
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.22045108675956726 
model_pd.l_d.mean(): -22.75084114074707 
model_pd.lagr.mean(): -22.5303897857666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6202], device='cuda:0')), ('power', tensor([-23.3711], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.22045108675956726
epoch£º0	 i:1 	 global-step:1	 l-p:0.1639728546142578
epoch£º0	 i:2 	 global-step:2	 l-p:0.20958468317985535
epoch£º0	 i:3 	 global-step:3	 l-p:0.18547755479812622
epoch£º0	 i:4 	 global-step:4	 l-p:0.1604577600955963
epoch£º0	 i:5 	 global-step:5	 l-p:0.13323137164115906
epoch£º0	 i:6 	 global-step:6	 l-p:0.07205234467983246
epoch£º0	 i:7 	 global-step:7	 l-p:0.15227164328098297
epoch£º0	 i:8 	 global-step:8	 l-p:0.12444312125444412
epoch£º0	 i:9 	 global-step:9	 l-p:0.1525035947561264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5290, 3.5294, 3.5290],
        [3.5290, 3.7409, 3.6980],
        [3.5290, 3.5896, 3.5508],
        [3.5290, 4.5221, 5.2100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.035105861723423004 
model_pd.l_d.mean(): -22.22926139831543 
model_pd.lagr.mean(): -22.194154739379883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1471], device='cuda:0')), ('power', tensor([-22.3764], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.035105861723423004
epoch£º1	 i:1 	 global-step:21	 l-p:0.0948038250207901
epoch£º1	 i:2 	 global-step:22	 l-p:0.1295902580022812
epoch£º1	 i:3 	 global-step:23	 l-p:0.12658867239952087
epoch£º1	 i:4 	 global-step:24	 l-p:0.08247102797031403
epoch£º1	 i:5 	 global-step:25	 l-p:0.10980378836393356
epoch£º1	 i:6 	 global-step:26	 l-p:0.14614053070545197
epoch£º1	 i:7 	 global-step:27	 l-p:0.1311836540699005
epoch£º1	 i:8 	 global-step:28	 l-p:0.11177258938550949
epoch£º1	 i:9 	 global-step:29	 l-p:0.09871819615364075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0404, 4.8941, 5.3155],
        [4.0404, 4.1189, 4.0708],
        [4.0404, 4.6872, 4.8975],
        [4.0404, 4.1062, 4.0631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.1317572444677353 
model_pd.l_d.mean(): -22.885700225830078 
model_pd.lagr.mean(): -22.753942489624023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0763], device='cuda:0')), ('power', tensor([-22.8094], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.1317572444677353
epoch£º2	 i:1 	 global-step:41	 l-p:0.08083644509315491
epoch£º2	 i:2 	 global-step:42	 l-p:0.13022761046886444
epoch£º2	 i:3 	 global-step:43	 l-p:0.11213386058807373
epoch£º2	 i:4 	 global-step:44	 l-p:0.12493521720170975
epoch£º2	 i:5 	 global-step:45	 l-p:0.1048060804605484
epoch£º2	 i:6 	 global-step:46	 l-p:0.10844957083463669
epoch£º2	 i:7 	 global-step:47	 l-p:0.11355181783437729
epoch£º2	 i:8 	 global-step:48	 l-p:0.10386304557323456
epoch£º2	 i:9 	 global-step:49	 l-p:0.12123552709817886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6315, 4.3173, 4.6195],
        [3.6315, 3.6643, 3.6394],
        [3.6315, 3.6995, 3.6575],
        [3.6315, 4.6156, 5.2709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.2710452079772949 
model_pd.l_d.mean(): -21.906286239624023 
model_pd.lagr.mean(): -21.63524055480957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1621], device='cuda:0')), ('power', tensor([-22.0684], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.2710452079772949
epoch£º3	 i:1 	 global-step:61	 l-p:0.27006494998931885
epoch£º3	 i:2 	 global-step:62	 l-p:0.19369959831237793
epoch£º3	 i:3 	 global-step:63	 l-p:0.12462922185659409
epoch£º3	 i:4 	 global-step:64	 l-p:0.11662866175174713
epoch£º3	 i:5 	 global-step:65	 l-p:0.11948095262050629
epoch£º3	 i:6 	 global-step:66	 l-p:0.14479830861091614
epoch£º3	 i:7 	 global-step:67	 l-p:0.15526223182678223
epoch£º3	 i:8 	 global-step:68	 l-p:0.110715351998806
epoch£º3	 i:9 	 global-step:69	 l-p:0.1126028522849083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5472, 4.0326, 4.1544],
        [3.5472, 4.1388, 4.3576],
        [3.5472, 4.5717, 5.3009],
        [3.5472, 3.6072, 3.5688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.15943460166454315 
model_pd.l_d.mean(): -22.146488189697266 
model_pd.lagr.mean(): -21.98705291748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1523], device='cuda:0')), ('power', tensor([-22.2988], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.15943460166454315
epoch£º4	 i:1 	 global-step:81	 l-p:0.12336207181215286
epoch£º4	 i:2 	 global-step:82	 l-p:0.4048042893409729
epoch£º4	 i:3 	 global-step:83	 l-p:0.11930131912231445
epoch£º4	 i:4 	 global-step:84	 l-p:0.12040650844573975
epoch£º4	 i:5 	 global-step:85	 l-p:0.14234118163585663
epoch£º4	 i:6 	 global-step:86	 l-p:0.13757428526878357
epoch£º4	 i:7 	 global-step:87	 l-p:0.32295581698417664
epoch£º4	 i:8 	 global-step:88	 l-p:0.13615940511226654
epoch£º4	 i:9 	 global-step:89	 l-p:0.09223657101392746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8067, 3.8069, 3.8067],
        [3.8067, 3.8116, 3.8071],
        [3.8067, 3.8805, 3.8356],
        [3.8067, 3.9124, 3.8589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.12029200047254562 
model_pd.l_d.mean(): -22.832006454467773 
model_pd.lagr.mean(): -22.711713790893555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0265], device='cuda:0')), ('power', tensor([-22.8585], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.12029200047254562
epoch£º5	 i:1 	 global-step:101	 l-p:6.550466060638428
epoch£º5	 i:2 	 global-step:102	 l-p:0.11233145743608475
epoch£º5	 i:3 	 global-step:103	 l-p:0.12942197918891907
epoch£º5	 i:4 	 global-step:104	 l-p:0.07733573019504547
epoch£º5	 i:5 	 global-step:105	 l-p:0.12809382379055023
epoch£º5	 i:6 	 global-step:106	 l-p:-1.7804425954818726
epoch£º5	 i:7 	 global-step:107	 l-p:0.11401429772377014
epoch£º5	 i:8 	 global-step:108	 l-p:0.12316779792308807
epoch£º5	 i:9 	 global-step:109	 l-p:0.11050175130367279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8387, 3.8597, 3.8424],
        [3.8387, 3.8387, 3.8387],
        [3.8387, 4.8962, 5.6053],
        [3.8387, 3.8423, 3.8389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.14074257016181946 
model_pd.l_d.mean(): -23.183931350708008 
model_pd.lagr.mean(): -23.043188095092773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0408], device='cuda:0')), ('power', tensor([-23.1431], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.14074257016181946
epoch£º6	 i:1 	 global-step:121	 l-p:0.11582385748624802
epoch£º6	 i:2 	 global-step:122	 l-p:0.10622211545705795
epoch£º6	 i:3 	 global-step:123	 l-p:0.13054072856903076
epoch£º6	 i:4 	 global-step:124	 l-p:0.10712090879678726
epoch£º6	 i:5 	 global-step:125	 l-p:0.009321410208940506
epoch£º6	 i:6 	 global-step:126	 l-p:0.12978655099868774
epoch£º6	 i:7 	 global-step:127	 l-p:0.6741876006126404
epoch£º6	 i:8 	 global-step:128	 l-p:0.09854267537593842
epoch£º6	 i:9 	 global-step:129	 l-p:0.11275802552700043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6214, 4.0770, 4.1697],
        [3.6214, 3.6655, 3.6344],
        [3.6214, 3.6214, 3.6214],
        [3.6214, 3.6364, 3.6236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.6808052659034729 
model_pd.l_d.mean(): -23.140625 
model_pd.lagr.mean(): -22.459819793701172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0511], device='cuda:0')), ('power', tensor([-23.1917], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.6808052659034729
epoch£º7	 i:1 	 global-step:141	 l-p:0.12045912444591522
epoch£º7	 i:2 	 global-step:142	 l-p:0.10574367642402649
epoch£º7	 i:3 	 global-step:143	 l-p:0.1348562389612198
epoch£º7	 i:4 	 global-step:144	 l-p:0.14500756561756134
epoch£º7	 i:5 	 global-step:145	 l-p:0.10494813323020935
epoch£º7	 i:6 	 global-step:146	 l-p:0.11308521032333374
epoch£º7	 i:7 	 global-step:147	 l-p:0.11202003061771393
epoch£º7	 i:8 	 global-step:148	 l-p:0.10918056964874268
epoch£º7	 i:9 	 global-step:149	 l-p:0.11247792840003967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7490, 3.7490, 3.7490],
        [3.7490, 3.7491, 3.7490],
        [3.7490, 4.3568, 4.5700],
        [3.7490, 3.8584, 3.8052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.15561510622501373 
model_pd.l_d.mean(): -21.972938537597656 
model_pd.lagr.mean(): -21.817323684692383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0890], device='cuda:0')), ('power', tensor([-22.0620], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.15561510622501373
epoch£º8	 i:1 	 global-step:161	 l-p:0.12010692059993744
epoch£º8	 i:2 	 global-step:162	 l-p:0.08678369969129562
epoch£º8	 i:3 	 global-step:163	 l-p:0.13845527172088623
epoch£º8	 i:4 	 global-step:164	 l-p:0.10763389617204666
epoch£º8	 i:5 	 global-step:165	 l-p:0.12447144836187363
epoch£º8	 i:6 	 global-step:166	 l-p:0.14984425902366638
epoch£º8	 i:7 	 global-step:167	 l-p:0.13415373861789703
epoch£º8	 i:8 	 global-step:168	 l-p:0.08105478435754776
epoch£º8	 i:9 	 global-step:169	 l-p:0.14001502096652985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7623, 3.7637, 3.7624],
        [3.7623, 3.7638, 3.7624],
        [3.7623, 3.8714, 3.8183],
        [3.7623, 3.7777, 3.7646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.0920049250125885 
model_pd.l_d.mean(): -21.241451263427734 
model_pd.lagr.mean(): -21.149446487426758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1146], device='cuda:0')), ('power', tensor([-21.3561], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.0920049250125885
epoch£º9	 i:1 	 global-step:181	 l-p:0.11670839041471481
epoch£º9	 i:2 	 global-step:182	 l-p:0.11232365667819977
epoch£º9	 i:3 	 global-step:183	 l-p:0.08288081735372543
epoch£º9	 i:4 	 global-step:184	 l-p:0.12121415138244629
epoch£º9	 i:5 	 global-step:185	 l-p:0.11359874159097672
epoch£º9	 i:6 	 global-step:186	 l-p:0.1159854307770729
epoch£º9	 i:7 	 global-step:187	 l-p:0.1259697526693344
epoch£º9	 i:8 	 global-step:188	 l-p:0.06590350717306137
epoch£º9	 i:9 	 global-step:189	 l-p:0.12519216537475586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7771, 4.6898, 5.2332],
        [3.7771, 4.4007, 4.6276],
        [3.7771, 3.7809, 3.7774],
        [3.7771, 3.7771, 3.7771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.17887984216213226 
model_pd.l_d.mean(): -22.927597045898438 
model_pd.lagr.mean(): -22.748716354370117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0218], device='cuda:0')), ('power', tensor([-22.9494], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.17887984216213226
epoch£º10	 i:1 	 global-step:201	 l-p:0.09280978888273239
epoch£º10	 i:2 	 global-step:202	 l-p:0.06468633562326431
epoch£º10	 i:3 	 global-step:203	 l-p:0.12416322529315948
epoch£º10	 i:4 	 global-step:204	 l-p:-0.4929613769054413
epoch£º10	 i:5 	 global-step:205	 l-p:0.12360351532697678
epoch£º10	 i:6 	 global-step:206	 l-p:0.09681133925914764
epoch£º10	 i:7 	 global-step:207	 l-p:0.11734689772129059
epoch£º10	 i:8 	 global-step:208	 l-p:0.12650646269321442
epoch£º10	 i:9 	 global-step:209	 l-p:0.12664125859737396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8724, 4.2737, 4.3100],
        [3.8724, 4.8797, 5.5226],
        [3.8724, 4.4213, 4.5703],
        [3.8724, 3.8903, 3.8753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.11920810490846634 
model_pd.l_d.mean(): -22.822105407714844 
model_pd.lagr.mean(): -22.702898025512695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0043], device='cuda:0')), ('power', tensor([-22.8178], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.11920810490846634
epoch£º11	 i:1 	 global-step:221	 l-p:0.11746801435947418
epoch£º11	 i:2 	 global-step:222	 l-p:0.11574048548936844
epoch£º11	 i:3 	 global-step:223	 l-p:0.10371629148721695
epoch£º11	 i:4 	 global-step:224	 l-p:0.1426635980606079
epoch£º11	 i:5 	 global-step:225	 l-p:0.11525001376867294
epoch£º11	 i:6 	 global-step:226	 l-p:0.08512263000011444
epoch£º11	 i:7 	 global-step:227	 l-p:0.11964727938175201
epoch£º11	 i:8 	 global-step:228	 l-p:0.20507746934890747
epoch£º11	 i:9 	 global-step:229	 l-p:0.11984644830226898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7241, 4.0126, 3.9949],
        [3.7241, 3.7505, 3.7297],
        [3.7241, 3.7465, 3.7284],
        [3.7241, 3.8464, 3.7927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.24274004995822906 
model_pd.l_d.mean(): -22.336225509643555 
model_pd.lagr.mean(): -22.09348487854004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1062], device='cuda:0')), ('power', tensor([-22.4424], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.24274004995822906
epoch£º12	 i:1 	 global-step:241	 l-p:0.11948978155851364
epoch£º12	 i:2 	 global-step:242	 l-p:0.12386121600866318
epoch£º12	 i:3 	 global-step:243	 l-p:0.14190725982189178
epoch£º12	 i:4 	 global-step:244	 l-p:0.16068139672279358
epoch£º12	 i:5 	 global-step:245	 l-p:0.09152315557003021
epoch£º12	 i:6 	 global-step:246	 l-p:0.11633598059415817
epoch£º12	 i:7 	 global-step:247	 l-p:0.11325563490390778
epoch£º12	 i:8 	 global-step:248	 l-p:0.13303764164447784
epoch£º12	 i:9 	 global-step:249	 l-p:0.04745427519083023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7689, 4.6085, 5.0698],
        [3.7689, 3.7689, 3.7689],
        [3.7689, 3.7713, 3.7690],
        [3.7689, 4.0600, 4.0418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.11455301195383072 
model_pd.l_d.mean(): -21.925800323486328 
model_pd.lagr.mean(): -21.811246871948242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0802], device='cuda:0')), ('power', tensor([-22.0060], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.11455301195383072
epoch£º13	 i:1 	 global-step:261	 l-p:0.1418960988521576
epoch£º13	 i:2 	 global-step:262	 l-p:0.11534202098846436
epoch£º13	 i:3 	 global-step:263	 l-p:0.11341249942779541
epoch£º13	 i:4 	 global-step:264	 l-p:0.11746612191200256
epoch£º13	 i:5 	 global-step:265	 l-p:0.1418510526418686
epoch£º13	 i:6 	 global-step:266	 l-p:0.07032300531864166
epoch£º13	 i:7 	 global-step:267	 l-p:0.20943377912044525
epoch£º13	 i:8 	 global-step:268	 l-p:0.1202109083533287
epoch£º13	 i:9 	 global-step:269	 l-p:0.10982896387577057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7609, 3.7609, 3.7609],
        [3.7609, 4.4219, 4.6915],
        [3.7609, 3.7861, 3.7660],
        [3.7609, 4.3735, 4.5955]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.1144104152917862 
model_pd.l_d.mean(): -21.86316680908203 
model_pd.lagr.mean(): -21.748756408691406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0713], device='cuda:0')), ('power', tensor([-21.9345], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.1144104152917862
epoch£º14	 i:1 	 global-step:281	 l-p:0.11519266664981842
epoch£º14	 i:2 	 global-step:282	 l-p:0.11810089647769928
epoch£º14	 i:3 	 global-step:283	 l-p:0.103403739631176
epoch£º14	 i:4 	 global-step:284	 l-p:0.09407654404640198
epoch£º14	 i:5 	 global-step:285	 l-p:0.07596375793218613
epoch£º14	 i:6 	 global-step:286	 l-p:0.2759380340576172
epoch£º14	 i:7 	 global-step:287	 l-p:0.181746244430542
epoch£º14	 i:8 	 global-step:288	 l-p:0.1544654369354248
epoch£º14	 i:9 	 global-step:289	 l-p:0.12279880046844482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8352, 3.8591, 3.8399],
        [3.8352, 3.8380, 3.8353],
        [3.8352, 3.9672, 3.9118],
        [3.8352, 3.8433, 3.8360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.13173095881938934 
model_pd.l_d.mean(): -23.132572174072266 
model_pd.lagr.mean(): -23.00084114074707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0307], device='cuda:0')), ('power', tensor([-23.1019], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.13173095881938934
epoch£º15	 i:1 	 global-step:301	 l-p:0.12631703913211823
epoch£º15	 i:2 	 global-step:302	 l-p:0.13736750185489655
epoch£º15	 i:3 	 global-step:303	 l-p:0.1172536239027977
epoch£º15	 i:4 	 global-step:304	 l-p:0.12032556533813477
epoch£º15	 i:5 	 global-step:305	 l-p:0.1289341002702713
epoch£º15	 i:6 	 global-step:306	 l-p:0.09493416547775269
epoch£º15	 i:7 	 global-step:307	 l-p:1.0171204805374146
epoch£º15	 i:8 	 global-step:308	 l-p:0.1041899248957634
epoch£º15	 i:9 	 global-step:309	 l-p:0.042322415858507156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8626, 3.8640, 3.8627],
        [3.8626, 3.8824, 3.8660],
        [3.8626, 3.8629, 3.8626],
        [3.8626, 4.1328, 4.1029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.10998689383268356 
model_pd.l_d.mean(): -22.141254425048828 
model_pd.lagr.mean(): -22.031267166137695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0100], device='cuda:0')), ('power', tensor([-22.1513], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.10998689383268356
epoch£º16	 i:1 	 global-step:321	 l-p:0.32813671231269836
epoch£º16	 i:2 	 global-step:322	 l-p:0.11119912564754486
epoch£º16	 i:3 	 global-step:323	 l-p:0.1036083996295929
epoch£º16	 i:4 	 global-step:324	 l-p:0.1450931578874588
epoch£º16	 i:5 	 global-step:325	 l-p:0.39963075518608093
epoch£º16	 i:6 	 global-step:326	 l-p:0.11005140095949173
epoch£º16	 i:7 	 global-step:327	 l-p:0.09145960956811905
epoch£º16	 i:8 	 global-step:328	 l-p:0.11583072692155838
epoch£º16	 i:9 	 global-step:329	 l-p:0.0921514704823494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9204, 4.6263, 4.9230],
        [3.9204, 4.1597, 4.1172],
        [3.9204, 3.9204, 3.9204],
        [3.9204, 4.6170, 4.9044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.11162633448839188 
model_pd.l_d.mean(): -22.87198829650879 
model_pd.lagr.mean(): -22.76036262512207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0310], device='cuda:0')), ('power', tensor([-22.8410], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.11162633448839188
epoch£º17	 i:1 	 global-step:341	 l-p:0.26322057843208313
epoch£º17	 i:2 	 global-step:342	 l-p:0.10875474661588669
epoch£º17	 i:3 	 global-step:343	 l-p:0.11330145597457886
epoch£º17	 i:4 	 global-step:344	 l-p:0.09059926867485046
epoch£º17	 i:5 	 global-step:345	 l-p:0.1289643496274948
epoch£º17	 i:6 	 global-step:346	 l-p:0.16306740045547485
epoch£º17	 i:7 	 global-step:347	 l-p:0.10663885623216629
epoch£º17	 i:8 	 global-step:348	 l-p:0.11521047353744507
epoch£º17	 i:9 	 global-step:349	 l-p:0.11583171784877777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7934, 3.8300, 3.8029],
        [3.7934, 3.7936, 3.7934],
        [3.7934, 3.8782, 3.8312],
        [3.7934, 3.8028, 3.7944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.086534783244133 
model_pd.l_d.mean(): -22.798383712768555 
model_pd.lagr.mean(): -22.711849212646484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0313], device='cuda:0')), ('power', tensor([-22.8297], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.086534783244133
epoch£º18	 i:1 	 global-step:361	 l-p:0.11233292520046234
epoch£º18	 i:2 	 global-step:362	 l-p:0.16528193652629852
epoch£º18	 i:3 	 global-step:363	 l-p:0.12352415919303894
epoch£º18	 i:4 	 global-step:364	 l-p:0.10213086754083633
epoch£º18	 i:5 	 global-step:365	 l-p:0.11971528828144073
epoch£º18	 i:6 	 global-step:366	 l-p:0.12316491454839706
epoch£º18	 i:7 	 global-step:367	 l-p:0.10506787151098251
epoch£º18	 i:8 	 global-step:368	 l-p:0.1471344530582428
epoch£º18	 i:9 	 global-step:369	 l-p:0.0881124958395958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7012, 3.8280, 3.7756],
        [3.7012, 3.7229, 3.7054],
        [3.7012, 3.7012, 3.7012],
        [3.7012, 3.8518, 3.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.11581721901893616 
model_pd.l_d.mean(): -23.166126251220703 
model_pd.lagr.mean(): -23.050308227539062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0085], device='cuda:0')), ('power', tensor([-23.1746], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.11581721901893616
epoch£º19	 i:1 	 global-step:381	 l-p:0.14557074010372162
epoch£º19	 i:2 	 global-step:382	 l-p:0.14351297914981842
epoch£º19	 i:3 	 global-step:383	 l-p:-0.03202516585588455
epoch£º19	 i:4 	 global-step:384	 l-p:0.1257089227437973
epoch£º19	 i:5 	 global-step:385	 l-p:0.1252789944410324
epoch£º19	 i:6 	 global-step:386	 l-p:0.13288135826587677
epoch£º19	 i:7 	 global-step:387	 l-p:0.15560060739517212
epoch£º19	 i:8 	 global-step:388	 l-p:0.07486934214830399
epoch£º19	 i:9 	 global-step:389	 l-p:0.11703819781541824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8219, 3.8239, 3.8220],
        [3.8219, 4.1326, 4.1248],
        [3.8219, 3.8531, 3.8293],
        [3.8219, 3.8669, 3.8353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.1507834792137146 
model_pd.l_d.mean(): -23.117788314819336 
model_pd.lagr.mean(): -22.967004776000977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0226], device='cuda:0')), ('power', tensor([-23.0952], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.1507834792137146
epoch£º20	 i:1 	 global-step:401	 l-p:0.13313636183738708
epoch£º20	 i:2 	 global-step:402	 l-p:0.10315738618373871
epoch£º20	 i:3 	 global-step:403	 l-p:0.11281028389930725
epoch£º20	 i:4 	 global-step:404	 l-p:0.1133965477347374
epoch£º20	 i:5 	 global-step:405	 l-p:0.034690406173467636
epoch£º20	 i:6 	 global-step:406	 l-p:0.11853242665529251
epoch£º20	 i:7 	 global-step:407	 l-p:-0.57090824842453
epoch£º20	 i:8 	 global-step:408	 l-p:0.11285186558961868
epoch£º20	 i:9 	 global-step:409	 l-p:0.12351882457733154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8516, 4.8987, 5.6068],
        [3.8516, 3.8974, 3.8653],
        [3.8516, 3.8516, 3.8516],
        [3.8516, 3.9260, 3.8821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.11030702292919159 
model_pd.l_d.mean(): -22.995887756347656 
model_pd.lagr.mean(): -22.88558006286621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0155], device='cuda:0')), ('power', tensor([-22.9804], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.11030702292919159
epoch£º21	 i:1 	 global-step:421	 l-p:0.02930297702550888
epoch£º21	 i:2 	 global-step:422	 l-p:0.10432837158441544
epoch£º21	 i:3 	 global-step:423	 l-p:0.08593253046274185
epoch£º21	 i:4 	 global-step:424	 l-p:0.13496249914169312
epoch£º21	 i:5 	 global-step:425	 l-p:0.17801757156848907
epoch£º21	 i:6 	 global-step:426	 l-p:0.10819253325462341
epoch£º21	 i:7 	 global-step:427	 l-p:0.14280156791210175
epoch£º21	 i:8 	 global-step:428	 l-p:0.14503158628940582
epoch£º21	 i:9 	 global-step:429	 l-p:0.10710394382476807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8264, 3.9014, 3.8575],
        [3.8264, 4.0448, 4.0013],
        [3.8264, 3.8281, 3.8265],
        [3.8264, 3.8499, 3.8311]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.1328440010547638 
model_pd.l_d.mean(): -23.111663818359375 
model_pd.lagr.mean(): -22.978818893432617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0248], device='cuda:0')), ('power', tensor([-23.0869], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.1328440010547638
epoch£º22	 i:1 	 global-step:441	 l-p:0.09535227715969086
epoch£º22	 i:2 	 global-step:442	 l-p:0.11970225721597672
epoch£º22	 i:3 	 global-step:443	 l-p:0.11250708252191544
epoch£º22	 i:4 	 global-step:444	 l-p:0.11690179258584976
epoch£º22	 i:5 	 global-step:445	 l-p:0.14204882085323334
epoch£º22	 i:6 	 global-step:446	 l-p:0.12790367007255554
epoch£º22	 i:7 	 global-step:447	 l-p:0.09478231519460678
epoch£º22	 i:8 	 global-step:448	 l-p:0.13422924280166626
epoch£º22	 i:9 	 global-step:449	 l-p:0.14300881326198578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7791, 4.7956, 5.4815],
        [3.7791, 4.1736, 4.2199],
        [3.7791, 3.7795, 3.7791],
        [3.7791, 4.4408, 4.7168]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.1078217476606369 
model_pd.l_d.mean(): -22.796955108642578 
model_pd.lagr.mean(): -22.689132690429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0335], device='cuda:0')), ('power', tensor([-22.8304], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.1078217476606369
epoch£º23	 i:1 	 global-step:461	 l-p:0.06418666988611221
epoch£º23	 i:2 	 global-step:462	 l-p:0.1517200767993927
epoch£º23	 i:3 	 global-step:463	 l-p:0.1308540254831314
epoch£º23	 i:4 	 global-step:464	 l-p:0.10266980528831482
epoch£º23	 i:5 	 global-step:465	 l-p:0.1304030865430832
epoch£º23	 i:6 	 global-step:466	 l-p:-0.3031245470046997
epoch£º23	 i:7 	 global-step:467	 l-p:0.10455478727817535
epoch£º23	 i:8 	 global-step:468	 l-p:0.1043085902929306
epoch£º23	 i:9 	 global-step:469	 l-p:0.10917925834655762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9055, 4.2512, 4.2594],
        [3.9055, 4.1244, 4.0792],
        [3.9055, 3.9675, 3.9280],
        [3.9055, 3.9055, 3.9055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.1131807267665863 
model_pd.l_d.mean(): -23.413278579711914 
model_pd.lagr.mean(): -23.300098419189453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0989], device='cuda:0')), ('power', tensor([-23.3144], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.1131807267665863
epoch£º24	 i:1 	 global-step:481	 l-p:0.12502053380012512
epoch£º24	 i:2 	 global-step:482	 l-p:0.1117279902100563
epoch£º24	 i:3 	 global-step:483	 l-p:0.13888707756996155
epoch£º24	 i:4 	 global-step:484	 l-p:0.05543690547347069
epoch£º24	 i:5 	 global-step:485	 l-p:0.5427950024604797
epoch£º24	 i:6 	 global-step:486	 l-p:0.12046089768409729
epoch£º24	 i:7 	 global-step:487	 l-p:0.11295974254608154
epoch£º24	 i:8 	 global-step:488	 l-p:0.11951182782649994
epoch£º24	 i:9 	 global-step:489	 l-p:0.12337249517440796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8888, 3.8888, 3.8888],
        [3.8888, 3.8888, 3.8888],
        [3.8888, 3.8888, 3.8888],
        [3.8888, 4.7421, 5.2104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.08396514505147934 
model_pd.l_d.mean(): -21.67487335205078 
model_pd.lagr.mean(): -21.59090805053711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0988], device='cuda:0')), ('power', tensor([-21.7737], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.08396514505147934
epoch£º25	 i:1 	 global-step:501	 l-p:0.11542723327875137
epoch£º25	 i:2 	 global-step:502	 l-p:0.12104300409555435
epoch£º25	 i:3 	 global-step:503	 l-p:0.20850250124931335
epoch£º25	 i:4 	 global-step:504	 l-p:0.08302369713783264
epoch£º25	 i:5 	 global-step:505	 l-p:0.1585359275341034
epoch£º25	 i:6 	 global-step:506	 l-p:0.12133196741342545
epoch£º25	 i:7 	 global-step:507	 l-p:0.10520526766777039
epoch£º25	 i:8 	 global-step:508	 l-p:0.13596190512180328
epoch£º25	 i:9 	 global-step:509	 l-p:0.11568302661180496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7649, 3.9050, 3.8526],
        [3.7649, 4.2053, 4.2875],
        [3.7649, 3.8085, 3.7780],
        [3.7649, 4.1448, 4.1840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.1303514987230301 
model_pd.l_d.mean(): -23.18290901184082 
model_pd.lagr.mean(): -23.05255699157715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0053], device='cuda:0')), ('power', tensor([-23.1776], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.1303514987230301
epoch£º26	 i:1 	 global-step:521	 l-p:0.11276902258396149
epoch£º26	 i:2 	 global-step:522	 l-p:0.13189493119716644
epoch£º26	 i:3 	 global-step:523	 l-p:-0.32749778032302856
epoch£º26	 i:4 	 global-step:524	 l-p:0.08744315057992935
epoch£º26	 i:5 	 global-step:525	 l-p:0.1134999543428421
epoch£º26	 i:6 	 global-step:526	 l-p:0.0933571606874466
epoch£º26	 i:7 	 global-step:527	 l-p:0.1361912190914154
epoch£º26	 i:8 	 global-step:528	 l-p:0.11190721392631531
epoch£º26	 i:9 	 global-step:529	 l-p:0.15822270512580872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8244, 3.9108, 3.8641],
        [3.8244, 4.0707, 4.0379],
        [3.8244, 3.8244, 3.8244],
        [3.8244, 3.8263, 3.8245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.06126986816525459 
model_pd.l_d.mean(): -21.645605087280273 
model_pd.lagr.mean(): -21.584335327148438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0769], device='cuda:0')), ('power', tensor([-21.7225], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.06126986816525459
epoch£º27	 i:1 	 global-step:541	 l-p:0.14141997694969177
epoch£º27	 i:2 	 global-step:542	 l-p:0.12238093465566635
epoch£º27	 i:3 	 global-step:543	 l-p:0.12276649475097656
epoch£º27	 i:4 	 global-step:544	 l-p:0.12213031202554703
epoch£º27	 i:5 	 global-step:545	 l-p:0.0794915184378624
epoch£º27	 i:6 	 global-step:546	 l-p:0.12185615301132202
epoch£º27	 i:7 	 global-step:547	 l-p:0.12261306494474411
epoch£º27	 i:8 	 global-step:548	 l-p:-0.017790168523788452
epoch£º27	 i:9 	 global-step:549	 l-p:0.11871621757745743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8125, 3.9607, 3.9081],
        [3.8125, 4.2561, 4.3379],
        [3.8125, 3.8484, 3.8220],
        [3.8125, 4.7859, 5.4145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.14631721377372742 
model_pd.l_d.mean(): -22.7139949798584 
model_pd.lagr.mean(): -22.567678451538086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0332], device='cuda:0')), ('power', tensor([-22.7472], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.14631721377372742
epoch£º28	 i:1 	 global-step:561	 l-p:0.11901554465293884
epoch£º28	 i:2 	 global-step:562	 l-p:0.10430889576673508
epoch£º28	 i:3 	 global-step:563	 l-p:0.2630632519721985
epoch£º28	 i:4 	 global-step:564	 l-p:0.08318997174501419
epoch£º28	 i:5 	 global-step:565	 l-p:0.1155972108244896
epoch£º28	 i:6 	 global-step:566	 l-p:0.14759446680545807
epoch£º28	 i:7 	 global-step:567	 l-p:0.13099536299705505
epoch£º28	 i:8 	 global-step:568	 l-p:0.08372509479522705
epoch£º28	 i:9 	 global-step:569	 l-p:0.11788740009069443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8309, 3.8309, 3.8309],
        [3.8309, 3.9781, 3.9253],
        [3.8309, 4.2974, 4.3960],
        [3.8309, 3.8339, 3.8310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.12106296420097351 
model_pd.l_d.mean(): -22.579269409179688 
model_pd.lagr.mean(): -22.458206176757812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0230], device='cuda:0')), ('power', tensor([-22.6023], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.12106296420097351
epoch£º29	 i:1 	 global-step:581	 l-p:0.12872885167598724
epoch£º29	 i:2 	 global-step:582	 l-p:0.11865059286355972
epoch£º29	 i:3 	 global-step:583	 l-p:0.057875242084264755
epoch£º29	 i:4 	 global-step:584	 l-p:0.16666270792484283
epoch£º29	 i:5 	 global-step:585	 l-p:0.11352206766605377
epoch£º29	 i:6 	 global-step:586	 l-p:0.08371465653181076
epoch£º29	 i:7 	 global-step:587	 l-p:0.1189640462398529
epoch£º29	 i:8 	 global-step:588	 l-p:0.09421863406896591
epoch£º29	 i:9 	 global-step:589	 l-p:0.10743863880634308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9097, 3.9109, 3.9097],
        [3.9097, 4.9010, 5.5344],
        [3.9097, 4.0728, 4.0196],
        [3.9097, 4.8098, 5.3329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.22506706416606903 
model_pd.l_d.mean(): -22.054780960083008 
model_pd.lagr.mean(): -21.829713821411133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0631], device='cuda:0')), ('power', tensor([-22.1178], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.22506706416606903
epoch£º30	 i:1 	 global-step:601	 l-p:-0.13524171710014343
epoch£º30	 i:2 	 global-step:602	 l-p:0.09466855973005295
epoch£º30	 i:3 	 global-step:603	 l-p:0.12224308401346207
epoch£º30	 i:4 	 global-step:604	 l-p:0.11623942852020264
epoch£º30	 i:5 	 global-step:605	 l-p:0.12858662009239197
epoch£º30	 i:6 	 global-step:606	 l-p:0.11233633011579514
epoch£º30	 i:7 	 global-step:607	 l-p:0.13015976548194885
epoch£º30	 i:8 	 global-step:608	 l-p:0.12818168103694916
epoch£º30	 i:9 	 global-step:609	 l-p:0.10266117006540298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7536, 3.7536, 3.7536],
        [3.7536, 3.7595, 3.7541],
        [3.7536, 4.7448, 5.4102],
        [3.7536, 3.7941, 3.7654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.12072721868753433 
model_pd.l_d.mean(): -21.506969451904297 
model_pd.lagr.mean(): -21.386241912841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1577], device='cuda:0')), ('power', tensor([-21.6647], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.12072721868753433
epoch£º31	 i:1 	 global-step:621	 l-p:0.1181817501783371
epoch£º31	 i:2 	 global-step:622	 l-p:0.15118952095508575
epoch£º31	 i:3 	 global-step:623	 l-p:0.11457696557044983
epoch£º31	 i:4 	 global-step:624	 l-p:0.12782204151153564
epoch£º31	 i:5 	 global-step:625	 l-p:0.13005779683589935
epoch£º31	 i:6 	 global-step:626	 l-p:0.11422624439001083
epoch£º31	 i:7 	 global-step:627	 l-p:-0.9495357275009155
epoch£º31	 i:8 	 global-step:628	 l-p:0.12521493434906006
epoch£º31	 i:9 	 global-step:629	 l-p:0.13826777040958405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5207, 4.4885, 5.1745],
        [3.5207, 4.1494, 4.4335],
        [3.5207, 3.5589, 3.5320],
        [3.5207, 3.5217, 3.5207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.1606907993555069 
model_pd.l_d.mean(): -22.784683227539062 
model_pd.lagr.mean(): -22.623992919921875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1105], device='cuda:0')), ('power', tensor([-22.8951], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.1606907993555069
epoch£º32	 i:1 	 global-step:641	 l-p:0.12936924397945404
epoch£º32	 i:2 	 global-step:642	 l-p:0.2086683213710785
epoch£º32	 i:3 	 global-step:643	 l-p:0.3446241617202759
epoch£º32	 i:4 	 global-step:644	 l-p:0.10323838889598846
epoch£º32	 i:5 	 global-step:645	 l-p:0.11943995952606201
epoch£º32	 i:6 	 global-step:646	 l-p:0.12306150794029236
epoch£º32	 i:7 	 global-step:647	 l-p:0.12167160958051682
epoch£º32	 i:8 	 global-step:648	 l-p:-0.05474244803190231
epoch£º32	 i:9 	 global-step:649	 l-p:0.20301808416843414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7209, 3.7211, 3.7209],
        [3.7209, 3.8943, 3.8474],
        [3.7209, 3.7214, 3.7209],
        [3.7209, 3.8945, 3.8475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): -0.2601291835308075 
model_pd.l_d.mean(): -23.05179786682129 
model_pd.lagr.mean(): -23.311927795410156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0309], device='cuda:0')), ('power', tensor([-23.0827], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:-0.2601291835308075
epoch£º33	 i:1 	 global-step:661	 l-p:0.11726976186037064
epoch£º33	 i:2 	 global-step:662	 l-p:0.11145034432411194
epoch£º33	 i:3 	 global-step:663	 l-p:0.1096508577466011
epoch£º33	 i:4 	 global-step:664	 l-p:0.1062401533126831
epoch£º33	 i:5 	 global-step:665	 l-p:0.12351619452238083
epoch£º33	 i:6 	 global-step:666	 l-p:0.11971017718315125
epoch£º33	 i:7 	 global-step:667	 l-p:0.11743811517953873
epoch£º33	 i:8 	 global-step:668	 l-p:-0.05081119015812874
epoch£º33	 i:9 	 global-step:669	 l-p:0.1243562325835228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8476, 4.2929, 4.3763],
        [3.8476, 3.8478, 3.8476],
        [3.8476, 4.0555, 4.0122],
        [3.8476, 3.8888, 3.8596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.11409062892198563 
model_pd.l_d.mean(): -23.04611587524414 
model_pd.lagr.mean(): -22.932025909423828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0165], device='cuda:0')), ('power', tensor([-23.0296], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.11409062892198563
epoch£º34	 i:1 	 global-step:681	 l-p:0.10812783986330032
epoch£º34	 i:2 	 global-step:682	 l-p:0.14481979608535767
epoch£º34	 i:3 	 global-step:683	 l-p:0.142521932721138
epoch£º34	 i:4 	 global-step:684	 l-p:0.10835043340921402
epoch£º34	 i:5 	 global-step:685	 l-p:0.13005156815052032
epoch£º34	 i:6 	 global-step:686	 l-p:0.1512283980846405
epoch£º34	 i:7 	 global-step:687	 l-p:0.1077546700835228
epoch£º34	 i:8 	 global-step:688	 l-p:0.1223428025841713
epoch£º34	 i:9 	 global-step:689	 l-p:0.15532949566841125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7232, 4.0434, 4.0530],
        [3.7232, 3.7232, 3.7232],
        [3.7232, 3.8159, 3.7695],
        [3.7232, 4.6790, 5.3086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): -0.2469143569469452 
model_pd.l_d.mean(): -22.73963737487793 
model_pd.lagr.mean(): -22.98655128479004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0696], device='cuda:0')), ('power', tensor([-22.8093], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:-0.2469143569469452
epoch£º35	 i:1 	 global-step:701	 l-p:0.10261866450309753
epoch£º35	 i:2 	 global-step:702	 l-p:0.1837375909090042
epoch£º35	 i:3 	 global-step:703	 l-p:0.12559743225574493
epoch£º35	 i:4 	 global-step:704	 l-p:0.11742273718118668
epoch£º35	 i:5 	 global-step:705	 l-p:0.11477991938591003
epoch£º35	 i:6 	 global-step:706	 l-p:0.11720804125070572
epoch£º35	 i:7 	 global-step:707	 l-p:0.13229088485240936
epoch£º35	 i:8 	 global-step:708	 l-p:0.07798749953508377
epoch£º35	 i:9 	 global-step:709	 l-p:0.11868618428707123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7253, 3.7253, 3.7253],
        [3.7253, 3.7253, 3.7253],
        [3.7253, 4.1784, 4.2795],
        [3.7253, 3.7578, 3.7337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.15492242574691772 
model_pd.l_d.mean(): -22.156057357788086 
model_pd.lagr.mean(): -22.001134872436523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0909], device='cuda:0')), ('power', tensor([-22.2469], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.15492242574691772
epoch£º36	 i:1 	 global-step:721	 l-p:0.1348421424627304
epoch£º36	 i:2 	 global-step:722	 l-p:0.11047099530696869
epoch£º36	 i:3 	 global-step:723	 l-p:0.17624357342720032
epoch£º36	 i:4 	 global-step:724	 l-p:0.36027881503105164
epoch£º36	 i:5 	 global-step:725	 l-p:0.07968902587890625
epoch£º36	 i:6 	 global-step:726	 l-p:0.07121496647596359
epoch£º36	 i:7 	 global-step:727	 l-p:0.10512115061283112
epoch£º36	 i:8 	 global-step:728	 l-p:0.12607645988464355
epoch£º36	 i:9 	 global-step:729	 l-p:0.12072388082742691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9075, 3.9121, 3.9079],
        [3.9075, 4.8953, 5.5294],
        [3.9075, 4.2071, 4.1965],
        [3.9075, 4.2835, 4.3156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): -0.0015855383826419711 
model_pd.l_d.mean(): -21.81189727783203 
model_pd.lagr.mean(): -21.8134822845459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0701], device='cuda:0')), ('power', tensor([-21.8820], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:-0.0015855383826419711
epoch£º37	 i:1 	 global-step:741	 l-p:0.111959308385849
epoch£º37	 i:2 	 global-step:742	 l-p:0.11492269486188889
epoch£º37	 i:3 	 global-step:743	 l-p:0.12536218762397766
epoch£º37	 i:4 	 global-step:744	 l-p:0.14282310009002686
epoch£º37	 i:5 	 global-step:745	 l-p:0.11150534451007843
epoch£º37	 i:6 	 global-step:746	 l-p:0.041627056896686554
epoch£º37	 i:7 	 global-step:747	 l-p:0.1305026113986969
epoch£º37	 i:8 	 global-step:748	 l-p:0.11133798211812973
epoch£º37	 i:9 	 global-step:749	 l-p:0.14288125932216644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7668, 3.8453, 3.8020],
        [3.7668, 3.8134, 3.7819],
        [3.7668, 3.7848, 3.7700],
        [3.7668, 3.8113, 3.7808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.14210107922554016 
model_pd.l_d.mean(): -22.71839714050293 
model_pd.lagr.mean(): -22.576295852661133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0538], device='cuda:0')), ('power', tensor([-22.7722], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.14210107922554016
epoch£º38	 i:1 	 global-step:761	 l-p:0.10551169514656067
epoch£º38	 i:2 	 global-step:762	 l-p:0.09144459664821625
epoch£º38	 i:3 	 global-step:763	 l-p:0.18325643241405487
epoch£º38	 i:4 	 global-step:764	 l-p:0.11348997056484222
epoch£º38	 i:5 	 global-step:765	 l-p:0.14166201651096344
epoch£º38	 i:6 	 global-step:766	 l-p:0.11542624235153198
epoch£º38	 i:7 	 global-step:767	 l-p:0.20301620662212372
epoch£º38	 i:8 	 global-step:768	 l-p:0.10499848425388336
epoch£º38	 i:9 	 global-step:769	 l-p:0.12951281666755676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8366, 3.8366, 3.8366],
        [3.8366, 4.6339, 5.0573],
        [3.8366, 3.8366, 3.8366],
        [3.8366, 3.8381, 3.8367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.1210254654288292 
model_pd.l_d.mean(): -23.30300521850586 
model_pd.lagr.mean(): -23.18198013305664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0512], device='cuda:0')), ('power', tensor([-23.2518], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.1210254654288292
epoch£º39	 i:1 	 global-step:781	 l-p:0.12058265507221222
epoch£º39	 i:2 	 global-step:782	 l-p:0.11495748162269592
epoch£º39	 i:3 	 global-step:783	 l-p:0.1533697545528412
epoch£º39	 i:4 	 global-step:784	 l-p:0.1179029792547226
epoch£º39	 i:5 	 global-step:785	 l-p:0.12010350078344345
epoch£º39	 i:6 	 global-step:786	 l-p:0.08843137323856354
epoch£º39	 i:7 	 global-step:787	 l-p:0.19958022236824036
epoch£º39	 i:8 	 global-step:788	 l-p:0.16204574704170227
epoch£º39	 i:9 	 global-step:789	 l-p:0.0057419678196311
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7402, 3.7404, 3.7402],
        [3.7402, 3.7531, 3.7421],
        [3.7402, 3.8186, 3.7757],
        [3.7402, 3.7454, 3.7407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.1914762407541275 
model_pd.l_d.mean(): -23.213720321655273 
model_pd.lagr.mean(): -23.02224349975586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0037], device='cuda:0')), ('power', tensor([-23.2174], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.1914762407541275
epoch£º40	 i:1 	 global-step:801	 l-p:0.13156422972679138
epoch£º40	 i:2 	 global-step:802	 l-p:0.047521624714136124
epoch£º40	 i:3 	 global-step:803	 l-p:0.10202828794717789
epoch£º40	 i:4 	 global-step:804	 l-p:-0.1542646437883377
epoch£º40	 i:5 	 global-step:805	 l-p:0.12505963444709778
epoch£º40	 i:6 	 global-step:806	 l-p:0.09734531491994858
epoch£º40	 i:7 	 global-step:807	 l-p:0.11132416129112244
epoch£º40	 i:8 	 global-step:808	 l-p:0.07735857367515564
epoch£º40	 i:9 	 global-step:809	 l-p:0.10063304752111435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0686, 4.0707, 4.0687],
        [4.0686, 4.0689, 4.0686],
        [4.0686, 4.2129, 4.1576],
        [4.0686, 4.1586, 4.1101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.10376781225204468 
model_pd.l_d.mean(): -23.053974151611328 
model_pd.lagr.mean(): -22.950206756591797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1045], device='cuda:0')), ('power', tensor([-22.9495], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.10376781225204468
epoch£º41	 i:1 	 global-step:821	 l-p:0.0960109680891037
epoch£º41	 i:2 	 global-step:822	 l-p:0.10072782635688782
epoch£º41	 i:3 	 global-step:823	 l-p:0.11080405861139297
epoch£º41	 i:4 	 global-step:824	 l-p:-1.694995403289795
epoch£º41	 i:5 	 global-step:825	 l-p:0.1250709444284439
epoch£º41	 i:6 	 global-step:826	 l-p:0.10846740752458572
epoch£º41	 i:7 	 global-step:827	 l-p:15.755861282348633
epoch£º41	 i:8 	 global-step:828	 l-p:0.11642655730247498
epoch£º41	 i:9 	 global-step:829	 l-p:0.11794549971818924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8171, 4.0242, 3.9836],
        [3.8171, 3.8171, 3.8171],
        [3.8171, 4.8333, 5.5240],
        [3.8171, 3.8286, 3.8187]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.08384248614311218 
model_pd.l_d.mean(): -22.12687873840332 
model_pd.lagr.mean(): -22.04303550720215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0560], device='cuda:0')), ('power', tensor([-22.1828], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.08384248614311218
epoch£º42	 i:1 	 global-step:841	 l-p:0.18224747478961945
epoch£º42	 i:2 	 global-step:842	 l-p:0.11273960024118423
epoch£º42	 i:3 	 global-step:843	 l-p:0.10488681495189667
epoch£º42	 i:4 	 global-step:844	 l-p:0.10099877417087555
epoch£º42	 i:5 	 global-step:845	 l-p:0.1817868947982788
epoch£º42	 i:6 	 global-step:846	 l-p:0.11873876303434372
epoch£º42	 i:7 	 global-step:847	 l-p:0.187395840883255
epoch£º42	 i:8 	 global-step:848	 l-p:0.16545291244983673
epoch£º42	 i:9 	 global-step:849	 l-p:0.09371233731508255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[3.7835, 4.6866, 5.2442],
        [3.7835, 4.1896, 4.2533],
        [3.7835, 4.0071, 3.9727],
        [3.7835, 4.6417, 5.1458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.1314018815755844 
model_pd.l_d.mean(): -22.729976654052734 
model_pd.lagr.mean(): -22.598575592041016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0448], device='cuda:0')), ('power', tensor([-22.7748], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.1314018815755844
epoch£º43	 i:1 	 global-step:861	 l-p:0.1710619330406189
epoch£º43	 i:2 	 global-step:862	 l-p:0.11308466643095016
epoch£º43	 i:3 	 global-step:863	 l-p:0.12290757894515991
epoch£º43	 i:4 	 global-step:864	 l-p:0.11126069724559784
epoch£º43	 i:5 	 global-step:865	 l-p:0.10947093367576599
epoch£º43	 i:6 	 global-step:866	 l-p:0.10022170841693878
epoch£º43	 i:7 	 global-step:867	 l-p:0.12813672423362732
epoch£º43	 i:8 	 global-step:868	 l-p:-0.023170342668890953
epoch£º43	 i:9 	 global-step:869	 l-p:0.0713217556476593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8882, 3.9825, 3.9349],
        [3.8882, 3.8934, 3.8886],
        [3.8882, 3.8899, 3.8882],
        [3.8882, 3.8882, 3.8882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.11000126600265503 
model_pd.l_d.mean(): -23.07328224182129 
model_pd.lagr.mean(): -22.963281631469727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0450], device='cuda:0')), ('power', tensor([-23.0283], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.11000126600265503
epoch£º44	 i:1 	 global-step:881	 l-p:-0.3349556028842926
epoch£º44	 i:2 	 global-step:882	 l-p:0.112992063164711
epoch£º44	 i:3 	 global-step:883	 l-p:0.102093406021595
epoch£º44	 i:4 	 global-step:884	 l-p:0.15907055139541626
epoch£º44	 i:5 	 global-step:885	 l-p:0.09092249721288681
epoch£º44	 i:6 	 global-step:886	 l-p:0.12279985100030899
epoch£º44	 i:7 	 global-step:887	 l-p:0.11172179877758026
epoch£º44	 i:8 	 global-step:888	 l-p:0.11702679097652435
epoch£º44	 i:9 	 global-step:889	 l-p:0.09869617223739624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8422, 3.8422, 3.8422],
        [3.8422, 3.8429, 3.8422],
        [3.8422, 4.0147, 3.9667],
        [3.8422, 3.8422, 3.8422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.04470096528530121 
model_pd.l_d.mean(): -23.363054275512695 
model_pd.lagr.mean(): -23.3183536529541 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0587], device='cuda:0')), ('power', tensor([-23.3044], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.04470096528530121
epoch£º45	 i:1 	 global-step:901	 l-p:0.13416525721549988
epoch£º45	 i:2 	 global-step:902	 l-p:0.09295126050710678
epoch£º45	 i:3 	 global-step:903	 l-p:0.12208964675664902
epoch£º45	 i:4 	 global-step:904	 l-p:0.09957744181156158
epoch£º45	 i:5 	 global-step:905	 l-p:0.1276693195104599
epoch£º45	 i:6 	 global-step:906	 l-p:0.10010629147291183
epoch£º45	 i:7 	 global-step:907	 l-p:0.10176090151071548
epoch£º45	 i:8 	 global-step:908	 l-p:0.1096508651971817
epoch£º45	 i:9 	 global-step:909	 l-p:0.14283521473407745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7962, 3.7992, 3.7964],
        [3.7962, 3.8153, 3.7998],
        [3.7962, 3.7962, 3.7962],
        [3.7962, 3.8149, 3.7997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.16943630576133728 
model_pd.l_d.mean(): -22.587942123413086 
model_pd.lagr.mean(): -22.418506622314453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0454], device='cuda:0')), ('power', tensor([-22.6333], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.16943630576133728
epoch£º46	 i:1 	 global-step:921	 l-p:0.14203721284866333
epoch£º46	 i:2 	 global-step:922	 l-p:0.12390656024217606
epoch£º46	 i:3 	 global-step:923	 l-p:0.08980462700128555
epoch£º46	 i:4 	 global-step:924	 l-p:0.09120116382837296
epoch£º46	 i:5 	 global-step:925	 l-p:0.1018088161945343
epoch£º46	 i:6 	 global-step:926	 l-p:0.11855334043502808
epoch£º46	 i:7 	 global-step:927	 l-p:0.13569165766239166
epoch£º46	 i:8 	 global-step:928	 l-p:0.14352011680603027
epoch£º46	 i:9 	 global-step:929	 l-p:0.11398963630199432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7894, 3.8302, 3.8017],
        [3.7894, 3.8674, 3.8246],
        [3.7894, 3.7907, 3.7894],
        [3.7894, 4.0407, 4.0172]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.20906217396259308 
model_pd.l_d.mean(): -21.779796600341797 
model_pd.lagr.mean(): -21.57073402404785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0723], device='cuda:0')), ('power', tensor([-21.8521], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.20906217396259308
epoch£º47	 i:1 	 global-step:941	 l-p:0.08582393825054169
epoch£º47	 i:2 	 global-step:942	 l-p:0.11831149458885193
epoch£º47	 i:3 	 global-step:943	 l-p:0.14644774794578552
epoch£º47	 i:4 	 global-step:944	 l-p:0.1046125590801239
epoch£º47	 i:5 	 global-step:945	 l-p:0.14741599559783936
epoch£º47	 i:6 	 global-step:946	 l-p:0.12251212447881699
epoch£º47	 i:7 	 global-step:947	 l-p:0.08189552277326584
epoch£º47	 i:8 	 global-step:948	 l-p:0.11358901858329773
epoch£º47	 i:9 	 global-step:949	 l-p:0.142506405711174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7932, 4.0455, 4.0225],
        [3.7932, 3.9276, 3.8781],
        [3.7932, 3.7932, 3.7932],
        [3.7932, 3.9646, 3.9183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.11857293546199799 
model_pd.l_d.mean(): -23.37205696105957 
model_pd.lagr.mean(): -23.25348472595215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0387], device='cuda:0')), ('power', tensor([-23.3333], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.11857293546199799
epoch£º48	 i:1 	 global-step:961	 l-p:0.11271422356367111
epoch£º48	 i:2 	 global-step:962	 l-p:0.10798383504152298
epoch£º48	 i:3 	 global-step:963	 l-p:-0.08618747442960739
epoch£º48	 i:4 	 global-step:964	 l-p:0.08909546583890915
epoch£º48	 i:5 	 global-step:965	 l-p:0.1532941609621048
epoch£º48	 i:6 	 global-step:966	 l-p:0.12722089886665344
epoch£º48	 i:7 	 global-step:967	 l-p:0.1159682646393776
epoch£º48	 i:8 	 global-step:968	 l-p:0.1068393662571907
epoch£º48	 i:9 	 global-step:969	 l-p:0.13214166462421417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7386, 3.7626, 3.7439],
        [3.7386, 3.7396, 3.7387],
        [3.7386, 3.8496, 3.8020],
        [3.7386, 3.7429, 3.7390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.12840589880943298 
model_pd.l_d.mean(): -23.414899826049805 
model_pd.lagr.mean(): -23.2864933013916 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0257], device='cuda:0')), ('power', tensor([-23.3892], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.12840589880943298
epoch£º49	 i:1 	 global-step:981	 l-p:0.11511091142892838
epoch£º49	 i:2 	 global-step:982	 l-p:0.1064368411898613
epoch£º49	 i:3 	 global-step:983	 l-p:0.12362699955701828
epoch£º49	 i:4 	 global-step:984	 l-p:0.1106785386800766
epoch£º49	 i:5 	 global-step:985	 l-p:-0.11602471023797989
epoch£º49	 i:6 	 global-step:986	 l-p:0.15530124306678772
epoch£º49	 i:7 	 global-step:987	 l-p:0.1275750696659088
epoch£º49	 i:8 	 global-step:988	 l-p:0.01789896935224533
epoch£º49	 i:9 	 global-step:989	 l-p:0.09462215006351471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4581, 4.0017, 4.2186],
        [3.4581, 3.4582, 3.4581],
        [3.4581, 3.6645, 3.6386],
        [3.4581, 3.4593, 3.4582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.10187211632728577 
model_pd.l_d.mean(): -22.903209686279297 
model_pd.lagr.mean(): -22.80133819580078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1509], device='cuda:0')), ('power', tensor([-23.0541], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.10187211632728577
epoch£º50	 i:1 	 global-step:1001	 l-p:-0.06384874880313873
epoch£º50	 i:2 	 global-step:1002	 l-p:0.091168612241745
epoch£º50	 i:3 	 global-step:1003	 l-p:0.08424242585897446
epoch£º50	 i:4 	 global-step:1004	 l-p:0.10334168374538422
epoch£º50	 i:5 	 global-step:1005	 l-p:0.11412205547094345
epoch£º50	 i:6 	 global-step:1006	 l-p:0.21485330164432526
epoch£º50	 i:7 	 global-step:1007	 l-p:0.12553179264068604
epoch£º50	 i:8 	 global-step:1008	 l-p:0.2220173478126526
epoch£º50	 i:9 	 global-step:1009	 l-p:0.10828074812889099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0597, 4.0597, 4.0597],
        [4.0597, 4.5324, 4.6255],
        [4.0597, 5.0986, 5.7735],
        [4.0597, 4.0598, 4.0597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.17797626554965973 
model_pd.l_d.mean(): -21.904205322265625 
model_pd.lagr.mean(): -21.726228713989258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0035], device='cuda:0')), ('power', tensor([-21.9007], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.17797626554965973
epoch£º51	 i:1 	 global-step:1021	 l-p:0.09614460915327072
epoch£º51	 i:2 	 global-step:1022	 l-p:-0.031077586114406586
epoch£º51	 i:3 	 global-step:1023	 l-p:0.1108386218547821
epoch£º51	 i:4 	 global-step:1024	 l-p:0.09953860938549042
epoch£º51	 i:5 	 global-step:1025	 l-p:0.10178016126155853
epoch£º51	 i:6 	 global-step:1026	 l-p:0.16796284914016724
epoch£º51	 i:7 	 global-step:1027	 l-p:0.1307106912136078
epoch£º51	 i:8 	 global-step:1028	 l-p:0.107612244784832
epoch£º51	 i:9 	 global-step:1029	 l-p:0.10644625127315521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3190, 4.8481, 4.9616],
        [4.3190, 4.7000, 4.7131],
        [4.3190, 4.3636, 4.3318],
        [4.3190, 4.3190, 4.3190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.09998742491006851 
model_pd.l_d.mean(): -23.005634307861328 
model_pd.lagr.mean(): -22.90564727783203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1976], device='cuda:0')), ('power', tensor([-22.8080], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.09998742491006851
epoch£º52	 i:1 	 global-step:1041	 l-p:0.11120330542325974
epoch£º52	 i:2 	 global-step:1042	 l-p:-0.24359145760536194
epoch£º52	 i:3 	 global-step:1043	 l-p:0.11900477111339569
epoch£º52	 i:4 	 global-step:1044	 l-p:0.10010886937379837
epoch£º52	 i:5 	 global-step:1045	 l-p:0.15980088710784912
epoch£º52	 i:6 	 global-step:1046	 l-p:0.09236966073513031
epoch£º52	 i:7 	 global-step:1047	 l-p:0.1757216453552246
epoch£º52	 i:8 	 global-step:1048	 l-p:0.11822882294654846
epoch£º52	 i:9 	 global-step:1049	 l-p:0.13573379814624786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9180, 3.9245, 3.9186],
        [3.9180, 3.9180, 3.9180],
        [3.9180, 3.9185, 3.9180],
        [3.9180, 3.9183, 3.9180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.12864689528942108 
model_pd.l_d.mean(): -22.196334838867188 
model_pd.lagr.mean(): -22.06768798828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0068], device='cuda:0')), ('power', tensor([-22.2031], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.12864689528942108
epoch£º53	 i:1 	 global-step:1061	 l-p:0.10201445966959
epoch£º53	 i:2 	 global-step:1062	 l-p:0.04397957772016525
epoch£º53	 i:3 	 global-step:1063	 l-p:0.09638390690088272
epoch£º53	 i:4 	 global-step:1064	 l-p:0.12693209946155548
epoch£º53	 i:5 	 global-step:1065	 l-p:0.4166395366191864
epoch£º53	 i:6 	 global-step:1066	 l-p:0.14425128698349
epoch£º53	 i:7 	 global-step:1067	 l-p:0.11256889998912811
epoch£º53	 i:8 	 global-step:1068	 l-p:0.1149512454867363
epoch£º53	 i:9 	 global-step:1069	 l-p:0.11105340719223022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8570, 3.8570, 3.8570],
        [3.8570, 4.3732, 4.5211],
        [3.8570, 4.4108, 4.5917],
        [3.8570, 3.9338, 3.8912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.03789215162396431 
model_pd.l_d.mean(): -23.212995529174805 
model_pd.lagr.mean(): -23.17510414123535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0439], device='cuda:0')), ('power', tensor([-23.1691], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.03789215162396431
epoch£º54	 i:1 	 global-step:1081	 l-p:0.12669987976551056
epoch£º54	 i:2 	 global-step:1082	 l-p:0.10356952250003815
epoch£º54	 i:3 	 global-step:1083	 l-p:0.13575750589370728
epoch£º54	 i:4 	 global-step:1084	 l-p:0.12945963442325592
epoch£º54	 i:5 	 global-step:1085	 l-p:0.10692352056503296
epoch£º54	 i:6 	 global-step:1086	 l-p:0.14397916197776794
epoch£º54	 i:7 	 global-step:1087	 l-p:0.1173357143998146
epoch£º54	 i:8 	 global-step:1088	 l-p:0.3278201222419739
epoch£º54	 i:9 	 global-step:1089	 l-p:0.11791090667247772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[3.7846, 4.3228, 4.4983],
        [3.7846, 4.5439, 4.9415],
        [3.7846, 4.1015, 4.1119],
        [3.7846, 4.3295, 4.5109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.08423526585102081 
model_pd.l_d.mean(): -22.81218719482422 
model_pd.lagr.mean(): -22.727951049804688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0353], device='cuda:0')), ('power', tensor([-22.8475], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.08423526585102081
epoch£º55	 i:1 	 global-step:1101	 l-p:0.11853199452161789
epoch£º55	 i:2 	 global-step:1102	 l-p:0.11204907298088074
epoch£º55	 i:3 	 global-step:1103	 l-p:0.11541593074798584
epoch£º55	 i:4 	 global-step:1104	 l-p:0.18059951066970825
epoch£º55	 i:5 	 global-step:1105	 l-p:0.1537768840789795
epoch£º55	 i:6 	 global-step:1106	 l-p:0.12152938544750214
epoch£º55	 i:7 	 global-step:1107	 l-p:0.19900260865688324
epoch£º55	 i:8 	 global-step:1108	 l-p:0.13704006373882294
epoch£º55	 i:9 	 global-step:1109	 l-p:0.11297508329153061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8462, 4.8100, 5.4362],
        [3.8462, 3.8462, 3.8462],
        [3.8462, 4.0210, 3.9749],
        [3.8462, 3.9022, 3.8668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.06316523253917694 
model_pd.l_d.mean(): -22.782812118530273 
model_pd.lagr.mean(): -22.719646453857422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0223], device='cuda:0')), ('power', tensor([-22.8052], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.06316523253917694
epoch£º56	 i:1 	 global-step:1121	 l-p:0.02434086799621582
epoch£º56	 i:2 	 global-step:1122	 l-p:0.11694090813398361
epoch£º56	 i:3 	 global-step:1123	 l-p:0.1220490112900734
epoch£º56	 i:4 	 global-step:1124	 l-p:0.1450560986995697
epoch£º56	 i:5 	 global-step:1125	 l-p:0.11663543432950974
epoch£º56	 i:6 	 global-step:1126	 l-p:0.13516786694526672
epoch£º56	 i:7 	 global-step:1127	 l-p:0.10056968778371811
epoch£º56	 i:8 	 global-step:1128	 l-p:0.1319413036108017
epoch£º56	 i:9 	 global-step:1129	 l-p:0.1154940128326416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8224, 4.5559, 4.9207],
        [3.8224, 4.6421, 5.1022],
        [3.8224, 3.8224, 3.8224],
        [3.8224, 3.8716, 3.8393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.13993003964424133 
model_pd.l_d.mean(): -22.999351501464844 
model_pd.lagr.mean(): -22.859420776367188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0028], device='cuda:0')), ('power', tensor([-23.0022], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.13993003964424133
epoch£º57	 i:1 	 global-step:1141	 l-p:0.07549356669187546
epoch£º57	 i:2 	 global-step:1142	 l-p:0.12238156795501709
epoch£º57	 i:3 	 global-step:1143	 l-p:0.1442248374223709
epoch£º57	 i:4 	 global-step:1144	 l-p:0.09374912828207016
epoch£º57	 i:5 	 global-step:1145	 l-p:0.06213873252272606
epoch£º57	 i:6 	 global-step:1146	 l-p:0.5635238885879517
epoch£º57	 i:7 	 global-step:1147	 l-p:0.1296115666627884
epoch£º57	 i:8 	 global-step:1148	 l-p:0.18203629553318024
epoch£º57	 i:9 	 global-step:1149	 l-p:0.0993211641907692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7513, 3.9715, 3.9404],
        [3.7513, 3.9052, 3.8590],
        [3.7513, 4.6421, 5.1988],
        [3.7513, 3.7513, 3.7513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.10059365630149841 
model_pd.l_d.mean(): -23.249481201171875 
model_pd.lagr.mean(): -23.148887634277344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0045], device='cuda:0')), ('power', tensor([-23.2450], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.10059365630149841
epoch£º58	 i:1 	 global-step:1161	 l-p:0.1452159732580185
epoch£º58	 i:2 	 global-step:1162	 l-p:0.09206261485815048
epoch£º58	 i:3 	 global-step:1163	 l-p:0.14176015555858612
epoch£º58	 i:4 	 global-step:1164	 l-p:0.1608857661485672
epoch£º58	 i:5 	 global-step:1165	 l-p:0.10178982466459274
epoch£º58	 i:6 	 global-step:1166	 l-p:0.09293942898511887
epoch£º58	 i:7 	 global-step:1167	 l-p:0.10854046791791916
epoch£º58	 i:8 	 global-step:1168	 l-p:0.11263397336006165
epoch£º58	 i:9 	 global-step:1169	 l-p:0.13491739332675934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9367, 4.2014, 4.1793],
        [3.9367, 3.9384, 3.9368],
        [3.9367, 3.9368, 3.9367],
        [3.9367, 4.0639, 4.0131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.12281300872564316 
model_pd.l_d.mean(): -22.057344436645508 
model_pd.lagr.mean(): -21.934532165527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0099], device='cuda:0')), ('power', tensor([-22.0673], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.12281300872564316
epoch£º59	 i:1 	 global-step:1181	 l-p:0.07728759944438934
epoch£º59	 i:2 	 global-step:1182	 l-p:0.1149112805724144
epoch£º59	 i:3 	 global-step:1183	 l-p:-0.31871798634529114
epoch£º59	 i:4 	 global-step:1184	 l-p:0.10969999432563782
epoch£º59	 i:5 	 global-step:1185	 l-p:0.10559429973363876
epoch£º59	 i:6 	 global-step:1186	 l-p:0.11268120259046555
epoch£º59	 i:7 	 global-step:1187	 l-p:0.18567676842212677
epoch£º59	 i:8 	 global-step:1188	 l-p:0.19134590029716492
epoch£º59	 i:9 	 global-step:1189	 l-p:0.11049094796180725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8051, 3.8813, 3.8397],
        [3.8051, 3.8052, 3.8051],
        [3.8051, 3.8065, 3.8052],
        [3.8051, 4.0499, 4.0259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.08115412294864655 
model_pd.l_d.mean(): -21.56775665283203 
model_pd.lagr.mean(): -21.486602783203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1420], device='cuda:0')), ('power', tensor([-21.7098], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.08115412294864655
epoch£º60	 i:1 	 global-step:1201	 l-p:0.11714833974838257
epoch£º60	 i:2 	 global-step:1202	 l-p:0.18173398077487946
epoch£º60	 i:3 	 global-step:1203	 l-p:0.10950838774442673
epoch£º60	 i:4 	 global-step:1204	 l-p:0.13693463802337646
epoch£º60	 i:5 	 global-step:1205	 l-p:0.10386842489242554
epoch£º60	 i:6 	 global-step:1206	 l-p:0.12396273016929626
epoch£º60	 i:7 	 global-step:1207	 l-p:0.12577737867832184
epoch£º60	 i:8 	 global-step:1208	 l-p:0.11200957000255585
epoch£º60	 i:9 	 global-step:1209	 l-p:0.0612141415476799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6624, 3.6624, 3.6624],
        [3.6624, 4.0992, 4.2036],
        [3.6624, 3.6669, 3.6627],
        [3.6624, 4.1610, 4.3173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.11633621156215668 
model_pd.l_d.mean(): -22.297348022460938 
model_pd.lagr.mean(): -22.181011199951172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0964], device='cuda:0')), ('power', tensor([-22.3937], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.11633621156215668
epoch£º61	 i:1 	 global-step:1221	 l-p:0.12096025049686432
epoch£º61	 i:2 	 global-step:1222	 l-p:0.13806530833244324
epoch£º61	 i:3 	 global-step:1223	 l-p:0.1805594265460968
epoch£º61	 i:4 	 global-step:1224	 l-p:0.09166537970304489
epoch£º61	 i:5 	 global-step:1225	 l-p:0.01587599329650402
epoch£º61	 i:6 	 global-step:1226	 l-p:0.13370339572429657
epoch£º61	 i:7 	 global-step:1227	 l-p:0.09418371319770813
epoch£º61	 i:8 	 global-step:1228	 l-p:0.1039898470044136
epoch£º61	 i:9 	 global-step:1229	 l-p:0.13470353186130524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7970, 3.8201, 3.8020],
        [3.7970, 4.4832, 4.8043],
        [3.7970, 3.8930, 3.8474],
        [3.7970, 3.9537, 3.9073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.09816031903028488 
model_pd.l_d.mean(): -22.7551326751709 
model_pd.lagr.mean(): -22.656972885131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0449], device='cuda:0')), ('power', tensor([-22.8000], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.09816031903028488
epoch£º62	 i:1 	 global-step:1241	 l-p:0.11947234719991684
epoch£º62	 i:2 	 global-step:1242	 l-p:0.15797707438468933
epoch£º62	 i:3 	 global-step:1243	 l-p:0.09387172758579254
epoch£º62	 i:4 	 global-step:1244	 l-p:0.113413006067276
epoch£º62	 i:5 	 global-step:1245	 l-p:0.09573112428188324
epoch£º62	 i:6 	 global-step:1246	 l-p:0.010175585746765137
epoch£º62	 i:7 	 global-step:1247	 l-p:0.11351576447486877
epoch£º62	 i:8 	 global-step:1248	 l-p:0.10369382798671722
epoch£º62	 i:9 	 global-step:1249	 l-p:0.15003162622451782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8316, 3.8335, 3.8317],
        [3.8316, 4.8138, 5.4698],
        [3.8316, 4.8173, 5.4776],
        [3.8316, 3.8316, 3.8316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.10748275369405746 
model_pd.l_d.mean(): -22.82132911682129 
model_pd.lagr.mean(): -22.71384620666504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0139], device='cuda:0')), ('power', tensor([-22.8352], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.10748275369405746
epoch£º63	 i:1 	 global-step:1261	 l-p:0.14728230237960815
epoch£º63	 i:2 	 global-step:1262	 l-p:0.12215261906385422
epoch£º63	 i:3 	 global-step:1263	 l-p:0.11828399449586868
epoch£º63	 i:4 	 global-step:1264	 l-p:-0.16280625760555267
epoch£º63	 i:5 	 global-step:1265	 l-p:0.1176772266626358
epoch£º63	 i:6 	 global-step:1266	 l-p:0.11738792061805725
epoch£º63	 i:7 	 global-step:1267	 l-p:0.3876582384109497
epoch£º63	 i:8 	 global-step:1268	 l-p:0.310699999332428
epoch£º63	 i:9 	 global-step:1269	 l-p:0.09905927628278732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5042, 3.5053, 3.5042],
        [3.5042, 3.5042, 3.5042],
        [3.5042, 3.5221, 3.5078],
        [3.5042, 3.5042, 3.5042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.09796934574842453 
model_pd.l_d.mean(): -22.689491271972656 
model_pd.lagr.mean(): -22.591522216796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1460], device='cuda:0')), ('power', tensor([-22.8355], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.09796934574842453
epoch£º64	 i:1 	 global-step:1281	 l-p:0.12263946980237961
epoch£º64	 i:2 	 global-step:1282	 l-p:0.1479654610157013
epoch£º64	 i:3 	 global-step:1283	 l-p:0.12382490187883377
epoch£º64	 i:4 	 global-step:1284	 l-p:0.17149415612220764
epoch£º64	 i:5 	 global-step:1285	 l-p:0.12173284590244293
epoch£º64	 i:6 	 global-step:1286	 l-p:0.12320753186941147
epoch£º64	 i:7 	 global-step:1287	 l-p:0.10793456435203552
epoch£º64	 i:8 	 global-step:1288	 l-p:0.12385257333517075
epoch£º64	 i:9 	 global-step:1289	 l-p:-0.016283173114061356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6544, 3.6547, 3.6544],
        [3.6544, 3.6544, 3.6544],
        [3.6544, 4.3845, 4.7755],
        [3.6544, 3.7349, 3.6938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.11809192597866058 
model_pd.l_d.mean(): -22.833274841308594 
model_pd.lagr.mean(): -22.71518325805664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0842], device='cuda:0')), ('power', tensor([-22.9175], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.11809192597866058
epoch£º65	 i:1 	 global-step:1301	 l-p:0.025977715849876404
epoch£º65	 i:2 	 global-step:1302	 l-p:0.11239174753427505
epoch£º65	 i:3 	 global-step:1303	 l-p:0.1003868579864502
epoch£º65	 i:4 	 global-step:1304	 l-p:0.07762882113456726
epoch£º65	 i:5 	 global-step:1305	 l-p:0.13282394409179688
epoch£º65	 i:6 	 global-step:1306	 l-p:0.16189056634902954
epoch£º65	 i:7 	 global-step:1307	 l-p:0.10224214941263199
epoch£º65	 i:8 	 global-step:1308	 l-p:0.13377165794372559
epoch£º65	 i:9 	 global-step:1309	 l-p:0.11374359577894211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8691, 3.8690, 3.8690],
        [3.8691, 4.6478, 5.0589],
        [3.8691, 4.2856, 4.3581],
        [3.8691, 3.8878, 3.8726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.13944467902183533 
model_pd.l_d.mean(): -23.072650909423828 
model_pd.lagr.mean(): -22.93320655822754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0296], device='cuda:0')), ('power', tensor([-23.0431], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.13944467902183533
epoch£º66	 i:1 	 global-step:1321	 l-p:0.05914897844195366
epoch£º66	 i:2 	 global-step:1322	 l-p:0.12000028043985367
epoch£º66	 i:3 	 global-step:1323	 l-p:0.12031446397304535
epoch£º66	 i:4 	 global-step:1324	 l-p:0.11235788464546204
epoch£º66	 i:5 	 global-step:1325	 l-p:0.10638020187616348
epoch£º66	 i:6 	 global-step:1326	 l-p:0.12017271667718887
epoch£º66	 i:7 	 global-step:1327	 l-p:0.1095474436879158
epoch£º66	 i:8 	 global-step:1328	 l-p:0.1048794612288475
epoch£º66	 i:9 	 global-step:1329	 l-p:0.0036256788298487663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8743, 3.8743, 3.8743],
        [3.8743, 3.8743, 3.8743],
        [3.8743, 4.6053, 4.9644],
        [3.8743, 4.4504, 4.6551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.11506356298923492 
model_pd.l_d.mean(): -22.762378692626953 
model_pd.lagr.mean(): -22.647315979003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0107], device='cuda:0')), ('power', tensor([-22.7731], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.11506356298923492
epoch£º67	 i:1 	 global-step:1341	 l-p:0.08150548487901688
epoch£º67	 i:2 	 global-step:1342	 l-p:0.17055633664131165
epoch£º67	 i:3 	 global-step:1343	 l-p:0.1120959222316742
epoch£º67	 i:4 	 global-step:1344	 l-p:0.12370555847883224
epoch£º67	 i:5 	 global-step:1345	 l-p:0.11113160848617554
epoch£º67	 i:6 	 global-step:1346	 l-p:0.10382665693759918
epoch£º67	 i:7 	 global-step:1347	 l-p:0.23257674276828766
epoch£º67	 i:8 	 global-step:1348	 l-p:0.19971558451652527
epoch£º67	 i:9 	 global-step:1349	 l-p:0.22748512029647827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7294, 4.6312, 5.2104],
        [3.7294, 4.6720, 5.3014],
        [3.7294, 3.7362, 3.7301],
        [3.7294, 4.4763, 4.8754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.11043628305196762 
model_pd.l_d.mean(): -22.18032455444336 
model_pd.lagr.mean(): -22.069889068603516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1196], device='cuda:0')), ('power', tensor([-22.2999], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.11043628305196762
epoch£º68	 i:1 	 global-step:1361	 l-p:0.10146764665842056
epoch£º68	 i:2 	 global-step:1362	 l-p:2.335174560546875
epoch£º68	 i:3 	 global-step:1363	 l-p:0.12789104878902435
epoch£º68	 i:4 	 global-step:1364	 l-p:0.10559434443712234
epoch£º68	 i:5 	 global-step:1365	 l-p:0.11440012603998184
epoch£º68	 i:6 	 global-step:1366	 l-p:0.10114419460296631
epoch£º68	 i:7 	 global-step:1367	 l-p:0.11751438677310944
epoch£º68	 i:8 	 global-step:1368	 l-p:0.15710635483264923
epoch£º68	 i:9 	 global-step:1369	 l-p:0.11410821974277496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8542, 3.9783, 3.9300],
        [3.8542, 4.6651, 5.1157],
        [3.8542, 4.0218, 3.9763],
        [3.8542, 3.8542, 3.8542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.11088580638170242 
model_pd.l_d.mean(): -21.859769821166992 
model_pd.lagr.mean(): -21.748884201049805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0913], device='cuda:0')), ('power', tensor([-21.9510], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.11088580638170242
epoch£º69	 i:1 	 global-step:1381	 l-p:0.15062610805034637
epoch£º69	 i:2 	 global-step:1382	 l-p:0.12332730740308762
epoch£º69	 i:3 	 global-step:1383	 l-p:0.11228133738040924
epoch£º69	 i:4 	 global-step:1384	 l-p:0.042190395295619965
epoch£º69	 i:5 	 global-step:1385	 l-p:0.12788908183574677
epoch£º69	 i:6 	 global-step:1386	 l-p:0.10919664800167084
epoch£º69	 i:7 	 global-step:1387	 l-p:0.11163102835416794
epoch£º69	 i:8 	 global-step:1388	 l-p:0.13823437690734863
epoch£º69	 i:9 	 global-step:1389	 l-p:0.1390262246131897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7399, 4.7196, 5.3946],
        [3.7399, 3.7591, 3.7438],
        [3.7399, 3.7399, 3.7399],
        [3.7399, 4.6560, 5.2516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.14222058653831482 
model_pd.l_d.mean(): -21.440847396850586 
model_pd.lagr.mean(): -21.298625946044922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1618], device='cuda:0')), ('power', tensor([-21.6027], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.14222058653831482
epoch£º70	 i:1 	 global-step:1401	 l-p:0.07688416540622711
epoch£º70	 i:2 	 global-step:1402	 l-p:0.7755475640296936
epoch£º70	 i:3 	 global-step:1403	 l-p:0.11335153132677078
epoch£º70	 i:4 	 global-step:1404	 l-p:0.10040807723999023
epoch£º70	 i:5 	 global-step:1405	 l-p:0.11660852283239365
epoch£º70	 i:6 	 global-step:1406	 l-p:0.15063080191612244
epoch£º70	 i:7 	 global-step:1407	 l-p:0.6735081076622009
epoch£º70	 i:8 	 global-step:1408	 l-p:0.12018781155347824
epoch£º70	 i:9 	 global-step:1409	 l-p:0.12278162688016891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6351, 3.7678, 3.7239],
        [3.6351, 3.6373, 3.6353],
        [3.6351, 3.6351, 3.6351],
        [3.6351, 4.0562, 4.1542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.12841171026229858 
model_pd.l_d.mean(): -23.040559768676758 
model_pd.lagr.mean(): -22.912147521972656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0714], device='cuda:0')), ('power', tensor([-23.1119], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.12841171026229858
epoch£º71	 i:1 	 global-step:1421	 l-p:0.1134246289730072
epoch£º71	 i:2 	 global-step:1422	 l-p:0.1545267105102539
epoch£º71	 i:3 	 global-step:1423	 l-p:0.19857345521450043
epoch£º71	 i:4 	 global-step:1424	 l-p:0.11939140409231186
epoch£º71	 i:5 	 global-step:1425	 l-p:0.10951987653970718
epoch£º71	 i:6 	 global-step:1426	 l-p:0.08341140300035477
epoch£º71	 i:7 	 global-step:1427	 l-p:0.035738419741392136
epoch£º71	 i:8 	 global-step:1428	 l-p:0.1205458790063858
epoch£º71	 i:9 	 global-step:1429	 l-p:0.21014635264873505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7626, 3.7626, 3.7626],
        [3.7626, 3.7626, 3.7626],
        [3.7626, 3.7658, 3.7628],
        [3.7626, 3.7626, 3.7626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.11072511225938797 
model_pd.l_d.mean(): -22.290231704711914 
model_pd.lagr.mean(): -22.179506301879883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0460], device='cuda:0')), ('power', tensor([-22.3362], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.11072511225938797
epoch£º72	 i:1 	 global-step:1441	 l-p:0.1501331627368927
epoch£º72	 i:2 	 global-step:1442	 l-p:0.17734882235527039
epoch£º72	 i:3 	 global-step:1443	 l-p:0.040760669857263565
epoch£º72	 i:4 	 global-step:1444	 l-p:0.10534727573394775
epoch£º72	 i:5 	 global-step:1445	 l-p:0.11927211284637451
epoch£º72	 i:6 	 global-step:1446	 l-p:0.11107036471366882
epoch£º72	 i:7 	 global-step:1447	 l-p:0.11607713252305984
epoch£º72	 i:8 	 global-step:1448	 l-p:0.10620550811290741
epoch£º72	 i:9 	 global-step:1449	 l-p:0.10983672738075256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0744, 4.0744, 4.0744],
        [4.0744, 4.2023, 4.1505],
        [4.0744, 4.8571, 5.2440],
        [4.0744, 4.2539, 4.2049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.11471521854400635 
model_pd.l_d.mean(): -22.328598022460938 
model_pd.lagr.mean(): -22.213882446289062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0695], device='cuda:0')), ('power', tensor([-22.2591], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.11471521854400635
epoch£º73	 i:1 	 global-step:1461	 l-p:0.09475098550319672
epoch£º73	 i:2 	 global-step:1462	 l-p:0.10933433473110199
epoch£º73	 i:3 	 global-step:1463	 l-p:0.1325564682483673
epoch£º73	 i:4 	 global-step:1464	 l-p:0.11828210204839706
epoch£º73	 i:5 	 global-step:1465	 l-p:0.10145419836044312
epoch£º73	 i:6 	 global-step:1466	 l-p:0.1123310998082161
epoch£º73	 i:7 	 global-step:1467	 l-p:0.15129655599594116
epoch£º73	 i:8 	 global-step:1468	 l-p:0.08929046243429184
epoch£º73	 i:9 	 global-step:1469	 l-p:0.11557287722826004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6917, 3.7040, 3.6936],
        [3.6917, 3.9489, 3.9385],
        [3.6917, 4.2014, 4.3694],
        [3.6917, 3.8448, 3.8020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.5145483016967773 
model_pd.l_d.mean(): -22.64354133605957 
model_pd.lagr.mean(): -22.12899398803711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0968], device='cuda:0')), ('power', tensor([-22.7403], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.5145483016967773
epoch£º74	 i:1 	 global-step:1481	 l-p:0.10996976494789124
epoch£º74	 i:2 	 global-step:1482	 l-p:0.11526046693325043
epoch£º74	 i:3 	 global-step:1483	 l-p:0.10334940999746323
epoch£º74	 i:4 	 global-step:1484	 l-p:-0.04405926540493965
epoch£º74	 i:5 	 global-step:1485	 l-p:0.12796033918857574
epoch£º74	 i:6 	 global-step:1486	 l-p:0.19075176119804382
epoch£º74	 i:7 	 global-step:1487	 l-p:-0.03463732823729515
epoch£º74	 i:8 	 global-step:1488	 l-p:0.11356061697006226
epoch£º74	 i:9 	 global-step:1489	 l-p:0.11407113820314407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8594, 4.1085, 4.0869],
        [3.8594, 4.0165, 3.9702],
        [3.8594, 3.8594, 3.8594],
        [3.8594, 4.4448, 4.6630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.10105815529823303 
model_pd.l_d.mean(): -22.844202041625977 
model_pd.lagr.mean(): -22.74314308166504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0017], device='cuda:0')), ('power', tensor([-22.8459], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.10105815529823303
epoch£º75	 i:1 	 global-step:1501	 l-p:0.12377192080020905
epoch£º75	 i:2 	 global-step:1502	 l-p:0.1516798436641693
epoch£º75	 i:3 	 global-step:1503	 l-p:0.11654020100831985
epoch£º75	 i:4 	 global-step:1504	 l-p:0.11643127351999283
epoch£º75	 i:5 	 global-step:1505	 l-p:0.11223770678043365
epoch£º75	 i:6 	 global-step:1506	 l-p:0.11098640412092209
epoch£º75	 i:7 	 global-step:1507	 l-p:0.35547834634780884
epoch£º75	 i:8 	 global-step:1508	 l-p:0.02895926870405674
epoch£º75	 i:9 	 global-step:1509	 l-p:0.07630062103271484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6721, 3.6724, 3.6721],
        [3.6721, 3.6721, 3.6721],
        [3.6721, 4.1914, 4.3712],
        [3.6721, 3.8322, 3.7912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.016500497236847878 
model_pd.l_d.mean(): -22.40678596496582 
model_pd.lagr.mean(): -22.39028549194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1127], device='cuda:0')), ('power', tensor([-22.5195], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.016500497236847878
epoch£º76	 i:1 	 global-step:1521	 l-p:0.11589164286851883
epoch£º76	 i:2 	 global-step:1522	 l-p:0.10926985740661621
epoch£º76	 i:3 	 global-step:1523	 l-p:0.0934833213686943
epoch£º76	 i:4 	 global-step:1524	 l-p:0.1214330643415451
epoch£º76	 i:5 	 global-step:1525	 l-p:1.0429948568344116
epoch£º76	 i:6 	 global-step:1526	 l-p:0.14451812207698822
epoch£º76	 i:7 	 global-step:1527	 l-p:0.15518298745155334
epoch£º76	 i:8 	 global-step:1528	 l-p:0.10976945608854294
epoch£º76	 i:9 	 global-step:1529	 l-p:0.11328086256980896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8754, 3.8881, 3.8773],
        [3.8754, 3.8757, 3.8754],
        [3.8754, 3.9065, 3.8836],
        [3.8754, 3.9504, 3.9093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.11972071975469589 
model_pd.l_d.mean(): -23.097997665405273 
model_pd.lagr.mean(): -22.9782772064209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0295], device='cuda:0')), ('power', tensor([-23.0685], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.11972071975469589
epoch£º77	 i:1 	 global-step:1541	 l-p:0.05797779932618141
epoch£º77	 i:2 	 global-step:1542	 l-p:0.09163094311952591
epoch£º77	 i:3 	 global-step:1543	 l-p:0.1382322758436203
epoch£º77	 i:4 	 global-step:1544	 l-p:0.11779626458883286
epoch£º77	 i:5 	 global-step:1545	 l-p:0.1488843411207199
epoch£º77	 i:6 	 global-step:1546	 l-p:0.14841678738594055
epoch£º77	 i:7 	 global-step:1547	 l-p:0.10795560479164124
epoch£º77	 i:8 	 global-step:1548	 l-p:0.09848577529191971
epoch£º77	 i:9 	 global-step:1549	 l-p:0.11210714280605316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8533, 4.3048, 4.4090],
        [3.8533, 4.5156, 4.8109],
        [3.8533, 3.9299, 3.8886],
        [3.8533, 3.8921, 3.8651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.09053020179271698 
model_pd.l_d.mean(): -22.714359283447266 
model_pd.lagr.mean(): -22.623828887939453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0264], device='cuda:0')), ('power', tensor([-22.7408], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.09053020179271698
epoch£º78	 i:1 	 global-step:1561	 l-p:0.1366969496011734
epoch£º78	 i:2 	 global-step:1562	 l-p:0.11846324056386948
epoch£º78	 i:3 	 global-step:1563	 l-p:0.1621374487876892
epoch£º78	 i:4 	 global-step:1564	 l-p:0.11886045336723328
epoch£º78	 i:5 	 global-step:1565	 l-p:0.4698978364467621
epoch£º78	 i:6 	 global-step:1566	 l-p:0.09165201336145401
epoch£º78	 i:7 	 global-step:1567	 l-p:-0.0926164910197258
epoch£º78	 i:8 	 global-step:1568	 l-p:0.12355457991361618
epoch£º78	 i:9 	 global-step:1569	 l-p:0.12907913327217102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8396, 4.3261, 4.4604],
        [3.8396, 3.8397, 3.8396],
        [3.8396, 3.8409, 3.8397],
        [3.8396, 3.8429, 3.8399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.18810388445854187 
model_pd.l_d.mean(): -23.086116790771484 
model_pd.lagr.mean(): -22.898012161254883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0100], device='cuda:0')), ('power', tensor([-23.0762], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.18810388445854187
epoch£º79	 i:1 	 global-step:1581	 l-p:0.05686008185148239
epoch£º79	 i:2 	 global-step:1582	 l-p:0.10765913128852844
epoch£º79	 i:3 	 global-step:1583	 l-p:0.10773581266403198
epoch£º79	 i:4 	 global-step:1584	 l-p:0.14924603700637817
epoch£º79	 i:5 	 global-step:1585	 l-p:0.0964357927441597
epoch£º79	 i:6 	 global-step:1586	 l-p:0.09851697832345963
epoch£º79	 i:7 	 global-step:1587	 l-p:0.09914389997720718
epoch£º79	 i:8 	 global-step:1588	 l-p:0.06929551810026169
epoch£º79	 i:9 	 global-step:1589	 l-p:0.11955779790878296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9132, 4.9553, 5.6775],
        [3.9132, 3.9259, 3.9151],
        [3.9132, 3.9132, 3.9132],
        [3.9132, 4.6431, 5.0008]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.11874526739120483 
model_pd.l_d.mean(): -22.17983055114746 
model_pd.lagr.mean(): -22.061084747314453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0137], device='cuda:0')), ('power', tensor([-22.1936], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.11874526739120483
epoch£º80	 i:1 	 global-step:1601	 l-p:0.10714629292488098
epoch£º80	 i:2 	 global-step:1602	 l-p:0.10204064846038818
epoch£º80	 i:3 	 global-step:1603	 l-p:0.11095912754535675
epoch£º80	 i:4 	 global-step:1604	 l-p:0.11076083779335022
epoch£º80	 i:5 	 global-step:1605	 l-p:0.12833356857299805
epoch£º80	 i:6 	 global-step:1606	 l-p:0.11665541678667068
epoch£º80	 i:7 	 global-step:1607	 l-p:0.1241033524274826
epoch£º80	 i:8 	 global-step:1608	 l-p:0.1293046772480011
epoch£º80	 i:9 	 global-step:1609	 l-p:0.5640587210655212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6766, 3.6806, 3.6769],
        [3.6766, 3.6766, 3.6766],
        [3.6766, 4.3440, 4.6713],
        [3.6766, 3.7245, 3.6940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.11279396712779999 
model_pd.l_d.mean(): -22.975061416625977 
model_pd.lagr.mean(): -22.862266540527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0637], device='cuda:0')), ('power', tensor([-23.0388], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.11279396712779999
epoch£º81	 i:1 	 global-step:1621	 l-p:0.1307430863380432
epoch£º81	 i:2 	 global-step:1622	 l-p:0.1138976514339447
epoch£º81	 i:3 	 global-step:1623	 l-p:0.1805134117603302
epoch£º81	 i:4 	 global-step:1624	 l-p:0.14353376626968384
epoch£º81	 i:5 	 global-step:1625	 l-p:0.10404655337333679
epoch£º81	 i:6 	 global-step:1626	 l-p:0.7607448697090149
epoch£º81	 i:7 	 global-step:1627	 l-p:0.1104099228978157
epoch£º81	 i:8 	 global-step:1628	 l-p:0.04553547874093056
epoch£º81	 i:9 	 global-step:1629	 l-p:0.09437677264213562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6507, 3.6978, 3.6678],
        [3.6507, 4.1140, 4.2493],
        [3.6507, 3.7944, 3.7524],
        [3.6507, 4.3927, 4.8056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.12220307439565659 
model_pd.l_d.mean(): -23.002782821655273 
model_pd.lagr.mean(): -22.880578994750977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0706], device='cuda:0')), ('power', tensor([-23.0734], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.12220307439565659
epoch£º82	 i:1 	 global-step:1641	 l-p:0.1230255588889122
epoch£º82	 i:2 	 global-step:1642	 l-p:0.12147263437509537
epoch£º82	 i:3 	 global-step:1643	 l-p:0.09328252822160721
epoch£º82	 i:4 	 global-step:1644	 l-p:0.10831448435783386
epoch£º82	 i:5 	 global-step:1645	 l-p:0.07092279195785522
epoch£º82	 i:6 	 global-step:1646	 l-p:0.7439368963241577
epoch£º82	 i:7 	 global-step:1647	 l-p:0.2725141644477844
epoch£º82	 i:8 	 global-step:1648	 l-p:0.11244171112775803
epoch£º82	 i:9 	 global-step:1649	 l-p:0.08329786360263824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8145, 4.4778, 4.7819],
        [3.8145, 3.9834, 3.9411],
        [3.8145, 3.8150, 3.8146],
        [3.8145, 4.1785, 4.2239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.24384628236293793 
model_pd.l_d.mean(): -22.301197052001953 
model_pd.lagr.mean(): -22.057350158691406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0620], device='cuda:0')), ('power', tensor([-22.3632], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.24384628236293793
epoch£º83	 i:1 	 global-step:1661	 l-p:0.14080850780010223
epoch£º83	 i:2 	 global-step:1662	 l-p:0.11449947208166122
epoch£º83	 i:3 	 global-step:1663	 l-p:0.1201411634683609
epoch£º83	 i:4 	 global-step:1664	 l-p:0.11774888634681702
epoch£º83	 i:5 	 global-step:1665	 l-p:0.05755851790308952
epoch£º83	 i:6 	 global-step:1666	 l-p:0.10929527133703232
epoch£º83	 i:7 	 global-step:1667	 l-p:0.1228267252445221
epoch£º83	 i:8 	 global-step:1668	 l-p:0.10782498866319656
epoch£º83	 i:9 	 global-step:1669	 l-p:-0.07576943933963776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9253, 3.9254, 3.9253],
        [3.9253, 3.9317, 3.9260],
        [3.9253, 3.9815, 3.9466],
        [3.9253, 4.1018, 4.0578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.1378435641527176 
model_pd.l_d.mean(): -22.954282760620117 
model_pd.lagr.mean(): -22.816438674926758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0188], device='cuda:0')), ('power', tensor([-22.9355], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.1378435641527176
epoch£º84	 i:1 	 global-step:1681	 l-p:-0.10533197969198227
epoch£º84	 i:2 	 global-step:1682	 l-p:0.10834220796823502
epoch£º84	 i:3 	 global-step:1683	 l-p:0.11038835346698761
epoch£º84	 i:4 	 global-step:1684	 l-p:0.10757113993167877
epoch£º84	 i:5 	 global-step:1685	 l-p:0.13324549794197083
epoch£º84	 i:6 	 global-step:1686	 l-p:0.1175195500254631
epoch£º84	 i:7 	 global-step:1687	 l-p:0.18126565217971802
epoch£º84	 i:8 	 global-step:1688	 l-p:0.10839997977018356
epoch£º84	 i:9 	 global-step:1689	 l-p:0.018758201971650124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[3.7225, 4.2469, 4.4291],
        [3.7225, 4.0558, 4.0885],
        [3.7225, 4.3225, 4.5763],
        [3.7225, 3.9554, 3.9355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 1.5060607194900513 
model_pd.l_d.mean(): -22.80066680908203 
model_pd.lagr.mean(): -21.294605255126953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0714], device='cuda:0')), ('power', tensor([-22.8721], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:1.5060607194900513
epoch£º85	 i:1 	 global-step:1701	 l-p:0.10783542692661285
epoch£º85	 i:2 	 global-step:1702	 l-p:0.1912061721086502
epoch£º85	 i:3 	 global-step:1703	 l-p:0.11405669897794724
epoch£º85	 i:4 	 global-step:1704	 l-p:0.16791045665740967
epoch£º85	 i:5 	 global-step:1705	 l-p:-0.36538347601890564
epoch£º85	 i:6 	 global-step:1706	 l-p:0.10908488929271698
epoch£º85	 i:7 	 global-step:1707	 l-p:0.10579390078783035
epoch£º85	 i:8 	 global-step:1708	 l-p:0.12214696407318115
epoch£º85	 i:9 	 global-step:1709	 l-p:0.10562259703874588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8741, 4.5133, 4.7861],
        [3.8741, 3.8813, 3.8749],
        [3.8741, 3.8742, 3.8741],
        [3.8741, 4.1072, 4.0808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.12436282634735107 
model_pd.l_d.mean(): -21.000173568725586 
model_pd.lagr.mean(): -20.875810623168945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1935], device='cuda:0')), ('power', tensor([-21.1937], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.12436282634735107
epoch£º86	 i:1 	 global-step:1721	 l-p:0.10976620763540268
epoch£º86	 i:2 	 global-step:1722	 l-p:0.10808009654283524
epoch£º86	 i:3 	 global-step:1723	 l-p:0.14422458410263062
epoch£º86	 i:4 	 global-step:1724	 l-p:0.1073378324508667
epoch£º86	 i:5 	 global-step:1725	 l-p:0.08762077242136002
epoch£º86	 i:6 	 global-step:1726	 l-p:0.11520711332559586
epoch£º86	 i:7 	 global-step:1727	 l-p:0.08172094076871872
epoch£º86	 i:8 	 global-step:1728	 l-p:0.11708665639162064
epoch£º86	 i:9 	 global-step:1729	 l-p:0.03450843691825867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7202, 3.7862, 3.7493],
        [3.7202, 3.7232, 3.7204],
        [3.7202, 3.7205, 3.7202],
        [3.7202, 4.0833, 4.1362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.10401897132396698 
model_pd.l_d.mean(): -22.552194595336914 
model_pd.lagr.mean(): -22.44817543029785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1054], device='cuda:0')), ('power', tensor([-22.6576], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.10401897132396698
epoch£º87	 i:1 	 global-step:1741	 l-p:0.13883313536643982
epoch£º87	 i:2 	 global-step:1742	 l-p:0.08269698917865753
epoch£º87	 i:3 	 global-step:1743	 l-p:0.12311616539955139
epoch£º87	 i:4 	 global-step:1744	 l-p:0.12654148042201996
epoch£º87	 i:5 	 global-step:1745	 l-p:0.0694955587387085
epoch£º87	 i:6 	 global-step:1746	 l-p:0.06850653886795044
epoch£º87	 i:7 	 global-step:1747	 l-p:0.07955296337604523
epoch£º87	 i:8 	 global-step:1748	 l-p:0.1234903559088707
epoch£º87	 i:9 	 global-step:1749	 l-p:0.12172814458608627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7183, 3.7183, 3.7183],
        [3.7183, 3.9251, 3.8962],
        [3.7183, 3.7891, 3.7509],
        [3.7183, 3.7220, 3.7186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.23297007381916046 
model_pd.l_d.mean(): -23.268470764160156 
model_pd.lagr.mean(): -23.03550148010254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0102], device='cuda:0')), ('power', tensor([-23.2787], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.23297007381916046
epoch£º88	 i:1 	 global-step:1761	 l-p:0.08852127194404602
epoch£º88	 i:2 	 global-step:1762	 l-p:0.11659575998783112
epoch£º88	 i:3 	 global-step:1763	 l-p:0.14962005615234375
epoch£º88	 i:4 	 global-step:1764	 l-p:0.16067813336849213
epoch£º88	 i:5 	 global-step:1765	 l-p:0.07949263602495193
epoch£º88	 i:6 	 global-step:1766	 l-p:0.11496181041002274
epoch£º88	 i:7 	 global-step:1767	 l-p:0.11261538416147232
epoch£º88	 i:8 	 global-step:1768	 l-p:0.1081513985991478
epoch£º88	 i:9 	 global-step:1769	 l-p:0.12520965933799744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9875, 4.7699, 5.1763],
        [3.9875, 4.0073, 3.9915],
        [3.9875, 4.0143, 3.9939],
        [3.9875, 4.1101, 4.0613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.11540170758962631 
model_pd.l_d.mean(): -22.320241928100586 
model_pd.lagr.mean(): -22.2048397064209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0419], device='cuda:0')), ('power', tensor([-22.3621], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.11540170758962631
epoch£º89	 i:1 	 global-step:1781	 l-p:0.8186686038970947
epoch£º89	 i:2 	 global-step:1782	 l-p:0.11680150777101517
epoch£º89	 i:3 	 global-step:1783	 l-p:0.1333341896533966
epoch£º89	 i:4 	 global-step:1784	 l-p:0.09316873550415039
epoch£º89	 i:5 	 global-step:1785	 l-p:0.11890356987714767
epoch£º89	 i:6 	 global-step:1786	 l-p:0.10253221541643143
epoch£º89	 i:7 	 global-step:1787	 l-p:0.12167476117610931
epoch£º89	 i:8 	 global-step:1788	 l-p:0.10166531056165695
epoch£º89	 i:9 	 global-step:1789	 l-p:0.10959426313638687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9563, 4.7986, 5.2768],
        [3.9563, 3.9564, 3.9563],
        [3.9563, 3.9564, 3.9563],
        [3.9563, 3.9851, 3.9636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.12418125569820404 
model_pd.l_d.mean(): -22.382722854614258 
model_pd.lagr.mean(): -22.258541107177734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0095], device='cuda:0')), ('power', tensor([-22.3733], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.12418125569820404
epoch£º90	 i:1 	 global-step:1801	 l-p:0.08432294428348541
epoch£º90	 i:2 	 global-step:1802	 l-p:0.16129231452941895
epoch£º90	 i:3 	 global-step:1803	 l-p:0.12308827042579651
epoch£º90	 i:4 	 global-step:1804	 l-p:0.08245734125375748
epoch£º90	 i:5 	 global-step:1805	 l-p:0.11247421056032181
epoch£º90	 i:6 	 global-step:1806	 l-p:0.13372941315174103
epoch£º90	 i:7 	 global-step:1807	 l-p:0.1685793250799179
epoch£º90	 i:8 	 global-step:1808	 l-p:0.10209643840789795
epoch£º90	 i:9 	 global-step:1809	 l-p:0.853828489780426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7217, 3.7217, 3.7217],
        [3.7217, 3.7220, 3.7217],
        [3.7217, 3.7261, 3.7221],
        [3.7217, 4.2741, 4.4843]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.11237874627113342 
model_pd.l_d.mean(): -22.386777877807617 
model_pd.lagr.mean(): -22.274398803710938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0503], device='cuda:0')), ('power', tensor([-22.4371], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.11237874627113342
epoch£º91	 i:1 	 global-step:1821	 l-p:0.13905365765094757
epoch£º91	 i:2 	 global-step:1822	 l-p:0.10313098877668381
epoch£º91	 i:3 	 global-step:1823	 l-p:0.013205127790570259
epoch£º91	 i:4 	 global-step:1824	 l-p:-0.03499855101108551
epoch£º91	 i:5 	 global-step:1825	 l-p:0.11626306176185608
epoch£º91	 i:6 	 global-step:1826	 l-p:0.012998859398066998
epoch£º91	 i:7 	 global-step:1827	 l-p:0.09387090802192688
epoch£º91	 i:8 	 global-step:1828	 l-p:0.09596297889947891
epoch£º91	 i:9 	 global-step:1829	 l-p:0.10300479829311371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8108, 3.8108, 3.8108],
        [3.8108, 4.5733, 4.9871],
        [3.8108, 3.9175, 3.8726],
        [3.8108, 3.8539, 3.8252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.40514808893203735 
model_pd.l_d.mean(): -22.516204833984375 
model_pd.lagr.mean(): -22.11105728149414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0872], device='cuda:0')), ('power', tensor([-22.6035], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.40514808893203735
epoch£º92	 i:1 	 global-step:1841	 l-p:0.11444912850856781
epoch£º92	 i:2 	 global-step:1842	 l-p:0.12173527479171753
epoch£º92	 i:3 	 global-step:1843	 l-p:0.11036357283592224
epoch£º92	 i:4 	 global-step:1844	 l-p:0.11295429617166519
epoch£º92	 i:5 	 global-step:1845	 l-p:0.11484718322753906
epoch£º92	 i:6 	 global-step:1846	 l-p:0.11634204536676407
epoch£º92	 i:7 	 global-step:1847	 l-p:0.10010486096143723
epoch£º92	 i:8 	 global-step:1848	 l-p:0.059405483305454254
epoch£º92	 i:9 	 global-step:1849	 l-p:0.12171337008476257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8249, 3.8249, 3.8249],
        [3.8249, 4.0934, 4.0857],
        [3.8249, 4.1183, 4.1231],
        [3.8249, 4.0260, 3.9923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.23716525733470917 
model_pd.l_d.mean(): -22.578630447387695 
model_pd.lagr.mean(): -22.34146499633789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0330], device='cuda:0')), ('power', tensor([-22.6116], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.23716525733470917
epoch£º93	 i:1 	 global-step:1861	 l-p:0.16004858911037445
epoch£º93	 i:2 	 global-step:1862	 l-p:0.11542342603206635
epoch£º93	 i:3 	 global-step:1863	 l-p:0.1355161964893341
epoch£º93	 i:4 	 global-step:1864	 l-p:0.05061827227473259
epoch£º93	 i:5 	 global-step:1865	 l-p:0.11773179471492767
epoch£º93	 i:6 	 global-step:1866	 l-p:0.13299940526485443
epoch£º93	 i:7 	 global-step:1867	 l-p:0.1178237572312355
epoch£º93	 i:8 	 global-step:1868	 l-p:0.10989245772361755
epoch£º93	 i:9 	 global-step:1869	 l-p:0.09637874364852905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8109, 4.2008, 4.2674],
        [3.8109, 3.8109, 3.8109],
        [3.8109, 3.8424, 3.8196],
        [3.8109, 3.8211, 3.8123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.1189582571387291 
model_pd.l_d.mean(): -23.289777755737305 
model_pd.lagr.mean(): -23.170820236206055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0308], device='cuda:0')), ('power', tensor([-23.2590], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.1189582571387291
epoch£º94	 i:1 	 global-step:1881	 l-p:0.26715487241744995
epoch£º94	 i:2 	 global-step:1882	 l-p:0.10843583196401596
epoch£º94	 i:3 	 global-step:1883	 l-p:0.14121150970458984
epoch£º94	 i:4 	 global-step:1884	 l-p:0.10833784192800522
epoch£º94	 i:5 	 global-step:1885	 l-p:0.10841180384159088
epoch£º94	 i:6 	 global-step:1886	 l-p:0.018438033759593964
epoch£º94	 i:7 	 global-step:1887	 l-p:0.13789330422878265
epoch£º94	 i:8 	 global-step:1888	 l-p:0.1170232817530632
epoch£º94	 i:9 	 global-step:1889	 l-p:0.12247846275568008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6462, 3.6462, 3.6462],
        [3.6462, 3.6462, 3.6462],
        [3.6462, 3.6471, 3.6462],
        [3.6462, 3.6565, 3.6477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.13225767016410828 
model_pd.l_d.mean(): -22.311220169067383 
model_pd.lagr.mean(): -22.17896270751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1315], device='cuda:0')), ('power', tensor([-22.4427], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.13225767016410828
epoch£º95	 i:1 	 global-step:1901	 l-p:0.11877806484699249
epoch£º95	 i:2 	 global-step:1902	 l-p:0.13720105588436127
epoch£º95	 i:3 	 global-step:1903	 l-p:0.12492121756076813
epoch£º95	 i:4 	 global-step:1904	 l-p:0.0861302837729454
epoch£º95	 i:5 	 global-step:1905	 l-p:0.12760010361671448
epoch£º95	 i:6 	 global-step:1906	 l-p:-0.48202040791511536
epoch£º95	 i:7 	 global-step:1907	 l-p:0.10894500464200974
epoch£º95	 i:8 	 global-step:1908	 l-p:0.25694674253463745
epoch£º95	 i:9 	 global-step:1909	 l-p:0.122269406914711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7783, 4.6219, 5.1351],
        [3.7783, 3.7788, 3.7783],
        [3.7783, 4.3345, 4.5435],
        [3.7783, 3.7783, 3.7783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.15177133679389954 
model_pd.l_d.mean(): -21.678091049194336 
model_pd.lagr.mean(): -21.52631950378418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1799], device='cuda:0')), ('power', tensor([-21.8580], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.15177133679389954
epoch£º96	 i:1 	 global-step:1921	 l-p:0.11432638764381409
epoch£º96	 i:2 	 global-step:1922	 l-p:0.10919084399938583
epoch£º96	 i:3 	 global-step:1923	 l-p:0.08338981121778488
epoch£º96	 i:4 	 global-step:1924	 l-p:0.14490608870983124
epoch£º96	 i:5 	 global-step:1925	 l-p:0.1577829271554947
epoch£º96	 i:6 	 global-step:1926	 l-p:0.22177402675151825
epoch£º96	 i:7 	 global-step:1927	 l-p:0.12130563706159592
epoch£º96	 i:8 	 global-step:1928	 l-p:0.1121235340833664
epoch£º96	 i:9 	 global-step:1929	 l-p:0.11190678179264069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8629, 4.2084, 4.2422],
        [3.8629, 4.4812, 4.7395],
        [3.8629, 3.8630, 3.8629],
        [3.8629, 4.8100, 5.4315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.11608345806598663 
model_pd.l_d.mean(): -23.013158798217773 
model_pd.lagr.mean(): -22.897075653076172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0139], device='cuda:0')), ('power', tensor([-22.9992], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.11608345806598663
epoch£º97	 i:1 	 global-step:1941	 l-p:0.12086093425750732
epoch£º97	 i:2 	 global-step:1942	 l-p:-0.13544507324695587
epoch£º97	 i:3 	 global-step:1943	 l-p:0.1343439221382141
epoch£º97	 i:4 	 global-step:1944	 l-p:0.11333538591861725
epoch£º97	 i:5 	 global-step:1945	 l-p:-0.05983210355043411
epoch£º97	 i:6 	 global-step:1946	 l-p:0.11238536238670349
epoch£º97	 i:7 	 global-step:1947	 l-p:0.11987216770648956
epoch£º97	 i:8 	 global-step:1948	 l-p:0.11936865746974945
epoch£º97	 i:9 	 global-step:1949	 l-p:-0.012597691267728806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6365, 3.8967, 3.8961],
        [3.6365, 3.6365, 3.6365],
        [3.6365, 3.6645, 3.6441],
        [3.6365, 4.0630, 4.1745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.11927854269742966 
model_pd.l_d.mean(): -23.0194149017334 
model_pd.lagr.mean(): -22.900136947631836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0788], device='cuda:0')), ('power', tensor([-23.0982], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.11927854269742966
epoch£º98	 i:1 	 global-step:1961	 l-p:0.10119277983903885
epoch£º98	 i:2 	 global-step:1962	 l-p:0.11862802505493164
epoch£º98	 i:3 	 global-step:1963	 l-p:0.11516374349594116
epoch£º98	 i:4 	 global-step:1964	 l-p:0.12099015712738037
epoch£º98	 i:5 	 global-step:1965	 l-p:0.30672502517700195
epoch£º98	 i:6 	 global-step:1966	 l-p:0.1789647787809372
epoch£º98	 i:7 	 global-step:1967	 l-p:0.1222546249628067
epoch£º98	 i:8 	 global-step:1968	 l-p:0.1101851612329483
epoch£º98	 i:9 	 global-step:1969	 l-p:0.0794716328382492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8741, 3.8741, 3.8741],
        [3.8741, 3.8742, 3.8741],
        [3.8741, 4.2192, 4.2525],
        [3.8741, 3.8742, 3.8741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.11378871649503708 
model_pd.l_d.mean(): -22.41547393798828 
model_pd.lagr.mean(): -22.301685333251953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0070], device='cuda:0')), ('power', tensor([-22.4085], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.11378871649503708
epoch£º99	 i:1 	 global-step:1981	 l-p:0.14716115593910217
epoch£º99	 i:2 	 global-step:1982	 l-p:0.1730126589536667
epoch£º99	 i:3 	 global-step:1983	 l-p:0.10542484372854233
epoch£º99	 i:4 	 global-step:1984	 l-p:0.07141048461198807
epoch£º99	 i:5 	 global-step:1985	 l-p:0.10803195089101791
epoch£º99	 i:6 	 global-step:1986	 l-p:0.10830730944871902
epoch£º99	 i:7 	 global-step:1987	 l-p:0.13059720396995544
epoch£º99	 i:8 	 global-step:1988	 l-p:-0.022468233481049538
epoch£º99	 i:9 	 global-step:1989	 l-p:0.11056388914585114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9142, 3.9147, 3.9143],
        [3.9142, 4.2985, 4.3549],
        [3.9142, 3.9822, 3.9439],
        [3.9142, 3.9142, 3.9142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.11540595442056656 
model_pd.l_d.mean(): -23.267841339111328 
model_pd.lagr.mean(): -23.152435302734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0670], device='cuda:0')), ('power', tensor([-23.2008], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.11540595442056656
epoch£º100	 i:1 	 global-step:2001	 l-p:0.11123434454202652
epoch£º100	 i:2 	 global-step:2002	 l-p:0.10790447145700455
epoch£º100	 i:3 	 global-step:2003	 l-p:-1.063608169555664
epoch£º100	 i:4 	 global-step:2004	 l-p:0.1780792772769928
epoch£º100	 i:5 	 global-step:2005	 l-p:0.1646842509508133
epoch£º100	 i:6 	 global-step:2006	 l-p:0.12398414313793182
epoch£º100	 i:7 	 global-step:2007	 l-p:0.10911022871732712
epoch£º100	 i:8 	 global-step:2008	 l-p:0.09590964764356613
epoch£º100	 i:9 	 global-step:2009	 l-p:0.1582440286874771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8214, 3.8661, 3.8370],
        [3.8214, 3.8215, 3.8214],
        [3.8214, 3.8230, 3.8215],
        [3.8214, 3.8214, 3.8214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.1034502163529396 
model_pd.l_d.mean(): -22.862558364868164 
model_pd.lagr.mean(): -22.75910758972168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0085], device='cuda:0')), ('power', tensor([-22.8710], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.1034502163529396
epoch£º101	 i:1 	 global-step:2021	 l-p:0.16040490567684174
epoch£º101	 i:2 	 global-step:2022	 l-p:0.08320850878953934
epoch£º101	 i:3 	 global-step:2023	 l-p:0.12116270512342453
epoch£º101	 i:4 	 global-step:2024	 l-p:0.20445097982883453
epoch£º101	 i:5 	 global-step:2025	 l-p:0.11267175525426865
epoch£º101	 i:6 	 global-step:2026	 l-p:0.12208884954452515
epoch£º101	 i:7 	 global-step:2027	 l-p:0.0989321693778038
epoch£º101	 i:8 	 global-step:2028	 l-p:0.11123045533895493
epoch£º101	 i:9 	 global-step:2029	 l-p:0.1152115911245346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[3.7939, 4.3037, 4.4711],
        [3.7939, 4.3072, 4.4776],
        [3.7939, 4.6704, 5.2222],
        [3.7939, 3.9149, 3.8707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.10662487149238586 
model_pd.l_d.mean(): -22.858339309692383 
model_pd.lagr.mean(): -22.7517147064209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0423], device='cuda:0')), ('power', tensor([-22.9007], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.10662487149238586
epoch£º102	 i:1 	 global-step:2041	 l-p:0.1097264364361763
epoch£º102	 i:2 	 global-step:2042	 l-p:-0.1895599663257599
epoch£º102	 i:3 	 global-step:2043	 l-p:0.13237646222114563
epoch£º102	 i:4 	 global-step:2044	 l-p:0.1347050964832306
epoch£º102	 i:5 	 global-step:2045	 l-p:0.12382377684116364
epoch£º102	 i:6 	 global-step:2046	 l-p:0.1227906197309494
epoch£º102	 i:7 	 global-step:2047	 l-p:0.11893640458583832
epoch£º102	 i:8 	 global-step:2048	 l-p:0.11914859712123871
epoch£º102	 i:9 	 global-step:2049	 l-p:0.09044454246759415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5820, 3.6464, 3.6117],
        [3.5820, 3.8141, 3.8040],
        [3.5820, 3.6419, 3.6085],
        [3.5820, 3.5890, 3.5829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.04886828362941742 
model_pd.l_d.mean(): -22.736835479736328 
model_pd.lagr.mean(): -22.68796730041504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1367], device='cuda:0')), ('power', tensor([-22.8735], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.04886828362941742
epoch£º103	 i:1 	 global-step:2061	 l-p:0.11849759519100189
epoch£º103	 i:2 	 global-step:2062	 l-p:0.12215076386928558
epoch£º103	 i:3 	 global-step:2063	 l-p:0.13973605632781982
epoch£º103	 i:4 	 global-step:2064	 l-p:0.13930891454219818
epoch£º103	 i:5 	 global-step:2065	 l-p:0.06575064361095428
epoch£º103	 i:6 	 global-step:2066	 l-p:0.1489342451095581
epoch£º103	 i:7 	 global-step:2067	 l-p:0.1085159108042717
epoch£º103	 i:8 	 global-step:2068	 l-p:0.07809743285179138
epoch£º103	 i:9 	 global-step:2069	 l-p:0.10047980397939682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8580, 3.8593, 3.8581],
        [3.8580, 3.9656, 3.9210],
        [3.8580, 3.9005, 3.8723],
        [3.8580, 4.5399, 4.8656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.09010010212659836 
model_pd.l_d.mean(): -23.296056747436523 
model_pd.lagr.mean(): -23.205957412719727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0502], device='cuda:0')), ('power', tensor([-23.2459], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.09010010212659836
epoch£º104	 i:1 	 global-step:2081	 l-p:0.14705052971839905
epoch£º104	 i:2 	 global-step:2082	 l-p:0.11702726036310196
epoch£º104	 i:3 	 global-step:2083	 l-p:0.1373540312051773
epoch£º104	 i:4 	 global-step:2084	 l-p:0.08804118633270264
epoch£º104	 i:5 	 global-step:2085	 l-p:0.13724054396152496
epoch£º104	 i:6 	 global-step:2086	 l-p:0.12373721599578857
epoch£º104	 i:7 	 global-step:2087	 l-p:0.11372676491737366
epoch£º104	 i:8 	 global-step:2088	 l-p:0.11974277347326279
epoch£º104	 i:9 	 global-step:2089	 l-p:0.09481335431337357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9317, 3.9317, 3.9317],
        [3.9317, 3.9317, 3.9317],
        [3.9317, 4.0920, 4.0481],
        [3.9317, 4.3414, 4.4159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.12320468574762344 
model_pd.l_d.mean(): -22.996509552001953 
model_pd.lagr.mean(): -22.87330436706543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0370], device='cuda:0')), ('power', tensor([-22.9595], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.12320468574762344
epoch£º105	 i:1 	 global-step:2101	 l-p:0.12004830688238144
epoch£º105	 i:2 	 global-step:2102	 l-p:0.11846324056386948
epoch£º105	 i:3 	 global-step:2103	 l-p:0.11634000390768051
epoch£º105	 i:4 	 global-step:2104	 l-p:0.20120173692703247
epoch£º105	 i:5 	 global-step:2105	 l-p:0.08912704139947891
epoch£º105	 i:6 	 global-step:2106	 l-p:0.11992806941270828
epoch£º105	 i:7 	 global-step:2107	 l-p:0.12926888465881348
epoch£º105	 i:8 	 global-step:2108	 l-p:0.12540386617183685
epoch£º105	 i:9 	 global-step:2109	 l-p:0.049317121505737305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6998, 3.7017, 3.6999],
        [3.6998, 4.1926, 4.3570],
        [3.6998, 3.7124, 3.7019],
        [3.6998, 4.3385, 4.6432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): -0.06877129524946213 
model_pd.l_d.mean(): -21.79462432861328 
model_pd.lagr.mean(): -21.86339569091797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1586], device='cuda:0')), ('power', tensor([-21.9532], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:-0.06877129524946213
epoch£º106	 i:1 	 global-step:2121	 l-p:0.1023530587553978
epoch£º106	 i:2 	 global-step:2122	 l-p:0.1347552090883255
epoch£º106	 i:3 	 global-step:2123	 l-p:0.048783548176288605
epoch£º106	 i:4 	 global-step:2124	 l-p:0.21500392258167267
epoch£º106	 i:5 	 global-step:2125	 l-p:0.12681861221790314
epoch£º106	 i:6 	 global-step:2126	 l-p:0.11435364186763763
epoch£º106	 i:7 	 global-step:2127	 l-p:0.10475578159093857
epoch£º106	 i:8 	 global-step:2128	 l-p:0.13096573948860168
epoch£º106	 i:9 	 global-step:2129	 l-p:0.06422891467809677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8914, 3.8934, 3.8915],
        [3.8914, 3.8953, 3.8917],
        [3.8914, 4.6687, 5.0926],
        [3.8914, 4.5291, 4.8066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.10263068974018097 
model_pd.l_d.mean(): -22.378372192382812 
model_pd.lagr.mean(): -22.275741577148438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0297], device='cuda:0')), ('power', tensor([-22.4081], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.10263068974018097
epoch£º107	 i:1 	 global-step:2141	 l-p:0.08764570206403732
epoch£º107	 i:2 	 global-step:2142	 l-p:0.07684523612260818
epoch£º107	 i:3 	 global-step:2143	 l-p:0.14432886242866516
epoch£º107	 i:4 	 global-step:2144	 l-p:0.10830957442522049
epoch£º107	 i:5 	 global-step:2145	 l-p:0.11858487129211426
epoch£º107	 i:6 	 global-step:2146	 l-p:0.11390138417482376
epoch£º107	 i:7 	 global-step:2147	 l-p:0.08993852138519287
epoch£º107	 i:8 	 global-step:2148	 l-p:0.13648447394371033
epoch£º107	 i:9 	 global-step:2149	 l-p:0.109153151512146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9129, 3.9210, 3.9139],
        [3.9129, 3.9130, 3.9129],
        [3.9129, 3.9882, 3.9483],
        [3.9129, 3.9129, 3.9129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.09298419952392578 
model_pd.l_d.mean(): -22.04151725769043 
model_pd.lagr.mean(): -21.948532104492188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0369], device='cuda:0')), ('power', tensor([-22.0784], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.09298419952392578
epoch£º108	 i:1 	 global-step:2161	 l-p:0.11616550385951996
epoch£º108	 i:2 	 global-step:2162	 l-p:0.11655757576227188
epoch£º108	 i:3 	 global-step:2163	 l-p:0.1005060076713562
epoch£º108	 i:4 	 global-step:2164	 l-p:0.12697270512580872
epoch£º108	 i:5 	 global-step:2165	 l-p:0.866655170917511
epoch£º108	 i:6 	 global-step:2166	 l-p:0.11753002554178238
epoch£º108	 i:7 	 global-step:2167	 l-p:-0.2326236069202423
epoch£º108	 i:8 	 global-step:2168	 l-p:0.134903222322464
epoch£º108	 i:9 	 global-step:2169	 l-p:-0.08249910920858383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7280, 3.7861, 3.7525],
        [3.7280, 3.7291, 3.7281],
        [3.7280, 4.6133, 5.1912],
        [3.7280, 3.7280, 3.7280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): -0.10225289314985275 
model_pd.l_d.mean(): -22.8266544342041 
model_pd.lagr.mean(): -22.92890739440918 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0634], device='cuda:0')), ('power', tensor([-22.8900], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:-0.10225289314985275
epoch£º109	 i:1 	 global-step:2181	 l-p:0.09926700592041016
epoch£º109	 i:2 	 global-step:2182	 l-p:0.1397102326154709
epoch£º109	 i:3 	 global-step:2183	 l-p:0.11705856025218964
epoch£º109	 i:4 	 global-step:2184	 l-p:0.11852792650461197
epoch£º109	 i:5 	 global-step:2185	 l-p:0.10878879576921463
epoch£º109	 i:6 	 global-step:2186	 l-p:0.10362540185451508
epoch£º109	 i:7 	 global-step:2187	 l-p:0.12009734660387039
epoch£º109	 i:8 	 global-step:2188	 l-p:0.14270435273647308
epoch£º109	 i:9 	 global-step:2189	 l-p:0.10022859275341034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9386, 3.9390, 3.9386],
        [3.9386, 3.9386, 3.9386],
        [3.9386, 3.9394, 3.9386],
        [3.9386, 3.9545, 3.9415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.11420285701751709 
model_pd.l_d.mean(): -23.32093048095703 
model_pd.lagr.mean(): -23.206727981567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0865], device='cuda:0')), ('power', tensor([-23.2345], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.11420285701751709
epoch£º110	 i:1 	 global-step:2201	 l-p:0.12038419395685196
epoch£º110	 i:2 	 global-step:2202	 l-p:0.36644381284713745
epoch£º110	 i:3 	 global-step:2203	 l-p:0.11331554502248764
epoch£º110	 i:4 	 global-step:2204	 l-p:0.11732518672943115
epoch£º110	 i:5 	 global-step:2205	 l-p:0.11052190512418747
epoch£º110	 i:6 	 global-step:2206	 l-p:0.1312207132577896
epoch£º110	 i:7 	 global-step:2207	 l-p:0.08706363290548325
epoch£º110	 i:8 	 global-step:2208	 l-p:0.20549632608890533
epoch£º110	 i:9 	 global-step:2209	 l-p:0.12869714200496674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5490, 3.5518, 3.5492],
        [3.5490, 3.9993, 4.1445],
        [3.5490, 3.8890, 3.9466],
        [3.5490, 4.1243, 4.3873]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.10483739525079727 
model_pd.l_d.mean(): -22.891603469848633 
model_pd.lagr.mean(): -22.786766052246094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1292], device='cuda:0')), ('power', tensor([-23.0209], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.10483739525079727
epoch£º111	 i:1 	 global-step:2221	 l-p:0.11365560442209244
epoch£º111	 i:2 	 global-step:2222	 l-p:0.13398756086826324
epoch£º111	 i:3 	 global-step:2223	 l-p:0.24883338809013367
epoch£º111	 i:4 	 global-step:2224	 l-p:-1.0571194887161255
epoch£º111	 i:5 	 global-step:2225	 l-p:0.11015975475311279
epoch£º111	 i:6 	 global-step:2226	 l-p:0.109039306640625
epoch£º111	 i:7 	 global-step:2227	 l-p:0.07517346739768982
epoch£º111	 i:8 	 global-step:2228	 l-p:0.0982992947101593
epoch£º111	 i:9 	 global-step:2229	 l-p:0.14547526836395264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7701, 3.8154, 3.7865],
        [3.7701, 3.8140, 3.7857],
        [3.7701, 3.7701, 3.7701],
        [3.7701, 3.7857, 3.7731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.10465938597917557 
model_pd.l_d.mean(): -23.19991111755371 
model_pd.lagr.mean(): -23.095251083374023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0059], device='cuda:0')), ('power', tensor([-23.2059], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.10465938597917557
epoch£º112	 i:1 	 global-step:2241	 l-p:0.11619525402784348
epoch£º112	 i:2 	 global-step:2242	 l-p:0.09521254152059555
epoch£º112	 i:3 	 global-step:2243	 l-p:0.11255916953086853
epoch£º112	 i:4 	 global-step:2244	 l-p:0.12418519705533981
epoch£º112	 i:5 	 global-step:2245	 l-p:0.10036558657884598
epoch£º112	 i:6 	 global-step:2246	 l-p:0.16347946226596832
epoch£º112	 i:7 	 global-step:2247	 l-p:0.04975318908691406
epoch£º112	 i:8 	 global-step:2248	 l-p:0.12102856487035751
epoch£º112	 i:9 	 global-step:2249	 l-p:0.1062868982553482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7979, 3.7979, 3.7979],
        [3.7979, 4.3021, 4.4684],
        [3.7979, 3.8436, 3.8144],
        [3.7979, 3.7979, 3.7979]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.10228856652975082 
model_pd.l_d.mean(): -22.468246459960938 
model_pd.lagr.mean(): -22.365957260131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0539], device='cuda:0')), ('power', tensor([-22.5221], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.10228856652975082
epoch£º113	 i:1 	 global-step:2261	 l-p:0.10609740018844604
epoch£º113	 i:2 	 global-step:2262	 l-p:0.11385899037122726
epoch£º113	 i:3 	 global-step:2263	 l-p:0.09837344288825989
epoch£º113	 i:4 	 global-step:2264	 l-p:0.10009631514549255
epoch£º113	 i:5 	 global-step:2265	 l-p:0.1454821527004242
epoch£º113	 i:6 	 global-step:2266	 l-p:0.205637127161026
epoch£º113	 i:7 	 global-step:2267	 l-p:0.08721292018890381
epoch£º113	 i:8 	 global-step:2268	 l-p:0.10563592612743378
epoch£º113	 i:9 	 global-step:2269	 l-p:0.12427794188261032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6845, 3.6846, 3.6845],
        [3.6845, 3.6856, 3.6846],
        [3.6845, 3.6851, 3.6846],
        [3.6845, 4.3068, 4.6003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): -0.022490467876195908 
model_pd.l_d.mean(): -22.525171279907227 
model_pd.lagr.mean(): -22.54766082763672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1230], device='cuda:0')), ('power', tensor([-22.6482], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:-0.022490467876195908
epoch£º114	 i:1 	 global-step:2281	 l-p:0.11365517973899841
epoch£º114	 i:2 	 global-step:2282	 l-p:-0.0447203628718853
epoch£º114	 i:3 	 global-step:2283	 l-p:0.12300846725702286
epoch£º114	 i:4 	 global-step:2284	 l-p:-0.0339084267616272
epoch£º114	 i:5 	 global-step:2285	 l-p:0.11968974769115448
epoch£º114	 i:6 	 global-step:2286	 l-p:0.10362217575311661
epoch£º114	 i:7 	 global-step:2287	 l-p:0.15077586472034454
epoch£º114	 i:8 	 global-step:2288	 l-p:0.11577920615673065
epoch£º114	 i:9 	 global-step:2289	 l-p:0.05972651392221451
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9318, 3.9318, 3.9318],
        [3.9318, 3.9318, 3.9318],
        [3.9318, 3.9318, 3.9318],
        [3.9318, 4.0855, 4.0419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.09620622545480728 
model_pd.l_d.mean(): -22.939884185791016 
model_pd.lagr.mean(): -22.843677520751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0247], device='cuda:0')), ('power', tensor([-22.9152], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.09620622545480728
epoch£º115	 i:1 	 global-step:2301	 l-p:0.13203030824661255
epoch£º115	 i:2 	 global-step:2302	 l-p:0.11104069650173187
epoch£º115	 i:3 	 global-step:2303	 l-p:0.11126135289669037
epoch£º115	 i:4 	 global-step:2304	 l-p:0.11566902697086334
epoch£º115	 i:5 	 global-step:2305	 l-p:0.15858551859855652
epoch£º115	 i:6 	 global-step:2306	 l-p:0.11090020090341568
epoch£º115	 i:7 	 global-step:2307	 l-p:0.19892558455467224
epoch£º115	 i:8 	 global-step:2308	 l-p:0.0837499126791954
epoch£º115	 i:9 	 global-step:2309	 l-p:0.11827470362186432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8667, 3.9267, 3.8919],
        [3.8667, 3.8943, 3.8740],
        [3.8667, 3.8667, 3.8667],
        [3.8667, 4.0188, 3.9768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.13731764256954193 
model_pd.l_d.mean(): -22.614334106445312 
model_pd.lagr.mean(): -22.47701644897461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0363], device='cuda:0')), ('power', tensor([-22.6506], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.13731764256954193
epoch£º116	 i:1 	 global-step:2321	 l-p:0.10986024886369705
epoch£º116	 i:2 	 global-step:2322	 l-p:0.11642243713140488
epoch£º116	 i:3 	 global-step:2323	 l-p:0.11741691082715988
epoch£º116	 i:4 	 global-step:2324	 l-p:0.13160920143127441
epoch£º116	 i:5 	 global-step:2325	 l-p:0.1299227625131607
epoch£º116	 i:6 	 global-step:2326	 l-p:0.2868199646472931
epoch£º116	 i:7 	 global-step:2327	 l-p:0.11603757739067078
epoch£º116	 i:8 	 global-step:2328	 l-p:0.08054105937480927
epoch£º116	 i:9 	 global-step:2329	 l-p:0.09949684143066406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7112, 3.7118, 3.7112],
        [3.7112, 3.7523, 3.7255],
        [3.7112, 3.7112, 3.7112],
        [3.7112, 3.7112, 3.7112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.10666126012802124 
model_pd.l_d.mean(): -23.394264221191406 
model_pd.lagr.mean(): -23.2876033782959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0037], device='cuda:0')), ('power', tensor([-23.3905], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.10666126012802124
epoch£º117	 i:1 	 global-step:2341	 l-p:0.11879922449588776
epoch£º117	 i:2 	 global-step:2342	 l-p:0.12619522213935852
epoch£º117	 i:3 	 global-step:2343	 l-p:0.1162896379828453
epoch£º117	 i:4 	 global-step:2344	 l-p:0.12500226497650146
epoch£º117	 i:5 	 global-step:2345	 l-p:0.08799617737531662
epoch£º117	 i:6 	 global-step:2346	 l-p:-0.11480002850294113
epoch£º117	 i:7 	 global-step:2347	 l-p:0.10869565606117249
epoch£º117	 i:8 	 global-step:2348	 l-p:-0.841594398021698
epoch£º117	 i:9 	 global-step:2349	 l-p:0.12389551103115082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5617, 4.3669, 4.8840],
        [3.5617, 3.7663, 3.7486],
        [3.5617, 3.5748, 3.5642],
        [3.5617, 3.5617, 3.5617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.2557535767555237 
model_pd.l_d.mean(): -22.640249252319336 
model_pd.lagr.mean(): -22.38449478149414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1510], device='cuda:0')), ('power', tensor([-22.7912], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.2557535767555237
epoch£º118	 i:1 	 global-step:2361	 l-p:0.11990557610988617
epoch£º118	 i:2 	 global-step:2362	 l-p:0.2222745418548584
epoch£º118	 i:3 	 global-step:2363	 l-p:0.08909501880407333
epoch£º118	 i:4 	 global-step:2364	 l-p:0.10169503837823868
epoch£º118	 i:5 	 global-step:2365	 l-p:0.13703790307044983
epoch£º118	 i:6 	 global-step:2366	 l-p:0.10084255784749985
epoch£º118	 i:7 	 global-step:2367	 l-p:0.00635715015232563
epoch£º118	 i:8 	 global-step:2368	 l-p:0.2581730782985687
epoch£º118	 i:9 	 global-step:2369	 l-p:0.01752953976392746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8420, 3.9868, 3.9448],
        [3.8420, 3.8479, 3.8426],
        [3.8420, 4.2415, 4.3208],
        [3.8420, 4.3423, 4.5028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.11830081045627594 
model_pd.l_d.mean(): -23.281330108642578 
model_pd.lagr.mean(): -23.163028717041016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0366], device='cuda:0')), ('power', tensor([-23.2448], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.11830081045627594
epoch£º119	 i:1 	 global-step:2381	 l-p:0.07916650921106339
epoch£º119	 i:2 	 global-step:2382	 l-p:0.10699031502008438
epoch£º119	 i:3 	 global-step:2383	 l-p:0.062218260020017624
epoch£º119	 i:4 	 global-step:2384	 l-p:0.1356329470872879
epoch£º119	 i:5 	 global-step:2385	 l-p:0.1367492973804474
epoch£º119	 i:6 	 global-step:2386	 l-p:0.09594074636697769
epoch£º119	 i:7 	 global-step:2387	 l-p:0.11140336841344833
epoch£º119	 i:8 	 global-step:2388	 l-p:0.10560502856969833
epoch£º119	 i:9 	 global-step:2389	 l-p:0.10319749265909195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9761, 3.9761, 3.9761],
        [3.9761, 3.9762, 3.9761],
        [3.9761, 3.9768, 3.9761],
        [3.9761, 4.2977, 4.3160]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.10571743547916412 
model_pd.l_d.mean(): -22.956815719604492 
model_pd.lagr.mean(): -22.851099014282227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0390], device='cuda:0')), ('power', tensor([-22.9178], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.10571743547916412
epoch£º120	 i:1 	 global-step:2401	 l-p:0.03154617175459862
epoch£º120	 i:2 	 global-step:2402	 l-p:0.08987188339233398
epoch£º120	 i:3 	 global-step:2403	 l-p:0.11227479577064514
epoch£º120	 i:4 	 global-step:2404	 l-p:0.11272287368774414
epoch£º120	 i:5 	 global-step:2405	 l-p:0.11335507035255432
epoch£º120	 i:6 	 global-step:2406	 l-p:6.028859615325928
epoch£º120	 i:7 	 global-step:2407	 l-p:1.0536831617355347
epoch£º120	 i:8 	 global-step:2408	 l-p:0.12059253454208374
epoch£º120	 i:9 	 global-step:2409	 l-p:-0.28775760531425476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7147, 3.8492, 3.8092],
        [3.7147, 3.7340, 3.7190],
        [3.7147, 3.7147, 3.7147],
        [3.7147, 3.7151, 3.7147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.11750134825706482 
model_pd.l_d.mean(): -22.507320404052734 
model_pd.lagr.mean(): -22.38981819152832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0386], device='cuda:0')), ('power', tensor([-22.5459], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.11750134825706482
epoch£º121	 i:1 	 global-step:2421	 l-p:-0.23372599482536316
epoch£º121	 i:2 	 global-step:2422	 l-p:0.0825141966342926
epoch£º121	 i:3 	 global-step:2423	 l-p:0.08464336395263672
epoch£º121	 i:4 	 global-step:2424	 l-p:0.1909261792898178
epoch£º121	 i:5 	 global-step:2425	 l-p:0.1446247696876526
epoch£º121	 i:6 	 global-step:2426	 l-p:0.12104049324989319
epoch£º121	 i:7 	 global-step:2427	 l-p:0.10753819346427917
epoch£º121	 i:8 	 global-step:2428	 l-p:0.11715611815452576
epoch£º121	 i:9 	 global-step:2429	 l-p:0.08425421267747879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8358, 3.8554, 3.8401],
        [3.8358, 3.8429, 3.8367],
        [3.8358, 3.8358, 3.8358],
        [3.8358, 3.8408, 3.8363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.10207876563072205 
model_pd.l_d.mean(): -21.880268096923828 
model_pd.lagr.mean(): -21.778188705444336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0748], device='cuda:0')), ('power', tensor([-21.9551], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.10207876563072205
epoch£º122	 i:1 	 global-step:2441	 l-p:0.13634589314460754
epoch£º122	 i:2 	 global-step:2442	 l-p:0.08995036780834198
epoch£º122	 i:3 	 global-step:2443	 l-p:0.11269441992044449
epoch£º122	 i:4 	 global-step:2444	 l-p:0.11822408437728882
epoch£º122	 i:5 	 global-step:2445	 l-p:0.09592633694410324
epoch£º122	 i:6 	 global-step:2446	 l-p:0.12012369930744171
epoch£º122	 i:7 	 global-step:2447	 l-p:0.1178988367319107
epoch£º122	 i:8 	 global-step:2448	 l-p:0.05720535293221474
epoch£º122	 i:9 	 global-step:2449	 l-p:0.1612151712179184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5269, 3.5272, 3.5269],
        [3.5269, 3.5607, 3.5381],
        [3.5269, 3.5306, 3.5272],
        [3.5269, 3.5269, 3.5269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): -0.17145173251628876 
model_pd.l_d.mean(): -23.200481414794922 
model_pd.lagr.mean(): -23.371932983398438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1017], device='cuda:0')), ('power', tensor([-23.3022], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:-0.17145173251628876
epoch£º123	 i:1 	 global-step:2461	 l-p:0.14360550045967102
epoch£º123	 i:2 	 global-step:2462	 l-p:0.3546532988548279
epoch£º123	 i:3 	 global-step:2463	 l-p:0.11392993479967117
epoch£º123	 i:4 	 global-step:2464	 l-p:0.12844634056091309
epoch£º123	 i:5 	 global-step:2465	 l-p:0.05364298075437546
epoch£º123	 i:6 	 global-step:2466	 l-p:0.12646061182022095
epoch£º123	 i:7 	 global-step:2467	 l-p:0.0886058583855629
epoch£º123	 i:8 	 global-step:2468	 l-p:0.250674307346344
epoch£º123	 i:9 	 global-step:2469	 l-p:0.10795509070158005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8699, 3.8702, 3.8699],
        [3.8699, 4.4332, 4.6487],
        [3.8699, 3.8969, 3.8771],
        [3.8699, 3.9099, 3.8833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.089481882750988 
model_pd.l_d.mean(): -22.87002944946289 
model_pd.lagr.mean(): -22.780548095703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0013], device='cuda:0')), ('power', tensor([-22.8688], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.089481882750988
epoch£º124	 i:1 	 global-step:2481	 l-p:0.08535856008529663
epoch£º124	 i:2 	 global-step:2482	 l-p:0.12973995506763458
epoch£º124	 i:3 	 global-step:2483	 l-p:0.12194589525461197
epoch£º124	 i:4 	 global-step:2484	 l-p:0.10319801419973373
epoch£º124	 i:5 	 global-step:2485	 l-p:0.13671353459358215
epoch£º124	 i:6 	 global-step:2486	 l-p:0.11708144843578339
epoch£º124	 i:7 	 global-step:2487	 l-p:0.09569363296031952
epoch£º124	 i:8 	 global-step:2488	 l-p:0.10713006556034088
epoch£º124	 i:9 	 global-step:2489	 l-p:0.09420961141586304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8967, 3.9067, 3.8982],
        [3.8967, 4.8613, 5.5096],
        [3.8967, 3.9094, 3.8988],
        [3.8967, 4.2041, 4.2204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.11136976629495621 
model_pd.l_d.mean(): -22.16746711730957 
model_pd.lagr.mean(): -22.05609703063965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0217], device='cuda:0')), ('power', tensor([-22.1892], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.11136976629495621
epoch£º125	 i:1 	 global-step:2501	 l-p:0.14130891859531403
epoch£º125	 i:2 	 global-step:2502	 l-p:0.08830638974905014
epoch£º125	 i:3 	 global-step:2503	 l-p:0.1853621006011963
epoch£º125	 i:4 	 global-step:2504	 l-p:0.12567642331123352
epoch£º125	 i:5 	 global-step:2505	 l-p:0.13373854756355286
epoch£º125	 i:6 	 global-step:2506	 l-p:0.05466202273964882
epoch£º125	 i:7 	 global-step:2507	 l-p:-1.0435324907302856
epoch£º125	 i:8 	 global-step:2508	 l-p:0.11822705715894699
epoch£º125	 i:9 	 global-step:2509	 l-p:0.11997226625680923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7827, 4.2992, 4.4829],
        [3.7827, 4.1103, 4.1464],
        [3.7827, 3.7827, 3.7827],
        [3.7827, 3.7867, 3.7831]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.11241716146469116 
model_pd.l_d.mean(): -23.260892868041992 
model_pd.lagr.mean(): -23.148475646972656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0199], device='cuda:0')), ('power', tensor([-23.2410], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.11241716146469116
epoch£º126	 i:1 	 global-step:2521	 l-p:0.12950149178504944
epoch£º126	 i:2 	 global-step:2522	 l-p:0.10631314665079117
epoch£º126	 i:3 	 global-step:2523	 l-p:0.11475053429603577
epoch£º126	 i:4 	 global-step:2524	 l-p:0.10166202485561371
epoch£º126	 i:5 	 global-step:2525	 l-p:0.16882605850696564
epoch£º126	 i:6 	 global-step:2526	 l-p:0.12264616042375565
epoch£º126	 i:7 	 global-step:2527	 l-p:0.16073261201381683
epoch£º126	 i:8 	 global-step:2528	 l-p:0.09889575093984604
epoch£º126	 i:9 	 global-step:2529	 l-p:0.07422055304050446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[3.6856, 3.7678, 3.7297],
        [3.6856, 3.7419, 3.7098],
        [3.6856, 3.8241, 3.7859],
        [3.6856, 4.2606, 4.5118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.12556453049182892 
model_pd.l_d.mean(): -22.63360023498535 
model_pd.lagr.mean(): -22.50803565979004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0748], device='cuda:0')), ('power', tensor([-22.7084], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.12556453049182892
epoch£º127	 i:1 	 global-step:2541	 l-p:0.09732670336961746
epoch£º127	 i:2 	 global-step:2542	 l-p:0.11459897458553314
epoch£º127	 i:3 	 global-step:2543	 l-p:0.13197237253189087
epoch£º127	 i:4 	 global-step:2544	 l-p:0.1855555921792984
epoch£º127	 i:5 	 global-step:2545	 l-p:0.18726815283298492
epoch£º127	 i:6 	 global-step:2546	 l-p:0.12232799082994461
epoch£º127	 i:7 	 global-step:2547	 l-p:0.10044967383146286
epoch£º127	 i:8 	 global-step:2548	 l-p:0.08086511492729187
epoch£º127	 i:9 	 global-step:2549	 l-p:0.11593209952116013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9091, 4.2406, 4.2710],
        [3.9091, 4.0391, 3.9954],
        [3.9091, 3.9111, 3.9092],
        [3.9091, 3.9096, 3.9091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.08435700088739395 
model_pd.l_d.mean(): -22.580577850341797 
model_pd.lagr.mean(): -22.4962215423584 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0333], device='cuda:0')), ('power', tensor([-22.6138], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.08435700088739395
epoch£º128	 i:1 	 global-step:2561	 l-p:0.10139806568622589
epoch£º128	 i:2 	 global-step:2562	 l-p:0.11855580657720566
epoch£º128	 i:3 	 global-step:2563	 l-p:0.11153195798397064
epoch£º128	 i:4 	 global-step:2564	 l-p:0.11692559719085693
epoch£º128	 i:5 	 global-step:2565	 l-p:0.10788250714540482
epoch£º128	 i:6 	 global-step:2566	 l-p:0.18469204008579254
epoch£º128	 i:7 	 global-step:2567	 l-p:0.09899501502513885
epoch£º128	 i:8 	 global-step:2568	 l-p:0.030894936993718147
epoch£º128	 i:9 	 global-step:2569	 l-p:0.13931146264076233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7132, 3.7138, 3.7133],
        [3.7132, 3.7255, 3.7154],
        [3.7132, 3.7133, 3.7132],
        [3.7132, 3.8058, 3.7663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): -0.0408967025578022 
model_pd.l_d.mean(): -22.77964210510254 
model_pd.lagr.mean(): -22.820539474487305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0950], device='cuda:0')), ('power', tensor([-22.8746], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:-0.0408967025578022
epoch£º129	 i:1 	 global-step:2581	 l-p:0.08666162192821503
epoch£º129	 i:2 	 global-step:2582	 l-p:0.11724203079938889
epoch£º129	 i:3 	 global-step:2583	 l-p:1.0358762741088867
epoch£º129	 i:4 	 global-step:2584	 l-p:0.10915601253509521
epoch£º129	 i:5 	 global-step:2585	 l-p:0.11633244901895523
epoch£º129	 i:6 	 global-step:2586	 l-p:0.017436550930142403
epoch£º129	 i:7 	 global-step:2587	 l-p:0.11883971840143204
epoch£º129	 i:8 	 global-step:2588	 l-p:0.11244119703769684
epoch£º129	 i:9 	 global-step:2589	 l-p:0.24533851444721222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8005, 3.9015, 3.8601],
        [3.8005, 3.8005, 3.8005],
        [3.8005, 3.8005, 3.8005],
        [3.8005, 3.8012, 3.8005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.19658078253269196 
model_pd.l_d.mean(): -21.980432510375977 
model_pd.lagr.mean(): -21.783851623535156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0887], device='cuda:0')), ('power', tensor([-22.0692], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.19658078253269196
epoch£º130	 i:1 	 global-step:2601	 l-p:0.11594557762145996
epoch£º130	 i:2 	 global-step:2602	 l-p:0.11242285370826721
epoch£º130	 i:3 	 global-step:2603	 l-p:0.1212315708398819
epoch£º130	 i:4 	 global-step:2604	 l-p:-0.01911958120763302
epoch£º130	 i:5 	 global-step:2605	 l-p:0.20548023283481598
epoch£º130	 i:6 	 global-step:2606	 l-p:0.08400709927082062
epoch£º130	 i:7 	 global-step:2607	 l-p:0.12106113135814667
epoch£º130	 i:8 	 global-step:2608	 l-p:0.14949269592761993
epoch£º130	 i:9 	 global-step:2609	 l-p:0.12709489464759827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8239, 3.8239, 3.8239],
        [3.8239, 3.8791, 3.8466],
        [3.8239, 3.8239, 3.8239],
        [3.8239, 3.8239, 3.8239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.12351414561271667 
model_pd.l_d.mean(): -22.606332778930664 
model_pd.lagr.mean(): -22.482818603515625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0499], device='cuda:0')), ('power', tensor([-22.6562], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.12351414561271667
epoch£º131	 i:1 	 global-step:2621	 l-p:0.11487285047769547
epoch£º131	 i:2 	 global-step:2622	 l-p:0.12095630168914795
epoch£º131	 i:3 	 global-step:2623	 l-p:0.11248818039894104
epoch£º131	 i:4 	 global-step:2624	 l-p:0.07660407572984695
epoch£º131	 i:5 	 global-step:2625	 l-p:0.11769935488700867
epoch£º131	 i:6 	 global-step:2626	 l-p:0.04764733836054802
epoch£º131	 i:7 	 global-step:2627	 l-p:0.12723487615585327
epoch£º131	 i:8 	 global-step:2628	 l-p:0.15581080317497253
epoch£º131	 i:9 	 global-step:2629	 l-p:0.196638822555542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6364, 3.6364, 3.6364],
        [3.6364, 4.3667, 4.7903],
        [3.6364, 3.6364, 3.6364],
        [3.6364, 3.9182, 3.9377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.10925056040287018 
model_pd.l_d.mean(): -22.219863891601562 
model_pd.lagr.mean(): -22.110612869262695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1521], device='cuda:0')), ('power', tensor([-22.3719], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.10925056040287018
epoch£º132	 i:1 	 global-step:2641	 l-p:0.10596568137407303
epoch£º132	 i:2 	 global-step:2642	 l-p:0.12193708121776581
epoch£º132	 i:3 	 global-step:2643	 l-p:0.11759992688894272
epoch£º132	 i:4 	 global-step:2644	 l-p:0.30237728357315063
epoch£º132	 i:5 	 global-step:2645	 l-p:0.10993985086679459
epoch£º132	 i:6 	 global-step:2646	 l-p:0.12712635099887848
epoch£º132	 i:7 	 global-step:2647	 l-p:0.1149689182639122
epoch£º132	 i:8 	 global-step:2648	 l-p:0.10249551385641098
epoch£º132	 i:9 	 global-step:2649	 l-p:0.11873342096805573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7901, 3.7901, 3.7901],
        [3.7901, 3.7901, 3.7901],
        [3.7901, 4.0074, 3.9875],
        [3.7901, 3.8689, 3.8306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.11517423391342163 
model_pd.l_d.mean(): -21.85698699951172 
model_pd.lagr.mean(): -21.74181365966797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0891], device='cuda:0')), ('power', tensor([-21.9461], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.11517423391342163
epoch£º133	 i:1 	 global-step:2661	 l-p:0.11889560520648956
epoch£º133	 i:2 	 global-step:2662	 l-p:0.135586678981781
epoch£º133	 i:3 	 global-step:2663	 l-p:0.037265170365571976
epoch£º133	 i:4 	 global-step:2664	 l-p:0.10781630873680115
epoch£º133	 i:5 	 global-step:2665	 l-p:0.1320091336965561
epoch£º133	 i:6 	 global-step:2666	 l-p:0.09903810173273087
epoch£º133	 i:7 	 global-step:2667	 l-p:0.025383876636624336
epoch£º133	 i:8 	 global-step:2668	 l-p:0.12612546980381012
epoch£º133	 i:9 	 global-step:2669	 l-p:0.11930467188358307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7179, 3.9170, 3.8935],
        [3.7179, 3.7715, 3.7403],
        [3.7179, 3.7285, 3.7197],
        [3.7179, 3.7196, 3.7180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.12422709912061691 
model_pd.l_d.mean(): -23.06687355041504 
model_pd.lagr.mean(): -22.942646026611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0405], device='cuda:0')), ('power', tensor([-23.1073], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.12422709912061691
epoch£º134	 i:1 	 global-step:2681	 l-p:-0.0022809933871030807
epoch£º134	 i:2 	 global-step:2682	 l-p:0.10841155797243118
epoch£º134	 i:3 	 global-step:2683	 l-p:0.10324157774448395
epoch£º134	 i:4 	 global-step:2684	 l-p:0.14356669783592224
epoch£º134	 i:5 	 global-step:2685	 l-p:0.1366904228925705
epoch£º134	 i:6 	 global-step:2686	 l-p:0.13580594956874847
epoch£º134	 i:7 	 global-step:2687	 l-p:-0.054344601929187775
epoch£º134	 i:8 	 global-step:2688	 l-p:0.08558157831430435
epoch£º134	 i:9 	 global-step:2689	 l-p:0.11345294862985611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7779, 3.7779, 3.7779],
        [3.7779, 4.5688, 5.0384],
        [3.7779, 3.8660, 3.8266],
        [3.7779, 4.4328, 4.7552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.2239244282245636 
model_pd.l_d.mean(): -21.9544734954834 
model_pd.lagr.mean(): -21.730548858642578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1263], device='cuda:0')), ('power', tensor([-22.0808], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.2239244282245636
epoch£º135	 i:1 	 global-step:2701	 l-p:0.1796635240316391
epoch£º135	 i:2 	 global-step:2702	 l-p:0.12908753752708435
epoch£º135	 i:3 	 global-step:2703	 l-p:0.11725572496652603
epoch£º135	 i:4 	 global-step:2704	 l-p:0.09521886706352234
epoch£º135	 i:5 	 global-step:2705	 l-p:0.15037107467651367
epoch£º135	 i:6 	 global-step:2706	 l-p:0.263581782579422
epoch£º135	 i:7 	 global-step:2707	 l-p:0.06692812591791153
epoch£º135	 i:8 	 global-step:2708	 l-p:0.07404889166355133
epoch£º135	 i:9 	 global-step:2709	 l-p:0.10964673012495041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0020, 4.0021, 4.0020],
        [4.0020, 4.1169, 4.0722],
        [4.0020, 4.0026, 4.0020],
        [4.0020, 4.1400, 4.0956]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.11145622283220291 
model_pd.l_d.mean(): -23.39284896850586 
model_pd.lagr.mean(): -23.28139305114746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1180], device='cuda:0')), ('power', tensor([-23.2748], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.11145622283220291
epoch£º136	 i:1 	 global-step:2721	 l-p:0.11454284191131592
epoch£º136	 i:2 	 global-step:2722	 l-p:-2.9602437019348145
epoch£º136	 i:3 	 global-step:2723	 l-p:0.10186759382486343
epoch£º136	 i:4 	 global-step:2724	 l-p:0.12532764673233032
epoch£º136	 i:5 	 global-step:2725	 l-p:0.15193697810173035
epoch£º136	 i:6 	 global-step:2726	 l-p:0.13263414800167084
epoch£º136	 i:7 	 global-step:2727	 l-p:0.09925874322652817
epoch£º136	 i:8 	 global-step:2728	 l-p:0.11528624594211578
epoch£º136	 i:9 	 global-step:2729	 l-p:0.10131192952394485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8178, 3.8178, 3.8178],
        [3.8178, 3.8298, 3.8199],
        [3.8178, 3.8178, 3.8178],
        [3.8178, 4.5443, 4.9369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.12423636764287949 
model_pd.l_d.mean(): -23.24881362915039 
model_pd.lagr.mean(): -23.124576568603516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0173], device='cuda:0')), ('power', tensor([-23.2315], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.12423636764287949
epoch£º137	 i:1 	 global-step:2741	 l-p:0.11522910743951797
epoch£º137	 i:2 	 global-step:2742	 l-p:-0.05734526365995407
epoch£º137	 i:3 	 global-step:2743	 l-p:0.07944653183221817
epoch£º137	 i:4 	 global-step:2744	 l-p:0.145072340965271
epoch£º137	 i:5 	 global-step:2745	 l-p:0.11004891991615295
epoch£º137	 i:6 	 global-step:2746	 l-p:0.1199750304222107
epoch£º137	 i:7 	 global-step:2747	 l-p:0.14605925977230072
epoch£º137	 i:8 	 global-step:2748	 l-p:0.12054380029439926
epoch£º137	 i:9 	 global-step:2749	 l-p:0.16871735453605652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6769, 3.7239, 3.6954],
        [3.6769, 3.6769, 3.6769],
        [3.6769, 3.6769, 3.6769],
        [3.6769, 3.6769, 3.6769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.11404921859502792 
model_pd.l_d.mean(): -23.290075302124023 
model_pd.lagr.mean(): -23.176025390625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0261], device='cuda:0')), ('power', tensor([-23.3162], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.11404921859502792
epoch£º138	 i:1 	 global-step:2761	 l-p:0.1048932746052742
epoch£º138	 i:2 	 global-step:2762	 l-p:0.09690336883068085
epoch£º138	 i:3 	 global-step:2763	 l-p:0.08012370765209198
epoch£º138	 i:4 	 global-step:2764	 l-p:0.078409843146801
epoch£º138	 i:5 	 global-step:2765	 l-p:0.10887868702411652
epoch£º138	 i:6 	 global-step:2766	 l-p:0.1204686313867569
epoch£º138	 i:7 	 global-step:2767	 l-p:0.25499582290649414
epoch£º138	 i:8 	 global-step:2768	 l-p:0.10419052094221115
epoch£º138	 i:9 	 global-step:2769	 l-p:0.13490310311317444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8301, 4.5571, 4.9492],
        [3.8301, 4.6964, 5.2450],
        [3.8301, 3.8301, 3.8301],
        [3.8301, 3.8317, 3.8302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.09234922379255295 
model_pd.l_d.mean(): -21.94554328918457 
model_pd.lagr.mean(): -21.853193283081055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0853], device='cuda:0')), ('power', tensor([-22.0309], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.09234922379255295
epoch£º139	 i:1 	 global-step:2781	 l-p:0.11624757200479507
epoch£º139	 i:2 	 global-step:2782	 l-p:0.10699854791164398
epoch£º139	 i:3 	 global-step:2783	 l-p:0.19994640350341797
epoch£º139	 i:4 	 global-step:2784	 l-p:0.1265302300453186
epoch£º139	 i:5 	 global-step:2785	 l-p:0.13256067037582397
epoch£º139	 i:6 	 global-step:2786	 l-p:0.11590434610843658
epoch£º139	 i:7 	 global-step:2787	 l-p:6.727773666381836
epoch£º139	 i:8 	 global-step:2788	 l-p:0.11248769611120224
epoch£º139	 i:9 	 global-step:2789	 l-p:-0.11185649782419205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7336, 4.4703, 4.8885],
        [3.7336, 3.7576, 3.7399],
        [3.7336, 4.1603, 4.2771],
        [3.7336, 4.4842, 4.9176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.1294815093278885 
model_pd.l_d.mean(): -22.865020751953125 
model_pd.lagr.mean(): -22.735538482666016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0676], device='cuda:0')), ('power', tensor([-22.9326], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.1294815093278885
epoch£º140	 i:1 	 global-step:2801	 l-p:0.10479560494422913
epoch£º140	 i:2 	 global-step:2802	 l-p:0.0636667013168335
epoch£º140	 i:3 	 global-step:2803	 l-p:0.1610463261604309
epoch£º140	 i:4 	 global-step:2804	 l-p:0.12428665161132812
epoch£º140	 i:5 	 global-step:2805	 l-p:0.07672753185033798
epoch£º140	 i:6 	 global-step:2806	 l-p:0.10744205862283707
epoch£º140	 i:7 	 global-step:2807	 l-p:0.1476714164018631
epoch£º140	 i:8 	 global-step:2808	 l-p:-0.023729009553790092
epoch£º140	 i:9 	 global-step:2809	 l-p:0.11805076152086258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8121, 3.8121, 3.8121],
        [3.8121, 3.8121, 3.8121],
        [3.8121, 3.8121, 3.8121],
        [3.8121, 3.9029, 3.8630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.10584291070699692 
model_pd.l_d.mean(): -22.915491104125977 
model_pd.lagr.mean(): -22.809648513793945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0257], device='cuda:0')), ('power', tensor([-22.9412], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.10584291070699692
epoch£º141	 i:1 	 global-step:2821	 l-p:0.19525687396526337
epoch£º141	 i:2 	 global-step:2822	 l-p:0.11953779309988022
epoch£º141	 i:3 	 global-step:2823	 l-p:0.1498374044895172
epoch£º141	 i:4 	 global-step:2824	 l-p:0.12641441822052002
epoch£º141	 i:5 	 global-step:2825	 l-p:0.1010752022266388
epoch£º141	 i:6 	 global-step:2826	 l-p:0.03750094771385193
epoch£º141	 i:7 	 global-step:2827	 l-p:0.12798704206943512
epoch£º141	 i:8 	 global-step:2828	 l-p:0.18188758194446564
epoch£º141	 i:9 	 global-step:2829	 l-p:0.10373445600271225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9342, 4.0149, 3.9753],
        [3.9342, 4.0709, 4.0284],
        [3.9342, 4.0569, 4.0137],
        [3.9342, 3.9346, 3.9342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.12004661560058594 
model_pd.l_d.mean(): -22.924909591674805 
model_pd.lagr.mean(): -22.80486297607422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0200], device='cuda:0')), ('power', tensor([-22.9049], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.12004661560058594
epoch£º142	 i:1 	 global-step:2841	 l-p:0.11398572474718094
epoch£º142	 i:2 	 global-step:2842	 l-p:0.15339942276477814
epoch£º142	 i:3 	 global-step:2843	 l-p:0.10891063511371613
epoch£º142	 i:4 	 global-step:2844	 l-p:0.10401024669408798
epoch£º142	 i:5 	 global-step:2845	 l-p:0.0757816880941391
epoch£º142	 i:6 	 global-step:2846	 l-p:0.17721742391586304
epoch£º142	 i:7 	 global-step:2847	 l-p:0.09370170533657074
epoch£º142	 i:8 	 global-step:2848	 l-p:0.09749967604875565
epoch£º142	 i:9 	 global-step:2849	 l-p:0.12318553775548935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9450, 3.9450, 3.9450],
        [3.9450, 3.9450, 3.9450],
        [3.9450, 3.9450, 3.9450],
        [3.9450, 4.4989, 4.7045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.11432162672281265 
model_pd.l_d.mean(): -22.102548599243164 
model_pd.lagr.mean(): -21.98822784423828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0365], device='cuda:0')), ('power', tensor([-22.1391], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.11432162672281265
epoch£º143	 i:1 	 global-step:2861	 l-p:0.09727595746517181
epoch£º143	 i:2 	 global-step:2862	 l-p:0.04844660684466362
epoch£º143	 i:3 	 global-step:2863	 l-p:0.11358404159545898
epoch£º143	 i:4 	 global-step:2864	 l-p:0.12727180123329163
epoch£º143	 i:5 	 global-step:2865	 l-p:0.09466022253036499
epoch£º143	 i:6 	 global-step:2866	 l-p:0.13903376460075378
epoch£º143	 i:7 	 global-step:2867	 l-p:0.1141882985830307
epoch£º143	 i:8 	 global-step:2868	 l-p:-0.30739885568618774
epoch£º143	 i:9 	 global-step:2869	 l-p:0.12088901549577713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7719, 4.6528, 5.2311],
        [3.7719, 4.1888, 4.2953],
        [3.7719, 3.8419, 3.8061],
        [3.7719, 3.7761, 3.7723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): -0.14208149909973145 
model_pd.l_d.mean(): -22.522380828857422 
model_pd.lagr.mean(): -22.66446304321289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0444], device='cuda:0')), ('power', tensor([-22.5667], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:-0.14208149909973145
epoch£º144	 i:1 	 global-step:2881	 l-p:0.10889133810997009
epoch£º144	 i:2 	 global-step:2882	 l-p:0.13196903467178345
epoch£º144	 i:3 	 global-step:2883	 l-p:0.11894652992486954
epoch£º144	 i:4 	 global-step:2884	 l-p:0.11047780513763428
epoch£º144	 i:5 	 global-step:2885	 l-p:0.08718706667423248
epoch£º144	 i:6 	 global-step:2886	 l-p:0.1370423138141632
epoch£º144	 i:7 	 global-step:2887	 l-p:0.10290363430976868
epoch£º144	 i:8 	 global-step:2888	 l-p:0.0793113112449646
epoch£º144	 i:9 	 global-step:2889	 l-p:0.1627461165189743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6785, 4.1503, 4.3130],
        [3.6785, 4.2144, 4.4357],
        [3.6785, 3.6840, 3.6791],
        [3.6785, 4.4000, 4.8123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.1437719762325287 
model_pd.l_d.mean(): -23.176246643066406 
model_pd.lagr.mean(): -23.032474517822266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0511], device='cuda:0')), ('power', tensor([-23.2273], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.1437719762325287
epoch£º145	 i:1 	 global-step:2901	 l-p:0.15171656012535095
epoch£º145	 i:2 	 global-step:2902	 l-p:0.14188627898693085
epoch£º145	 i:3 	 global-step:2903	 l-p:0.35458531975746155
epoch£º145	 i:4 	 global-step:2904	 l-p:0.11554229259490967
epoch£º145	 i:5 	 global-step:2905	 l-p:0.10002335160970688
epoch£º145	 i:6 	 global-step:2906	 l-p:0.14043928682804108
epoch£º145	 i:7 	 global-step:2907	 l-p:0.22587698698043823
epoch£º145	 i:8 	 global-step:2908	 l-p:0.13481377065181732
epoch£º145	 i:9 	 global-step:2909	 l-p:0.11350911110639572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9516, 3.9516, 3.9516],
        [3.9516, 3.9773, 3.9583],
        [3.9516, 4.1515, 4.1214],
        [3.9516, 3.9997, 3.9697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.11097691208124161 
model_pd.l_d.mean(): -22.54132080078125 
model_pd.lagr.mean(): -22.430343627929688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0028], device='cuda:0')), ('power', tensor([-22.5385], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.11097691208124161
epoch£º146	 i:1 	 global-step:2921	 l-p:0.0991816520690918
epoch£º146	 i:2 	 global-step:2922	 l-p:0.10645414888858795
epoch£º146	 i:3 	 global-step:2923	 l-p:0.14844393730163574
epoch£º146	 i:4 	 global-step:2924	 l-p:0.13820402324199677
epoch£º146	 i:5 	 global-step:2925	 l-p:0.19204197824001312
epoch£º146	 i:6 	 global-step:2926	 l-p:0.11925306171178818
epoch£º146	 i:7 	 global-step:2927	 l-p:0.24985812604427338
epoch£º146	 i:8 	 global-step:2928	 l-p:0.051504988223314285
epoch£º146	 i:9 	 global-step:2929	 l-p:0.1061924621462822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8042, 3.9327, 3.8927],
        [3.8042, 3.8042, 3.8042],
        [3.8042, 3.8380, 3.8149],
        [3.8042, 3.9929, 3.9647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.1992952823638916 
model_pd.l_d.mean(): -23.33035659790039 
model_pd.lagr.mean(): -23.131061553955078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0275], device='cuda:0')), ('power', tensor([-23.3028], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.1992952823638916
epoch£º147	 i:1 	 global-step:2941	 l-p:0.09400712698698044
epoch£º147	 i:2 	 global-step:2942	 l-p:-0.26332196593284607
epoch£º147	 i:3 	 global-step:2943	 l-p:0.10950460284948349
epoch£º147	 i:4 	 global-step:2944	 l-p:0.11242564022541046
epoch£º147	 i:5 	 global-step:2945	 l-p:0.1454959660768509
epoch£º147	 i:6 	 global-step:2946	 l-p:0.09891292452812195
epoch£º147	 i:7 	 global-step:2947	 l-p:0.1257941573858261
epoch£º147	 i:8 	 global-step:2948	 l-p:0.10302901268005371
epoch£º147	 i:9 	 global-step:2949	 l-p:0.09814777970314026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9381, 3.9543, 3.9413],
        [3.9381, 3.9456, 3.9390],
        [3.9381, 4.7628, 5.2487],
        [3.9381, 3.9381, 3.9381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.11226879060268402 
model_pd.l_d.mean(): -23.131603240966797 
model_pd.lagr.mean(): -23.01933479309082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0496], device='cuda:0')), ('power', tensor([-23.0820], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.11226879060268402
epoch£º148	 i:1 	 global-step:2961	 l-p:0.13013583421707153
epoch£º148	 i:2 	 global-step:2962	 l-p:0.2244853973388672
epoch£º148	 i:3 	 global-step:2963	 l-p:0.07408683747053146
epoch£º148	 i:4 	 global-step:2964	 l-p:0.1508835107088089
epoch£º148	 i:5 	 global-step:2965	 l-p:0.10181824117898941
epoch£º148	 i:6 	 global-step:2966	 l-p:0.11358983814716339
epoch£º148	 i:7 	 global-step:2967	 l-p:0.1117996871471405
epoch£º148	 i:8 	 global-step:2968	 l-p:0.2044515758752823
epoch£º148	 i:9 	 global-step:2969	 l-p:0.0922970175743103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8325, 3.8800, 3.8509],
        [3.8325, 3.8553, 3.8383],
        [3.8325, 4.6089, 5.0582],
        [3.8325, 4.1736, 4.2211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.10544758290052414 
model_pd.l_d.mean(): -21.959224700927734 
model_pd.lagr.mean(): -21.853776931762695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1106], device='cuda:0')), ('power', tensor([-22.0698], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.10544758290052414
epoch£º149	 i:1 	 global-step:2981	 l-p:0.16608431935310364
epoch£º149	 i:2 	 global-step:2982	 l-p:0.11883809417486191
epoch£º149	 i:3 	 global-step:2983	 l-p:0.11137567460536957
epoch£º149	 i:4 	 global-step:2984	 l-p:0.12945111095905304
epoch£º149	 i:5 	 global-step:2985	 l-p:0.11487653106451035
epoch£º149	 i:6 	 global-step:2986	 l-p:0.09554820507764816
epoch£º149	 i:7 	 global-step:2987	 l-p:0.07593131065368652
epoch£º149	 i:8 	 global-step:2988	 l-p:-0.03484879434108734
epoch£º149	 i:9 	 global-step:2989	 l-p:0.10860934853553772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7497, 3.9602, 3.9422],
        [3.7497, 4.0504, 4.0781],
        [3.7497, 3.9557, 3.9360],
        [3.7497, 3.7510, 3.7497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.11322139203548431 
model_pd.l_d.mean(): -21.915742874145508 
model_pd.lagr.mean(): -21.802520751953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1132], device='cuda:0')), ('power', tensor([-22.0289], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.11322139203548431
epoch£º150	 i:1 	 global-step:3001	 l-p:0.2980736196041107
epoch£º150	 i:2 	 global-step:3002	 l-p:0.1256849765777588
epoch£º150	 i:3 	 global-step:3003	 l-p:-0.05536656081676483
epoch£º150	 i:4 	 global-step:3004	 l-p:0.16215039789676666
epoch£º150	 i:5 	 global-step:3005	 l-p:0.13243259489536285
epoch£º150	 i:6 	 global-step:3006	 l-p:0.09261356294155121
epoch£º150	 i:7 	 global-step:3007	 l-p:0.1243085265159607
epoch£º150	 i:8 	 global-step:3008	 l-p:-0.08224120736122131
epoch£º150	 i:9 	 global-step:3009	 l-p:0.11651172488927841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9992, 4.2800, 4.2820],
        [3.9992, 3.9992, 3.9992],
        [3.9992, 4.6086, 4.8627],
        [3.9992, 4.3735, 4.4318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.11541704088449478 
model_pd.l_d.mean(): -23.24679183959961 
model_pd.lagr.mean(): -23.13137435913086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0894], device='cuda:0')), ('power', tensor([-23.1574], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.11541704088449478
epoch£º151	 i:1 	 global-step:3021	 l-p:0.10213275998830795
epoch£º151	 i:2 	 global-step:3022	 l-p:0.13171769678592682
epoch£º151	 i:3 	 global-step:3023	 l-p:0.12426581233739853
epoch£º151	 i:4 	 global-step:3024	 l-p:0.11372801661491394
epoch£º151	 i:5 	 global-step:3025	 l-p:0.11621016263961792
epoch£º151	 i:6 	 global-step:3026	 l-p:0.0761387050151825
epoch£º151	 i:7 	 global-step:3027	 l-p:0.1069565936923027
epoch£º151	 i:8 	 global-step:3028	 l-p:-0.15561097860336304
epoch£º151	 i:9 	 global-step:3029	 l-p:0.12227082997560501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[3.5620, 3.6450, 3.6094],
        [3.5620, 4.1690, 4.4791],
        [3.5620, 3.6702, 3.6340],
        [3.5620, 3.6670, 3.6307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.07911480963230133 
model_pd.l_d.mean(): -22.72870635986328 
model_pd.lagr.mean(): -22.64959144592285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1678], device='cuda:0')), ('power', tensor([-22.8965], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.07911480963230133
epoch£º152	 i:1 	 global-step:3041	 l-p:0.1481994092464447
epoch£º152	 i:2 	 global-step:3042	 l-p:0.11854219436645508
epoch£º152	 i:3 	 global-step:3043	 l-p:0.10379359126091003
epoch£º152	 i:4 	 global-step:3044	 l-p:0.16069254279136658
epoch£º152	 i:5 	 global-step:3045	 l-p:-0.01929342746734619
epoch£º152	 i:6 	 global-step:3046	 l-p:0.17747417092323303
epoch£º152	 i:7 	 global-step:3047	 l-p:0.12106364220380783
epoch£º152	 i:8 	 global-step:3048	 l-p:0.13329119980335236
epoch£º152	 i:9 	 global-step:3049	 l-p:0.10287602990865707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8053, 3.8080, 3.8055],
        [3.8053, 3.8053, 3.8053],
        [3.8053, 3.8126, 3.8063],
        [3.8053, 3.8053, 3.8053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.11762876063585281 
model_pd.l_d.mean(): -22.406906127929688 
model_pd.lagr.mean(): -22.289278030395508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0318], device='cuda:0')), ('power', tensor([-22.4387], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.11762876063585281
epoch£º153	 i:1 	 global-step:3061	 l-p:0.09393484145402908
epoch£º153	 i:2 	 global-step:3062	 l-p:0.13407999277114868
epoch£º153	 i:3 	 global-step:3063	 l-p:0.09564057737588882
epoch£º153	 i:4 	 global-step:3064	 l-p:0.1380721777677536
epoch£º153	 i:5 	 global-step:3065	 l-p:-0.03372080624103546
epoch£º153	 i:6 	 global-step:3066	 l-p:-0.17545337975025177
epoch£º153	 i:7 	 global-step:3067	 l-p:0.11048655956983566
epoch£º153	 i:8 	 global-step:3068	 l-p:0.10520239919424057
epoch£º153	 i:9 	 global-step:3069	 l-p:-0.29630160331726074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7652, 4.2002, 4.3253],
        [3.7652, 3.8688, 3.8293],
        [3.7652, 3.7821, 3.7689],
        [3.7652, 3.7683, 3.7655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 1.007957935333252 
model_pd.l_d.mean(): -21.970346450805664 
model_pd.lagr.mean(): -20.96238899230957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1127], device='cuda:0')), ('power', tensor([-22.0831], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:1.007957935333252
epoch£º154	 i:1 	 global-step:3081	 l-p:0.10762514919042587
epoch£º154	 i:2 	 global-step:3082	 l-p:0.0985892117023468
epoch£º154	 i:3 	 global-step:3083	 l-p:0.12217077612876892
epoch£º154	 i:4 	 global-step:3084	 l-p:0.19672444462776184
epoch£º154	 i:5 	 global-step:3085	 l-p:0.14927899837493896
epoch£º154	 i:6 	 global-step:3086	 l-p:0.121507428586483
epoch£º154	 i:7 	 global-step:3087	 l-p:0.10067756474018097
epoch£º154	 i:8 	 global-step:3088	 l-p:0.10851819813251495
epoch£º154	 i:9 	 global-step:3089	 l-p:0.1315619945526123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8444, 4.2991, 4.4330],
        [3.8444, 4.2362, 4.3203],
        [3.8444, 3.8744, 3.8533],
        [3.8444, 4.0728, 4.0588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.11030981689691544 
model_pd.l_d.mean(): -22.784753799438477 
model_pd.lagr.mean(): -22.6744441986084 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0270], device='cuda:0')), ('power', tensor([-22.8117], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.11030981689691544
epoch£º155	 i:1 	 global-step:3101	 l-p:0.12726178765296936
epoch£º155	 i:2 	 global-step:3102	 l-p:0.13985949754714966
epoch£º155	 i:3 	 global-step:3103	 l-p:0.11108142882585526
epoch£º155	 i:4 	 global-step:3104	 l-p:-0.07116162776947021
epoch£º155	 i:5 	 global-step:3105	 l-p:0.1298997700214386
epoch£º155	 i:6 	 global-step:3106	 l-p:0.10661210864782333
epoch£º155	 i:7 	 global-step:3107	 l-p:0.12603646516799927
epoch£º155	 i:8 	 global-step:3108	 l-p:0.1332111358642578
epoch£º155	 i:9 	 global-step:3109	 l-p:0.097691111266613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[3.3543, 4.0036, 4.3915],
        [3.3543, 4.0178, 4.4214],
        [3.3543, 3.4343, 3.4016],
        [3.3543, 4.0016, 4.3872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.10982201993465424 
model_pd.l_d.mean(): -22.84318733215332 
model_pd.lagr.mean(): -22.733366012573242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2195], device='cuda:0')), ('power', tensor([-23.0627], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.10982201993465424
epoch£º156	 i:1 	 global-step:3121	 l-p:0.11296331882476807
epoch£º156	 i:2 	 global-step:3122	 l-p:0.10983742773532867
epoch£º156	 i:3 	 global-step:3123	 l-p:0.08589506894350052
epoch£º156	 i:4 	 global-step:3124	 l-p:0.16191062331199646
epoch£º156	 i:5 	 global-step:3125	 l-p:0.10698167234659195
epoch£º156	 i:6 	 global-step:3126	 l-p:0.12700894474983215
epoch£º156	 i:7 	 global-step:3127	 l-p:0.12341879308223724
epoch£º156	 i:8 	 global-step:3128	 l-p:0.30907490849494934
epoch£º156	 i:9 	 global-step:3129	 l-p:0.1142096295952797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[3.7041, 3.7457, 3.7196],
        [3.7041, 4.0384, 4.0932],
        [3.7041, 4.3089, 4.5986],
        [3.7041, 3.8590, 3.8264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.06034930422902107 
model_pd.l_d.mean(): -22.93255615234375 
model_pd.lagr.mean(): -22.872207641601562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0804], device='cuda:0')), ('power', tensor([-23.0129], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.06034930422902107
epoch£º157	 i:1 	 global-step:3141	 l-p:0.12461289763450623
epoch£º157	 i:2 	 global-step:3142	 l-p:0.13732266426086426
epoch£º157	 i:3 	 global-step:3143	 l-p:0.03971032425761223
epoch£º157	 i:4 	 global-step:3144	 l-p:0.11659707874059677
epoch£º157	 i:5 	 global-step:3145	 l-p:0.09752868115901947
epoch£º157	 i:6 	 global-step:3146	 l-p:0.09555169939994812
epoch£º157	 i:7 	 global-step:3147	 l-p:0.12402209639549255
epoch£º157	 i:8 	 global-step:3148	 l-p:0.14710453152656555
epoch£º157	 i:9 	 global-step:3149	 l-p:0.161495640873909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8995, 3.8996, 3.8995],
        [3.8995, 3.9059, 3.9003],
        [3.8995, 3.8996, 3.8995],
        [3.8995, 4.2062, 4.2299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.11528237909078598 
model_pd.l_d.mean(): -22.52440071105957 
model_pd.lagr.mean(): -22.40911865234375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0020], device='cuda:0')), ('power', tensor([-22.5264], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.11528237909078598
epoch£º158	 i:1 	 global-step:3161	 l-p:0.13959535956382751
epoch£º158	 i:2 	 global-step:3162	 l-p:0.08495277911424637
epoch£º158	 i:3 	 global-step:3163	 l-p:0.30159276723861694
epoch£º158	 i:4 	 global-step:3164	 l-p:0.13484647870063782
epoch£º158	 i:5 	 global-step:3165	 l-p:0.1284642219543457
epoch£º158	 i:6 	 global-step:3166	 l-p:0.10592100024223328
epoch£º158	 i:7 	 global-step:3167	 l-p:0.11810088902711868
epoch£º158	 i:8 	 global-step:3168	 l-p:0.11334861814975739
epoch£º158	 i:9 	 global-step:3169	 l-p:0.11283455789089203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8661, 3.9728, 3.9320],
        [3.8661, 3.9131, 3.8842],
        [3.8661, 3.9463, 3.9081],
        [3.8661, 3.8921, 3.8732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.1334565132856369 
model_pd.l_d.mean(): -23.10110855102539 
model_pd.lagr.mean(): -22.9676513671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0070], device='cuda:0')), ('power', tensor([-23.0941], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.1334565132856369
epoch£º159	 i:1 	 global-step:3181	 l-p:0.14664340019226074
epoch£º159	 i:2 	 global-step:3182	 l-p:0.07785046845674515
epoch£º159	 i:3 	 global-step:3183	 l-p:0.1236807331442833
epoch£º159	 i:4 	 global-step:3184	 l-p:0.09309855848550797
epoch£º159	 i:5 	 global-step:3185	 l-p:0.09826009720563889
epoch£º159	 i:6 	 global-step:3186	 l-p:0.11253765225410461
epoch£º159	 i:7 	 global-step:3187	 l-p:0.11750964820384979
epoch£º159	 i:8 	 global-step:3188	 l-p:0.12046021968126297
epoch£º159	 i:9 	 global-step:3189	 l-p:0.2242349237203598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5166, 3.5494, 3.5277],
        [3.5166, 3.9575, 4.1157],
        [3.5166, 3.5166, 3.5166],
        [3.5166, 3.5169, 3.5166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.13000941276550293 
model_pd.l_d.mean(): -23.493432998657227 
model_pd.lagr.mean(): -23.36342430114746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0667], device='cuda:0')), ('power', tensor([-23.5601], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.13000941276550293
epoch£º160	 i:1 	 global-step:3201	 l-p:0.08216386288404465
epoch£º160	 i:2 	 global-step:3202	 l-p:0.0935000479221344
epoch£º160	 i:3 	 global-step:3203	 l-p:0.1196303516626358
epoch£º160	 i:4 	 global-step:3204	 l-p:0.09661316871643066
epoch£º160	 i:5 	 global-step:3205	 l-p:0.12004407495260239
epoch£º160	 i:6 	 global-step:3206	 l-p:0.07169680297374725
epoch£º160	 i:7 	 global-step:3207	 l-p:0.12681077420711517
epoch£º160	 i:8 	 global-step:3208	 l-p:0.11795282363891602
epoch£º160	 i:9 	 global-step:3209	 l-p:0.1609026938676834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[3.7328, 3.8253, 3.7872],
        [3.7328, 3.7767, 3.7496],
        [3.7328, 4.1346, 4.2381],
        [3.7328, 4.1866, 4.3329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.12127990275621414 
model_pd.l_d.mean(): -23.43933868408203 
model_pd.lagr.mean(): -23.318058013916016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0109], device='cuda:0')), ('power', tensor([-23.4285], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.12127990275621414
epoch£º161	 i:1 	 global-step:3221	 l-p:0.003697702893987298
epoch£º161	 i:2 	 global-step:3222	 l-p:0.11540502309799194
epoch£º161	 i:3 	 global-step:3223	 l-p:0.12325576692819595
epoch£º161	 i:4 	 global-step:3224	 l-p:0.11637625098228455
epoch£º161	 i:5 	 global-step:3225	 l-p:0.1218983456492424
epoch£º161	 i:6 	 global-step:3226	 l-p:-0.5482574105262756
epoch£º161	 i:7 	 global-step:3227	 l-p:0.2581649720668793
epoch£º161	 i:8 	 global-step:3228	 l-p:0.08205138891935349
epoch£º161	 i:9 	 global-step:3229	 l-p:0.11655686795711517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8879, 4.2789, 4.3612],
        [3.8879, 3.8884, 3.8880],
        [3.8879, 3.9560, 3.9203],
        [3.8879, 4.3730, 4.5309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.10479357838630676 
model_pd.l_d.mean(): -22.968969345092773 
model_pd.lagr.mean(): -22.86417579650879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0045], device='cuda:0')), ('power', tensor([-22.9735], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.10479357838630676
epoch£º162	 i:1 	 global-step:3241	 l-p:0.1457482874393463
epoch£º162	 i:2 	 global-step:3242	 l-p:0.08825687319040298
epoch£º162	 i:3 	 global-step:3243	 l-p:0.11347445845603943
epoch£º162	 i:4 	 global-step:3244	 l-p:0.12558616697788239
epoch£º162	 i:5 	 global-step:3245	 l-p:0.2564452588558197
epoch£º162	 i:6 	 global-step:3246	 l-p:0.10283973813056946
epoch£º162	 i:7 	 global-step:3247	 l-p:0.12034379690885544
epoch£º162	 i:8 	 global-step:3248	 l-p:0.11627011746168137
epoch£º162	 i:9 	 global-step:3249	 l-p:0.26524466276168823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7809, 4.3967, 4.6892],
        [3.7809, 3.7809, 3.7809],
        [3.7809, 4.0044, 3.9926],
        [3.7809, 3.7809, 3.7809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): -0.01995777152478695 
model_pd.l_d.mean(): -21.816936492919922 
model_pd.lagr.mean(): -21.836894989013672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1403], device='cuda:0')), ('power', tensor([-21.9572], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:-0.01995777152478695
epoch£º163	 i:1 	 global-step:3261	 l-p:0.12030943483114243
epoch£º163	 i:2 	 global-step:3262	 l-p:0.3475494980812073
epoch£º163	 i:3 	 global-step:3263	 l-p:0.1884571760892868
epoch£º163	 i:4 	 global-step:3264	 l-p:0.11274883151054382
epoch£º163	 i:5 	 global-step:3265	 l-p:0.12239500135183334
epoch£º163	 i:6 	 global-step:3266	 l-p:0.1193288117647171
epoch£º163	 i:7 	 global-step:3267	 l-p:0.11124131083488464
epoch£º163	 i:8 	 global-step:3268	 l-p:0.10147164016962051
epoch£º163	 i:9 	 global-step:3269	 l-p:0.11584722250699997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7226, 3.8729, 3.8394],
        [3.7226, 3.7253, 3.7228],
        [3.7226, 3.7232, 3.7226],
        [3.7226, 3.7377, 3.7258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.11624360084533691 
model_pd.l_d.mean(): -22.94367027282715 
model_pd.lagr.mean(): -22.82742691040039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0596], device='cuda:0')), ('power', tensor([-23.0032], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.11624360084533691
epoch£º164	 i:1 	 global-step:3281	 l-p:0.10831265896558762
epoch£º164	 i:2 	 global-step:3282	 l-p:0.27228277921676636
epoch£º164	 i:3 	 global-step:3283	 l-p:0.12534385919570923
epoch£º164	 i:4 	 global-step:3284	 l-p:0.14773045480251312
epoch£º164	 i:5 	 global-step:3285	 l-p:0.12033997476100922
epoch£º164	 i:6 	 global-step:3286	 l-p:0.12113156914710999
epoch£º164	 i:7 	 global-step:3287	 l-p:0.19013778865337372
epoch£º164	 i:8 	 global-step:3288	 l-p:0.12211337685585022
epoch£º164	 i:9 	 global-step:3289	 l-p:0.08999812602996826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5756, 3.5756, 3.5756],
        [3.5756, 4.0593, 4.2515],
        [3.5756, 3.9117, 3.9805],
        [3.5756, 3.6903, 3.6550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): -0.07018300890922546 
model_pd.l_d.mean(): -22.990455627441406 
model_pd.lagr.mean(): -23.060638427734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1128], device='cuda:0')), ('power', tensor([-23.1032], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:-0.07018300890922546
epoch£º165	 i:1 	 global-step:3301	 l-p:0.10845106095075607
epoch£º165	 i:2 	 global-step:3302	 l-p:0.10640939325094223
epoch£º165	 i:3 	 global-step:3303	 l-p:0.14978575706481934
epoch£º165	 i:4 	 global-step:3304	 l-p:0.07943512499332428
epoch£º165	 i:5 	 global-step:3305	 l-p:0.12625636160373688
epoch£º165	 i:6 	 global-step:3306	 l-p:0.12639355659484863
epoch£º165	 i:7 	 global-step:3307	 l-p:0.10203305631875992
epoch£º165	 i:8 	 global-step:3308	 l-p:0.09760622680187225
epoch£º165	 i:9 	 global-step:3309	 l-p:0.13122683763504028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6370, 3.6380, 3.6371],
        [3.6370, 3.6810, 3.6542],
        [3.6370, 3.6872, 3.6581],
        [3.6370, 3.6371, 3.6370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.09958618879318237 
model_pd.l_d.mean(): -22.1273193359375 
model_pd.lagr.mean(): -22.027732849121094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1516], device='cuda:0')), ('power', tensor([-22.2789], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.09958618879318237
epoch£º166	 i:1 	 global-step:3321	 l-p:0.2888984680175781
epoch£º166	 i:2 	 global-step:3322	 l-p:0.12181099504232407
epoch£º166	 i:3 	 global-step:3323	 l-p:0.10891912132501602
epoch£º166	 i:4 	 global-step:3324	 l-p:0.11046458780765533
epoch£º166	 i:5 	 global-step:3325	 l-p:0.1250143200159073
epoch£º166	 i:6 	 global-step:3326	 l-p:0.11931270360946655
epoch£º166	 i:7 	 global-step:3327	 l-p:0.08130473643541336
epoch£º166	 i:8 	 global-step:3328	 l-p:0.12783312797546387
epoch£º166	 i:9 	 global-step:3329	 l-p:0.10032743960618973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7200, 3.7200, 3.7200],
        [3.7200, 3.8204, 3.7823],
        [3.7200, 4.5314, 5.0456],
        [3.7200, 3.7275, 3.7211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.1379385143518448 
model_pd.l_d.mean(): -22.46410369873047 
model_pd.lagr.mean(): -22.32616424560547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0940], device='cuda:0')), ('power', tensor([-22.5581], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.1379385143518448
epoch£º167	 i:1 	 global-step:3341	 l-p:0.10435298085212708
epoch£º167	 i:2 	 global-step:3342	 l-p:0.09195778518915176
epoch£º167	 i:3 	 global-step:3343	 l-p:0.11200597882270813
epoch£º167	 i:4 	 global-step:3344	 l-p:0.08838464319705963
epoch£º167	 i:5 	 global-step:3345	 l-p:0.122810497879982
epoch£º167	 i:6 	 global-step:3346	 l-p:0.1389217972755432
epoch£º167	 i:7 	 global-step:3347	 l-p:0.11592692881822586
epoch£º167	 i:8 	 global-step:3348	 l-p:0.17434293031692505
epoch£º167	 i:9 	 global-step:3349	 l-p:0.14578650891780853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8612, 3.8617, 3.8612],
        [3.8612, 4.6419, 5.0988],
        [3.8612, 3.8612, 3.8612],
        [3.8612, 3.8612, 3.8612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): -0.059512753039598465 
model_pd.l_d.mean(): -21.90869903564453 
model_pd.lagr.mean(): -21.968212127685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0883], device='cuda:0')), ('power', tensor([-21.9970], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:-0.059512753039598465
epoch£º168	 i:1 	 global-step:3361	 l-p:0.11431612819433212
epoch£º168	 i:2 	 global-step:3362	 l-p:0.12001626938581467
epoch£º168	 i:3 	 global-step:3363	 l-p:0.12371809035539627
epoch£º168	 i:4 	 global-step:3364	 l-p:0.180408775806427
epoch£º168	 i:5 	 global-step:3365	 l-p:0.1951082944869995
epoch£º168	 i:6 	 global-step:3366	 l-p:0.09932824224233627
epoch£º168	 i:7 	 global-step:3367	 l-p:0.2311476469039917
epoch£º168	 i:8 	 global-step:3368	 l-p:0.11218835413455963
epoch£º168	 i:9 	 global-step:3369	 l-p:0.10829561948776245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8771, 3.8772, 3.8771],
        [3.8771, 3.8918, 3.8800],
        [3.8771, 3.9786, 3.9383],
        [3.8771, 3.9169, 3.8910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.10671031475067139 
model_pd.l_d.mean(): -23.27895736694336 
model_pd.lagr.mean(): -23.1722469329834 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0456], device='cuda:0')), ('power', tensor([-23.2333], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.10671031475067139
epoch£º169	 i:1 	 global-step:3381	 l-p:0.11416387557983398
epoch£º169	 i:2 	 global-step:3382	 l-p:0.14955468475818634
epoch£º169	 i:3 	 global-step:3383	 l-p:-0.15888407826423645
epoch£º169	 i:4 	 global-step:3384	 l-p:0.10164369642734528
epoch£º169	 i:5 	 global-step:3385	 l-p:0.1421174257993698
epoch£º169	 i:6 	 global-step:3386	 l-p:-0.13271555304527283
epoch£º169	 i:7 	 global-step:3387	 l-p:-0.010533475317060947
epoch£º169	 i:8 	 global-step:3388	 l-p:0.11919459700584412
epoch£º169	 i:9 	 global-step:3389	 l-p:0.1268923133611679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8255, 3.9680, 3.9311],
        [3.8255, 3.8262, 3.8255],
        [3.8255, 3.8429, 3.8294],
        [3.8255, 3.8260, 3.8255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.12154260277748108 
model_pd.l_d.mean(): -21.988056182861328 
model_pd.lagr.mean(): -21.866514205932617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0943], device='cuda:0')), ('power', tensor([-22.0823], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.12154260277748108
epoch£º170	 i:1 	 global-step:3401	 l-p:0.21740035712718964
epoch£º170	 i:2 	 global-step:3402	 l-p:0.11722785979509354
epoch£º170	 i:3 	 global-step:3403	 l-p:0.11873555928468704
epoch£º170	 i:4 	 global-step:3404	 l-p:0.11110919713973999
epoch£º170	 i:5 	 global-step:3405	 l-p:0.12153918296098709
epoch£º170	 i:6 	 global-step:3406	 l-p:0.03259215131402016
epoch£º170	 i:7 	 global-step:3407	 l-p:0.1537812054157257
epoch£º170	 i:8 	 global-step:3408	 l-p:0.15858598053455353
epoch£º170	 i:9 	 global-step:3409	 l-p:0.08331899344921112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7453, 4.0847, 4.1439],
        [3.7453, 3.7463, 3.7454],
        [3.7453, 3.7624, 3.7491],
        [3.7453, 3.7454, 3.7453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.10587005317211151 
model_pd.l_d.mean(): -21.917451858520508 
model_pd.lagr.mean(): -21.811582565307617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1398], device='cuda:0')), ('power', tensor([-22.0572], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.10587005317211151
epoch£º171	 i:1 	 global-step:3421	 l-p:0.10673080384731293
epoch£º171	 i:2 	 global-step:3422	 l-p:0.12654460966587067
epoch£º171	 i:3 	 global-step:3423	 l-p:2.2070932388305664
epoch£º171	 i:4 	 global-step:3424	 l-p:0.10516437143087387
epoch£º171	 i:5 	 global-step:3425	 l-p:0.3560754954814911
epoch£º171	 i:6 	 global-step:3426	 l-p:0.11100667715072632
epoch£º171	 i:7 	 global-step:3427	 l-p:0.14775556325912476
epoch£º171	 i:8 	 global-step:3428	 l-p:0.1319134533405304
epoch£º171	 i:9 	 global-step:3429	 l-p:0.2035217434167862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9617, 3.9623, 3.9618],
        [3.9617, 3.9669, 3.9623],
        [3.9617, 4.1705, 4.1469],
        [3.9617, 3.9617, 3.9617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.12159951776266098 
model_pd.l_d.mean(): -23.284387588500977 
model_pd.lagr.mean(): -23.16278839111328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0759], device='cuda:0')), ('power', tensor([-23.2085], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.12159951776266098
epoch£º172	 i:1 	 global-step:3441	 l-p:0.12909534573554993
epoch£º172	 i:2 	 global-step:3442	 l-p:0.09258554875850677
epoch£º172	 i:3 	 global-step:3443	 l-p:0.10593011975288391
epoch£º172	 i:4 	 global-step:3444	 l-p:0.1557401418685913
epoch£º172	 i:5 	 global-step:3445	 l-p:0.10472847521305084
epoch£º172	 i:6 	 global-step:3446	 l-p:0.12877608835697174
epoch£º172	 i:7 	 global-step:3447	 l-p:0.10947154462337494
epoch£º172	 i:8 	 global-step:3448	 l-p:0.07616840302944183
epoch£º172	 i:9 	 global-step:3449	 l-p:0.09635724127292633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[3.7125, 4.4453, 4.8744],
        [3.7125, 3.7736, 3.7409],
        [3.7125, 4.1598, 4.3070],
        [3.7125, 3.7812, 3.7467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.09467022866010666 
model_pd.l_d.mean(): -22.467241287231445 
model_pd.lagr.mean(): -22.37257194519043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1241], device='cuda:0')), ('power', tensor([-22.5914], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.09467022866010666
epoch£º173	 i:1 	 global-step:3461	 l-p:0.10558724403381348
epoch£º173	 i:2 	 global-step:3462	 l-p:0.14692485332489014
epoch£º173	 i:3 	 global-step:3463	 l-p:0.12369412928819656
epoch£º173	 i:4 	 global-step:3464	 l-p:0.127583846449852
epoch£º173	 i:5 	 global-step:3465	 l-p:0.08265595138072968
epoch£º173	 i:6 	 global-step:3466	 l-p:0.10674192011356354
epoch£º173	 i:7 	 global-step:3467	 l-p:0.13093875348567963
epoch£º173	 i:8 	 global-step:3468	 l-p:0.12091118842363358
epoch£º173	 i:9 	 global-step:3469	 l-p:0.044406525790691376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5983, 4.1393, 4.3864],
        [3.5983, 3.6066, 3.5995],
        [3.5983, 3.6458, 3.6176],
        [3.5983, 3.6841, 3.6482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.12937413156032562 
model_pd.l_d.mean(): -22.997034072875977 
model_pd.lagr.mean(): -22.867660522460938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1044], device='cuda:0')), ('power', tensor([-23.1014], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.12937413156032562
epoch£º174	 i:1 	 global-step:3481	 l-p:0.055674679577350616
epoch£º174	 i:2 	 global-step:3482	 l-p:0.13791359961032867
epoch£º174	 i:3 	 global-step:3483	 l-p:0.11376161128282547
epoch£º174	 i:4 	 global-step:3484	 l-p:0.13211119174957275
epoch£º174	 i:5 	 global-step:3485	 l-p:0.15187177062034607
epoch£º174	 i:6 	 global-step:3486	 l-p:0.12697577476501465
epoch£º174	 i:7 	 global-step:3487	 l-p:0.11864658445119858
epoch£º174	 i:8 	 global-step:3488	 l-p:0.013247479684650898
epoch£º174	 i:9 	 global-step:3489	 l-p:0.1207059770822525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7925, 3.7942, 3.7926],
        [3.7925, 3.7996, 3.7935],
        [3.7925, 3.8268, 3.8036],
        [3.7925, 3.8460, 3.8150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.11269626021385193 
model_pd.l_d.mean(): -23.29031753540039 
model_pd.lagr.mean(): -23.177621841430664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0216], device='cuda:0')), ('power', tensor([-23.2687], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.11269626021385193
epoch£º175	 i:1 	 global-step:3501	 l-p:0.11251978576183319
epoch£º175	 i:2 	 global-step:3502	 l-p:0.1129399836063385
epoch£º175	 i:3 	 global-step:3503	 l-p:0.141794815659523
epoch£º175	 i:4 	 global-step:3504	 l-p:0.12424741685390472
epoch£º175	 i:5 	 global-step:3505	 l-p:0.07631327211856842
epoch£º175	 i:6 	 global-step:3506	 l-p:0.09362737089395523
epoch£º175	 i:7 	 global-step:3507	 l-p:0.10127637535333633
epoch£º175	 i:8 	 global-step:3508	 l-p:0.07731986790895462
epoch£º175	 i:9 	 global-step:3509	 l-p:0.02212563529610634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8392, 3.8392, 3.8392],
        [3.8392, 3.8531, 3.8419],
        [3.8392, 4.2293, 4.3194],
        [3.8392, 3.9927, 3.9578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.12681709229946136 
model_pd.l_d.mean(): -22.344985961914062 
model_pd.lagr.mean(): -22.218168258666992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0562], device='cuda:0')), ('power', tensor([-22.4012], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.12681709229946136
epoch£º176	 i:1 	 global-step:3521	 l-p:0.11666040122509003
epoch£º176	 i:2 	 global-step:3522	 l-p:0.11678140610456467
epoch£º176	 i:3 	 global-step:3523	 l-p:0.11710988730192184
epoch£º176	 i:4 	 global-step:3524	 l-p:0.11430076509714127
epoch£º176	 i:5 	 global-step:3525	 l-p:0.12527307868003845
epoch£º176	 i:6 	 global-step:3526	 l-p:0.15194858610630035
epoch£º176	 i:7 	 global-step:3527	 l-p:0.2015293836593628
epoch£º176	 i:8 	 global-step:3528	 l-p:0.10707734525203705
epoch£º176	 i:9 	 global-step:3529	 l-p:0.12137222290039062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7989, 4.6008, 5.0941],
        [3.7989, 3.7993, 3.7989],
        [3.7989, 3.8735, 3.8371],
        [3.7989, 3.7991, 3.7989]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.10379253327846527 
model_pd.l_d.mean(): -22.69077491760254 
model_pd.lagr.mean(): -22.58698272705078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0557], device='cuda:0')), ('power', tensor([-22.7464], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.10379253327846527
epoch£º177	 i:1 	 global-step:3541	 l-p:0.0668766126036644
epoch£º177	 i:2 	 global-step:3542	 l-p:0.11004374921321869
epoch£º177	 i:3 	 global-step:3543	 l-p:0.15670619904994965
epoch£º177	 i:4 	 global-step:3544	 l-p:0.19961148500442505
epoch£º177	 i:5 	 global-step:3545	 l-p:0.12485048174858093
epoch£º177	 i:6 	 global-step:3546	 l-p:0.11837651580572128
epoch£º177	 i:7 	 global-step:3547	 l-p:0.09920259565114975
epoch£º177	 i:8 	 global-step:3548	 l-p:0.12240183353424072
epoch£º177	 i:9 	 global-step:3549	 l-p:0.10761758685112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6346, 3.6347, 3.6346],
        [3.6346, 3.6346, 3.6346],
        [3.6346, 3.6346, 3.6346],
        [3.6346, 3.6774, 3.6507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.27991554141044617 
model_pd.l_d.mean(): -21.674957275390625 
model_pd.lagr.mean(): -21.395042419433594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1660], device='cuda:0')), ('power', tensor([-21.8409], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.27991554141044617
epoch£º178	 i:1 	 global-step:3561	 l-p:0.13487982749938965
epoch£º178	 i:2 	 global-step:3562	 l-p:0.12451071292161942
epoch£º178	 i:3 	 global-step:3563	 l-p:0.0684085339307785
epoch£º178	 i:4 	 global-step:3564	 l-p:0.12911666929721832
epoch£º178	 i:5 	 global-step:3565	 l-p:0.1261204183101654
epoch£º178	 i:6 	 global-step:3566	 l-p:0.05559757724404335
epoch£º178	 i:7 	 global-step:3567	 l-p:0.15260107815265656
epoch£º178	 i:8 	 global-step:3568	 l-p:-0.684955358505249
epoch£º178	 i:9 	 global-step:3569	 l-p:0.14386270940303802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5359, 3.5364, 3.5359],
        [3.5359, 3.5477, 3.5379],
        [3.5359, 3.5470, 3.5377],
        [3.5359, 3.5361, 3.5359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.12448952347040176 
model_pd.l_d.mean(): -23.247798919677734 
model_pd.lagr.mean(): -23.123310089111328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0969], device='cuda:0')), ('power', tensor([-23.3447], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.12448952347040176
epoch£º179	 i:1 	 global-step:3581	 l-p:0.1195605993270874
epoch£º179	 i:2 	 global-step:3582	 l-p:0.2528781592845917
epoch£º179	 i:3 	 global-step:3583	 l-p:0.0947825238108635
epoch£º179	 i:4 	 global-step:3584	 l-p:0.05366402119398117
epoch£º179	 i:5 	 global-step:3585	 l-p:0.1414157748222351
epoch£º179	 i:6 	 global-step:3586	 l-p:0.09608104825019836
epoch£º179	 i:7 	 global-step:3587	 l-p:-0.15658463537693024
epoch£º179	 i:8 	 global-step:3588	 l-p:0.17167076468467712
epoch£º179	 i:9 	 global-step:3589	 l-p:0.11866437643766403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8007, 3.8012, 3.8007],
        [3.8007, 3.8011, 3.8007],
        [3.8007, 3.9397, 3.9033],
        [3.8007, 3.8007, 3.8007]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.12095822393894196 
model_pd.l_d.mean(): -23.42131996154785 
model_pd.lagr.mean(): -23.30036163330078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0347], device='cuda:0')), ('power', tensor([-23.3867], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.12095822393894196
epoch£º180	 i:1 	 global-step:3601	 l-p:0.3324633836746216
epoch£º180	 i:2 	 global-step:3602	 l-p:0.09561603516340256
epoch£º180	 i:3 	 global-step:3603	 l-p:0.971052348613739
epoch£º180	 i:4 	 global-step:3604	 l-p:0.07928044348955154
epoch£º180	 i:5 	 global-step:3605	 l-p:0.10775605589151382
epoch£º180	 i:6 	 global-step:3606	 l-p:0.1219596192240715
epoch£º180	 i:7 	 global-step:3607	 l-p:0.1187792718410492
epoch£º180	 i:8 	 global-step:3608	 l-p:-0.20670001208782196
epoch£º180	 i:9 	 global-step:3609	 l-p:0.10958239436149597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1161, 4.1227, 4.1169],
        [4.1161, 4.1829, 4.1462],
        [4.1161, 4.4525, 4.4864],
        [4.1161, 4.6671, 4.8656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.0975428968667984 
model_pd.l_d.mean(): -23.04081153869629 
model_pd.lagr.mean(): -22.943267822265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0923], device='cuda:0')), ('power', tensor([-22.9485], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.0975428968667984
epoch£º181	 i:1 	 global-step:3621	 l-p:0.19803571701049805
epoch£º181	 i:2 	 global-step:3622	 l-p:0.07713641971349716
epoch£º181	 i:3 	 global-step:3623	 l-p:0.12076375633478165
epoch£º181	 i:4 	 global-step:3624	 l-p:0.10287071019411087
epoch£º181	 i:5 	 global-step:3625	 l-p:0.09738996624946594
epoch£º181	 i:6 	 global-step:3626	 l-p:0.1264868676662445
epoch£º181	 i:7 	 global-step:3627	 l-p:0.11674943566322327
epoch£º181	 i:8 	 global-step:3628	 l-p:0.10886822640895844
epoch£º181	 i:9 	 global-step:3629	 l-p:0.17532654106616974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7999, 3.8057, 3.8006],
        [3.7999, 3.8357, 3.8116],
        [3.7999, 3.8877, 3.8493],
        [3.7999, 3.7999, 3.7999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.10797467082738876 
model_pd.l_d.mean(): -23.008302688598633 
model_pd.lagr.mean(): -22.900327682495117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0038], device='cuda:0')), ('power', tensor([-23.0121], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.10797467082738876
epoch£º182	 i:1 	 global-step:3641	 l-p:0.0922408252954483
epoch£º182	 i:2 	 global-step:3642	 l-p:0.1602967381477356
epoch£º182	 i:3 	 global-step:3643	 l-p:0.14447738230228424
epoch£º182	 i:4 	 global-step:3644	 l-p:0.2336474508047104
epoch£º182	 i:5 	 global-step:3645	 l-p:0.03409511595964432
epoch£º182	 i:6 	 global-step:3646	 l-p:0.11808647215366364
epoch£º182	 i:7 	 global-step:3647	 l-p:0.12621170282363892
epoch£º182	 i:8 	 global-step:3648	 l-p:0.1093796044588089
epoch£º182	 i:9 	 global-step:3649	 l-p:-0.14251886308193207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5862, 3.7736, 3.7576],
        [3.5862, 3.6654, 3.6297],
        [3.5862, 3.5901, 3.5866],
        [3.5862, 4.0327, 4.1960]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.10658340156078339 
model_pd.l_d.mean(): -23.12063217163086 
model_pd.lagr.mean(): -23.014049530029297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0919], device='cuda:0')), ('power', tensor([-23.2126], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.10658340156078339
epoch£º183	 i:1 	 global-step:3661	 l-p:0.10322712361812592
epoch£º183	 i:2 	 global-step:3662	 l-p:0.10407479852437973
epoch£º183	 i:3 	 global-step:3663	 l-p:0.1633525788784027
epoch£º183	 i:4 	 global-step:3664	 l-p:0.10875022411346436
epoch£º183	 i:5 	 global-step:3665	 l-p:0.12873893976211548
epoch£º183	 i:6 	 global-step:3666	 l-p:0.14292746782302856
epoch£º183	 i:7 	 global-step:3667	 l-p:0.16743996739387512
epoch£º183	 i:8 	 global-step:3668	 l-p:-0.11681725829839706
epoch£º183	 i:9 	 global-step:3669	 l-p:0.10084673017263412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5862, 3.6273, 3.6009],
        [3.5862, 4.0845, 4.2953],
        [3.5862, 3.6046, 3.5901],
        [3.5862, 3.5862, 3.5862]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.13648894429206848 
model_pd.l_d.mean(): -22.16240119934082 
model_pd.lagr.mean(): -22.025911331176758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1650], device='cuda:0')), ('power', tensor([-22.3274], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.13648894429206848
epoch£º184	 i:1 	 global-step:3681	 l-p:0.09287320077419281
epoch£º184	 i:2 	 global-step:3682	 l-p:0.10989926755428314
epoch£º184	 i:3 	 global-step:3683	 l-p:0.10567508637905121
epoch£º184	 i:4 	 global-step:3684	 l-p:0.10835771262645721
epoch£º184	 i:5 	 global-step:3685	 l-p:0.01406923308968544
epoch£º184	 i:6 	 global-step:3686	 l-p:0.07617735862731934
epoch£º184	 i:7 	 global-step:3687	 l-p:0.17485713958740234
epoch£º184	 i:8 	 global-step:3688	 l-p:0.11920486390590668
epoch£º184	 i:9 	 global-step:3689	 l-p:0.11375236511230469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0378, 4.1388, 4.0968],
        [4.0378, 4.0378, 4.0378],
        [4.0378, 4.0514, 4.0403],
        [4.0378, 4.6280, 4.8722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.12649986147880554 
model_pd.l_d.mean(): -22.345945358276367 
model_pd.lagr.mean(): -22.219446182250977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0038], device='cuda:0')), ('power', tensor([-22.3497], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.12649986147880554
epoch£º185	 i:1 	 global-step:3701	 l-p:0.11492911726236343
epoch£º185	 i:2 	 global-step:3702	 l-p:0.05465022101998329
epoch£º185	 i:3 	 global-step:3703	 l-p:1.2702465057373047
epoch£º185	 i:4 	 global-step:3704	 l-p:0.1059051901102066
epoch£º185	 i:5 	 global-step:3705	 l-p:0.10736318677663803
epoch£º185	 i:6 	 global-step:3706	 l-p:0.11306628584861755
epoch£º185	 i:7 	 global-step:3707	 l-p:0.11794204264879227
epoch£º185	 i:8 	 global-step:3708	 l-p:0.09904585033655167
epoch£º185	 i:9 	 global-step:3709	 l-p:0.16430918872356415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9314, 3.9314, 3.9314],
        [3.9314, 4.2077, 4.2186],
        [3.9314, 4.1751, 4.1692],
        [3.9314, 4.0159, 3.9767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.09325996786355972 
model_pd.l_d.mean(): -22.917001724243164 
model_pd.lagr.mean(): -22.823741912841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0061], device='cuda:0')), ('power', tensor([-22.9109], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.09325996786355972
epoch£º186	 i:1 	 global-step:3721	 l-p:0.16263018548488617
epoch£º186	 i:2 	 global-step:3722	 l-p:0.18100176751613617
epoch£º186	 i:3 	 global-step:3723	 l-p:0.12656082212924957
epoch£º186	 i:4 	 global-step:3724	 l-p:0.8069546818733215
epoch£º186	 i:5 	 global-step:3725	 l-p:0.09957776218652725
epoch£º186	 i:6 	 global-step:3726	 l-p:0.11477117240428925
epoch£º186	 i:7 	 global-step:3727	 l-p:0.10977382212877274
epoch£º186	 i:8 	 global-step:3728	 l-p:0.12023254483938217
epoch£º186	 i:9 	 global-step:3729	 l-p:0.13565942645072937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7892, 3.7893, 3.7892],
        [3.7892, 3.9068, 3.8682],
        [3.7892, 3.9993, 3.9848],
        [3.7892, 3.7892, 3.7892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.12946796417236328 
model_pd.l_d.mean(): -22.420082092285156 
model_pd.lagr.mean(): -22.29061508178711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0700], device='cuda:0')), ('power', tensor([-22.4901], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.12946796417236328
epoch£º187	 i:1 	 global-step:3741	 l-p:0.097938172519207
epoch£º187	 i:2 	 global-step:3742	 l-p:0.10817844420671463
epoch£º187	 i:3 	 global-step:3743	 l-p:0.12298373878002167
epoch£º187	 i:4 	 global-step:3744	 l-p:0.12170406430959702
epoch£º187	 i:5 	 global-step:3745	 l-p:0.10076543688774109
epoch£º187	 i:6 	 global-step:3746	 l-p:0.08307453989982605
epoch£º187	 i:7 	 global-step:3747	 l-p:0.1403253972530365
epoch£º187	 i:8 	 global-step:3748	 l-p:-25.029298782348633
epoch£º187	 i:9 	 global-step:3749	 l-p:0.12420126050710678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2265, 3.6213, 3.7792],
        [3.2265, 3.2524, 3.2314],
        [3.2265, 3.2265, 3.2265],
        [3.2265, 3.6146, 3.7665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.1257420927286148 
model_pd.l_d.mean(): -23.361129760742188 
model_pd.lagr.mean(): -23.235387802124023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2152], device='cuda:0')), ('power', tensor([-23.5763], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.1257420927286148
epoch£º188	 i:1 	 global-step:3761	 l-p:0.1198597177863121
epoch£º188	 i:2 	 global-step:3762	 l-p:0.1354856938123703
epoch£º188	 i:3 	 global-step:3763	 l-p:0.11519696563482285
epoch£º188	 i:4 	 global-step:3764	 l-p:0.14507612586021423
epoch£º188	 i:5 	 global-step:3765	 l-p:0.12916524708271027
epoch£º188	 i:6 	 global-step:3766	 l-p:0.13843832910060883
epoch£º188	 i:7 	 global-step:3767	 l-p:0.10322634130716324
epoch£º188	 i:8 	 global-step:3768	 l-p:0.11618989706039429
epoch£º188	 i:9 	 global-step:3769	 l-p:0.15918958187103271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5859, 4.2057, 4.5375],
        [3.5859, 4.2161, 4.5586],
        [3.5859, 3.5859, 3.5859],
        [3.5859, 3.5859, 3.5859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): -0.004272031597793102 
model_pd.l_d.mean(): -22.63607406616211 
model_pd.lagr.mean(): -22.64034652709961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1369], device='cuda:0')), ('power', tensor([-22.7730], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:-0.004272031597793102
epoch£º189	 i:1 	 global-step:3781	 l-p:0.11501140147447586
epoch£º189	 i:2 	 global-step:3782	 l-p:0.1095130443572998
epoch£º189	 i:3 	 global-step:3783	 l-p:0.11296126991510391
epoch£º189	 i:4 	 global-step:3784	 l-p:0.11671044677495956
epoch£º189	 i:5 	 global-step:3785	 l-p:0.11253110319375992
epoch£º189	 i:6 	 global-step:3786	 l-p:0.3555036783218384
epoch£º189	 i:7 	 global-step:3787	 l-p:0.09852621704339981
epoch£º189	 i:8 	 global-step:3788	 l-p:0.15554726123809814
epoch£º189	 i:9 	 global-step:3789	 l-p:0.13637623190879822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8920, 3.8920, 3.8920],
        [3.8920, 3.8920, 3.8920],
        [3.8920, 3.8920, 3.8920],
        [3.8920, 3.9025, 3.8936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.11815937608480453 
model_pd.l_d.mean(): -22.961732864379883 
model_pd.lagr.mean(): -22.84357261657715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0092], device='cuda:0')), ('power', tensor([-22.9525], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.11815937608480453
epoch£º190	 i:1 	 global-step:3801	 l-p:0.1082223504781723
epoch£º190	 i:2 	 global-step:3802	 l-p:0.11682403087615967
epoch£º190	 i:3 	 global-step:3803	 l-p:-0.03448587283492088
epoch£º190	 i:4 	 global-step:3804	 l-p:-0.12349841743707657
epoch£º190	 i:5 	 global-step:3805	 l-p:0.11008044332265854
epoch£º190	 i:6 	 global-step:3806	 l-p:0.05871708691120148
epoch£º190	 i:7 	 global-step:3807	 l-p:0.16470235586166382
epoch£º190	 i:8 	 global-step:3808	 l-p:0.11457975208759308
epoch£º190	 i:9 	 global-step:3809	 l-p:0.06165280193090439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0928, 4.8466, 5.2485],
        [4.0928, 4.1376, 4.1087],
        [4.0928, 4.0928, 4.0928],
        [4.0928, 4.0928, 4.0928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.13000796735286713 
model_pd.l_d.mean(): -21.98600959777832 
model_pd.lagr.mean(): -21.856000900268555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0450], device='cuda:0')), ('power', tensor([-22.0310], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.13000796735286713
epoch£º191	 i:1 	 global-step:3821	 l-p:0.11421319842338562
epoch£º191	 i:2 	 global-step:3822	 l-p:0.10592729598283768
epoch£º191	 i:3 	 global-step:3823	 l-p:0.20120899379253387
epoch£º191	 i:4 	 global-step:3824	 l-p:0.10409355908632278
epoch£º191	 i:5 	 global-step:3825	 l-p:0.10441581159830093
epoch£º191	 i:6 	 global-step:3826	 l-p:0.13409523665905
epoch£º191	 i:7 	 global-step:3827	 l-p:0.09139426797628403
epoch£º191	 i:8 	 global-step:3828	 l-p:0.10767783969640732
epoch£º191	 i:9 	 global-step:3829	 l-p:0.07316205650568008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9213, 3.9344, 3.9236],
        [3.9213, 3.9293, 3.9224],
        [3.9213, 4.5561, 4.8594],
        [3.9213, 3.9399, 3.9253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.09323293715715408 
model_pd.l_d.mean(): -22.91828155517578 
model_pd.lagr.mean(): -22.825048446655273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0034], device='cuda:0')), ('power', tensor([-22.9149], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.09323293715715408
epoch£º192	 i:1 	 global-step:3841	 l-p:0.12098924070596695
epoch£º192	 i:2 	 global-step:3842	 l-p:0.06060788407921791
epoch£º192	 i:3 	 global-step:3843	 l-p:0.09657451510429382
epoch£º192	 i:4 	 global-step:3844	 l-p:0.13870419561862946
epoch£º192	 i:5 	 global-step:3845	 l-p:0.13532716035842896
epoch£º192	 i:6 	 global-step:3846	 l-p:0.4036189615726471
epoch£º192	 i:7 	 global-step:3847	 l-p:0.45325714349746704
epoch£º192	 i:8 	 global-step:3848	 l-p:0.09311562031507492
epoch£º192	 i:9 	 global-step:3849	 l-p:0.08536039292812347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7222, 4.2507, 4.4757],
        [3.7222, 3.7312, 3.7234],
        [3.7222, 3.7232, 3.7223],
        [3.7222, 3.7223, 3.7222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.05907503888010979 
model_pd.l_d.mean(): -23.211286544799805 
model_pd.lagr.mean(): -23.152212142944336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0364], device='cuda:0')), ('power', tensor([-23.2476], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.05907503888010979
epoch£º193	 i:1 	 global-step:3861	 l-p:0.11781835556030273
epoch£º193	 i:2 	 global-step:3862	 l-p:0.1300317943096161
epoch£º193	 i:3 	 global-step:3863	 l-p:0.125797301530838
epoch£º193	 i:4 	 global-step:3864	 l-p:0.05039171129465103
epoch£º193	 i:5 	 global-step:3865	 l-p:0.1141386404633522
epoch£º193	 i:6 	 global-step:3866	 l-p:0.10597533732652664
epoch£º193	 i:7 	 global-step:3867	 l-p:0.13879650831222534
epoch£º193	 i:8 	 global-step:3868	 l-p:0.112459197640419
epoch£º193	 i:9 	 global-step:3869	 l-p:0.11737456917762756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9741, 3.9741, 3.9741],
        [3.9741, 4.3837, 4.4824],
        [3.9741, 4.1206, 4.0820],
        [3.9741, 3.9994, 3.9805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.11365492641925812 
model_pd.l_d.mean(): -23.349201202392578 
model_pd.lagr.mean(): -23.235546112060547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0867], device='cuda:0')), ('power', tensor([-23.2625], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.11365492641925812
epoch£º194	 i:1 	 global-step:3881	 l-p:0.13225366175174713
epoch£º194	 i:2 	 global-step:3882	 l-p:0.12595728039741516
epoch£º194	 i:3 	 global-step:3883	 l-p:0.17382284998893738
epoch£º194	 i:4 	 global-step:3884	 l-p:0.11482604593038559
epoch£º194	 i:5 	 global-step:3885	 l-p:0.16959823668003082
epoch£º194	 i:6 	 global-step:3886	 l-p:0.09905700385570526
epoch£º194	 i:7 	 global-step:3887	 l-p:0.11376310884952545
epoch£º194	 i:8 	 global-step:3888	 l-p:0.1061561182141304
epoch£º194	 i:9 	 global-step:3889	 l-p:0.06653754413127899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8369, 3.8819, 3.8530],
        [3.8369, 4.1813, 4.2432],
        [3.8369, 3.8383, 3.8370],
        [3.8369, 4.1140, 4.1323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.623388946056366 
model_pd.l_d.mean(): -23.04718589782715 
model_pd.lagr.mean(): -22.423797607421875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0089], device='cuda:0')), ('power', tensor([-23.0561], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.623388946056366
epoch£º195	 i:1 	 global-step:3901	 l-p:0.19842326641082764
epoch£º195	 i:2 	 global-step:3902	 l-p:0.1210075244307518
epoch£º195	 i:3 	 global-step:3903	 l-p:0.10076870769262314
epoch£º195	 i:4 	 global-step:3904	 l-p:0.1105225533246994
epoch£º195	 i:5 	 global-step:3905	 l-p:0.1265256404876709
epoch£º195	 i:6 	 global-step:3906	 l-p:0.11554321646690369
epoch£º195	 i:7 	 global-step:3907	 l-p:0.10207150131464005
epoch£º195	 i:8 	 global-step:3908	 l-p:0.11881522089242935
epoch£º195	 i:9 	 global-step:3909	 l-p:0.14100883901119232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6494, 3.6494, 3.6494],
        [3.6494, 3.6520, 3.6495],
        [3.6494, 3.6495, 3.6494],
        [3.6494, 3.6513, 3.6495]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.2897554636001587 
model_pd.l_d.mean(): -22.55752944946289 
model_pd.lagr.mean(): -22.26777458190918 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1546], device='cuda:0')), ('power', tensor([-22.7121], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.2897554636001587
epoch£º196	 i:1 	 global-step:3921	 l-p:-0.10784801840782166
epoch£º196	 i:2 	 global-step:3922	 l-p:0.0230090469121933
epoch£º196	 i:3 	 global-step:3923	 l-p:0.12472491711378098
epoch£º196	 i:4 	 global-step:3924	 l-p:0.15004298090934753
epoch£º196	 i:5 	 global-step:3925	 l-p:0.12378307431936264
epoch£º196	 i:6 	 global-step:3926	 l-p:0.10619246959686279
epoch£º196	 i:7 	 global-step:3927	 l-p:0.08898536115884781
epoch£º196	 i:8 	 global-step:3928	 l-p:0.12039607763290405
epoch£º196	 i:9 	 global-step:3929	 l-p:0.10821709036827087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6055, 3.6056, 3.6055],
        [3.6055, 3.8593, 3.8777],
        [3.6055, 3.6055, 3.6055],
        [3.6055, 3.6117, 3.6058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.1605089157819748 
model_pd.l_d.mean(): -22.192350387573242 
model_pd.lagr.mean(): -22.031841278076172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1732], device='cuda:0')), ('power', tensor([-22.3656], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.1605089157819748
epoch£º197	 i:1 	 global-step:3941	 l-p:0.12255468219518661
epoch£º197	 i:2 	 global-step:3942	 l-p:0.09533920139074326
epoch£º197	 i:3 	 global-step:3943	 l-p:0.04559585079550743
epoch£º197	 i:4 	 global-step:3944	 l-p:0.03922183811664581
epoch£º197	 i:5 	 global-step:3945	 l-p:0.12369470298290253
epoch£º197	 i:6 	 global-step:3946	 l-p:0.11287243664264679
epoch£º197	 i:7 	 global-step:3947	 l-p:0.11220571398735046
epoch£º197	 i:8 	 global-step:3948	 l-p:0.14113375544548035
epoch£º197	 i:9 	 global-step:3949	 l-p:0.06592072546482086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6022, 3.6114, 3.6029],
        [3.6022, 3.6023, 3.6022],
        [3.6022, 3.6022, 3.6022],
        [3.6022, 3.6025, 3.6022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.11167591065168381 
model_pd.l_d.mean(): -23.2390079498291 
model_pd.lagr.mean(): -23.12733268737793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0734], device='cuda:0')), ('power', tensor([-23.3124], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.11167591065168381
epoch£º198	 i:1 	 global-step:3961	 l-p:0.09651676565408707
epoch£º198	 i:2 	 global-step:3962	 l-p:0.1060090959072113
epoch£º198	 i:3 	 global-step:3963	 l-p:0.12568259239196777
epoch£º198	 i:4 	 global-step:3964	 l-p:0.11083143204450607
epoch£º198	 i:5 	 global-step:3965	 l-p:-0.02541273459792137
epoch£º198	 i:6 	 global-step:3966	 l-p:0.11658788472414017
epoch£º198	 i:7 	 global-step:3967	 l-p:0.11047302186489105
epoch£º198	 i:8 	 global-step:3968	 l-p:0.05960114300251007
epoch£º198	 i:9 	 global-step:3969	 l-p:0.09499023854732513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5942, 3.5944, 3.5942],
        [3.5942, 3.5982, 3.5942],
        [3.5942, 3.5942, 3.5942],
        [3.5942, 3.6558, 3.6211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.12239681929349899 
model_pd.l_d.mean(): -22.190305709838867 
model_pd.lagr.mean(): -22.067909240722656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1495], device='cuda:0')), ('power', tensor([-22.3398], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.12239681929349899
epoch£º199	 i:1 	 global-step:3981	 l-p:-0.019198240712285042
epoch£º199	 i:2 	 global-step:3982	 l-p:0.06547036021947861
epoch£º199	 i:3 	 global-step:3983	 l-p:0.12029964476823807
epoch£º199	 i:4 	 global-step:3984	 l-p:0.10933656245470047
epoch£º199	 i:5 	 global-step:3985	 l-p:0.1528332680463791
epoch£º199	 i:6 	 global-step:3986	 l-p:0.12815503776073456
epoch£º199	 i:7 	 global-step:3987	 l-p:0.11189895123243332
epoch£º199	 i:8 	 global-step:3988	 l-p:0.10955031216144562
epoch£º199	 i:9 	 global-step:3989	 l-p:0.1116514503955841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7710, 3.7710, 3.7710],
        [3.7710, 3.7712, 3.7710],
        [3.7710, 3.7778, 3.7716],
        [3.7710, 3.9937, 3.9871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.11551365256309509 
model_pd.l_d.mean(): -23.204452514648438 
model_pd.lagr.mean(): -23.088939666748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0146], device='cuda:0')), ('power', tensor([-23.2191], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.11551365256309509
epoch£º200	 i:1 	 global-step:4001	 l-p:0.12021655589342117
epoch£º200	 i:2 	 global-step:4002	 l-p:0.11188095808029175
epoch£º200	 i:3 	 global-step:4003	 l-p:0.12018725275993347
epoch£º200	 i:4 	 global-step:4004	 l-p:0.18039651215076447
epoch£º200	 i:5 	 global-step:4005	 l-p:0.12466415017843246
epoch£º200	 i:6 	 global-step:4006	 l-p:0.34696605801582336
epoch£º200	 i:7 	 global-step:4007	 l-p:0.0992116779088974
epoch£º200	 i:8 	 global-step:4008	 l-p:0.10444273054599762
epoch£º200	 i:9 	 global-step:4009	 l-p:0.117812879383564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6064, 3.6064, 3.6064],
        [3.6064, 3.9007, 3.9457],
        [3.6064, 3.6064, 3.6064],
        [3.6064, 3.6567, 3.6247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.11804915964603424 
model_pd.l_d.mean(): -21.83437156677246 
model_pd.lagr.mean(): -21.71632194519043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1933], device='cuda:0')), ('power', tensor([-22.0276], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.11804915964603424
epoch£º201	 i:1 	 global-step:4021	 l-p:0.11772789061069489
epoch£º201	 i:2 	 global-step:4022	 l-p:-0.4967578053474426
epoch£º201	 i:3 	 global-step:4023	 l-p:0.1273370385169983
epoch£º201	 i:4 	 global-step:4024	 l-p:0.1180957555770874
epoch£º201	 i:5 	 global-step:4025	 l-p:0.10949275642633438
epoch£º201	 i:6 	 global-step:4026	 l-p:0.12123240530490875
epoch£º201	 i:7 	 global-step:4027	 l-p:1.6601296663284302
epoch£º201	 i:8 	 global-step:4028	 l-p:0.1093098446726799
epoch£º201	 i:9 	 global-step:4029	 l-p:0.12076478451490402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6469, 3.6469, 3.6469],
        [3.6469, 3.6469, 3.6469],
        [3.6469, 3.6954, 3.6641],
        [3.6469, 3.6524, 3.6470]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.10333358496427536 
model_pd.l_d.mean(): -22.906967163085938 
model_pd.lagr.mean(): -22.803632736206055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0909], device='cuda:0')), ('power', tensor([-22.9978], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.10333358496427536
epoch£º202	 i:1 	 global-step:4041	 l-p:0.03736616671085358
epoch£º202	 i:2 	 global-step:4042	 l-p:0.10799092799425125
epoch£º202	 i:3 	 global-step:4043	 l-p:0.11959724128246307
epoch£º202	 i:4 	 global-step:4044	 l-p:0.10044952481985092
epoch£º202	 i:5 	 global-step:4045	 l-p:0.11788590997457504
epoch£º202	 i:6 	 global-step:4046	 l-p:0.17662152647972107
epoch£º202	 i:7 	 global-step:4047	 l-p:0.136745423078537
epoch£º202	 i:8 	 global-step:4048	 l-p:-0.22021327912807465
epoch£º202	 i:9 	 global-step:4049	 l-p:0.11392893642187119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6240, 3.9726, 4.0558],
        [3.6240, 3.6753, 3.6428],
        [3.6240, 3.6544, 3.6308],
        [3.6240, 3.6240, 3.6240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.11407021433115005 
model_pd.l_d.mean(): -22.115245819091797 
model_pd.lagr.mean(): -22.001174926757812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1425], device='cuda:0')), ('power', tensor([-22.2578], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.11407021433115005
epoch£º203	 i:1 	 global-step:4061	 l-p:-0.2525447905063629
epoch£º203	 i:2 	 global-step:4062	 l-p:0.10856189578771591
epoch£º203	 i:3 	 global-step:4063	 l-p:0.10139493644237518
epoch£º203	 i:4 	 global-step:4064	 l-p:0.1275567263364792
epoch£º203	 i:5 	 global-step:4065	 l-p:0.14368002116680145
epoch£º203	 i:6 	 global-step:4066	 l-p:0.13865505158901215
epoch£º203	 i:7 	 global-step:4067	 l-p:0.05361022800207138
epoch£º203	 i:8 	 global-step:4068	 l-p:0.1071413904428482
epoch£º203	 i:9 	 global-step:4069	 l-p:0.11642023175954819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5994, 3.9134, 3.9730],
        [3.5994, 3.5994, 3.5994],
        [3.5994, 3.9072, 3.9624],
        [3.5994, 3.6578, 3.6231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.11299917101860046 
model_pd.l_d.mean(): -23.115461349487305 
model_pd.lagr.mean(): -23.00246238708496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1063], device='cuda:0')), ('power', tensor([-23.2218], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.11299917101860046
epoch£º204	 i:1 	 global-step:4081	 l-p:0.12489087879657745
epoch£º204	 i:2 	 global-step:4082	 l-p:0.10966631025075912
epoch£º204	 i:3 	 global-step:4083	 l-p:0.13817258179187775
epoch£º204	 i:4 	 global-step:4084	 l-p:0.11784694343805313
epoch£º204	 i:5 	 global-step:4085	 l-p:0.08933146297931671
epoch£º204	 i:6 	 global-step:4086	 l-p:0.08612789213657379
epoch£º204	 i:7 	 global-step:4087	 l-p:0.041242145001888275
epoch£º204	 i:8 	 global-step:4088	 l-p:0.12426899373531342
epoch£º204	 i:9 	 global-step:4089	 l-p:0.11034981161355972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5296, 3.5603, 3.5356],
        [3.5296, 3.5296, 3.5296],
        [3.5296, 3.5296, 3.5296],
        [3.5296, 3.5296, 3.5296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.10178465396165848 
model_pd.l_d.mean(): -22.642440795898438 
model_pd.lagr.mean(): -22.54065704345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1737], device='cuda:0')), ('power', tensor([-22.8162], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.10178465396165848
epoch£º205	 i:1 	 global-step:4101	 l-p:0.1237209215760231
epoch£º205	 i:2 	 global-step:4102	 l-p:0.07601097971200943
epoch£º205	 i:3 	 global-step:4103	 l-p:0.09664604812860489
epoch£º205	 i:4 	 global-step:4104	 l-p:0.11272534728050232
epoch£º205	 i:5 	 global-step:4105	 l-p:0.18408460915088654
epoch£º205	 i:6 	 global-step:4106	 l-p:0.1152862161397934
epoch£º205	 i:7 	 global-step:4107	 l-p:0.12251901626586914
epoch£º205	 i:8 	 global-step:4108	 l-p:0.11174894124269485
epoch£º205	 i:9 	 global-step:4109	 l-p:0.1233053058385849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6115, 3.6290, 3.6132],
        [3.6115, 4.1985, 4.4971],
        [3.6115, 3.9530, 4.0323],
        [3.6115, 3.6115, 3.6115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.1757936179637909 
model_pd.l_d.mean(): -23.014543533325195 
model_pd.lagr.mean(): -22.8387508392334 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1229], device='cuda:0')), ('power', tensor([-23.1375], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.1757936179637909
epoch£º206	 i:1 	 global-step:4121	 l-p:0.0943848192691803
epoch£º206	 i:2 	 global-step:4122	 l-p:-1.353187084197998
epoch£º206	 i:3 	 global-step:4123	 l-p:0.6085569262504578
epoch£º206	 i:4 	 global-step:4124	 l-p:0.09634646028280258
epoch£º206	 i:5 	 global-step:4125	 l-p:0.23240329325199127
epoch£º206	 i:6 	 global-step:4126	 l-p:0.11930442601442337
epoch£º206	 i:7 	 global-step:4127	 l-p:0.12193429470062256
epoch£º206	 i:8 	 global-step:4128	 l-p:0.1172080934047699
epoch£º206	 i:9 	 global-step:4129	 l-p:0.10184086859226227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7922, 3.8054, 3.7937],
        [3.7922, 3.7922, 3.7922],
        [3.7922, 3.9860, 3.9661],
        [3.7922, 3.9066, 3.8667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.11113711446523666 
model_pd.l_d.mean(): -22.733057022094727 
model_pd.lagr.mean(): -22.621919631958008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0361], device='cuda:0')), ('power', tensor([-22.7692], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.11113711446523666
epoch£º207	 i:1 	 global-step:4141	 l-p:0.12245478481054306
epoch£º207	 i:2 	 global-step:4142	 l-p:0.10912787169218063
epoch£º207	 i:3 	 global-step:4143	 l-p:0.10674641281366348
epoch£º207	 i:4 	 global-step:4144	 l-p:0.06877003610134125
epoch£º207	 i:5 	 global-step:4145	 l-p:0.05769459530711174
epoch£º207	 i:6 	 global-step:4146	 l-p:0.15611276030540466
epoch£º207	 i:7 	 global-step:4147	 l-p:0.13128535449504852
epoch£º207	 i:8 	 global-step:4148	 l-p:0.115146204829216
epoch£º207	 i:9 	 global-step:4149	 l-p:0.10176349431276321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8427, 3.8428, 3.8427],
        [3.8427, 4.2005, 4.2736],
        [3.8427, 3.9376, 3.8968],
        [3.8427, 3.8427, 3.8427]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): -0.6234142780303955 
model_pd.l_d.mean(): -23.4005184173584 
model_pd.lagr.mean(): -24.02393341064453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0437], device='cuda:0')), ('power', tensor([-23.3568], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:-0.6234142780303955
epoch£º208	 i:1 	 global-step:4161	 l-p:0.1225108727812767
epoch£º208	 i:2 	 global-step:4162	 l-p:0.12479596585035324
epoch£º208	 i:3 	 global-step:4163	 l-p:0.08230472356081009
epoch£º208	 i:4 	 global-step:4164	 l-p:0.23894889652729034
epoch£º208	 i:5 	 global-step:4165	 l-p:-0.31351444125175476
epoch£º208	 i:6 	 global-step:4166	 l-p:0.108383908867836
epoch£º208	 i:7 	 global-step:4167	 l-p:0.11459647864103317
epoch£º208	 i:8 	 global-step:4168	 l-p:0.10098939388990402
epoch£º208	 i:9 	 global-step:4169	 l-p:0.11490049958229065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[3.8374, 4.0876, 4.0923],
        [3.8374, 4.5214, 4.8889],
        [3.8374, 3.8745, 3.8478],
        [3.8374, 4.0228, 3.9983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): -0.01004029717296362 
model_pd.l_d.mean(): -22.94452476501465 
model_pd.lagr.mean(): -22.954565048217773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0439], device='cuda:0')), ('power', tensor([-22.9884], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:-0.01004029717296362
epoch£º209	 i:1 	 global-step:4181	 l-p:0.11405190825462341
epoch£º209	 i:2 	 global-step:4182	 l-p:0.10851967334747314
epoch£º209	 i:3 	 global-step:4183	 l-p:0.07590360939502716
epoch£º209	 i:4 	 global-step:4184	 l-p:0.1195317879319191
epoch£º209	 i:5 	 global-step:4185	 l-p:0.12271701544523239
epoch£º209	 i:6 	 global-step:4186	 l-p:0.10080249607563019
epoch£º209	 i:7 	 global-step:4187	 l-p:0.11449357122182846
epoch£º209	 i:8 	 global-step:4188	 l-p:-1.5529412031173706
epoch£º209	 i:9 	 global-step:4189	 l-p:0.2605223059654236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6806, 3.6806, 3.6806],
        [3.6806, 3.6806, 3.6806],
        [3.6806, 3.7713, 3.7316],
        [3.6806, 3.6806, 3.6806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.10292068123817444 
model_pd.l_d.mean(): -23.294063568115234 
model_pd.lagr.mean(): -23.191143035888672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0446], device='cuda:0')), ('power', tensor([-23.3386], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.10292068123817444
epoch£º210	 i:1 	 global-step:4201	 l-p:0.09020936489105225
epoch£º210	 i:2 	 global-step:4202	 l-p:0.10361141711473465
epoch£º210	 i:3 	 global-step:4203	 l-p:-0.17474456131458282
epoch£º210	 i:4 	 global-step:4204	 l-p:0.1040605679154396
epoch£º210	 i:5 	 global-step:4205	 l-p:0.13190779089927673
epoch£º210	 i:6 	 global-step:4206	 l-p:0.12691715359687805
epoch£º210	 i:7 	 global-step:4207	 l-p:0.11040898412466049
epoch£º210	 i:8 	 global-step:4208	 l-p:0.3679875135421753
epoch£º210	 i:9 	 global-step:4209	 l-p:0.3325755298137665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6657, 3.9084, 3.9181],
        [3.6657, 3.6657, 3.6657],
        [3.6657, 3.7435, 3.7046],
        [3.6657, 3.6657, 3.6657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.100618876516819 
model_pd.l_d.mean(): -22.5646915435791 
model_pd.lagr.mean(): -22.464073181152344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0953], device='cuda:0')), ('power', tensor([-22.6600], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.100618876516819
epoch£º211	 i:1 	 global-step:4221	 l-p:0.13003239035606384
epoch£º211	 i:2 	 global-step:4222	 l-p:0.25256696343421936
epoch£º211	 i:3 	 global-step:4223	 l-p:0.137944757938385
epoch£º211	 i:4 	 global-step:4224	 l-p:0.11072512716054916
epoch£º211	 i:5 	 global-step:4225	 l-p:1.3590790033340454
epoch£º211	 i:6 	 global-step:4226	 l-p:0.1220686286687851
epoch£º211	 i:7 	 global-step:4227	 l-p:0.16693900525569916
epoch£º211	 i:8 	 global-step:4228	 l-p:0.08510701358318329
epoch£º211	 i:9 	 global-step:4229	 l-p:0.10525684803724289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7515, 4.0675, 4.1180],
        [3.7515, 3.7515, 3.7515],
        [3.7515, 3.7515, 3.7515],
        [3.7515, 4.0464, 4.0829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.11932599544525146 
model_pd.l_d.mean(): -23.499755859375 
model_pd.lagr.mean(): -23.380430221557617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0214], device='cuda:0')), ('power', tensor([-23.4783], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.11932599544525146
epoch£º212	 i:1 	 global-step:4241	 l-p:0.08002644032239914
epoch£º212	 i:2 	 global-step:4242	 l-p:0.10639688372612
epoch£º212	 i:3 	 global-step:4243	 l-p:0.12400747835636139
epoch£º212	 i:4 	 global-step:4244	 l-p:0.12439687550067902
epoch£º212	 i:5 	 global-step:4245	 l-p:0.11590418219566345
epoch£º212	 i:6 	 global-step:4246	 l-p:0.04742622748017311
epoch£º212	 i:7 	 global-step:4247	 l-p:0.04528367891907692
epoch£º212	 i:8 	 global-step:4248	 l-p:0.1267269253730774
epoch£º212	 i:9 	 global-step:4249	 l-p:0.13685865700244904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8299, 3.8303, 3.8298],
        [3.8299, 3.8298, 3.8299],
        [3.8299, 3.9441, 3.9035],
        [3.8299, 3.8356, 3.8301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.09635267406702042 
model_pd.l_d.mean(): -22.473968505859375 
model_pd.lagr.mean(): -22.377614974975586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0899], device='cuda:0')), ('power', tensor([-22.5639], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.09635267406702042
epoch£º213	 i:1 	 global-step:4261	 l-p:0.000687618216034025
epoch£º213	 i:2 	 global-step:4262	 l-p:0.10320908576250076
epoch£º213	 i:3 	 global-step:4263	 l-p:0.12033837288618088
epoch£º213	 i:4 	 global-step:4264	 l-p:0.045447878539562225
epoch£º213	 i:5 	 global-step:4265	 l-p:0.10437067598104477
epoch£º213	 i:6 	 global-step:4266	 l-p:0.11632923036813736
epoch£º213	 i:7 	 global-step:4267	 l-p:-0.09705585241317749
epoch£º213	 i:8 	 global-step:4268	 l-p:0.12218766659498215
epoch£º213	 i:9 	 global-step:4269	 l-p:0.11474543064832687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8020, 3.8020, 3.8020],
        [3.8020, 3.8208, 3.8046],
        [3.8020, 3.9069, 3.8660],
        [3.8020, 3.8020, 3.8020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.03928454592823982 
model_pd.l_d.mean(): -23.221145629882812 
model_pd.lagr.mean(): -23.181861877441406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0019], device='cuda:0')), ('power', tensor([-23.2231], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.03928454592823982
epoch£º214	 i:1 	 global-step:4281	 l-p:0.10114111006259918
epoch£º214	 i:2 	 global-step:4282	 l-p:0.05849379301071167
epoch£º214	 i:3 	 global-step:4283	 l-p:0.11187337338924408
epoch£º214	 i:4 	 global-step:4284	 l-p:0.16685126721858978
epoch£º214	 i:5 	 global-step:4285	 l-p:0.11421951651573181
epoch£º214	 i:6 	 global-step:4286	 l-p:0.15475919842720032
epoch£º214	 i:7 	 global-step:4287	 l-p:0.0902167335152626
epoch£º214	 i:8 	 global-step:4288	 l-p:0.08099090307950974
epoch£º214	 i:9 	 global-step:4289	 l-p:0.14327071607112885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8194, 3.9255, 3.8844],
        [3.8194, 3.8195, 3.8194],
        [3.8194, 3.8265, 3.8197],
        [3.8194, 3.8194, 3.8194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.1371040642261505 
model_pd.l_d.mean(): -23.179731369018555 
model_pd.lagr.mean(): -23.042627334594727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0106], device='cuda:0')), ('power', tensor([-23.1903], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.1371040642261505
epoch£º215	 i:1 	 global-step:4301	 l-p:-0.08188865333795547
epoch£º215	 i:2 	 global-step:4302	 l-p:0.08755926042795181
epoch£º215	 i:3 	 global-step:4303	 l-p:0.11780687421560287
epoch£º215	 i:4 	 global-step:4304	 l-p:0.10606589913368225
epoch£º215	 i:5 	 global-step:4305	 l-p:0.11348220705986023
epoch£º215	 i:6 	 global-step:4306	 l-p:0.14720794558525085
epoch£º215	 i:7 	 global-step:4307	 l-p:0.021213848143815994
epoch£º215	 i:8 	 global-step:4308	 l-p:0.10150078684091568
epoch£º215	 i:9 	 global-step:4309	 l-p:0.13621161878108978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8107, 3.8109, 3.8107],
        [3.8107, 4.0410, 4.0370],
        [3.8107, 3.8194, 3.8111],
        [3.8107, 3.8107, 3.8107]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.08291199803352356 
model_pd.l_d.mean(): -22.245845794677734 
model_pd.lagr.mean(): -22.162933349609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0607], device='cuda:0')), ('power', tensor([-22.3065], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.08291199803352356
epoch£º216	 i:1 	 global-step:4321	 l-p:0.026550201699137688
epoch£º216	 i:2 	 global-step:4322	 l-p:0.09882871061563492
epoch£º216	 i:3 	 global-step:4323	 l-p:0.11728471517562866
epoch£º216	 i:4 	 global-step:4324	 l-p:0.21831437945365906
epoch£º216	 i:5 	 global-step:4325	 l-p:0.12805236876010895
epoch£º216	 i:6 	 global-step:4326	 l-p:0.10995950549840927
epoch£º216	 i:7 	 global-step:4327	 l-p:0.12207096070051193
epoch£º216	 i:8 	 global-step:4328	 l-p:0.16569861769676208
epoch£º216	 i:9 	 global-step:4329	 l-p:0.10991537570953369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8929, 3.8991, 3.8932],
        [3.8929, 4.2178, 4.2651],
        [3.8929, 4.0302, 3.9911],
        [3.8929, 3.8929, 3.8929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.11043883860111237 
model_pd.l_d.mean(): -23.23253059387207 
model_pd.lagr.mean(): -23.12209129333496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0414], device='cuda:0')), ('power', tensor([-23.1912], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.11043883860111237
epoch£º217	 i:1 	 global-step:4341	 l-p:0.11949611455202103
epoch£º217	 i:2 	 global-step:4342	 l-p:0.08294517546892166
epoch£º217	 i:3 	 global-step:4343	 l-p:0.13495779037475586
epoch£º217	 i:4 	 global-step:4344	 l-p:0.11670941859483719
epoch£º217	 i:5 	 global-step:4345	 l-p:0.09639915078878403
epoch£º217	 i:6 	 global-step:4346	 l-p:0.07579550892114639
epoch£º217	 i:7 	 global-step:4347	 l-p:0.10771016031503677
epoch£º217	 i:8 	 global-step:4348	 l-p:0.06001358851790428
epoch£º217	 i:9 	 global-step:4349	 l-p:0.09053905308246613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8059, 3.8072, 3.8059],
        [3.8059, 3.8059, 3.8059],
        [3.8059, 3.8060, 3.8059],
        [3.8059, 3.8059, 3.8059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.11665935069322586 
model_pd.l_d.mean(): -23.28151512145996 
model_pd.lagr.mean(): -23.16485595703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0095], device='cuda:0')), ('power', tensor([-23.2720], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.11665935069322586
epoch£º218	 i:1 	 global-step:4361	 l-p:0.13293080031871796
epoch£º218	 i:2 	 global-step:4362	 l-p:0.1288754791021347
epoch£º218	 i:3 	 global-step:4363	 l-p:0.11972744017839432
epoch£º218	 i:4 	 global-step:4364	 l-p:0.09157811850309372
epoch£º218	 i:5 	 global-step:4365	 l-p:0.08951429277658463
epoch£º218	 i:6 	 global-step:4366	 l-p:0.12329592555761337
epoch£º218	 i:7 	 global-step:4367	 l-p:0.005781273823231459
epoch£º218	 i:8 	 global-step:4368	 l-p:0.07259789109230042
epoch£º218	 i:9 	 global-step:4369	 l-p:0.12800750136375427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8285, 3.8308, 3.8284],
        [3.8285, 3.8343, 3.8285],
        [3.8285, 3.9336, 3.8921],
        [3.8285, 4.5430, 4.9442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.11267852783203125 
model_pd.l_d.mean(): -22.05988121032715 
model_pd.lagr.mean(): -21.947202682495117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0807], device='cuda:0')), ('power', tensor([-22.1406], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.11267852783203125
epoch£º219	 i:1 	 global-step:4381	 l-p:0.09875976294279099
epoch£º219	 i:2 	 global-step:4382	 l-p:0.12298885732889175
epoch£º219	 i:3 	 global-step:4383	 l-p:0.1609700322151184
epoch£º219	 i:4 	 global-step:4384	 l-p:0.010003375820815563
epoch£º219	 i:5 	 global-step:4385	 l-p:-0.017592396587133408
epoch£º219	 i:6 	 global-step:4386	 l-p:0.03912748396396637
epoch£º219	 i:7 	 global-step:4387	 l-p:0.11364349722862244
epoch£º219	 i:8 	 global-step:4388	 l-p:0.10391015559434891
epoch£º219	 i:9 	 global-step:4389	 l-p:0.1250019073486328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8789, 3.8790, 3.8789],
        [3.8789, 3.9205, 3.8911],
        [3.8789, 3.8789, 3.8789],
        [3.8789, 3.8862, 3.8793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.09775004535913467 
model_pd.l_d.mean(): -23.06137466430664 
model_pd.lagr.mean(): -22.963624954223633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0089], device='cuda:0')), ('power', tensor([-23.0525], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.09775004535913467
epoch£º220	 i:1 	 global-step:4401	 l-p:0.09519018232822418
epoch£º220	 i:2 	 global-step:4402	 l-p:0.2564520835876465
epoch£º220	 i:3 	 global-step:4403	 l-p:0.11229117214679718
epoch£º220	 i:4 	 global-step:4404	 l-p:0.11213857680559158
epoch£º220	 i:5 	 global-step:4405	 l-p:0.12307824939489365
epoch£º220	 i:6 	 global-step:4406	 l-p:0.143657386302948
epoch£º220	 i:7 	 global-step:4407	 l-p:0.04272611066699028
epoch£º220	 i:8 	 global-step:4408	 l-p:0.037772078067064285
epoch£º220	 i:9 	 global-step:4409	 l-p:0.11666543781757355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7953, 4.1126, 4.1615],
        [3.7953, 3.9909, 3.9717],
        [3.7953, 3.8679, 3.8287],
        [3.7953, 3.7953, 3.7953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.12394663691520691 
model_pd.l_d.mean(): -22.926124572753906 
model_pd.lagr.mean(): -22.80217742919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0424], device='cuda:0')), ('power', tensor([-22.9685], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.12394663691520691
epoch£º221	 i:1 	 global-step:4421	 l-p:0.10018526017665863
epoch£º221	 i:2 	 global-step:4422	 l-p:0.11333070695400238
epoch£º221	 i:3 	 global-step:4423	 l-p:0.13889415562152863
epoch£º221	 i:4 	 global-step:4424	 l-p:0.08401785045862198
epoch£º221	 i:5 	 global-step:4425	 l-p:0.12580369412899017
epoch£º221	 i:6 	 global-step:4426	 l-p:3.3184337615966797
epoch£º221	 i:7 	 global-step:4427	 l-p:0.08068250119686127
epoch£º221	 i:8 	 global-step:4428	 l-p:0.1415148824453354
epoch£º221	 i:9 	 global-step:4429	 l-p:0.21006812155246735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[3.6940, 4.3661, 4.7420],
        [3.6940, 4.0409, 4.1190],
        [3.6940, 4.3356, 4.6802],
        [3.6940, 4.0116, 4.0682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): -0.6734728813171387 
model_pd.l_d.mean(): -22.540613174438477 
model_pd.lagr.mean(): -23.214086532592773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1514], device='cuda:0')), ('power', tensor([-22.6920], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:-0.6734728813171387
epoch£º222	 i:1 	 global-step:4441	 l-p:0.11936887353658676
epoch£º222	 i:2 	 global-step:4442	 l-p:0.14419852197170258
epoch£º222	 i:3 	 global-step:4443	 l-p:0.07436292618513107
epoch£º222	 i:4 	 global-step:4444	 l-p:0.12014974653720856
epoch£º222	 i:5 	 global-step:4445	 l-p:0.11664892733097076
epoch£º222	 i:6 	 global-step:4446	 l-p:0.11202128976583481
epoch£º222	 i:7 	 global-step:4447	 l-p:0.09228264540433884
epoch£º222	 i:8 	 global-step:4448	 l-p:0.11983796954154968
epoch£º222	 i:9 	 global-step:4449	 l-p:-0.011963739059865475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6316, 3.6315, 3.6316],
        [3.6316, 3.6945, 3.6566],
        [3.6316, 3.7487, 3.7104],
        [3.6316, 3.6349, 3.6307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.1287854164838791 
model_pd.l_d.mean(): -23.351181030273438 
model_pd.lagr.mean(): -23.222394943237305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0605], device='cuda:0')), ('power', tensor([-23.4116], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.1287854164838791
epoch£º223	 i:1 	 global-step:4461	 l-p:0.11443118751049042
epoch£º223	 i:2 	 global-step:4462	 l-p:0.07010224461555481
epoch£º223	 i:3 	 global-step:4463	 l-p:0.07031474262475967
epoch£º223	 i:4 	 global-step:4464	 l-p:0.05388989299535751
epoch£º223	 i:5 	 global-step:4465	 l-p:0.12503215670585632
epoch£º223	 i:6 	 global-step:4466	 l-p:0.1560160517692566
epoch£º223	 i:7 	 global-step:4467	 l-p:0.21416911482810974
epoch£º223	 i:8 	 global-step:4468	 l-p:0.12478319555521011
epoch£º223	 i:9 	 global-step:4469	 l-p:0.12272174656391144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6001, 3.6001, 3.6001],
        [3.6001, 3.6001, 3.6001],
        [3.6001, 3.6110, 3.5986],
        [3.6001, 3.5999, 3.6000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.11214656382799149 
model_pd.l_d.mean(): -23.296894073486328 
model_pd.lagr.mean(): -23.18474769592285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0613], device='cuda:0')), ('power', tensor([-23.3582], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.11214656382799149
epoch£º224	 i:1 	 global-step:4481	 l-p:0.10947173833847046
epoch£º224	 i:2 	 global-step:4482	 l-p:0.12349434942007065
epoch£º224	 i:3 	 global-step:4483	 l-p:0.16572539508342743
epoch£º224	 i:4 	 global-step:4484	 l-p:0.08500689268112183
epoch£º224	 i:5 	 global-step:4485	 l-p:0.12365640699863434
epoch£º224	 i:6 	 global-step:4486	 l-p:0.12591569125652313
epoch£º224	 i:7 	 global-step:4487	 l-p:-0.08690670132637024
epoch£º224	 i:8 	 global-step:4488	 l-p:0.10989770293235779
epoch£º224	 i:9 	 global-step:4489	 l-p:0.1105928122997284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5825, 3.5825, 3.5825],
        [3.5825, 3.5825, 3.5825],
        [3.5825, 3.5825, 3.5825],
        [3.5825, 3.6471, 3.6084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.10906476527452469 
model_pd.l_d.mean(): -22.70089340209961 
model_pd.lagr.mean(): -22.591829299926758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1235], device='cuda:0')), ('power', tensor([-22.8244], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.10906476527452469
epoch£º225	 i:1 	 global-step:4501	 l-p:0.0349109061062336
epoch£º225	 i:2 	 global-step:4502	 l-p:0.0988604873418808
epoch£º225	 i:3 	 global-step:4503	 l-p:0.12549284100532532
epoch£º225	 i:4 	 global-step:4504	 l-p:0.1264520287513733
epoch£º225	 i:5 	 global-step:4505	 l-p:0.12524843215942383
epoch£º225	 i:6 	 global-step:4506	 l-p:0.1441686451435089
epoch£º225	 i:7 	 global-step:4507	 l-p:0.23940519988536835
epoch£º225	 i:8 	 global-step:4508	 l-p:0.07245156913995743
epoch£º225	 i:9 	 global-step:4509	 l-p:0.1322832703590393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5898, 3.6739, 3.6332],
        [3.5898, 3.9073, 3.9722],
        [3.5898, 3.6192, 3.5929],
        [3.5898, 3.5898, 3.5898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.10042674839496613 
model_pd.l_d.mean(): -23.291183471679688 
model_pd.lagr.mean(): -23.19075584411621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0810], device='cuda:0')), ('power', tensor([-23.3721], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.10042674839496613
epoch£º226	 i:1 	 global-step:4521	 l-p:0.13062268495559692
epoch£º226	 i:2 	 global-step:4522	 l-p:0.12755954265594482
epoch£º226	 i:3 	 global-step:4523	 l-p:0.2788507640361786
epoch£º226	 i:4 	 global-step:4524	 l-p:0.12573140859603882
epoch£º226	 i:5 	 global-step:4525	 l-p:0.10473346710205078
epoch£º226	 i:6 	 global-step:4526	 l-p:0.07795743644237518
epoch£º226	 i:7 	 global-step:4527	 l-p:0.117650106549263
epoch£º226	 i:8 	 global-step:4528	 l-p:0.12421915680170059
epoch£º226	 i:9 	 global-step:4529	 l-p:0.08003195375204086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5936, 3.6512, 3.6137],
        [3.5936, 3.5933, 3.5935],
        [3.5936, 3.6007, 3.5916],
        [3.5936, 3.5935, 3.5936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.1258614957332611 
model_pd.l_d.mean(): -22.868223190307617 
model_pd.lagr.mean(): -22.742361068725586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1106], device='cuda:0')), ('power', tensor([-22.9788], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.1258614957332611
epoch£º227	 i:1 	 global-step:4541	 l-p:0.09013407677412033
epoch£º227	 i:2 	 global-step:4542	 l-p:0.11833290010690689
epoch£º227	 i:3 	 global-step:4543	 l-p:0.12125028669834137
epoch£º227	 i:4 	 global-step:4544	 l-p:0.03987422212958336
epoch£º227	 i:5 	 global-step:4545	 l-p:0.12365353107452393
epoch£º227	 i:6 	 global-step:4546	 l-p:0.039619289338588715
epoch£º227	 i:7 	 global-step:4547	 l-p:0.15621627867221832
epoch£º227	 i:8 	 global-step:4548	 l-p:0.09853514283895493
epoch£º227	 i:9 	 global-step:4549	 l-p:0.13541296124458313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6872, 3.6872, 3.6872],
        [3.6872, 3.6872, 3.6872],
        [3.6872, 3.6871, 3.6872],
        [3.6872, 3.7005, 3.6865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.12008482962846756 
model_pd.l_d.mean(): -23.2222843170166 
model_pd.lagr.mean(): -23.10219955444336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0561], device='cuda:0')), ('power', tensor([-23.2784], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.12008482962846756
epoch£º228	 i:1 	 global-step:4561	 l-p:0.10788216441869736
epoch£º228	 i:2 	 global-step:4562	 l-p:0.11648263782262802
epoch£º228	 i:3 	 global-step:4563	 l-p:0.12764078378677368
epoch£º228	 i:4 	 global-step:4564	 l-p:0.4551751911640167
epoch£º228	 i:5 	 global-step:4565	 l-p:0.5535106658935547
epoch£º228	 i:6 	 global-step:4566	 l-p:0.12124408781528473
epoch£º228	 i:7 	 global-step:4567	 l-p:0.13423363864421844
epoch£º228	 i:8 	 global-step:4568	 l-p:0.1048019751906395
epoch£º228	 i:9 	 global-step:4569	 l-p:0.08592988550662994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6929, 3.6928, 3.6929],
        [3.6929, 4.0504, 4.1371],
        [3.6929, 3.6979, 3.6917],
        [3.6929, 3.6955, 3.6920]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.10483293980360031 
model_pd.l_d.mean(): -21.905366897583008 
model_pd.lagr.mean(): -21.800533294677734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1600], device='cuda:0')), ('power', tensor([-22.0653], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.10483293980360031
epoch£º229	 i:1 	 global-step:4581	 l-p:0.09969204664230347
epoch£º229	 i:2 	 global-step:4582	 l-p:0.11031133681535721
epoch£º229	 i:3 	 global-step:4583	 l-p:0.12472875416278839
epoch£º229	 i:4 	 global-step:4584	 l-p:0.15298832952976227
epoch£º229	 i:5 	 global-step:4585	 l-p:0.13107606768608093
epoch£º229	 i:6 	 global-step:4586	 l-p:0.11974488943815231
epoch£º229	 i:7 	 global-step:4587	 l-p:0.2070736289024353
epoch£º229	 i:8 	 global-step:4588	 l-p:1.3685111999511719
epoch£º229	 i:9 	 global-step:4589	 l-p:0.15858666598796844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[3.7232, 3.7317, 3.7221],
        [3.7232, 4.3523, 4.6807],
        [3.7232, 3.7361, 3.7225],
        [3.7232, 3.8417, 3.8016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.1429859697818756 
model_pd.l_d.mean(): -22.187419891357422 
model_pd.lagr.mean(): -22.04443359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1254], device='cuda:0')), ('power', tensor([-22.3128], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.1429859697818756
epoch£º230	 i:1 	 global-step:4601	 l-p:0.12800468504428864
epoch£º230	 i:2 	 global-step:4602	 l-p:0.12041803449392319
epoch£º230	 i:3 	 global-step:4603	 l-p:0.12326746433973312
epoch£º230	 i:4 	 global-step:4604	 l-p:0.09512796998023987
epoch£º230	 i:5 	 global-step:4605	 l-p:0.20122353732585907
epoch£º230	 i:6 	 global-step:4606	 l-p:0.1119428500533104
epoch£º230	 i:7 	 global-step:4607	 l-p:0.08574166148900986
epoch£º230	 i:8 	 global-step:4608	 l-p:0.11592371761798859
epoch£º230	 i:9 	 global-step:4609	 l-p:0.10552190244197845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7996, 3.7996, 3.7996],
        [3.7996, 3.7996, 3.7996],
        [3.7996, 4.4322, 4.7542],
        [3.7996, 3.8316, 3.8052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.19546344876289368 
model_pd.l_d.mean(): -22.89893913269043 
model_pd.lagr.mean(): -22.703475952148438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0603], device='cuda:0')), ('power', tensor([-22.9592], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.19546344876289368
epoch£º231	 i:1 	 global-step:4621	 l-p:0.11410193890333176
epoch£º231	 i:2 	 global-step:4622	 l-p:0.030201971530914307
epoch£º231	 i:3 	 global-step:4623	 l-p:0.13285504281520844
epoch£º231	 i:4 	 global-step:4624	 l-p:0.11554866284132004
epoch£º231	 i:5 	 global-step:4625	 l-p:0.11809605360031128
epoch£º231	 i:6 	 global-step:4626	 l-p:0.09329047799110413
epoch£º231	 i:7 	 global-step:4627	 l-p:0.08869700133800507
epoch£º231	 i:8 	 global-step:4628	 l-p:0.08652500808238983
epoch£º231	 i:9 	 global-step:4629	 l-p:0.09676449745893478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7630, 3.7630, 3.7630],
        [3.7630, 3.7784, 3.7628],
        [3.7630, 4.0310, 4.0508],
        [3.7630, 3.7632, 3.7628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.1317528933286667 
model_pd.l_d.mean(): -23.264009475708008 
model_pd.lagr.mean(): -23.13225746154785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0222], device='cuda:0')), ('power', tensor([-23.2862], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.1317528933286667
epoch£º232	 i:1 	 global-step:4641	 l-p:0.1040012314915657
epoch£º232	 i:2 	 global-step:4642	 l-p:0.09719100594520569
epoch£º232	 i:3 	 global-step:4643	 l-p:0.1095111221075058
epoch£º232	 i:4 	 global-step:4644	 l-p:0.10953094065189362
epoch£º232	 i:5 	 global-step:4645	 l-p:0.1399146169424057
epoch£º232	 i:6 	 global-step:4646	 l-p:0.11508172750473022
epoch£º232	 i:7 	 global-step:4647	 l-p:0.06223546341061592
epoch£º232	 i:8 	 global-step:4648	 l-p:0.016726402565836906
epoch£º232	 i:9 	 global-step:4649	 l-p:0.10496978461742401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6180, 3.6336, 3.6158],
        [3.6180, 3.6225, 3.6156],
        [3.6180, 3.7853, 3.7606],
        [3.6180, 4.1204, 4.3392]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.15577451884746552 
model_pd.l_d.mean(): -22.858592987060547 
model_pd.lagr.mean(): -22.702817916870117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1427], device='cuda:0')), ('power', tensor([-23.0013], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.15577451884746552
epoch£º233	 i:1 	 global-step:4661	 l-p:0.11929338425397873
epoch£º233	 i:2 	 global-step:4662	 l-p:0.02518165111541748
epoch£º233	 i:3 	 global-step:4663	 l-p:0.11875490844249725
epoch£º233	 i:4 	 global-step:4664	 l-p:0.05189771577715874
epoch£º233	 i:5 	 global-step:4665	 l-p:0.11973528563976288
epoch£º233	 i:6 	 global-step:4666	 l-p:0.0637282133102417
epoch£º233	 i:7 	 global-step:4667	 l-p:0.1222481057047844
epoch£º233	 i:8 	 global-step:4668	 l-p:0.4180300533771515
epoch£º233	 i:9 	 global-step:4669	 l-p:0.13177482783794403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7176, 3.7177, 3.7173],
        [3.7176, 3.7202, 3.7165],
        [3.7176, 3.7228, 3.7161],
        [3.7176, 3.7176, 3.7176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.1264781802892685 
model_pd.l_d.mean(): -22.849910736083984 
model_pd.lagr.mean(): -22.723432540893555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0981], device='cuda:0')), ('power', tensor([-22.9480], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.1264781802892685
epoch£º234	 i:1 	 global-step:4681	 l-p:0.11954346299171448
epoch£º234	 i:2 	 global-step:4682	 l-p:0.12307192385196686
epoch£º234	 i:3 	 global-step:4683	 l-p:0.3624638617038727
epoch£º234	 i:4 	 global-step:4684	 l-p:0.09999416023492813
epoch£º234	 i:5 	 global-step:4685	 l-p:0.11635423451662064
epoch£º234	 i:6 	 global-step:4686	 l-p:0.19099344313144684
epoch£º234	 i:7 	 global-step:4687	 l-p:0.12512001395225525
epoch£º234	 i:8 	 global-step:4688	 l-p:0.08081437647342682
epoch£º234	 i:9 	 global-step:4689	 l-p:0.10562659054994583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7294, 4.1849, 4.3493],
        [3.7294, 3.7294, 3.7294],
        [3.7294, 3.7298, 3.7290],
        [3.7294, 3.7294, 3.7294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.10770761221647263 
model_pd.l_d.mean(): -23.19947052001953 
model_pd.lagr.mean(): -23.09176254272461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0299], device='cuda:0')), ('power', tensor([-23.2294], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.10770761221647263
epoch£º235	 i:1 	 global-step:4701	 l-p:0.107538141310215
epoch£º235	 i:2 	 global-step:4702	 l-p:0.1251300573348999
epoch£º235	 i:3 	 global-step:4703	 l-p:0.42235106229782104
epoch£º235	 i:4 	 global-step:4704	 l-p:0.11993007361888885
epoch£º235	 i:5 	 global-step:4705	 l-p:0.37858524918556213
epoch£º235	 i:6 	 global-step:4706	 l-p:0.1158280000090599
epoch£º235	 i:7 	 global-step:4707	 l-p:-0.11522984504699707
epoch£º235	 i:8 	 global-step:4708	 l-p:0.10839442908763885
epoch£º235	 i:9 	 global-step:4709	 l-p:0.09793101996183395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7306, 3.8654, 3.8276],
        [3.7306, 3.7305, 3.7306],
        [3.7306, 3.8152, 3.7727],
        [3.7306, 3.7418, 3.7289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.12528595328330994 
model_pd.l_d.mean(): -23.169822692871094 
model_pd.lagr.mean(): -23.044536590576172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0422], device='cuda:0')), ('power', tensor([-23.2120], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.12528595328330994
epoch£º236	 i:1 	 global-step:4721	 l-p:0.18562564253807068
epoch£º236	 i:2 	 global-step:4722	 l-p:0.198973149061203
epoch£º236	 i:3 	 global-step:4723	 l-p:0.1217736005783081
epoch£º236	 i:4 	 global-step:4724	 l-p:0.12429061532020569
epoch£º236	 i:5 	 global-step:4725	 l-p:0.062200963497161865
epoch£º236	 i:6 	 global-step:4726	 l-p:0.0994814857840538
epoch£º236	 i:7 	 global-step:4727	 l-p:0.12026071548461914
epoch£º236	 i:8 	 global-step:4728	 l-p:0.12412747740745544
epoch£º236	 i:9 	 global-step:4729	 l-p:0.1603979766368866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7486, 4.2690, 4.4897],
        [3.7486, 3.8000, 3.7635],
        [3.7486, 3.7486, 3.7486],
        [3.7486, 3.8344, 3.7916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.1010083332657814 
model_pd.l_d.mean(): -22.53696060180664 
model_pd.lagr.mean(): -22.43595314025879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0959], device='cuda:0')), ('power', tensor([-22.6329], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.1010083332657814
epoch£º237	 i:1 	 global-step:4741	 l-p:0.1282545030117035
epoch£º237	 i:2 	 global-step:4742	 l-p:0.12947091460227966
epoch£º237	 i:3 	 global-step:4743	 l-p:0.11628690361976624
epoch£º237	 i:4 	 global-step:4744	 l-p:0.1012735366821289
epoch£º237	 i:5 	 global-step:4745	 l-p:0.6582921743392944
epoch£º237	 i:6 	 global-step:4746	 l-p:0.21956457197666168
epoch£º237	 i:7 	 global-step:4747	 l-p:0.09306260198354721
epoch£º237	 i:8 	 global-step:4748	 l-p:0.17880843579769135
epoch£º237	 i:9 	 global-step:4749	 l-p:0.0908295139670372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7505, 4.3471, 4.6404],
        [3.7505, 3.7768, 3.7521],
        [3.7505, 4.0853, 4.1510],
        [3.7505, 3.7505, 3.7505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.15667462348937988 
model_pd.l_d.mean(): -20.969863891601562 
model_pd.lagr.mean(): -20.813188552856445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2310], device='cuda:0')), ('power', tensor([-21.2009], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.15667462348937988
epoch£º238	 i:1 	 global-step:4761	 l-p:0.11397168785333633
epoch£º238	 i:2 	 global-step:4762	 l-p:0.1565457284450531
epoch£º238	 i:3 	 global-step:4763	 l-p:0.090561643242836
epoch£º238	 i:4 	 global-step:4764	 l-p:0.11408275365829468
epoch£º238	 i:5 	 global-step:4765	 l-p:0.059940677136182785
epoch£º238	 i:6 	 global-step:4766	 l-p:-0.2579403221607208
epoch£º238	 i:7 	 global-step:4767	 l-p:0.11992210149765015
epoch£º238	 i:8 	 global-step:4768	 l-p:0.37495237588882446
epoch£º238	 i:9 	 global-step:4769	 l-p:0.09589020162820816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[3.9168, 4.2809, 4.3557],
        [3.9168, 4.5044, 4.7692],
        [3.9168, 3.9790, 3.9400],
        [3.9168, 4.1787, 4.1869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.11213348060846329 
model_pd.l_d.mean(): -23.01787567138672 
model_pd.lagr.mean(): -22.905742645263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0033], device='cuda:0')), ('power', tensor([-23.0212], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.11213348060846329
epoch£º239	 i:1 	 global-step:4781	 l-p:0.11290349066257477
epoch£º239	 i:2 	 global-step:4782	 l-p:0.10979292541742325
epoch£º239	 i:3 	 global-step:4783	 l-p:0.13980622589588165
epoch£º239	 i:4 	 global-step:4784	 l-p:-2.6492440700531006
epoch£º239	 i:5 	 global-step:4785	 l-p:-0.0052748010493814945
epoch£º239	 i:6 	 global-step:4786	 l-p:-0.5167500376701355
epoch£º239	 i:7 	 global-step:4787	 l-p:0.1177527979016304
epoch£º239	 i:8 	 global-step:4788	 l-p:0.11105436086654663
epoch£º239	 i:9 	 global-step:4789	 l-p:0.10558289289474487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8343, 4.0794, 4.0825],
        [3.8343, 3.8597, 3.8364],
        [3.8343, 3.8822, 3.8473],
        [3.8343, 3.8342, 3.8343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.1124923899769783 
model_pd.l_d.mean(): -23.280088424682617 
model_pd.lagr.mean(): -23.1675968170166 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0226], device='cuda:0')), ('power', tensor([-23.2575], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.1124923899769783
epoch£º240	 i:1 	 global-step:4801	 l-p:0.09256517142057419
epoch£º240	 i:2 	 global-step:4802	 l-p:0.13859595358371735
epoch£º240	 i:3 	 global-step:4803	 l-p:0.10415547341108322
epoch£º240	 i:4 	 global-step:4804	 l-p:0.10758840292692184
epoch£º240	 i:5 	 global-step:4805	 l-p:0.10466071963310242
epoch£º240	 i:6 	 global-step:4806	 l-p:0.1630641520023346
epoch£º240	 i:7 	 global-step:4807	 l-p:0.11037328839302063
epoch£º240	 i:8 	 global-step:4808	 l-p:-1.033389687538147
epoch£º240	 i:9 	 global-step:4809	 l-p:0.1244952455163002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7091, 3.7354, 3.7097],
        [3.7091, 3.7091, 3.7091],
        [3.7091, 4.0386, 4.1039],
        [3.7091, 4.2122, 4.4219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.12058187276124954 
model_pd.l_d.mean(): -23.148656845092773 
model_pd.lagr.mean(): -23.028074264526367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0550], device='cuda:0')), ('power', tensor([-23.2036], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.12058187276124954
epoch£º241	 i:1 	 global-step:4821	 l-p:0.14581729471683502
epoch£º241	 i:2 	 global-step:4822	 l-p:0.11096717417240143
epoch£º241	 i:3 	 global-step:4823	 l-p:0.12389133125543594
epoch£º241	 i:4 	 global-step:4824	 l-p:0.11810927838087082
epoch£º241	 i:5 	 global-step:4825	 l-p:0.10124172270298004
epoch£º241	 i:6 	 global-step:4826	 l-p:0.9151229858398438
epoch£º241	 i:7 	 global-step:4827	 l-p:0.18518900871276855
epoch£º241	 i:8 	 global-step:4828	 l-p:0.10178951174020767
epoch£º241	 i:9 	 global-step:4829	 l-p:0.09336090087890625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7312, 3.9097, 3.8854],
        [3.7312, 3.7371, 3.7288],
        [3.7312, 3.7311, 3.7312],
        [3.7312, 4.1795, 4.3382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.13726769387722015 
model_pd.l_d.mean(): -23.04534912109375 
model_pd.lagr.mean(): -22.9080810546875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0617], device='cuda:0')), ('power', tensor([-23.1070], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.13726769387722015
epoch£º242	 i:1 	 global-step:4841	 l-p:0.08998438715934753
epoch£º242	 i:2 	 global-step:4842	 l-p:0.2926260232925415
epoch£º242	 i:3 	 global-step:4843	 l-p:0.1605197936296463
epoch£º242	 i:4 	 global-step:4844	 l-p:0.15915675461292267
epoch£º242	 i:5 	 global-step:4845	 l-p:0.07922899723052979
epoch£º242	 i:6 	 global-step:4846	 l-p:0.12635236978530884
epoch£º242	 i:7 	 global-step:4847	 l-p:0.10388841480016708
epoch£º242	 i:8 	 global-step:4848	 l-p:0.0030951928347349167
epoch£º242	 i:9 	 global-step:4849	 l-p:-0.010745754465460777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8716, 3.8878, 3.8713],
        [3.8716, 3.8716, 3.8716],
        [3.8716, 3.8716, 3.8716],
        [3.8716, 4.3144, 4.4548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.11499429494142532 
model_pd.l_d.mean(): -22.862775802612305 
model_pd.lagr.mean(): -22.74778175354004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0034], device='cuda:0')), ('power', tensor([-22.8662], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.11499429494142532
epoch£º243	 i:1 	 global-step:4861	 l-p:0.11945777386426926
epoch£º243	 i:2 	 global-step:4862	 l-p:-0.09161422401666641
epoch£º243	 i:3 	 global-step:4863	 l-p:0.12200764566659927
epoch£º243	 i:4 	 global-step:4864	 l-p:0.14192168414592743
epoch£º243	 i:5 	 global-step:4865	 l-p:0.11423186212778091
epoch£º243	 i:6 	 global-step:4866	 l-p:0.5392011404037476
epoch£º243	 i:7 	 global-step:4867	 l-p:0.1258680671453476
epoch£º243	 i:8 	 global-step:4868	 l-p:-0.07664739340543747
epoch£º243	 i:9 	 global-step:4869	 l-p:0.07297174632549286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[3.8561, 3.8647, 3.8548],
        [3.8561, 4.0626, 4.0456],
        [3.8561, 3.8786, 3.8571],
        [3.8561, 4.2564, 4.3638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.15190154314041138 
model_pd.l_d.mean(): -23.128679275512695 
model_pd.lagr.mean(): -22.976778030395508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0070], device='cuda:0')), ('power', tensor([-23.1356], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.15190154314041138
epoch£º244	 i:1 	 global-step:4881	 l-p:0.10478293895721436
epoch£º244	 i:2 	 global-step:4882	 l-p:0.11932373046875
epoch£º244	 i:3 	 global-step:4883	 l-p:0.12994222342967987
epoch£º244	 i:4 	 global-step:4884	 l-p:0.07176391780376434
epoch£º244	 i:5 	 global-step:4885	 l-p:0.05570691451430321
epoch£º244	 i:6 	 global-step:4886	 l-p:0.09462810307741165
epoch£º244	 i:7 	 global-step:4887	 l-p:0.11258964240550995
epoch£º244	 i:8 	 global-step:4888	 l-p:0.15899954736232758
epoch£º244	 i:9 	 global-step:4889	 l-p:0.11697021126747131
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7724, 3.7724, 3.7724],
        [3.7724, 3.8695, 3.8256],
        [3.7724, 3.9699, 3.9520],
        [3.7724, 3.7724, 3.7724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.1262499988079071 
model_pd.l_d.mean(): -22.809743881225586 
model_pd.lagr.mean(): -22.683494567871094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0846], device='cuda:0')), ('power', tensor([-22.8943], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.1262499988079071
epoch£º245	 i:1 	 global-step:4901	 l-p:0.11590087413787842
epoch£º245	 i:2 	 global-step:4902	 l-p:0.09253139048814774
epoch£º245	 i:3 	 global-step:4903	 l-p:0.08574467152357101
epoch£º245	 i:4 	 global-step:4904	 l-p:0.2214457243680954
epoch£º245	 i:5 	 global-step:4905	 l-p:0.1187179908156395
epoch£º245	 i:6 	 global-step:4906	 l-p:0.12705928087234497
epoch£º245	 i:7 	 global-step:4907	 l-p:0.12173780053853989
epoch£º245	 i:8 	 global-step:4908	 l-p:0.04790297523140907
epoch£º245	 i:9 	 global-step:4909	 l-p:0.008192204870283604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6567, 3.9340, 3.9669],
        [3.6567, 3.6565, 3.6567],
        [3.6567, 3.6752, 3.6532],
        [3.6567, 3.6567, 3.6567]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.10023442655801773 
model_pd.l_d.mean(): -22.487422943115234 
model_pd.lagr.mean(): -22.387187957763672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1418], device='cuda:0')), ('power', tensor([-22.6292], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.10023442655801773
epoch£º246	 i:1 	 global-step:4921	 l-p:0.11341280490159988
epoch£º246	 i:2 	 global-step:4922	 l-p:0.11958596855401993
epoch£º246	 i:3 	 global-step:4923	 l-p:0.09847915172576904
epoch£º246	 i:4 	 global-step:4924	 l-p:0.11015134304761887
epoch£º246	 i:5 	 global-step:4925	 l-p:0.076043039560318
epoch£º246	 i:6 	 global-step:4926	 l-p:0.8105543255805969
epoch£º246	 i:7 	 global-step:4927	 l-p:0.13876715302467346
epoch£º246	 i:8 	 global-step:4928	 l-p:0.09275361150503159
epoch£º246	 i:9 	 global-step:4929	 l-p:0.0864928811788559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6014, 3.6014, 3.6014],
        [3.6014, 3.6006, 3.5989],
        [3.6014, 3.6385, 3.6038],
        [3.6014, 3.6014, 3.6014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.11035551130771637 
model_pd.l_d.mean(): -22.8499698638916 
model_pd.lagr.mean(): -22.739614486694336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1090], device='cuda:0')), ('power', tensor([-22.9590], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.11035551130771637
epoch£º247	 i:1 	 global-step:4941	 l-p:0.10308824479579926
epoch£º247	 i:2 	 global-step:4942	 l-p:0.12187637388706207
epoch£º247	 i:3 	 global-step:4943	 l-p:0.13049326837062836
epoch£º247	 i:4 	 global-step:4944	 l-p:0.04993254691362381
epoch£º247	 i:5 	 global-step:4945	 l-p:0.1827409565448761
epoch£º247	 i:6 	 global-step:4946	 l-p:0.1247556284070015
epoch£º247	 i:7 	 global-step:4947	 l-p:0.04685916379094124
epoch£º247	 i:8 	 global-step:4948	 l-p:0.2639980912208557
epoch£º247	 i:9 	 global-step:4949	 l-p:0.11079414188861847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7830, 3.7830, 3.7830],
        [3.7830, 3.7830, 3.7830],
        [3.7830, 3.7847, 3.7814],
        [3.7830, 3.7830, 3.7830]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.1318933665752411 
model_pd.l_d.mean(): -23.27016258239746 
model_pd.lagr.mean(): -23.138269424438477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0138], device='cuda:0')), ('power', tensor([-23.2839], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.1318933665752411
epoch£º248	 i:1 	 global-step:4961	 l-p:0.13676117360591888
epoch£º248	 i:2 	 global-step:4962	 l-p:0.0562380813062191
epoch£º248	 i:3 	 global-step:4963	 l-p:0.04368503391742706
epoch£º248	 i:4 	 global-step:4964	 l-p:0.11277686804533005
epoch£º248	 i:5 	 global-step:4965	 l-p:0.20573721826076508
epoch£º248	 i:6 	 global-step:4966	 l-p:0.12633642554283142
epoch£º248	 i:7 	 global-step:4967	 l-p:-0.08197100460529327
epoch£º248	 i:8 	 global-step:4968	 l-p:0.10168992727994919
epoch£º248	 i:9 	 global-step:4969	 l-p:0.09833396971225739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[4.0107, 4.3302, 4.3682],
        [4.0107, 4.3682, 4.4319],
        [4.0107, 4.0682, 4.0298],
        [4.0107, 4.5985, 4.8537]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.11972349137067795 
model_pd.l_d.mean(): -23.20427894592285 
model_pd.lagr.mean(): -23.08455467224121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0662], device='cuda:0')), ('power', tensor([-23.1381], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.11972349137067795
epoch£º249	 i:1 	 global-step:4981	 l-p:0.10926742106676102
epoch£º249	 i:2 	 global-step:4982	 l-p:0.19738565385341644
epoch£º249	 i:3 	 global-step:4983	 l-p:0.1125534251332283
epoch£º249	 i:4 	 global-step:4984	 l-p:0.13578635454177856
epoch£º249	 i:5 	 global-step:4985	 l-p:0.14808136224746704
epoch£º249	 i:6 	 global-step:4986	 l-p:0.1244838610291481
epoch£º249	 i:7 	 global-step:4987	 l-p:0.11797364801168442
epoch£º249	 i:8 	 global-step:4988	 l-p:0.09907914698123932
epoch£º249	 i:9 	 global-step:4989	 l-p:0.11485515534877777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9996, 4.1414, 4.0994],
        [3.9996, 3.9996, 3.9996],
        [3.9996, 4.0091, 3.9990],
        [3.9996, 3.9996, 3.9996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.1324138641357422 
model_pd.l_d.mean(): -22.461715698242188 
model_pd.lagr.mean(): -22.329301834106445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0146], device='cuda:0')), ('power', tensor([-22.4763], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.1324138641357422
epoch£º250	 i:1 	 global-step:5001	 l-p:0.1084255576133728
epoch£º250	 i:2 	 global-step:5002	 l-p:0.11033494025468826
epoch£º250	 i:3 	 global-step:5003	 l-p:0.10954707860946655
epoch£º250	 i:4 	 global-step:5004	 l-p:0.10525643080472946
epoch£º250	 i:5 	 global-step:5005	 l-p:0.19825880229473114
epoch£º250	 i:6 	 global-step:5006	 l-p:0.09240423142910004
epoch£º250	 i:7 	 global-step:5007	 l-p:0.11919905990362167
epoch£º250	 i:8 	 global-step:5008	 l-p:0.045605357736349106
epoch£º250	 i:9 	 global-step:5009	 l-p:0.08952435106039047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8339, 3.9331, 3.8882],
        [3.8339, 3.8338, 3.8339],
        [3.8339, 3.8801, 3.8441],
        [3.8339, 3.8339, 3.8339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.04400965943932533 
model_pd.l_d.mean(): -22.931293487548828 
model_pd.lagr.mean(): -22.887283325195312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0424], device='cuda:0')), ('power', tensor([-22.9737], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.04400965943932533
epoch£º251	 i:1 	 global-step:5021	 l-p:0.13481420278549194
epoch£º251	 i:2 	 global-step:5022	 l-p:0.15457536280155182
epoch£º251	 i:3 	 global-step:5023	 l-p:0.1443236619234085
epoch£º251	 i:4 	 global-step:5024	 l-p:0.09053218364715576
epoch£º251	 i:5 	 global-step:5025	 l-p:-0.017374346032738686
epoch£º251	 i:6 	 global-step:5026	 l-p:0.09537570923566818
epoch£º251	 i:7 	 global-step:5027	 l-p:0.12799520790576935
epoch£º251	 i:8 	 global-step:5028	 l-p:0.10903243720531464
epoch£º251	 i:9 	 global-step:5029	 l-p:0.12162893265485764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7798, 3.7796, 3.7798],
        [3.7798, 4.3924, 4.6985],
        [3.7798, 3.8212, 3.7863],
        [3.7798, 3.8789, 3.8341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.1003119945526123 
model_pd.l_d.mean(): -22.887378692626953 
model_pd.lagr.mean(): -22.787067413330078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0837], device='cuda:0')), ('power', tensor([-22.9711], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.1003119945526123
epoch£º252	 i:1 	 global-step:5041	 l-p:0.12359170615673065
epoch£º252	 i:2 	 global-step:5042	 l-p:0.0983697846531868
epoch£º252	 i:3 	 global-step:5043	 l-p:0.10764538496732712
epoch£º252	 i:4 	 global-step:5044	 l-p:-0.18366381525993347
epoch£º252	 i:5 	 global-step:5045	 l-p:0.11377669870853424
epoch£º252	 i:6 	 global-step:5046	 l-p:0.12238084524869919
epoch£º252	 i:7 	 global-step:5047	 l-p:0.1504167765378952
epoch£º252	 i:8 	 global-step:5048	 l-p:0.06477084755897522
epoch£º252	 i:9 	 global-step:5049	 l-p:0.12297366559505463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[3.6032, 4.1449, 4.4038],
        [3.6032, 3.6372, 3.6026],
        [3.6032, 3.6323, 3.6003],
        [3.6032, 3.8473, 3.8625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.08934465050697327 
model_pd.l_d.mean(): -23.267900466918945 
model_pd.lagr.mean(): -23.178556442260742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0827], device='cuda:0')), ('power', tensor([-23.3506], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.08934465050697327
epoch£º253	 i:1 	 global-step:5061	 l-p:0.10439544171094894
epoch£º253	 i:2 	 global-step:5062	 l-p:0.132158100605011
epoch£º253	 i:3 	 global-step:5063	 l-p:0.03705594316124916
epoch£º253	 i:4 	 global-step:5064	 l-p:0.16749493777751923
epoch£º253	 i:5 	 global-step:5065	 l-p:0.1079755574464798
epoch£º253	 i:6 	 global-step:5066	 l-p:0.13214953243732452
epoch£º253	 i:7 	 global-step:5067	 l-p:0.12563660740852356
epoch£º253	 i:8 	 global-step:5068	 l-p:0.11850367486476898
epoch£º253	 i:9 	 global-step:5069	 l-p:0.11218772828578949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7887, 3.7915, 3.7861],
        [3.7887, 3.8095, 3.7865],
        [3.7887, 3.7887, 3.7887],
        [3.7887, 3.7887, 3.7887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.0782843679189682 
model_pd.l_d.mean(): -22.974266052246094 
model_pd.lagr.mean(): -22.895980834960938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0482], device='cuda:0')), ('power', tensor([-23.0224], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.0782843679189682
epoch£º254	 i:1 	 global-step:5081	 l-p:0.1640709638595581
epoch£º254	 i:2 	 global-step:5082	 l-p:0.049116332083940506
epoch£º254	 i:3 	 global-step:5083	 l-p:-0.02195410616695881
epoch£º254	 i:4 	 global-step:5084	 l-p:0.13885527849197388
epoch£º254	 i:5 	 global-step:5085	 l-p:0.11496789008378983
epoch£º254	 i:6 	 global-step:5086	 l-p:0.1069750115275383
epoch£º254	 i:7 	 global-step:5087	 l-p:0.10523176193237305
epoch£º254	 i:8 	 global-step:5088	 l-p:0.1150076612830162
epoch£º254	 i:9 	 global-step:5089	 l-p:0.09820292145013809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[3.9840, 4.6517, 4.9872],
        [3.9840, 4.0813, 4.0352],
        [3.9840, 4.3105, 4.3552],
        [3.9840, 4.7668, 5.2203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.09238296002149582 
model_pd.l_d.mean(): -22.022809982299805 
model_pd.lagr.mean(): -21.93042755126953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0492], device='cuda:0')), ('power', tensor([-22.0720], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.09238296002149582
epoch£º255	 i:1 	 global-step:5101	 l-p:0.48099634051322937
epoch£º255	 i:2 	 global-step:5102	 l-p:0.1583731323480606
epoch£º255	 i:3 	 global-step:5103	 l-p:0.13112770020961761
epoch£º255	 i:4 	 global-step:5104	 l-p:0.13356700539588928
epoch£º255	 i:5 	 global-step:5105	 l-p:0.09958788752555847
epoch£º255	 i:6 	 global-step:5106	 l-p:0.08799566328525543
epoch£º255	 i:7 	 global-step:5107	 l-p:0.1125880628824234
epoch£º255	 i:8 	 global-step:5108	 l-p:0.11290527880191803
epoch£º255	 i:9 	 global-step:5109	 l-p:0.10369423031806946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9611, 3.9829, 3.9614],
        [3.9611, 3.9617, 3.9606],
        [3.9611, 3.9611, 3.9611],
        [3.9611, 4.2355, 4.2489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.10542348772287369 
model_pd.l_d.mean(): -23.206541061401367 
model_pd.lagr.mean(): -23.101118087768555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0503], device='cuda:0')), ('power', tensor([-23.1563], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.10542348772287369
epoch£º256	 i:1 	 global-step:5121	 l-p:0.1356031894683838
epoch£º256	 i:2 	 global-step:5122	 l-p:0.11414836347103119
epoch£º256	 i:3 	 global-step:5123	 l-p:0.07112789154052734
epoch£º256	 i:4 	 global-step:5124	 l-p:0.12773911654949188
epoch£º256	 i:5 	 global-step:5125	 l-p:0.1114494651556015
epoch£º256	 i:6 	 global-step:5126	 l-p:0.0990668460726738
epoch£º256	 i:7 	 global-step:5127	 l-p:0.12989702820777893
epoch£º256	 i:8 	 global-step:5128	 l-p:-0.09693820774555206
epoch£º256	 i:9 	 global-step:5129	 l-p:0.10367643088102341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6299, 3.6299, 3.6299],
        [3.6299, 3.6299, 3.6299],
        [3.6299, 3.7264, 3.6813],
        [3.6299, 3.6279, 3.6273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.09109725058078766 
model_pd.l_d.mean(): -22.59400749206543 
model_pd.lagr.mean(): -22.502910614013672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1070], device='cuda:0')), ('power', tensor([-22.7010], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.09109725058078766
epoch£º257	 i:1 	 global-step:5141	 l-p:0.08431942015886307
epoch£º257	 i:2 	 global-step:5142	 l-p:0.1449417918920517
epoch£º257	 i:3 	 global-step:5143	 l-p:0.14743871986865997
epoch£º257	 i:4 	 global-step:5144	 l-p:0.10567216575145721
epoch£º257	 i:5 	 global-step:5145	 l-p:0.20523391664028168
epoch£º257	 i:6 	 global-step:5146	 l-p:0.10840550810098648
epoch£º257	 i:7 	 global-step:5147	 l-p:0.09343761205673218
epoch£º257	 i:8 	 global-step:5148	 l-p:0.051662009209394455
epoch£º257	 i:9 	 global-step:5149	 l-p:0.10691146552562714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6566, 3.6563, 3.6566],
        [3.6566, 3.6650, 3.6493],
        [3.6566, 3.6566, 3.6566],
        [3.6566, 3.8750, 3.8722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.10841959714889526 
model_pd.l_d.mean(): -22.926651000976562 
model_pd.lagr.mean(): -22.8182315826416 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1039], device='cuda:0')), ('power', tensor([-23.0305], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.10841959714889526
epoch£º258	 i:1 	 global-step:5161	 l-p:0.10374733060598373
epoch£º258	 i:2 	 global-step:5162	 l-p:0.09345723688602448
epoch£º258	 i:3 	 global-step:5163	 l-p:0.12667015194892883
epoch£º258	 i:4 	 global-step:5164	 l-p:0.1279166340827942
epoch£º258	 i:5 	 global-step:5165	 l-p:0.12751361727714539
epoch£º258	 i:6 	 global-step:5166	 l-p:0.08954405784606934
epoch£º258	 i:7 	 global-step:5167	 l-p:-0.019640864804387093
epoch£º258	 i:8 	 global-step:5168	 l-p:0.10392025113105774
epoch£º258	 i:9 	 global-step:5169	 l-p:0.11528205871582031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5594, 3.7580, 3.7493],
        [3.5594, 3.5594, 3.5594],
        [3.5594, 3.5594, 3.5594],
        [3.5594, 3.5813, 3.5509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): -0.0683588758111 
model_pd.l_d.mean(): -22.15199851989746 
model_pd.lagr.mean(): -22.22035789489746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1721], device='cuda:0')), ('power', tensor([-22.3241], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:-0.0683588758111
epoch£º259	 i:1 	 global-step:5181	 l-p:0.11363638937473297
epoch£º259	 i:2 	 global-step:5182	 l-p:0.1068938672542572
epoch£º259	 i:3 	 global-step:5183	 l-p:0.11427783966064453
epoch£º259	 i:4 	 global-step:5184	 l-p:0.12741391360759735
epoch£º259	 i:5 	 global-step:5185	 l-p:0.11665746569633484
epoch£º259	 i:6 	 global-step:5186	 l-p:0.1081138476729393
epoch£º259	 i:7 	 global-step:5187	 l-p:0.09308221936225891
epoch£º259	 i:8 	 global-step:5188	 l-p:0.31088918447494507
epoch£º259	 i:9 	 global-step:5189	 l-p:0.13065263628959656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5118, 3.8077, 3.8650],
        [3.5118, 3.6432, 3.6067],
        [3.5118, 3.5118, 3.5118],
        [3.5118, 3.5358, 3.5022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.12421679496765137 
model_pd.l_d.mean(): -22.962055206298828 
model_pd.lagr.mean(): -22.837839126586914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1756], device='cuda:0')), ('power', tensor([-23.1377], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.12421679496765137
epoch£º260	 i:1 	 global-step:5201	 l-p:0.11722099035978317
epoch£º260	 i:2 	 global-step:5202	 l-p:0.1332354098558426
epoch£º260	 i:3 	 global-step:5203	 l-p:0.1628994345664978
epoch£º260	 i:4 	 global-step:5204	 l-p:0.12090752273797989
epoch£º260	 i:5 	 global-step:5205	 l-p:0.10411025583744049
epoch£º260	 i:6 	 global-step:5206	 l-p:0.12549817562103271
epoch£º260	 i:7 	 global-step:5207	 l-p:0.13140571117401123
epoch£º260	 i:8 	 global-step:5208	 l-p:0.09016864746809006
epoch£º260	 i:9 	 global-step:5209	 l-p:0.10837055742740631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5165, 3.5164, 3.5165],
        [3.5165, 3.6791, 3.6544],
        [3.5165, 3.7043, 3.6919],
        [3.5165, 3.5120, 3.5123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.12880964577198029 
model_pd.l_d.mean(): -22.93623924255371 
model_pd.lagr.mean(): -22.807430267333984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1350], device='cuda:0')), ('power', tensor([-23.0712], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.12880964577198029
epoch£º261	 i:1 	 global-step:5221	 l-p:0.12671977281570435
epoch£º261	 i:2 	 global-step:5222	 l-p:0.09682025760412216
epoch£º261	 i:3 	 global-step:5223	 l-p:-0.029168879613280296
epoch£º261	 i:4 	 global-step:5224	 l-p:0.10748729109764099
epoch£º261	 i:5 	 global-step:5225	 l-p:0.15939360857009888
epoch£º261	 i:6 	 global-step:5226	 l-p:0.1149137020111084
epoch£º261	 i:7 	 global-step:5227	 l-p:0.5064190030097961
epoch£º261	 i:8 	 global-step:5228	 l-p:0.12674503028392792
epoch£º261	 i:9 	 global-step:5229	 l-p:0.13186724483966827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4929, 3.4899, 3.4921],
        [3.4929, 3.4880, 3.4864],
        [3.4929, 3.4918, 3.4928],
        [3.4929, 3.4919, 3.4928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.10602552443742752 
model_pd.l_d.mean(): -22.558801651000977 
model_pd.lagr.mean(): -22.452775955200195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1867], device='cuda:0')), ('power', tensor([-22.7455], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.10602552443742752
epoch£º262	 i:1 	 global-step:5241	 l-p:0.12397164851427078
epoch£º262	 i:2 	 global-step:5242	 l-p:0.12998011708259583
epoch£º262	 i:3 	 global-step:5243	 l-p:0.09206484258174896
epoch£º262	 i:4 	 global-step:5244	 l-p:0.12392082065343857
epoch£º262	 i:5 	 global-step:5245	 l-p:0.12321877479553223
epoch£º262	 i:6 	 global-step:5246	 l-p:0.15425361692905426
epoch£º262	 i:7 	 global-step:5247	 l-p:0.17784611880779266
epoch£º262	 i:8 	 global-step:5248	 l-p:-0.03972690925002098
epoch£º262	 i:9 	 global-step:5249	 l-p:0.129112109541893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6206, 3.8922, 3.9239],
        [3.6206, 3.7748, 3.7429],
        [3.6206, 3.6174, 3.6182],
        [3.6206, 3.8559, 3.8645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.10953427106142044 
model_pd.l_d.mean(): -22.40976333618164 
model_pd.lagr.mean(): -22.300228118896484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1390], device='cuda:0')), ('power', tensor([-22.5487], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.10953427106142044
epoch£º263	 i:1 	 global-step:5261	 l-p:0.12550953030586243
epoch£º263	 i:2 	 global-step:5262	 l-p:0.11703021824359894
epoch£º263	 i:3 	 global-step:5263	 l-p:0.10531911253929138
epoch£º263	 i:4 	 global-step:5264	 l-p:0.07368534058332443
epoch£º263	 i:5 	 global-step:5265	 l-p:0.14665476977825165
epoch£º263	 i:6 	 global-step:5266	 l-p:0.026068005710840225
epoch£º263	 i:7 	 global-step:5267	 l-p:0.11276982724666595
epoch£º263	 i:8 	 global-step:5268	 l-p:0.11155340820550919
epoch£º263	 i:9 	 global-step:5269	 l-p:0.1138903871178627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6867, 3.6852, 3.6864],
        [3.6867, 3.9829, 4.0269],
        [3.6867, 4.1546, 4.3361],
        [3.6867, 3.8813, 3.8644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.12469369918107986 
model_pd.l_d.mean(): -23.219470977783203 
model_pd.lagr.mean(): -23.094778060913086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0633], device='cuda:0')), ('power', tensor([-23.2828], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.12469369918107986
epoch£º264	 i:1 	 global-step:5281	 l-p:0.1103769838809967
epoch£º264	 i:2 	 global-step:5282	 l-p:0.1266384869813919
epoch£º264	 i:3 	 global-step:5283	 l-p:0.13944727182388306
epoch£º264	 i:4 	 global-step:5284	 l-p:0.3413119912147522
epoch£º264	 i:5 	 global-step:5285	 l-p:0.11829748749732971
epoch£º264	 i:6 	 global-step:5286	 l-p:0.08467099815607071
epoch£º264	 i:7 	 global-step:5287	 l-p:0.12126224488019943
epoch£º264	 i:8 	 global-step:5288	 l-p:0.09671218693256378
epoch£º264	 i:9 	 global-step:5289	 l-p:0.09477502852678299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5913, 4.0830, 4.2964],
        [3.5913, 3.6574, 3.6099],
        [3.5913, 3.5913, 3.5913],
        [3.5913, 3.5912, 3.5913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.15437051653862 
model_pd.l_d.mean(): -22.723907470703125 
model_pd.lagr.mean(): -22.569536209106445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1679], device='cuda:0')), ('power', tensor([-22.8918], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.15437051653862
epoch£º265	 i:1 	 global-step:5301	 l-p:0.12288197129964828
epoch£º265	 i:2 	 global-step:5302	 l-p:0.10762859135866165
epoch£º265	 i:3 	 global-step:5303	 l-p:0.05885424464941025
epoch£º265	 i:4 	 global-step:5304	 l-p:0.016059264540672302
epoch£º265	 i:5 	 global-step:5305	 l-p:0.16101619601249695
epoch£º265	 i:6 	 global-step:5306	 l-p:-0.1731070727109909
epoch£º265	 i:7 	 global-step:5307	 l-p:0.08687926828861237
epoch£º265	 i:8 	 global-step:5308	 l-p:0.11426937580108643
epoch£º265	 i:9 	 global-step:5309	 l-p:0.10646545141935349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8135, 3.8127, 3.8134],
        [3.8135, 3.8133, 3.8135],
        [3.8135, 3.8119, 3.8119],
        [3.8135, 3.8134, 3.8135]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.08903589099645615 
model_pd.l_d.mean(): -23.079179763793945 
model_pd.lagr.mean(): -22.990144729614258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0314], device='cuda:0')), ('power', tensor([-23.1106], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.08903589099645615
epoch£º266	 i:1 	 global-step:5321	 l-p:0.13990026712417603
epoch£º266	 i:2 	 global-step:5322	 l-p:0.1225070059299469
epoch£º266	 i:3 	 global-step:5323	 l-p:0.1152624785900116
epoch£º266	 i:4 	 global-step:5324	 l-p:0.09440058469772339
epoch£º266	 i:5 	 global-step:5325	 l-p:0.040845803916454315
epoch£º266	 i:6 	 global-step:5326	 l-p:0.10810600966215134
epoch£º266	 i:7 	 global-step:5327	 l-p:0.1263323873281479
epoch£º266	 i:8 	 global-step:5328	 l-p:0.14120231568813324
epoch£º266	 i:9 	 global-step:5329	 l-p:0.0407756045460701
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8539, 3.8539, 3.8539],
        [3.8539, 3.9063, 3.8644],
        [3.8539, 3.8539, 3.8539],
        [3.8539, 3.8525, 3.8531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.014751533977687359 
model_pd.l_d.mean(): -22.788516998291016 
model_pd.lagr.mean(): -22.773765563964844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0609], device='cuda:0')), ('power', tensor([-22.8494], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.014751533977687359
epoch£º267	 i:1 	 global-step:5341	 l-p:0.13762807846069336
epoch£º267	 i:2 	 global-step:5342	 l-p:0.11982868611812592
epoch£º267	 i:3 	 global-step:5343	 l-p:0.1087438091635704
epoch£º267	 i:4 	 global-step:5344	 l-p:0.07275087386369705
epoch£º267	 i:5 	 global-step:5345	 l-p:-1.1565730571746826
epoch£º267	 i:6 	 global-step:5346	 l-p:0.10347867757081985
epoch£º267	 i:7 	 global-step:5347	 l-p:0.1183168962597847
epoch£º267	 i:8 	 global-step:5348	 l-p:0.11133895814418793
epoch£º267	 i:9 	 global-step:5349	 l-p:0.11807217448949814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8840, 3.8839, 3.8840],
        [3.8840, 4.0261, 3.9842],
        [3.8840, 3.8828, 3.8831],
        [3.8840, 3.8840, 3.8840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.06042218953371048 
model_pd.l_d.mean(): -22.965011596679688 
model_pd.lagr.mean(): -22.90458869934082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0241], device='cuda:0')), ('power', tensor([-22.9891], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.06042218953371048
epoch£º268	 i:1 	 global-step:5361	 l-p:0.111592598259449
epoch£º268	 i:2 	 global-step:5362	 l-p:0.12230867892503738
epoch£º268	 i:3 	 global-step:5363	 l-p:0.1428975611925125
epoch£º268	 i:4 	 global-step:5364	 l-p:0.0016778087010607123
epoch£º268	 i:5 	 global-step:5365	 l-p:0.09973624348640442
epoch£º268	 i:6 	 global-step:5366	 l-p:0.12247186154127121
epoch£º268	 i:7 	 global-step:5367	 l-p:0.11163626611232758
epoch£º268	 i:8 	 global-step:5368	 l-p:0.1273416131734848
epoch£º268	 i:9 	 global-step:5369	 l-p:0.11187034100294113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8160, 3.8158, 3.8160],
        [3.8160, 3.8148, 3.8157],
        [3.8160, 3.8181, 3.8105],
        [3.8160, 3.8149, 3.8128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.15228775143623352 
model_pd.l_d.mean(): -23.112342834472656 
model_pd.lagr.mean(): -22.960054397583008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0383], device='cuda:0')), ('power', tensor([-23.1506], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.15228775143623352
epoch£º269	 i:1 	 global-step:5381	 l-p:0.12057673931121826
epoch£º269	 i:2 	 global-step:5382	 l-p:0.11638110876083374
epoch£º269	 i:3 	 global-step:5383	 l-p:0.09740207344293594
epoch£º269	 i:4 	 global-step:5384	 l-p:0.1920037865638733
epoch£º269	 i:5 	 global-step:5385	 l-p:0.11035095155239105
epoch£º269	 i:6 	 global-step:5386	 l-p:0.012519492767751217
epoch£º269	 i:7 	 global-step:5387	 l-p:0.10831407457590103
epoch£º269	 i:8 	 global-step:5388	 l-p:0.11682654917240143
epoch£º269	 i:9 	 global-step:5389	 l-p:0.09250765293836594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6675, 3.6660, 3.6672],
        [3.6675, 3.8112, 3.7735],
        [3.6675, 3.6672, 3.6675],
        [3.6675, 3.7200, 3.6743]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.12298847734928131 
model_pd.l_d.mean(): -23.304855346679688 
model_pd.lagr.mean(): -23.181867599487305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0545], device='cuda:0')), ('power', tensor([-23.3594], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.12298847734928131
epoch£º270	 i:1 	 global-step:5401	 l-p:0.1311371624469757
epoch£º270	 i:2 	 global-step:5402	 l-p:0.14350052177906036
epoch£º270	 i:3 	 global-step:5403	 l-p:0.1293412148952484
epoch£º270	 i:4 	 global-step:5404	 l-p:0.10039804130792618
epoch£º270	 i:5 	 global-step:5405	 l-p:0.10730407387018204
epoch£º270	 i:6 	 global-step:5406	 l-p:0.049432311207056046
epoch£º270	 i:7 	 global-step:5407	 l-p:0.08642078191041946
epoch£º270	 i:8 	 global-step:5408	 l-p:0.1131792813539505
epoch£º270	 i:9 	 global-step:5409	 l-p:0.08898534625768661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5424, 3.5424, 3.5424],
        [3.5424, 3.5730, 3.5315],
        [3.5424, 3.7503, 3.7470],
        [3.5424, 3.9589, 4.1106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.04711901396512985 
model_pd.l_d.mean(): -23.2048282623291 
model_pd.lagr.mean(): -23.1577091217041 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1301], device='cuda:0')), ('power', tensor([-23.3350], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.04711901396512985
epoch£º271	 i:1 	 global-step:5421	 l-p:0.1237235963344574
epoch£º271	 i:2 	 global-step:5422	 l-p:0.16611991822719574
epoch£º271	 i:3 	 global-step:5423	 l-p:0.15435701608657837
epoch£º271	 i:4 	 global-step:5424	 l-p:0.10444638133049011
epoch£º271	 i:5 	 global-step:5425	 l-p:0.07625105232000351
epoch£º271	 i:6 	 global-step:5426	 l-p:0.12081900238990784
epoch£º271	 i:7 	 global-step:5427	 l-p:0.1072780191898346
epoch£º271	 i:8 	 global-step:5428	 l-p:0.11878453195095062
epoch£º271	 i:9 	 global-step:5429	 l-p:0.13465341925621033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5677, 3.5640, 3.5665],
        [3.5677, 3.8318, 3.8623],
        [3.5677, 3.5621, 3.5641],
        [3.5677, 3.5651, 3.5672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.1187930777668953 
model_pd.l_d.mean(): -22.319469451904297 
model_pd.lagr.mean(): -22.20067596435547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1663], device='cuda:0')), ('power', tensor([-22.4858], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.1187930777668953
epoch£º272	 i:1 	 global-step:5441	 l-p:0.08618400245904922
epoch£º272	 i:2 	 global-step:5442	 l-p:0.11830265820026398
epoch£º272	 i:3 	 global-step:5443	 l-p:0.13759039342403412
epoch£º272	 i:4 	 global-step:5444	 l-p:0.12407460808753967
epoch£º272	 i:5 	 global-step:5445	 l-p:0.10344588756561279
epoch£º272	 i:6 	 global-step:5446	 l-p:0.10371242463588715
epoch£º272	 i:7 	 global-step:5447	 l-p:0.046238746494054794
epoch£º272	 i:8 	 global-step:5448	 l-p:0.12811367213726044
epoch£º272	 i:9 	 global-step:5449	 l-p:0.21669524908065796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5398, 3.5537, 3.5213],
        [3.5398, 3.5389, 3.5397],
        [3.5398, 3.5330, 3.5347],
        [3.5398, 3.5358, 3.5386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.1142849549651146 
model_pd.l_d.mean(): -21.820772171020508 
model_pd.lagr.mean(): -21.70648765563965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2265], device='cuda:0')), ('power', tensor([-22.0473], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.1142849549651146
epoch£º273	 i:1 	 global-step:5461	 l-p:0.10662537068128586
epoch£º273	 i:2 	 global-step:5462	 l-p:0.13168160617351532
epoch£º273	 i:3 	 global-step:5463	 l-p:0.1551094353199005
epoch£º273	 i:4 	 global-step:5464	 l-p:0.101866215467453
epoch£º273	 i:5 	 global-step:5465	 l-p:0.17531661689281464
epoch£º273	 i:6 	 global-step:5466	 l-p:0.10933604836463928
epoch£º273	 i:7 	 global-step:5467	 l-p:-0.0076051331125199795
epoch£º273	 i:8 	 global-step:5468	 l-p:0.13082502782344818
epoch£º273	 i:9 	 global-step:5469	 l-p:0.06627455353736877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6067, 3.7120, 3.6643],
        [3.6067, 4.0364, 4.1929],
        [3.6067, 3.6056, 3.6066],
        [3.6067, 4.0351, 4.1906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.11889969557523727 
model_pd.l_d.mean(): -23.33539390563965 
model_pd.lagr.mean(): -23.216493606567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0764], device='cuda:0')), ('power', tensor([-23.4118], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.11889969557523727
epoch£º274	 i:1 	 global-step:5481	 l-p:0.08108006417751312
epoch£º274	 i:2 	 global-step:5482	 l-p:0.23525096476078033
epoch£º274	 i:3 	 global-step:5483	 l-p:0.11457958072423935
epoch£º274	 i:4 	 global-step:5484	 l-p:0.14636893570423126
epoch£º274	 i:5 	 global-step:5485	 l-p:0.11108323186635971
epoch£º274	 i:6 	 global-step:5486	 l-p:0.0887678861618042
epoch£º274	 i:7 	 global-step:5487	 l-p:0.0317353755235672
epoch£º274	 i:8 	 global-step:5488	 l-p:0.0480184368789196
epoch£º274	 i:9 	 global-step:5489	 l-p:0.12233356386423111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7125, 3.8756, 3.8431],
        [3.7125, 3.8911, 3.8651],
        [3.7125, 3.7125, 3.7125],
        [3.7125, 3.7094, 3.7114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.10847711563110352 
model_pd.l_d.mean(): -23.386253356933594 
model_pd.lagr.mean(): -23.27777671813965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0218], device='cuda:0')), ('power', tensor([-23.4081], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.10847711563110352
epoch£º275	 i:1 	 global-step:5501	 l-p:0.1360405832529068
epoch£º275	 i:2 	 global-step:5502	 l-p:61.475677490234375
epoch£º275	 i:3 	 global-step:5503	 l-p:0.08064749836921692
epoch£º275	 i:4 	 global-step:5504	 l-p:0.7087756395339966
epoch£º275	 i:5 	 global-step:5505	 l-p:0.12050937116146088
epoch£º275	 i:6 	 global-step:5506	 l-p:0.11742592602968216
epoch£º275	 i:7 	 global-step:5507	 l-p:0.05510912463068962
epoch£º275	 i:8 	 global-step:5508	 l-p:0.11808574199676514
epoch£º275	 i:9 	 global-step:5509	 l-p:0.10801371932029724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8182, 4.2542, 4.3957],
        [3.8182, 3.8156, 3.8170],
        [3.8182, 3.8182, 3.8182],
        [3.8182, 3.8177, 3.8181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.1254262775182724 
model_pd.l_d.mean(): -23.39988136291504 
model_pd.lagr.mean(): -23.274456024169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0221], device='cuda:0')), ('power', tensor([-23.3778], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.1254262775182724
epoch£º276	 i:1 	 global-step:5521	 l-p:0.11710606515407562
epoch£º276	 i:2 	 global-step:5522	 l-p:0.18840479850769043
epoch£º276	 i:3 	 global-step:5523	 l-p:0.11922167241573334
epoch£º276	 i:4 	 global-step:5524	 l-p:0.12413860857486725
epoch£º276	 i:5 	 global-step:5525	 l-p:0.11556745320558548
epoch£º276	 i:6 	 global-step:5526	 l-p:-0.055624283850193024
epoch£º276	 i:7 	 global-step:5527	 l-p:0.051180399954319
epoch£º276	 i:8 	 global-step:5528	 l-p:0.14471867680549622
epoch£º276	 i:9 	 global-step:5529	 l-p:0.14790841937065125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6450, 3.9204, 3.9530],
        [3.6450, 3.9766, 4.0498],
        [3.6450, 3.6449, 3.6450],
        [3.6450, 3.8501, 3.8397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.11065905541181564 
model_pd.l_d.mean(): -22.999786376953125 
model_pd.lagr.mean(): -22.889127731323242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1187], device='cuda:0')), ('power', tensor([-23.1185], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.11065905541181564
epoch£º277	 i:1 	 global-step:5541	 l-p:0.15947137773036957
epoch£º277	 i:2 	 global-step:5542	 l-p:0.09581232070922852
epoch£º277	 i:3 	 global-step:5543	 l-p:0.09503692388534546
epoch£º277	 i:4 	 global-step:5544	 l-p:0.10647107660770416
epoch£º277	 i:5 	 global-step:5545	 l-p:0.20383529365062714
epoch£º277	 i:6 	 global-step:5546	 l-p:0.06036815047264099
epoch£º277	 i:7 	 global-step:5547	 l-p:0.12751124799251556
epoch£º277	 i:8 	 global-step:5548	 l-p:0.040024176239967346
epoch£º277	 i:9 	 global-step:5549	 l-p:0.11675795912742615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6860, 3.6839, 3.6856],
        [3.6860, 3.6811, 3.6830],
        [3.6860, 3.6849, 3.6740],
        [3.6860, 4.0011, 4.0588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.131928950548172 
model_pd.l_d.mean(): -22.292654037475586 
model_pd.lagr.mean(): -22.160724639892578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1184], device='cuda:0')), ('power', tensor([-22.4111], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.131928950548172
epoch£º278	 i:1 	 global-step:5561	 l-p:0.03166519105434418
epoch£º278	 i:2 	 global-step:5562	 l-p:0.10116073489189148
epoch£º278	 i:3 	 global-step:5563	 l-p:0.09508706629276276
epoch£º278	 i:4 	 global-step:5564	 l-p:0.11231748014688492
epoch£º278	 i:5 	 global-step:5565	 l-p:0.1778421401977539
epoch£º278	 i:6 	 global-step:5566	 l-p:0.21431323885917664
epoch£º278	 i:7 	 global-step:5567	 l-p:0.08554472774267197
epoch£º278	 i:8 	 global-step:5568	 l-p:0.10679176449775696
epoch£º278	 i:9 	 global-step:5569	 l-p:0.11372963339090347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7865, 3.8022, 3.7749],
        [3.7865, 3.7827, 3.7828],
        [3.7865, 4.0923, 4.1363],
        [3.7865, 3.7970, 3.7746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.08384355157613754 
model_pd.l_d.mean(): -23.180892944335938 
model_pd.lagr.mean(): -23.097049713134766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0269], device='cuda:0')), ('power', tensor([-23.2078], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.08384355157613754
epoch£º279	 i:1 	 global-step:5581	 l-p:0.16322490572929382
epoch£º279	 i:2 	 global-step:5582	 l-p:0.087163545191288
epoch£º279	 i:3 	 global-step:5583	 l-p:0.11690802127122879
epoch£º279	 i:4 	 global-step:5584	 l-p:0.09179475903511047
epoch£º279	 i:5 	 global-step:5585	 l-p:0.1136685237288475
epoch£º279	 i:6 	 global-step:5586	 l-p:1.1199864149093628
epoch£º279	 i:7 	 global-step:5587	 l-p:0.20708036422729492
epoch£º279	 i:8 	 global-step:5588	 l-p:0.14313699305057526
epoch£º279	 i:9 	 global-step:5589	 l-p:0.12104614824056625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7287, 3.7284, 3.7287],
        [3.7287, 3.7238, 3.7248],
        [3.7287, 3.7259, 3.7192],
        [3.7287, 3.7369, 3.7143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.45881524682044983 
model_pd.l_d.mean(): -22.90093421936035 
model_pd.lagr.mean(): -22.442119598388672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0565], device='cuda:0')), ('power', tensor([-22.9574], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.45881524682044983
epoch£º280	 i:1 	 global-step:5601	 l-p:0.00556398369371891
epoch£º280	 i:2 	 global-step:5602	 l-p:0.11127621680498123
epoch£º280	 i:3 	 global-step:5603	 l-p:0.11722025275230408
epoch£º280	 i:4 	 global-step:5604	 l-p:0.129331573843956
epoch£º280	 i:5 	 global-step:5605	 l-p:-0.2670225501060486
epoch£º280	 i:6 	 global-step:5606	 l-p:0.1150684505701065
epoch£º280	 i:7 	 global-step:5607	 l-p:0.12752526998519897
epoch£º280	 i:8 	 global-step:5608	 l-p:0.08104066550731659
epoch£º280	 i:9 	 global-step:5609	 l-p:0.11983232200145721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6534, 3.6951, 3.6481],
        [3.6534, 3.6532, 3.6534],
        [3.6534, 3.6499, 3.6524],
        [3.6534, 3.6475, 3.6433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.11776910722255707 
model_pd.l_d.mean(): -23.273319244384766 
model_pd.lagr.mean(): -23.155550003051758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0723], device='cuda:0')), ('power', tensor([-23.3456], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.11776910722255707
epoch£º281	 i:1 	 global-step:5621	 l-p:0.08929794281721115
epoch£º281	 i:2 	 global-step:5622	 l-p:0.12036299705505371
epoch£º281	 i:3 	 global-step:5623	 l-p:-0.3618907034397125
epoch£º281	 i:4 	 global-step:5624	 l-p:0.12478571385145187
epoch£º281	 i:5 	 global-step:5625	 l-p:0.14605067670345306
epoch£º281	 i:6 	 global-step:5626	 l-p:0.11620867252349854
epoch£º281	 i:7 	 global-step:5627	 l-p:0.1018417701125145
epoch£º281	 i:8 	 global-step:5628	 l-p:0.0999525934457779
epoch£º281	 i:9 	 global-step:5629	 l-p:0.16845737397670746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[3.5920, 3.5842, 3.5801],
        [3.5920, 3.6422, 3.5911],
        [3.5920, 3.5851, 3.5779],
        [3.5920, 3.9758, 4.0951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): -0.219570592045784 
model_pd.l_d.mean(): -21.85289192199707 
model_pd.lagr.mean(): -22.07246208190918 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2518], device='cuda:0')), ('power', tensor([-22.1047], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:-0.219570592045784
epoch£º282	 i:1 	 global-step:5641	 l-p:0.12181241810321808
epoch£º282	 i:2 	 global-step:5642	 l-p:0.13005155324935913
epoch£º282	 i:3 	 global-step:5643	 l-p:0.10940010845661163
epoch£º282	 i:4 	 global-step:5644	 l-p:0.11942026764154434
epoch£º282	 i:5 	 global-step:5645	 l-p:0.11543149501085281
epoch£º282	 i:6 	 global-step:5646	 l-p:0.1516083925962448
epoch£º282	 i:7 	 global-step:5647	 l-p:0.11201899498701096
epoch£º282	 i:8 	 global-step:5648	 l-p:0.08413077890872955
epoch£º282	 i:9 	 global-step:5649	 l-p:0.10857650637626648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6225, 3.7404, 3.6935],
        [3.6225, 4.0132, 4.1353],
        [3.6225, 3.7404, 3.6934],
        [3.6225, 3.6210, 3.6224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.07821277529001236 
model_pd.l_d.mean(): -22.92681121826172 
model_pd.lagr.mean(): -22.84859848022461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1344], device='cuda:0')), ('power', tensor([-23.0612], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.07821277529001236
epoch£º283	 i:1 	 global-step:5661	 l-p:0.11107994616031647
epoch£º283	 i:2 	 global-step:5662	 l-p:0.10569369047880173
epoch£º283	 i:3 	 global-step:5663	 l-p:0.23409003019332886
epoch£º283	 i:4 	 global-step:5664	 l-p:0.13017313182353973
epoch£º283	 i:5 	 global-step:5665	 l-p:0.12094654887914658
epoch£º283	 i:6 	 global-step:5666	 l-p:0.09654304385185242
epoch£º283	 i:7 	 global-step:5667	 l-p:0.13646751642227173
epoch£º283	 i:8 	 global-step:5668	 l-p:0.12527383863925934
epoch£º283	 i:9 	 global-step:5669	 l-p:0.11934586614370346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6434, 3.6396, 3.6424],
        [3.6434, 3.6411, 3.6430],
        [3.6434, 3.6433, 3.6434],
        [3.6434, 3.7035, 3.6509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.1282031089067459 
model_pd.l_d.mean(): -23.408151626586914 
model_pd.lagr.mean(): -23.279949188232422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0550], device='cuda:0')), ('power', tensor([-23.4632], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.1282031089067459
epoch£º284	 i:1 	 global-step:5681	 l-p:0.09620480239391327
epoch£º284	 i:2 	 global-step:5682	 l-p:0.1476157307624817
epoch£º284	 i:3 	 global-step:5683	 l-p:0.08752305805683136
epoch£º284	 i:4 	 global-step:5684	 l-p:0.23039910197257996
epoch£º284	 i:5 	 global-step:5685	 l-p:0.10799285024404526
epoch£º284	 i:6 	 global-step:5686	 l-p:0.1171000748872757
epoch£º284	 i:7 	 global-step:5687	 l-p:0.10111148655414581
epoch£º284	 i:8 	 global-step:5688	 l-p:-0.0020577097311615944
epoch£º284	 i:9 	 global-step:5689	 l-p:0.11936206370592117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7084, 3.7084, 3.7084],
        [3.7084, 3.7053, 3.7077],
        [3.7084, 3.7029, 3.6973],
        [3.7084, 3.8265, 3.7782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.05269128829240799 
model_pd.l_d.mean(): -23.201866149902344 
model_pd.lagr.mean(): -23.1491756439209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0570], device='cuda:0')), ('power', tensor([-23.2589], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.05269128829240799
epoch£º285	 i:1 	 global-step:5701	 l-p:0.6015608310699463
epoch£º285	 i:2 	 global-step:5702	 l-p:0.12760120630264282
epoch£º285	 i:3 	 global-step:5703	 l-p:0.12949572503566742
epoch£º285	 i:4 	 global-step:5704	 l-p:0.09634637832641602
epoch£º285	 i:5 	 global-step:5705	 l-p:0.07839855551719666
epoch£º285	 i:6 	 global-step:5706	 l-p:0.048969317227602005
epoch£º285	 i:7 	 global-step:5707	 l-p:0.12129639089107513
epoch£º285	 i:8 	 global-step:5708	 l-p:0.11024919152259827
epoch£º285	 i:9 	 global-step:5709	 l-p:-0.0032988691236823797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8981, 3.8949, 3.8966],
        [3.8981, 3.8980, 3.8981],
        [3.8981, 3.8981, 3.8981],
        [3.8981, 3.8974, 3.8980]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): -0.09162607043981552 
model_pd.l_d.mean(): -23.229106903076172 
model_pd.lagr.mean(): -23.32073211669922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0252], device='cuda:0')), ('power', tensor([-23.2040], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:-0.09162607043981552
epoch£º286	 i:1 	 global-step:5721	 l-p:0.12782369554042816
epoch£º286	 i:2 	 global-step:5722	 l-p:0.27145153284072876
epoch£º286	 i:3 	 global-step:5723	 l-p:0.12259908765554428
epoch£º286	 i:4 	 global-step:5724	 l-p:0.11031913757324219
epoch£º286	 i:5 	 global-step:5725	 l-p:1.1167157888412476
epoch£º286	 i:6 	 global-step:5726	 l-p:0.12334801256656647
epoch£º286	 i:7 	 global-step:5727	 l-p:0.11546790599822998
epoch£º286	 i:8 	 global-step:5728	 l-p:0.10687455534934998
epoch£º286	 i:9 	 global-step:5729	 l-p:0.12071358412504196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9104, 4.0376, 3.9890],
        [3.9104, 4.0932, 4.0617],
        [3.9104, 3.9212, 3.8990],
        [3.9104, 3.9069, 3.9072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): -0.3121723532676697 
model_pd.l_d.mean(): -23.099023818969727 
model_pd.lagr.mean(): -23.411195755004883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0053], device='cuda:0')), ('power', tensor([-23.0937], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:-0.3121723532676697
epoch£º287	 i:1 	 global-step:5741	 l-p:0.1164039596915245
epoch£º287	 i:2 	 global-step:5742	 l-p:0.0900578647851944
epoch£º287	 i:3 	 global-step:5743	 l-p:0.11456415057182312
epoch£º287	 i:4 	 global-step:5744	 l-p:0.1426399052143097
epoch£º287	 i:5 	 global-step:5745	 l-p:0.16931387782096863
epoch£º287	 i:6 	 global-step:5746	 l-p:0.24850386381149292
epoch£º287	 i:7 	 global-step:5747	 l-p:0.10275167971849442
epoch£º287	 i:8 	 global-step:5748	 l-p:0.11326539516448975
epoch£º287	 i:9 	 global-step:5749	 l-p:0.12830360233783722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7364, 3.7320, 3.7348],
        [3.7364, 3.7364, 3.7364],
        [3.7364, 3.7359, 3.7364],
        [3.7364, 3.8086, 3.7550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.5084161758422852 
model_pd.l_d.mean(): -23.23297119140625 
model_pd.lagr.mean(): -22.72455596923828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0467], device='cuda:0')), ('power', tensor([-23.2797], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.5084161758422852
epoch£º288	 i:1 	 global-step:5761	 l-p:0.12188684195280075
epoch£º288	 i:2 	 global-step:5762	 l-p:-0.23268035054206848
epoch£º288	 i:3 	 global-step:5763	 l-p:0.10573036223649979
epoch£º288	 i:4 	 global-step:5764	 l-p:0.1151479184627533
epoch£º288	 i:5 	 global-step:5765	 l-p:0.13543611764907837
epoch£º288	 i:6 	 global-step:5766	 l-p:0.05523302033543587
epoch£º288	 i:7 	 global-step:5767	 l-p:0.11224489659070969
epoch£º288	 i:8 	 global-step:5768	 l-p:0.12157741189002991
epoch£º288	 i:9 	 global-step:5769	 l-p:0.11511193960905075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5859, 3.7538, 3.7258],
        [3.5859, 3.5853, 3.5859],
        [3.5859, 3.5855, 3.5859],
        [3.5859, 3.5803, 3.5842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.14103788137435913 
model_pd.l_d.mean(): -22.18323516845703 
model_pd.lagr.mean(): -22.042198181152344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1991], device='cuda:0')), ('power', tensor([-22.3823], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.14103788137435913
epoch£º289	 i:1 	 global-step:5781	 l-p:0.11859077215194702
epoch£º289	 i:2 	 global-step:5782	 l-p:0.10705216974020004
epoch£º289	 i:3 	 global-step:5783	 l-p:0.12611526250839233
epoch£º289	 i:4 	 global-step:5784	 l-p:0.13438697159290314
epoch£º289	 i:5 	 global-step:5785	 l-p:0.12699362635612488
epoch£º289	 i:6 	 global-step:5786	 l-p:0.10658193379640579
epoch£º289	 i:7 	 global-step:5787	 l-p:0.11039946973323822
epoch£º289	 i:8 	 global-step:5788	 l-p:0.09397421777248383
epoch£º289	 i:9 	 global-step:5789	 l-p:0.1272926926612854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5364, 3.5364, 3.5364],
        [3.5364, 3.5346, 3.5362],
        [3.5364, 3.5364, 3.5364],
        [3.5364, 3.5265, 3.5127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.11533018201589584 
model_pd.l_d.mean(): -23.333019256591797 
model_pd.lagr.mean(): -23.217689514160156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0976], device='cuda:0')), ('power', tensor([-23.4306], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.11533018201589584
epoch£º290	 i:1 	 global-step:5801	 l-p:0.10878611356019974
epoch£º290	 i:2 	 global-step:5802	 l-p:0.1169896125793457
epoch£º290	 i:3 	 global-step:5803	 l-p:-0.08941546827554703
epoch£º290	 i:4 	 global-step:5804	 l-p:0.119694285094738
epoch£º290	 i:5 	 global-step:5805	 l-p:0.11744319647550583
epoch£º290	 i:6 	 global-step:5806	 l-p:0.0722104087471962
epoch£º290	 i:7 	 global-step:5807	 l-p:0.1319746971130371
epoch£º290	 i:8 	 global-step:5808	 l-p:0.11067911237478256
epoch£º290	 i:9 	 global-step:5809	 l-p:0.16350847482681274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4673, 3.4572, 3.4353],
        [3.4673, 3.5622, 3.5091],
        [3.4673, 3.4603, 3.4652],
        [3.4673, 3.4673, 3.4673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.11671016365289688 
model_pd.l_d.mean(): -23.53845977783203 
model_pd.lagr.mean(): -23.421749114990234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0986], device='cuda:0')), ('power', tensor([-23.6370], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.11671016365289688
epoch£º291	 i:1 	 global-step:5821	 l-p:0.04077787324786186
epoch£º291	 i:2 	 global-step:5822	 l-p:0.13254311680793762
epoch£º291	 i:3 	 global-step:5823	 l-p:0.12191759794950485
epoch£º291	 i:4 	 global-step:5824	 l-p:0.7137208580970764
epoch£º291	 i:5 	 global-step:5825	 l-p:0.09199567884206772
epoch£º291	 i:6 	 global-step:5826	 l-p:0.12174882739782333
epoch£º291	 i:7 	 global-step:5827	 l-p:0.12254195660352707
epoch£º291	 i:8 	 global-step:5828	 l-p:0.04985947161912918
epoch£º291	 i:9 	 global-step:5829	 l-p:0.09515691548585892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6257, 3.6257, 3.6257],
        [3.6257, 3.6230, 3.6253],
        [3.6257, 3.6255, 3.6257],
        [3.6257, 3.6257, 3.6257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.11847831308841705 
model_pd.l_d.mean(): -22.926807403564453 
model_pd.lagr.mean(): -22.80832862854004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1111], device='cuda:0')), ('power', tensor([-23.0379], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.11847831308841705
epoch£º292	 i:1 	 global-step:5841	 l-p:0.0840870663523674
epoch£º292	 i:2 	 global-step:5842	 l-p:0.10812763124704361
epoch£º292	 i:3 	 global-step:5843	 l-p:0.10685452818870544
epoch£º292	 i:4 	 global-step:5844	 l-p:0.15513606369495392
epoch£º292	 i:5 	 global-step:5845	 l-p:0.0805794894695282
epoch£º292	 i:6 	 global-step:5846	 l-p:0.07918580621480942
epoch£º292	 i:7 	 global-step:5847	 l-p:0.1201557070016861
epoch£º292	 i:8 	 global-step:5848	 l-p:0.12861697375774384
epoch£º292	 i:9 	 global-step:5849	 l-p:0.11545723676681519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[3.7522, 3.7679, 3.7315],
        [3.7522, 3.7664, 3.7312],
        [3.7522, 3.7449, 3.7400],
        [3.7522, 3.7442, 3.7430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.1155434176325798 
model_pd.l_d.mean(): -23.29296875 
model_pd.lagr.mean(): -23.177425384521484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0182], device='cuda:0')), ('power', tensor([-23.3112], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.1155434176325798
epoch£º293	 i:1 	 global-step:5861	 l-p:0.21298742294311523
epoch£º293	 i:2 	 global-step:5862	 l-p:0.08448459208011627
epoch£º293	 i:3 	 global-step:5863	 l-p:0.3105955421924591
epoch£º293	 i:4 	 global-step:5864	 l-p:0.10085318237543106
epoch£º293	 i:5 	 global-step:5865	 l-p:0.10720522701740265
epoch£º293	 i:6 	 global-step:5866	 l-p:0.11300215125083923
epoch£º293	 i:7 	 global-step:5867	 l-p:0.10857865959405899
epoch£º293	 i:8 	 global-step:5868	 l-p:0.11944536119699478
epoch£º293	 i:9 	 global-step:5869	 l-p:0.16662795841693878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[3.7833, 3.7783, 3.7684],
        [3.7833, 3.9499, 3.9142],
        [3.7833, 3.9989, 3.9864],
        [3.7833, 3.8120, 3.7678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.11375468969345093 
model_pd.l_d.mean(): -22.499610900878906 
model_pd.lagr.mean(): -22.38585662841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0719], device='cuda:0')), ('power', tensor([-22.5715], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.11375468969345093
epoch£º294	 i:1 	 global-step:5881	 l-p:0.09823009371757507
epoch£º294	 i:2 	 global-step:5882	 l-p:0.30338478088378906
epoch£º294	 i:3 	 global-step:5883	 l-p:0.03409992530941963
epoch£º294	 i:4 	 global-step:5884	 l-p:0.08894630521535873
epoch£º294	 i:5 	 global-step:5885	 l-p:0.13434205949306488
epoch£º294	 i:6 	 global-step:5886	 l-p:0.12016311287879944
epoch£º294	 i:7 	 global-step:5887	 l-p:0.13857458531856537
epoch£º294	 i:8 	 global-step:5888	 l-p:0.14350862801074982
epoch£º294	 i:9 	 global-step:5889	 l-p:0.09562757611274719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6351, 3.6297, 3.6090],
        [3.6351, 3.6349, 3.6351],
        [3.6351, 3.6342, 3.6350],
        [3.6351, 3.6351, 3.6351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.12540751695632935 
model_pd.l_d.mean(): -23.483503341674805 
model_pd.lagr.mean(): -23.358095169067383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0469], device='cuda:0')), ('power', tensor([-23.5304], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.12540751695632935
epoch£º295	 i:1 	 global-step:5901	 l-p:0.1108885332942009
epoch£º295	 i:2 	 global-step:5902	 l-p:0.1432589441537857
epoch£º295	 i:3 	 global-step:5903	 l-p:0.10950994491577148
epoch£º295	 i:4 	 global-step:5904	 l-p:0.09800466150045395
epoch£º295	 i:5 	 global-step:5905	 l-p:0.126079723238945
epoch£º295	 i:6 	 global-step:5906	 l-p:0.056487035006284714
epoch£º295	 i:7 	 global-step:5907	 l-p:0.11973258852958679
epoch£º295	 i:8 	 global-step:5908	 l-p:0.1203838512301445
epoch£º295	 i:9 	 global-step:5909	 l-p:0.1555808037519455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5666, 3.5613, 3.5340],
        [3.5666, 3.5631, 3.5661],
        [3.5666, 3.5662, 3.5666],
        [3.5666, 3.5666, 3.5666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.11277088522911072 
model_pd.l_d.mean(): -23.233850479125977 
model_pd.lagr.mean(): -23.12108039855957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1108], device='cuda:0')), ('power', tensor([-23.3447], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.11277088522911072
epoch£º296	 i:1 	 global-step:5921	 l-p:0.0691443681716919
epoch£º296	 i:2 	 global-step:5922	 l-p:0.10596798360347748
epoch£º296	 i:3 	 global-step:5923	 l-p:0.20842991769313812
epoch£º296	 i:4 	 global-step:5924	 l-p:0.12466079741716385
epoch£º296	 i:5 	 global-step:5925	 l-p:0.08456455171108246
epoch£º296	 i:6 	 global-step:5926	 l-p:0.14492300152778625
epoch£º296	 i:7 	 global-step:5927	 l-p:0.1254342794418335
epoch£º296	 i:8 	 global-step:5928	 l-p:0.11507947742938995
epoch£º296	 i:9 	 global-step:5929	 l-p:0.07197141647338867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6608, 4.0700, 4.2034],
        [3.6608, 3.6527, 3.6369],
        [3.6608, 3.6546, 3.6588],
        [3.6608, 3.6603, 3.6607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.12727464735507965 
model_pd.l_d.mean(): -22.52372932434082 
model_pd.lagr.mean(): -22.396453857421875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1500], device='cuda:0')), ('power', tensor([-22.6737], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.12727464735507965
epoch£º297	 i:1 	 global-step:5941	 l-p:0.10302364081144333
epoch£º297	 i:2 	 global-step:5942	 l-p:0.09351783245801926
epoch£º297	 i:3 	 global-step:5943	 l-p:0.10999881476163864
epoch£º297	 i:4 	 global-step:5944	 l-p:0.043938081711530685
epoch£º297	 i:5 	 global-step:5945	 l-p:0.38234075903892517
epoch£º297	 i:6 	 global-step:5946	 l-p:0.11577827483415604
epoch£º297	 i:7 	 global-step:5947	 l-p:0.12004104256629944
epoch£º297	 i:8 	 global-step:5948	 l-p:0.1560996025800705
epoch£º297	 i:9 	 global-step:5949	 l-p:0.1263570785522461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8460, 3.8460, 3.8460],
        [3.8460, 3.8386, 3.8376],
        [3.8460, 3.8459, 3.8460],
        [3.8460, 4.4383, 4.7163]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.06187356635928154 
model_pd.l_d.mean(): -22.71071434020996 
model_pd.lagr.mean(): -22.648839950561523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0431], device='cuda:0')), ('power', tensor([-22.7539], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.06187356635928154
epoch£º298	 i:1 	 global-step:5961	 l-p:0.11754018813371658
epoch£º298	 i:2 	 global-step:5962	 l-p:0.05482644587755203
epoch£º298	 i:3 	 global-step:5963	 l-p:0.12473607808351517
epoch£º298	 i:4 	 global-step:5964	 l-p:0.13122735917568207
epoch£º298	 i:5 	 global-step:5965	 l-p:0.12332508713006973
epoch£º298	 i:6 	 global-step:5966	 l-p:0.12803466618061066
epoch£º298	 i:7 	 global-step:5967	 l-p:0.1037549152970314
epoch£º298	 i:8 	 global-step:5968	 l-p:0.1702824980020523
epoch£º298	 i:9 	 global-step:5969	 l-p:0.07450267672538757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8189, 3.8148, 3.8179],
        [3.8189, 3.8130, 3.8032],
        [3.8189, 3.8347, 3.7975],
        [3.8189, 3.8857, 3.8290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.12085683643817902 
model_pd.l_d.mean(): -23.15955924987793 
model_pd.lagr.mean(): -23.0387020111084 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0264], device='cuda:0')), ('power', tensor([-23.1860], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.12085683643817902
epoch£º299	 i:1 	 global-step:5981	 l-p:0.08779686689376831
epoch£º299	 i:2 	 global-step:5982	 l-p:0.09218550473451614
epoch£º299	 i:3 	 global-step:5983	 l-p:0.2235552966594696
epoch£º299	 i:4 	 global-step:5984	 l-p:0.11918973922729492
epoch£º299	 i:5 	 global-step:5985	 l-p:0.09046614170074463
epoch£º299	 i:6 	 global-step:5986	 l-p:0.13466273248195648
epoch£º299	 i:7 	 global-step:5987	 l-p:0.04847465455532074
epoch£º299	 i:8 	 global-step:5988	 l-p:0.06986941397190094
epoch£º299	 i:9 	 global-step:5989	 l-p:0.13456200063228607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6625, 4.0614, 4.1861],
        [3.6625, 3.6518, 3.6561],
        [3.6625, 3.6625, 3.6625],
        [3.6625, 3.6521, 3.6565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.11268226057291031 
model_pd.l_d.mean(): -23.33841323852539 
model_pd.lagr.mean(): -23.225730895996094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0591], device='cuda:0')), ('power', tensor([-23.3975], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.11268226057291031
epoch£º300	 i:1 	 global-step:6001	 l-p:0.10195335000753403
epoch£º300	 i:2 	 global-step:6002	 l-p:0.1512933075428009
epoch£º300	 i:3 	 global-step:6003	 l-p:0.10749700665473938
epoch£º300	 i:4 	 global-step:6004	 l-p:0.12294947355985641
epoch£º300	 i:5 	 global-step:6005	 l-p:0.10879213362932205
epoch£º300	 i:6 	 global-step:6006	 l-p:0.22994676232337952
epoch£º300	 i:7 	 global-step:6007	 l-p:0.08184940367937088
epoch£º300	 i:8 	 global-step:6008	 l-p:0.06215574964880943
epoch£º300	 i:9 	 global-step:6009	 l-p:0.12391839921474457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6718, 3.6591, 3.6538],
        [3.6718, 3.6718, 3.6718],
        [3.6718, 3.7341, 3.6745],
        [3.6718, 3.6718, 3.6718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.14769569039344788 
model_pd.l_d.mean(): -22.334224700927734 
model_pd.lagr.mean(): -22.1865291595459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1385], device='cuda:0')), ('power', tensor([-22.4727], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.14769569039344788
epoch£º301	 i:1 	 global-step:6021	 l-p:0.040858324617147446
epoch£º301	 i:2 	 global-step:6022	 l-p:-0.07250608503818512
epoch£º301	 i:3 	 global-step:6023	 l-p:0.11669328063726425
epoch£º301	 i:4 	 global-step:6024	 l-p:-0.0892983004450798
epoch£º301	 i:5 	 global-step:6025	 l-p:0.10809654742479324
epoch£º301	 i:6 	 global-step:6026	 l-p:0.11799527704715729
epoch£º301	 i:7 	 global-step:6027	 l-p:0.1445099115371704
epoch£º301	 i:8 	 global-step:6028	 l-p:0.10821069777011871
epoch£º301	 i:9 	 global-step:6029	 l-p:0.11718175560235977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7519, 3.7442, 3.7306],
        [3.7519, 3.7513, 3.7519],
        [3.7519, 4.1302, 4.2302],
        [3.7519, 3.7513, 3.7519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.11396903544664383 
model_pd.l_d.mean(): -23.338825225830078 
model_pd.lagr.mean(): -23.224855422973633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0189], device='cuda:0')), ('power', tensor([-23.3577], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.11396903544664383
epoch£º302	 i:1 	 global-step:6041	 l-p:0.10905195027589798
epoch£º302	 i:2 	 global-step:6042	 l-p:0.11538520455360413
epoch£º302	 i:3 	 global-step:6043	 l-p:0.2359786182641983
epoch£º302	 i:4 	 global-step:6044	 l-p:0.12319662421941757
epoch£º302	 i:5 	 global-step:6045	 l-p:0.11094249784946442
epoch£º302	 i:6 	 global-step:6046	 l-p:0.10890474170446396
epoch£º302	 i:7 	 global-step:6047	 l-p:0.12098564207553864
epoch£º302	 i:8 	 global-step:6048	 l-p:0.12692739069461823
epoch£º302	 i:9 	 global-step:6049	 l-p:-3.4997942447662354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4991, 3.4991, 3.4991],
        [3.4991, 3.4791, 3.4740],
        [3.4991, 3.9437, 4.1225],
        [3.4991, 3.4860, 3.4575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.11659533530473709 
model_pd.l_d.mean(): -22.294248580932617 
model_pd.lagr.mean(): -22.17765235900879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1947], device='cuda:0')), ('power', tensor([-22.4889], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.11659533530473709
epoch£º303	 i:1 	 global-step:6061	 l-p:0.11834825575351715
epoch£º303	 i:2 	 global-step:6062	 l-p:0.12653997540473938
epoch£º303	 i:3 	 global-step:6063	 l-p:0.1287248730659485
epoch£º303	 i:4 	 global-step:6064	 l-p:0.13117273151874542
epoch£º303	 i:5 	 global-step:6065	 l-p:0.12009208649396896
epoch£º303	 i:6 	 global-step:6066	 l-p:0.5339182019233704
epoch£º303	 i:7 	 global-step:6067	 l-p:0.11916093528270721
epoch£º303	 i:8 	 global-step:6068	 l-p:0.13395462930202484
epoch£º303	 i:9 	 global-step:6069	 l-p:0.0787658616900444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3297, 3.3297, 3.3297],
        [3.3297, 3.3297, 3.3297],
        [3.3297, 3.3289, 3.3297],
        [3.3297, 3.3294, 3.3297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.1341867446899414 
model_pd.l_d.mean(): -22.226177215576172 
model_pd.lagr.mean(): -22.091991424560547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2562], device='cuda:0')), ('power', tensor([-22.4824], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.1341867446899414
epoch£º304	 i:1 	 global-step:6081	 l-p:0.018145499750971794
epoch£º304	 i:2 	 global-step:6082	 l-p:0.1339675933122635
epoch£º304	 i:3 	 global-step:6083	 l-p:0.12773063778877258
epoch£º304	 i:4 	 global-step:6084	 l-p:0.09983924776315689
epoch£º304	 i:5 	 global-step:6085	 l-p:0.12427880614995956
epoch£º304	 i:6 	 global-step:6086	 l-p:0.09899759292602539
epoch£º304	 i:7 	 global-step:6087	 l-p:0.11649075895547867
epoch£º304	 i:8 	 global-step:6088	 l-p:0.11109541356563568
epoch£º304	 i:9 	 global-step:6089	 l-p:0.11975239217281342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5372, 3.5264, 3.4946],
        [3.5372, 3.5371, 3.5372],
        [3.5372, 3.8827, 3.9747],
        [3.5372, 3.5182, 3.5191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.10378318279981613 
model_pd.l_d.mean(): -23.016845703125 
model_pd.lagr.mean(): -22.913063049316406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1615], device='cuda:0')), ('power', tensor([-23.1784], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.10378318279981613
epoch£º305	 i:1 	 global-step:6101	 l-p:0.12629064917564392
epoch£º305	 i:2 	 global-step:6102	 l-p:0.12272500991821289
epoch£º305	 i:3 	 global-step:6103	 l-p:0.05471383407711983
epoch£º305	 i:4 	 global-step:6104	 l-p:0.1180807501077652
epoch£º305	 i:5 	 global-step:6105	 l-p:0.12071233242750168
epoch£º305	 i:6 	 global-step:6106	 l-p:0.17044809460639954
epoch£º305	 i:7 	 global-step:6107	 l-p:0.1264847218990326
epoch£º305	 i:8 	 global-step:6108	 l-p:0.11076001077890396
epoch£º305	 i:9 	 global-step:6109	 l-p:-0.015023416839540005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5124, 3.5124, 3.5124],
        [3.5124, 3.8330, 3.9072],
        [3.5124, 3.5124, 3.5124],
        [3.5124, 3.5203, 3.4667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.11212018132209778 
model_pd.l_d.mean(): -22.20175552368164 
model_pd.lagr.mean(): -22.089635848999023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2241], device='cuda:0')), ('power', tensor([-22.4259], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.11212018132209778
epoch£º306	 i:1 	 global-step:6121	 l-p:0.09365402162075043
epoch£º306	 i:2 	 global-step:6122	 l-p:0.09220308810472488
epoch£º306	 i:3 	 global-step:6123	 l-p:0.10231517255306244
epoch£º306	 i:4 	 global-step:6124	 l-p:0.1230854019522667
epoch£º306	 i:5 	 global-step:6125	 l-p:0.10733796656131744
epoch£º306	 i:6 	 global-step:6126	 l-p:0.12348306179046631
epoch£º306	 i:7 	 global-step:6127	 l-p:0.16298320889472961
epoch£º306	 i:8 	 global-step:6128	 l-p:0.12502413988113403
epoch£º306	 i:9 	 global-step:6129	 l-p:0.12977442145347595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5845, 3.5845, 3.5845],
        [3.5845, 3.5687, 3.5742],
        [3.5845, 3.7922, 3.7826],
        [3.5845, 3.5698, 3.5760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.12329093366861343 
model_pd.l_d.mean(): -23.302703857421875 
model_pd.lagr.mean(): -23.179412841796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1032], device='cuda:0')), ('power', tensor([-23.4059], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.12329093366861343
epoch£º307	 i:1 	 global-step:6141	 l-p:0.1084422692656517
epoch£º307	 i:2 	 global-step:6142	 l-p:0.16781418025493622
epoch£º307	 i:3 	 global-step:6143	 l-p:0.6711136698722839
epoch£º307	 i:4 	 global-step:6144	 l-p:0.07980816811323166
epoch£º307	 i:5 	 global-step:6145	 l-p:0.1142442598938942
epoch£º307	 i:6 	 global-step:6146	 l-p:0.12974800169467926
epoch£º307	 i:7 	 global-step:6147	 l-p:0.08620495349168777
epoch£º307	 i:8 	 global-step:6148	 l-p:0.12024158239364624
epoch£º307	 i:9 	 global-step:6149	 l-p:0.1275281459093094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5726, 3.5668, 3.5714],
        [3.5726, 3.5647, 3.5704],
        [3.5726, 3.5545, 3.5589],
        [3.5726, 3.5810, 3.5283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.17780475318431854 
model_pd.l_d.mean(): -23.57278060913086 
model_pd.lagr.mean(): -23.394975662231445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0534], device='cuda:0')), ('power', tensor([-23.6262], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.17780475318431854
epoch£º308	 i:1 	 global-step:6161	 l-p:0.10425054281949997
epoch£º308	 i:2 	 global-step:6162	 l-p:0.10926621407270432
epoch£º308	 i:3 	 global-step:6163	 l-p:0.008535835891962051
epoch£º308	 i:4 	 global-step:6164	 l-p:0.11896941065788269
epoch£º308	 i:5 	 global-step:6165	 l-p:0.1276097148656845
epoch£º308	 i:6 	 global-step:6166	 l-p:0.10475030541419983
epoch£º308	 i:7 	 global-step:6167	 l-p:0.12883324921131134
epoch£º308	 i:8 	 global-step:6168	 l-p:0.11273688822984695
epoch£º308	 i:9 	 global-step:6169	 l-p:0.049683015793561935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7086, 3.6949, 3.6798],
        [3.7086, 3.8241, 3.7681],
        [3.7086, 3.7085, 3.7086],
        [3.7086, 3.9115, 3.8929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.1029752641916275 
model_pd.l_d.mean(): -22.218454360961914 
model_pd.lagr.mean(): -22.115478515625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1356], device='cuda:0')), ('power', tensor([-22.3541], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.1029752641916275
epoch£º309	 i:1 	 global-step:6181	 l-p:0.10149241238832474
epoch£º309	 i:2 	 global-step:6182	 l-p:0.11772026121616364
epoch£º309	 i:3 	 global-step:6183	 l-p:0.15172341465950012
epoch£º309	 i:4 	 global-step:6184	 l-p:0.07905194908380508
epoch£º309	 i:5 	 global-step:6185	 l-p:0.1345495581626892
epoch£º309	 i:6 	 global-step:6186	 l-p:0.09383466839790344
epoch£º309	 i:7 	 global-step:6187	 l-p:0.10305985808372498
epoch£º309	 i:8 	 global-step:6188	 l-p:0.01216668076813221
epoch£º309	 i:9 	 global-step:6189	 l-p:0.11699999868869781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7160, 3.7115, 3.7152],
        [3.7160, 3.7159, 3.7160],
        [3.7160, 3.8234, 3.7649],
        [3.7160, 3.9997, 4.0310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.12505197525024414 
model_pd.l_d.mean(): -23.396211624145508 
model_pd.lagr.mean(): -23.271160125732422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0346], device='cuda:0')), ('power', tensor([-23.4308], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.12505197525024414
epoch£º310	 i:1 	 global-step:6201	 l-p:0.12286356836557388
epoch£º310	 i:2 	 global-step:6202	 l-p:0.12458772212266922
epoch£º310	 i:3 	 global-step:6203	 l-p:0.09494946151971817
epoch£º310	 i:4 	 global-step:6204	 l-p:0.10882658511400223
epoch£º310	 i:5 	 global-step:6205	 l-p:0.10938699543476105
epoch£º310	 i:6 	 global-step:6206	 l-p:0.08842616528272629
epoch£º310	 i:7 	 global-step:6207	 l-p:0.10260047018527985
epoch£º310	 i:8 	 global-step:6208	 l-p:0.04552735388278961
epoch£º310	 i:9 	 global-step:6209	 l-p:0.05968967452645302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[3.4770, 3.4563, 3.4236],
        [3.4770, 3.4552, 3.4248],
        [3.4770, 3.4502, 3.4436],
        [3.4770, 3.4643, 3.4193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.036843977868556976 
model_pd.l_d.mean(): -22.236865997314453 
model_pd.lagr.mean(): -22.200021743774414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2143], device='cuda:0')), ('power', tensor([-22.4512], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.036843977868556976
epoch£º311	 i:1 	 global-step:6221	 l-p:0.12751519680023193
epoch£º311	 i:2 	 global-step:6222	 l-p:0.11699648946523666
epoch£º311	 i:3 	 global-step:6223	 l-p:0.12743349373340607
epoch£º311	 i:4 	 global-step:6224	 l-p:0.09079096466302872
epoch£º311	 i:5 	 global-step:6225	 l-p:0.08994409441947937
epoch£º311	 i:6 	 global-step:6226	 l-p:0.17738647758960724
epoch£º311	 i:7 	 global-step:6227	 l-p:0.11507000029087067
epoch£º311	 i:8 	 global-step:6228	 l-p:0.1326495110988617
epoch£º311	 i:9 	 global-step:6229	 l-p:0.10593514889478683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5155, 3.4978, 3.4627],
        [3.5155, 3.5109, 3.5148],
        [3.5155, 3.5155, 3.5155],
        [3.5155, 3.7320, 3.7310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.13628311455249786 
model_pd.l_d.mean(): -23.446319580078125 
model_pd.lagr.mean(): -23.310035705566406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1091], device='cuda:0')), ('power', tensor([-23.5554], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.13628311455249786
epoch£º312	 i:1 	 global-step:6241	 l-p:0.15329107642173767
epoch£º312	 i:2 	 global-step:6242	 l-p:0.06569376587867737
epoch£º312	 i:3 	 global-step:6243	 l-p:0.114475779235363
epoch£º312	 i:4 	 global-step:6244	 l-p:0.12509667873382568
epoch£º312	 i:5 	 global-step:6245	 l-p:0.11310233920812607
epoch£º312	 i:6 	 global-step:6246	 l-p:0.612159013748169
epoch£º312	 i:7 	 global-step:6247	 l-p:0.11361394822597504
epoch£º312	 i:8 	 global-step:6248	 l-p:0.10343044996261597
epoch£º312	 i:9 	 global-step:6249	 l-p:0.18214000761508942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5688, 3.5514, 3.5587],
        [3.5688, 3.8718, 3.9272],
        [3.5688, 3.5541, 3.5182],
        [3.5688, 3.5616, 3.5672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.1110420972108841 
model_pd.l_d.mean(): -23.367385864257812 
model_pd.lagr.mean(): -23.256343841552734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0982], device='cuda:0')), ('power', tensor([-23.4655], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.1110420972108841
epoch£º313	 i:1 	 global-step:6261	 l-p:0.11530891060829163
epoch£º313	 i:2 	 global-step:6262	 l-p:0.006096463184803724
epoch£º313	 i:3 	 global-step:6263	 l-p:0.10376434028148651
epoch£º313	 i:4 	 global-step:6264	 l-p:0.11185603588819504
epoch£º313	 i:5 	 global-step:6265	 l-p:0.1317630261182785
epoch£º313	 i:6 	 global-step:6266	 l-p:0.1201620101928711
epoch£º313	 i:7 	 global-step:6267	 l-p:0.10291340947151184
epoch£º313	 i:8 	 global-step:6268	 l-p:0.1647804081439972
epoch£º313	 i:9 	 global-step:6269	 l-p:0.11841747164726257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4643, 3.4643, 3.4643],
        [3.4643, 3.4543, 3.4615],
        [3.4643, 3.4383, 3.4429],
        [3.4643, 3.4351, 3.4314]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.09318268299102783 
model_pd.l_d.mean(): -22.879423141479492 
model_pd.lagr.mean(): -22.786239624023438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2228], device='cuda:0')), ('power', tensor([-23.1022], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.09318268299102783
epoch£º314	 i:1 	 global-step:6281	 l-p:0.13183793425559998
epoch£º314	 i:2 	 global-step:6282	 l-p:0.11238615959882736
epoch£º314	 i:3 	 global-step:6283	 l-p:0.15789400041103363
epoch£º314	 i:4 	 global-step:6284	 l-p:0.06758773326873779
epoch£º314	 i:5 	 global-step:6285	 l-p:0.10041660070419312
epoch£º314	 i:6 	 global-step:6286	 l-p:0.12210322171449661
epoch£º314	 i:7 	 global-step:6287	 l-p:0.11619824916124344
epoch£º314	 i:8 	 global-step:6288	 l-p:0.11355819553136826
epoch£º314	 i:9 	 global-step:6289	 l-p:0.15589380264282227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6497, 3.7250, 3.6578],
        [3.6497, 3.6343, 3.6044],
        [3.6497, 3.6497, 3.6497],
        [3.6497, 3.8501, 3.8313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.09264037013053894 
model_pd.l_d.mean(): -23.043840408325195 
model_pd.lagr.mean(): -22.951200485229492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0992], device='cuda:0')), ('power', tensor([-23.1431], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.09264037013053894
epoch£º315	 i:1 	 global-step:6301	 l-p:0.11140435189008713
epoch£º315	 i:2 	 global-step:6302	 l-p:0.04601439833641052
epoch£º315	 i:3 	 global-step:6303	 l-p:0.12217707186937332
epoch£º315	 i:4 	 global-step:6304	 l-p:0.08556181937456131
epoch£º315	 i:5 	 global-step:6305	 l-p:0.09638679027557373
epoch£º315	 i:6 	 global-step:6306	 l-p:0.11104156076908112
epoch£º315	 i:7 	 global-step:6307	 l-p:0.11526019871234894
epoch£º315	 i:8 	 global-step:6308	 l-p:-3.19730544090271
epoch£º315	 i:9 	 global-step:6309	 l-p:0.11528933048248291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7926, 3.7872, 3.7915],
        [3.7926, 3.7917, 3.7926],
        [3.7926, 3.7999, 3.7528],
        [3.7926, 3.7778, 3.7824]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.11873628199100494 
model_pd.l_d.mean(): -23.51881217956543 
model_pd.lagr.mean(): -23.400075912475586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0264], device='cuda:0')), ('power', tensor([-23.4924], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.11873628199100494
epoch£º316	 i:1 	 global-step:6321	 l-p:0.11662662029266357
epoch£º316	 i:2 	 global-step:6322	 l-p:0.09013548493385315
epoch£º316	 i:3 	 global-step:6323	 l-p:-0.12436284869909286
epoch£º316	 i:4 	 global-step:6324	 l-p:0.11099861562252045
epoch£º316	 i:5 	 global-step:6325	 l-p:0.16952452063560486
epoch£º316	 i:6 	 global-step:6326	 l-p:0.12528786063194275
epoch£º316	 i:7 	 global-step:6327	 l-p:0.14521001279354095
epoch£º316	 i:8 	 global-step:6328	 l-p:0.0990402027964592
epoch£º316	 i:9 	 global-step:6329	 l-p:0.08774420619010925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6869, 3.6683, 3.6736],
        [3.6869, 3.6861, 3.6868],
        [3.6869, 3.6657, 3.6527],
        [3.6869, 3.6650, 3.6583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.1336790770292282 
model_pd.l_d.mean(): -23.100204467773438 
model_pd.lagr.mean(): -22.96652603149414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0939], device='cuda:0')), ('power', tensor([-23.1941], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.1336790770292282
epoch£º317	 i:1 	 global-step:6341	 l-p:0.07741090655326843
epoch£º317	 i:2 	 global-step:6342	 l-p:0.10401318967342377
epoch£º317	 i:3 	 global-step:6343	 l-p:0.1080092117190361
epoch£º317	 i:4 	 global-step:6344	 l-p:0.12432723492383957
epoch£º317	 i:5 	 global-step:6345	 l-p:0.11944791674613953
epoch£º317	 i:6 	 global-step:6346	 l-p:0.09892750531435013
epoch£º317	 i:7 	 global-step:6347	 l-p:0.07414961606264114
epoch£º317	 i:8 	 global-step:6348	 l-p:0.12088727951049805
epoch£º317	 i:9 	 global-step:6349	 l-p:0.09985239058732986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7216, 3.7030, 3.7074],
        [3.7216, 3.7215, 3.7216],
        [3.7216, 3.7153, 3.7202],
        [3.7216, 3.7211, 3.7216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.11468597501516342 
model_pd.l_d.mean(): -23.016469955444336 
model_pd.lagr.mean(): -22.901784896850586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0859], device='cuda:0')), ('power', tensor([-23.1023], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.11468597501516342
epoch£º318	 i:1 	 global-step:6361	 l-p:0.10636750608682632
epoch£º318	 i:2 	 global-step:6362	 l-p:0.1329621970653534
epoch£º318	 i:3 	 global-step:6363	 l-p:0.10876651853322983
epoch£º318	 i:4 	 global-step:6364	 l-p:0.11218633502721786
epoch£º318	 i:5 	 global-step:6365	 l-p:0.05256563052535057
epoch£º318	 i:6 	 global-step:6366	 l-p:0.14183108508586884
epoch£º318	 i:7 	 global-step:6367	 l-p:0.02056342549622059
epoch£º318	 i:8 	 global-step:6368	 l-p:0.12484817206859589
epoch£º318	 i:9 	 global-step:6369	 l-p:0.10135865211486816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7274, 3.7060, 3.6985],
        [3.7274, 3.7121, 3.6838],
        [3.7274, 3.7274, 3.7274],
        [3.7274, 3.7274, 3.7274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.11912863701581955 
model_pd.l_d.mean(): -23.23506736755371 
model_pd.lagr.mean(): -23.115938186645508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0593], device='cuda:0')), ('power', tensor([-23.2944], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.11912863701581955
epoch£º319	 i:1 	 global-step:6381	 l-p:0.04690318554639816
epoch£º319	 i:2 	 global-step:6382	 l-p:0.10285857319831848
epoch£º319	 i:3 	 global-step:6383	 l-p:0.24134939908981323
epoch£º319	 i:4 	 global-step:6384	 l-p:0.11343861371278763
epoch£º319	 i:5 	 global-step:6385	 l-p:0.0895492359995842
epoch£º319	 i:6 	 global-step:6386	 l-p:0.11474376171827316
epoch£º319	 i:7 	 global-step:6387	 l-p:0.11944723129272461
epoch£º319	 i:8 	 global-step:6388	 l-p:0.1667964905500412
epoch£º319	 i:9 	 global-step:6389	 l-p:0.13028597831726074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6350, 3.6123, 3.5874],
        [3.6350, 3.6314, 3.5784],
        [3.6350, 3.6254, 3.5783],
        [3.6350, 3.6320, 3.6346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.12340113520622253 
model_pd.l_d.mean(): -23.009002685546875 
model_pd.lagr.mean(): -22.885601043701172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0870], device='cuda:0')), ('power', tensor([-23.0960], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.12340113520622253
epoch£º320	 i:1 	 global-step:6401	 l-p:0.1162908747792244
epoch£º320	 i:2 	 global-step:6402	 l-p:-0.0018084632465615869
epoch£º320	 i:3 	 global-step:6403	 l-p:0.12407577037811279
epoch£º320	 i:4 	 global-step:6404	 l-p:0.12949997186660767
epoch£º320	 i:5 	 global-step:6405	 l-p:0.10488300770521164
epoch£º320	 i:6 	 global-step:6406	 l-p:-2.145686626434326
epoch£º320	 i:7 	 global-step:6407	 l-p:0.1308736652135849
epoch£º320	 i:8 	 global-step:6408	 l-p:0.1164991706609726
epoch£º320	 i:9 	 global-step:6409	 l-p:0.09100433439016342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4855, 3.4701, 3.4798],
        [3.4855, 3.5944, 3.5348],
        [3.4855, 3.4809, 3.4849],
        [3.4855, 3.4853, 3.4855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.09441251307725906 
model_pd.l_d.mean(): -22.606290817260742 
model_pd.lagr.mean(): -22.511878967285156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1998], device='cuda:0')), ('power', tensor([-22.8061], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.09441251307725906
epoch£º321	 i:1 	 global-step:6421	 l-p:0.13289636373519897
epoch£º321	 i:2 	 global-step:6422	 l-p:0.10544121265411377
epoch£º321	 i:3 	 global-step:6423	 l-p:0.12397797405719757
epoch£º321	 i:4 	 global-step:6424	 l-p:0.11331532150506973
epoch£º321	 i:5 	 global-step:6425	 l-p:0.10539267957210541
epoch£º321	 i:6 	 global-step:6426	 l-p:0.03628937527537346
epoch£º321	 i:7 	 global-step:6427	 l-p:0.13429301977157593
epoch£º321	 i:8 	 global-step:6428	 l-p:0.11464343965053558
epoch£º321	 i:9 	 global-step:6429	 l-p:0.1280798465013504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5225, 3.9521, 4.1115],
        [3.5225, 3.5224, 3.5225],
        [3.5225, 3.5221, 3.5225],
        [3.5225, 3.5225, 3.5225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.12801507115364075 
model_pd.l_d.mean(): -23.362342834472656 
model_pd.lagr.mean(): -23.23432731628418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1172], device='cuda:0')), ('power', tensor([-23.4796], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.12801507115364075
epoch£º322	 i:1 	 global-step:6441	 l-p:0.1316181868314743
epoch£º322	 i:2 	 global-step:6442	 l-p:0.09082263708114624
epoch£º322	 i:3 	 global-step:6443	 l-p:0.1269315630197525
epoch£º322	 i:4 	 global-step:6444	 l-p:0.15452788770198822
epoch£º322	 i:5 	 global-step:6445	 l-p:0.07049980014562607
epoch£º322	 i:6 	 global-step:6446	 l-p:0.09571930766105652
epoch£º322	 i:7 	 global-step:6447	 l-p:0.11281301081180573
epoch£º322	 i:8 	 global-step:6448	 l-p:0.21182960271835327
epoch£º322	 i:9 	 global-step:6449	 l-p:0.12409903109073639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6320, 3.6320, 3.6320],
        [3.6320, 3.6276, 3.6314],
        [3.6320, 3.6691, 3.5958],
        [3.6320, 3.6160, 3.6249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.08691594749689102 
model_pd.l_d.mean(): -23.1495418548584 
model_pd.lagr.mean(): -23.062625885009766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1120], device='cuda:0')), ('power', tensor([-23.2616], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.08691594749689102
epoch£º323	 i:1 	 global-step:6461	 l-p:0.11695940047502518
epoch£º323	 i:2 	 global-step:6462	 l-p:0.40021222829818726
epoch£º323	 i:3 	 global-step:6463	 l-p:0.14224059879779816
epoch£º323	 i:4 	 global-step:6464	 l-p:0.10497962683439255
epoch£º323	 i:5 	 global-step:6465	 l-p:5.079006671905518
epoch£º323	 i:6 	 global-step:6466	 l-p:0.12432239204645157
epoch£º323	 i:7 	 global-step:6467	 l-p:0.1044783741235733
epoch£º323	 i:8 	 global-step:6468	 l-p:0.09683305770158768
epoch£º323	 i:9 	 global-step:6469	 l-p:0.1043820008635521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8059, 3.8059, 3.8059],
        [3.8059, 3.8059, 3.8059],
        [3.8059, 3.7982, 3.8039],
        [3.8059, 3.7846, 3.7754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.207946315407753 
model_pd.l_d.mean(): -22.637271881103516 
model_pd.lagr.mean(): -22.429325103759766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1276], device='cuda:0')), ('power', tensor([-22.7649], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.207946315407753
epoch£º324	 i:1 	 global-step:6481	 l-p:0.0834592804312706
epoch£º324	 i:2 	 global-step:6482	 l-p:0.11953967064619064
epoch£º324	 i:3 	 global-step:6483	 l-p:0.10756176710128784
epoch£º324	 i:4 	 global-step:6484	 l-p:0.10361499339342117
epoch£º324	 i:5 	 global-step:6485	 l-p:0.13613666594028473
epoch£º324	 i:6 	 global-step:6486	 l-p:0.060868870466947556
epoch£º324	 i:7 	 global-step:6487	 l-p:0.09105999022722244
epoch£º324	 i:8 	 global-step:6488	 l-p:0.11213760077953339
epoch£º324	 i:9 	 global-step:6489	 l-p:-0.3049261271953583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6434, 3.6190, 3.5865],
        [3.6434, 3.6344, 3.6412],
        [3.6434, 3.6433, 3.6434],
        [3.6434, 3.6522, 3.5853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.09698991477489471 
model_pd.l_d.mean(): -22.392364501953125 
model_pd.lagr.mean(): -22.295373916625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1361], device='cuda:0')), ('power', tensor([-22.5284], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.09698991477489471
epoch£º325	 i:1 	 global-step:6501	 l-p:0.16027343273162842
epoch£º325	 i:2 	 global-step:6502	 l-p:0.12056047469377518
epoch£º325	 i:3 	 global-step:6503	 l-p:0.10135480761528015
epoch£º325	 i:4 	 global-step:6504	 l-p:0.49573084712028503
epoch£º325	 i:5 	 global-step:6505	 l-p:0.11288057267665863
epoch£º325	 i:6 	 global-step:6506	 l-p:0.07952598482370377
epoch£º325	 i:7 	 global-step:6507	 l-p:0.11577583849430084
epoch£º325	 i:8 	 global-step:6508	 l-p:0.11017654836177826
epoch£º325	 i:9 	 global-step:6509	 l-p:0.12716363370418549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7056, 3.7139, 3.6500],
        [3.7056, 3.7047, 3.7056],
        [3.7056, 4.1646, 4.3319],
        [3.7056, 3.7068, 3.6474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.09096036851406097 
model_pd.l_d.mean(): -23.07521629333496 
model_pd.lagr.mean(): -22.984256744384766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0841], device='cuda:0')), ('power', tensor([-23.1593], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.09096036851406097
epoch£º326	 i:1 	 global-step:6521	 l-p:0.335449755191803
epoch£º326	 i:2 	 global-step:6522	 l-p:0.12712010741233826
epoch£º326	 i:3 	 global-step:6523	 l-p:0.10497672110795975
epoch£º326	 i:4 	 global-step:6524	 l-p:0.0993904396891594
epoch£º326	 i:5 	 global-step:6525	 l-p:0.13097716867923737
epoch£º326	 i:6 	 global-step:6526	 l-p:0.17208091914653778
epoch£º326	 i:7 	 global-step:6527	 l-p:0.10722485929727554
epoch£º326	 i:8 	 global-step:6528	 l-p:0.32071390748023987
epoch£º326	 i:9 	 global-step:6529	 l-p:0.11735614389181137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6053, 4.0395, 4.1943],
        [3.6053, 3.6052, 3.6053],
        [3.6053, 3.5767, 3.5816],
        [3.6053, 3.6043, 3.6053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.13051080703735352 
model_pd.l_d.mean(): -22.987037658691406 
model_pd.lagr.mean(): -22.85652732849121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1274], device='cuda:0')), ('power', tensor([-23.1145], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.13051080703735352
epoch£º327	 i:1 	 global-step:6541	 l-p:0.21456533670425415
epoch£º327	 i:2 	 global-step:6542	 l-p:0.13046050071716309
epoch£º327	 i:3 	 global-step:6543	 l-p:0.11459334194660187
epoch£º327	 i:4 	 global-step:6544	 l-p:0.2021099179983139
epoch£º327	 i:5 	 global-step:6545	 l-p:0.13046784698963165
epoch£º327	 i:6 	 global-step:6546	 l-p:0.0961587131023407
epoch£º327	 i:7 	 global-step:6547	 l-p:0.111077681183815
epoch£º327	 i:8 	 global-step:6548	 l-p:0.08381809294223785
epoch£º327	 i:9 	 global-step:6549	 l-p:0.1168174222111702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5518, 3.5449, 3.5506],
        [3.5518, 3.5490, 3.5515],
        [3.5518, 3.5362, 3.4749],
        [3.5518, 3.7676, 3.7608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.1159682646393776 
model_pd.l_d.mean(): -22.96830940246582 
model_pd.lagr.mean(): -22.852340698242188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1692], device='cuda:0')), ('power', tensor([-23.1375], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.1159682646393776
epoch£º328	 i:1 	 global-step:6561	 l-p:0.14851146936416626
epoch£º328	 i:2 	 global-step:6562	 l-p:0.11747120320796967
epoch£º328	 i:3 	 global-step:6563	 l-p:0.0819224938750267
epoch£º328	 i:4 	 global-step:6564	 l-p:0.13391967117786407
epoch£º328	 i:5 	 global-step:6565	 l-p:0.09902510792016983
epoch£º328	 i:6 	 global-step:6566	 l-p:0.08937417715787888
epoch£º328	 i:7 	 global-step:6567	 l-p:0.1250733733177185
epoch£º328	 i:8 	 global-step:6568	 l-p:0.11305040121078491
epoch£º328	 i:9 	 global-step:6569	 l-p:0.09908054023981094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4732, 3.4732, 3.4732],
        [3.4732, 3.5252, 3.4443],
        [3.4732, 3.4722, 3.4732],
        [3.4732, 3.4436, 3.4542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.11218574643135071 
model_pd.l_d.mean(): -22.21763038635254 
model_pd.lagr.mean(): -22.105443954467773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2574], device='cuda:0')), ('power', tensor([-22.4750], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.11218574643135071
epoch£º329	 i:1 	 global-step:6581	 l-p:0.10853397101163864
epoch£º329	 i:2 	 global-step:6582	 l-p:0.10234204679727554
epoch£º329	 i:3 	 global-step:6583	 l-p:0.12838396430015564
epoch£º329	 i:4 	 global-step:6584	 l-p:0.08797973394393921
epoch£º329	 i:5 	 global-step:6585	 l-p:0.1303977370262146
epoch£º329	 i:6 	 global-step:6586	 l-p:0.11988923698663712
epoch£º329	 i:7 	 global-step:6587	 l-p:0.08869154006242752
epoch£º329	 i:8 	 global-step:6588	 l-p:0.13348506391048431
epoch£º329	 i:9 	 global-step:6589	 l-p:0.1264558583498001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4593, 3.4584, 3.4592],
        [3.4593, 3.4580, 3.4592],
        [3.4593, 3.4585, 3.4592],
        [3.4593, 3.4593, 3.4593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.12155639380216599 
model_pd.l_d.mean(): -23.29623031616211 
model_pd.lagr.mean(): -23.174673080444336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1520], device='cuda:0')), ('power', tensor([-23.4482], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.12155639380216599
epoch£º330	 i:1 	 global-step:6601	 l-p:0.136170893907547
epoch£º330	 i:2 	 global-step:6602	 l-p:-0.013762621209025383
epoch£º330	 i:3 	 global-step:6603	 l-p:0.10114116221666336
epoch£º330	 i:4 	 global-step:6604	 l-p:0.11895814538002014
epoch£º330	 i:5 	 global-step:6605	 l-p:0.13581180572509766
epoch£º330	 i:6 	 global-step:6606	 l-p:0.11384746432304382
epoch£º330	 i:7 	 global-step:6607	 l-p:0.13316376507282257
epoch£º330	 i:8 	 global-step:6608	 l-p:0.12307170778512955
epoch£º330	 i:9 	 global-step:6609	 l-p:0.12089154869318008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3648, 3.3191, 3.2656],
        [3.3648, 3.6739, 3.7443],
        [3.3648, 3.3608, 3.3643],
        [3.3648, 3.6109, 3.6347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.10238349437713623 
model_pd.l_d.mean(): -23.099637985229492 
model_pd.lagr.mean(): -22.997255325317383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2308], device='cuda:0')), ('power', tensor([-23.3304], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.10238349437713623
epoch£º331	 i:1 	 global-step:6621	 l-p:0.14155422151088715
epoch£º331	 i:2 	 global-step:6622	 l-p:0.02419370599091053
epoch£º331	 i:3 	 global-step:6623	 l-p:0.13889919221401215
epoch£º331	 i:4 	 global-step:6624	 l-p:0.11064556986093521
epoch£º331	 i:5 	 global-step:6625	 l-p:0.12498790770769119
epoch£º331	 i:6 	 global-step:6626	 l-p:0.12899938225746155
epoch£º331	 i:7 	 global-step:6627	 l-p:0.126851424574852
epoch£º331	 i:8 	 global-step:6628	 l-p:0.1112772524356842
epoch£º331	 i:9 	 global-step:6629	 l-p:0.11175084114074707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4692, 3.4228, 3.4011],
        [3.4692, 3.4754, 3.3903],
        [3.4692, 3.4321, 3.3772],
        [3.4692, 3.4659, 3.4689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.08368179947137833 
model_pd.l_d.mean(): -23.136377334594727 
model_pd.lagr.mean(): -23.052696228027344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1923], device='cuda:0')), ('power', tensor([-23.3286], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.08368179947137833
epoch£º332	 i:1 	 global-step:6641	 l-p:0.12355317175388336
epoch£º332	 i:2 	 global-step:6642	 l-p:0.1306125372648239
epoch£º332	 i:3 	 global-step:6643	 l-p:0.15399786829948425
epoch£º332	 i:4 	 global-step:6644	 l-p:0.1351458579301834
epoch£º332	 i:5 	 global-step:6645	 l-p:0.12393758445978165
epoch£º332	 i:6 	 global-step:6646	 l-p:0.12538768351078033
epoch£º332	 i:7 	 global-step:6647	 l-p:0.08531030267477036
epoch£º332	 i:8 	 global-step:6648	 l-p:0.11257771402597427
epoch£º332	 i:9 	 global-step:6649	 l-p:0.09781701862812042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5093, 3.5093, 3.5093],
        [3.5093, 3.5552, 3.4698],
        [3.5093, 3.5093, 3.5093],
        [3.5093, 3.5092, 3.5093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.11703519523143768 
model_pd.l_d.mean(): -23.193706512451172 
model_pd.lagr.mean(): -23.076671600341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1504], device='cuda:0')), ('power', tensor([-23.3441], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.11703519523143768
epoch£º333	 i:1 	 global-step:6661	 l-p:0.10313871502876282
epoch£º333	 i:2 	 global-step:6662	 l-p:0.08412165194749832
epoch£º333	 i:3 	 global-step:6663	 l-p:0.12715551257133484
epoch£º333	 i:4 	 global-step:6664	 l-p:0.12474187463521957
epoch£º333	 i:5 	 global-step:6665	 l-p:0.12870770692825317
epoch£º333	 i:6 	 global-step:6666	 l-p:0.12268203496932983
epoch£º333	 i:7 	 global-step:6667	 l-p:0.14359016716480255
epoch£º333	 i:8 	 global-step:6668	 l-p:0.12045840173959732
epoch£º333	 i:9 	 global-step:6669	 l-p:0.10091403871774673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3791, 3.3786, 3.3791],
        [3.3791, 3.3759, 3.3788],
        [3.3791, 3.3249, 3.2855],
        [3.3791, 3.3276, 3.3236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.14026887714862823 
model_pd.l_d.mean(): -23.585861206054688 
model_pd.lagr.mean(): -23.445592880249023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1481], device='cuda:0')), ('power', tensor([-23.7340], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.14026887714862823
epoch£º334	 i:1 	 global-step:6681	 l-p:0.134147047996521
epoch£º334	 i:2 	 global-step:6682	 l-p:0.1334328055381775
epoch£º334	 i:3 	 global-step:6683	 l-p:0.1309221386909485
epoch£º334	 i:4 	 global-step:6684	 l-p:0.11161082983016968
epoch£º334	 i:5 	 global-step:6685	 l-p:0.12565694749355316
epoch£º334	 i:6 	 global-step:6686	 l-p:0.1334540694952011
epoch£º334	 i:7 	 global-step:6687	 l-p:0.12677296996116638
epoch£º334	 i:8 	 global-step:6688	 l-p:0.13359403610229492
epoch£º334	 i:9 	 global-step:6689	 l-p:0.08816157281398773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3668, 3.3253, 3.3370],
        [3.3668, 3.6538, 3.7047],
        [3.3668, 3.3668, 3.3668],
        [3.3668, 3.6997, 3.7857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.06625581532716751 
model_pd.l_d.mean(): -22.374080657958984 
model_pd.lagr.mean(): -22.307825088500977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2892], device='cuda:0')), ('power', tensor([-22.6633], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.06625581532716751
epoch£º335	 i:1 	 global-step:6701	 l-p:0.1332298368215561
epoch£º335	 i:2 	 global-step:6702	 l-p:0.11643270403146744
epoch£º335	 i:3 	 global-step:6703	 l-p:0.13879315555095673
epoch£º335	 i:4 	 global-step:6704	 l-p:0.11698686331510544
epoch£º335	 i:5 	 global-step:6705	 l-p:0.10166022926568985
epoch£º335	 i:6 	 global-step:6706	 l-p:0.13158313930034637
epoch£º335	 i:7 	 global-step:6707	 l-p:0.0882205069065094
epoch£º335	 i:8 	 global-step:6708	 l-p:0.13330602645874023
epoch£º335	 i:9 	 global-step:6709	 l-p:0.12009447067975998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[3.4620, 3.4238, 3.4353],
        [3.4620, 3.4109, 3.4025],
        [3.4620, 3.4940, 3.4022],
        [3.4620, 3.4201, 3.4288]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.11539657413959503 
model_pd.l_d.mean(): -22.48823356628418 
model_pd.lagr.mean(): -22.37283706665039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1836], device='cuda:0')), ('power', tensor([-22.6718], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.11539657413959503
epoch£º336	 i:1 	 global-step:6721	 l-p:0.12504532933235168
epoch£º336	 i:2 	 global-step:6722	 l-p:0.07367316633462906
epoch£º336	 i:3 	 global-step:6723	 l-p:0.1302759200334549
epoch£º336	 i:4 	 global-step:6724	 l-p:0.118593730032444
epoch£º336	 i:5 	 global-step:6725	 l-p:0.12609024345874786
epoch£º336	 i:6 	 global-step:6726	 l-p:0.13874554634094238
epoch£º336	 i:7 	 global-step:6727	 l-p:0.11766505241394043
epoch£º336	 i:8 	 global-step:6728	 l-p:0.12399306893348694
epoch£º336	 i:9 	 global-step:6729	 l-p:0.1224038377404213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4473, 3.8712, 4.0240],
        [3.4473, 3.4141, 3.4285],
        [3.4473, 3.4103, 3.3345],
        [3.4473, 3.5984, 3.5527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.1128631979227066 
model_pd.l_d.mean(): -21.829343795776367 
model_pd.lagr.mean(): -21.716480255126953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2572], device='cuda:0')), ('power', tensor([-22.0866], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.1128631979227066
epoch£º337	 i:1 	 global-step:6741	 l-p:0.1363651007413864
epoch£º337	 i:2 	 global-step:6742	 l-p:0.11876018345355988
epoch£º337	 i:3 	 global-step:6743	 l-p:0.1127549335360527
epoch£º337	 i:4 	 global-step:6744	 l-p:0.1368681639432907
epoch£º337	 i:5 	 global-step:6745	 l-p:0.07571842521429062
epoch£º337	 i:6 	 global-step:6746	 l-p:0.10881059616804123
epoch£º337	 i:7 	 global-step:6747	 l-p:0.13159671425819397
epoch£º337	 i:8 	 global-step:6748	 l-p:0.13719694316387177
epoch£º337	 i:9 	 global-step:6749	 l-p:0.12986567616462708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3657, 3.3148, 3.3230],
        [3.3657, 3.3383, 3.3541],
        [3.3657, 3.3597, 3.3650],
        [3.3657, 3.3657, 3.3657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.122224822640419 
model_pd.l_d.mean(): -23.121320724487305 
model_pd.lagr.mean(): -22.999095916748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2277], device='cuda:0')), ('power', tensor([-23.3491], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.122224822640419
epoch£º338	 i:1 	 global-step:6761	 l-p:0.11625034362077713
epoch£º338	 i:2 	 global-step:6762	 l-p:0.14257173240184784
epoch£º338	 i:3 	 global-step:6763	 l-p:0.1279842108488083
epoch£º338	 i:4 	 global-step:6764	 l-p:0.13610711693763733
epoch£º338	 i:5 	 global-step:6765	 l-p:0.1474752426147461
epoch£º338	 i:6 	 global-step:6766	 l-p:0.14662227034568787
epoch£º338	 i:7 	 global-step:6767	 l-p:0.11530566960573196
epoch£º338	 i:8 	 global-step:6768	 l-p:0.12949925661087036
epoch£º338	 i:9 	 global-step:6769	 l-p:0.1425207406282425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3524, 3.5272, 3.4983],
        [3.3524, 3.3113, 3.3272],
        [3.3524, 3.3232, 3.3397],
        [3.3524, 3.3524, 3.3524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.1179368868470192 
model_pd.l_d.mean(): -23.24671173095703 
model_pd.lagr.mean(): -23.128774642944336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2226], device='cuda:0')), ('power', tensor([-23.4693], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.1179368868470192
epoch£º339	 i:1 	 global-step:6781	 l-p:0.13530495762825012
epoch£º339	 i:2 	 global-step:6782	 l-p:0.11020554602146149
epoch£º339	 i:3 	 global-step:6783	 l-p:0.12947455048561096
epoch£º339	 i:4 	 global-step:6784	 l-p:0.12173446267843246
epoch£º339	 i:5 	 global-step:6785	 l-p:-0.19349151849746704
epoch£º339	 i:6 	 global-step:6786	 l-p:0.135359987616539
epoch£º339	 i:7 	 global-step:6787	 l-p:0.09318521618843079
epoch£º339	 i:8 	 global-step:6788	 l-p:0.12839849293231964
epoch£º339	 i:9 	 global-step:6789	 l-p:0.10028216987848282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5335, 3.6446, 3.5723],
        [3.5335, 3.5308, 3.5333],
        [3.5335, 3.5334, 3.5335],
        [3.5335, 3.4879, 3.4956]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.03594297170639038 
model_pd.l_d.mean(): -22.874937057495117 
model_pd.lagr.mean(): -22.8389949798584 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1878], device='cuda:0')), ('power', tensor([-23.0627], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.03594297170639038
epoch£º340	 i:1 	 global-step:6801	 l-p:0.11614816635847092
epoch£º340	 i:2 	 global-step:6802	 l-p:0.11664312332868576
epoch£º340	 i:3 	 global-step:6803	 l-p:0.1177106723189354
epoch£º340	 i:4 	 global-step:6804	 l-p:0.10823952406644821
epoch£º340	 i:5 	 global-step:6805	 l-p:0.12862905859947205
epoch£º340	 i:6 	 global-step:6806	 l-p:0.11941546201705933
epoch£º340	 i:7 	 global-step:6807	 l-p:0.11286007612943649
epoch£º340	 i:8 	 global-step:6808	 l-p:0.12846913933753967
epoch£º340	 i:9 	 global-step:6809	 l-p:-1.909732460975647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6071, 3.6062, 3.6071],
        [3.6071, 3.5560, 3.5478],
        [3.6071, 3.7824, 3.7403],
        [3.6071, 3.7930, 3.7570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.10684970021247864 
model_pd.l_d.mean(): -22.990432739257812 
model_pd.lagr.mean(): -22.883583068847656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1582], device='cuda:0')), ('power', tensor([-23.1486], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.10684970021247864
epoch£º341	 i:1 	 global-step:6821	 l-p:0.11696210503578186
epoch£º341	 i:2 	 global-step:6822	 l-p:0.12060409039258957
epoch£º341	 i:3 	 global-step:6823	 l-p:0.1409893035888672
epoch£º341	 i:4 	 global-step:6824	 l-p:0.11406556516885757
epoch£º341	 i:5 	 global-step:6825	 l-p:0.08890703320503235
epoch£º341	 i:6 	 global-step:6826	 l-p:0.12187036126852036
epoch£º341	 i:7 	 global-step:6827	 l-p:0.119758240878582
epoch£º341	 i:8 	 global-step:6828	 l-p:0.13064393401145935
epoch£º341	 i:9 	 global-step:6829	 l-p:0.10529450327157974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4511, 3.4511, 3.4511],
        [3.4511, 3.6408, 3.6141],
        [3.4511, 3.7596, 3.8156],
        [3.4511, 3.6413, 3.6150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.12601648271083832 
model_pd.l_d.mean(): -23.436874389648438 
model_pd.lagr.mean(): -23.31085777282715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1510], device='cuda:0')), ('power', tensor([-23.5879], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.12601648271083832
epoch£º342	 i:1 	 global-step:6841	 l-p:0.12539680302143097
epoch£º342	 i:2 	 global-step:6842	 l-p:0.12839949131011963
epoch£º342	 i:3 	 global-step:6843	 l-p:0.11520309001207352
epoch£º342	 i:4 	 global-step:6844	 l-p:0.11027299612760544
epoch£º342	 i:5 	 global-step:6845	 l-p:0.13372758030891418
epoch£º342	 i:6 	 global-step:6846	 l-p:0.1265564113855362
epoch£º342	 i:7 	 global-step:6847	 l-p:0.141677588224411
epoch£º342	 i:8 	 global-step:6848	 l-p:0.08435177803039551
epoch£º342	 i:9 	 global-step:6849	 l-p:0.13706135749816895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3850, 3.5531, 3.5153],
        [3.3850, 3.3850, 3.3850],
        [3.3850, 3.6619, 3.6973],
        [3.3850, 3.3599, 3.3760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.12793254852294922 
model_pd.l_d.mean(): -23.499814987182617 
model_pd.lagr.mean(): -23.371883392333984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1638], device='cuda:0')), ('power', tensor([-23.6636], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.12793254852294922
epoch£º343	 i:1 	 global-step:6861	 l-p:0.13100391626358032
epoch£º343	 i:2 	 global-step:6862	 l-p:0.13492022454738617
epoch£º343	 i:3 	 global-step:6863	 l-p:0.13451939821243286
epoch£º343	 i:4 	 global-step:6864	 l-p:0.13720999658107758
epoch£º343	 i:5 	 global-step:6865	 l-p:0.15385746955871582
epoch£º343	 i:6 	 global-step:6866	 l-p:0.1308637410402298
epoch£º343	 i:7 	 global-step:6867	 l-p:0.131340891122818
epoch£º343	 i:8 	 global-step:6868	 l-p:0.12698565423488617
epoch£º343	 i:9 	 global-step:6869	 l-p:0.13132810592651367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[3.2698, 3.5064, 3.5177],
        [3.2698, 3.4135, 3.3645],
        [3.2698, 3.1990, 3.2018],
        [3.2698, 3.2013, 3.2071]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.1393207162618637 
model_pd.l_d.mean(): -23.527570724487305 
model_pd.lagr.mean(): -23.38825035095215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2095], device='cuda:0')), ('power', tensor([-23.7371], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.1393207162618637
epoch£º344	 i:1 	 global-step:6881	 l-p:0.1462354063987732
epoch£º344	 i:2 	 global-step:6882	 l-p:0.12374284118413925
epoch£º344	 i:3 	 global-step:6883	 l-p:0.1169305220246315
epoch£º344	 i:4 	 global-step:6884	 l-p:0.12197349220514297
epoch£º344	 i:5 	 global-step:6885	 l-p:0.1384061723947525
epoch£º344	 i:6 	 global-step:6886	 l-p:0.11403034627437592
epoch£º344	 i:7 	 global-step:6887	 l-p:0.0352100133895874
epoch£º344	 i:8 	 global-step:6888	 l-p:0.1349174678325653
epoch£º344	 i:9 	 global-step:6889	 l-p:0.1322156935930252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4964, 3.4791, 3.4918],
        [3.4964, 3.8081, 3.8611],
        [3.4964, 3.4747, 3.4894],
        [3.4964, 3.4413, 3.3613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.1218504011631012 
model_pd.l_d.mean(): -22.57862091064453 
model_pd.lagr.mean(): -22.456769943237305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1716], device='cuda:0')), ('power', tensor([-22.7503], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.1218504011631012
epoch£º345	 i:1 	 global-step:6901	 l-p:0.11967219412326813
epoch£º345	 i:2 	 global-step:6902	 l-p:0.14329186081886292
epoch£º345	 i:3 	 global-step:6903	 l-p:0.129819855093956
epoch£º345	 i:4 	 global-step:6904	 l-p:0.11974114924669266
epoch£º345	 i:5 	 global-step:6905	 l-p:0.12515603005886078
epoch£º345	 i:6 	 global-step:6906	 l-p:0.09740138798952103
epoch£º345	 i:7 	 global-step:6907	 l-p:0.1310308873653412
epoch£º345	 i:8 	 global-step:6908	 l-p:0.12279774993658066
epoch£º345	 i:9 	 global-step:6909	 l-p:0.07515247911214828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4774, 3.4666, 3.4755],
        [3.4774, 3.4773, 3.4774],
        [3.4774, 3.4480, 3.4653],
        [3.4774, 3.4321, 3.3351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.08039860427379608 
model_pd.l_d.mean(): -22.85228157043457 
model_pd.lagr.mean(): -22.771883010864258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2359], device='cuda:0')), ('power', tensor([-23.0881], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.08039860427379608
epoch£º346	 i:1 	 global-step:6921	 l-p:0.12540274858474731
epoch£º346	 i:2 	 global-step:6922	 l-p:0.11397503316402435
epoch£º346	 i:3 	 global-step:6923	 l-p:0.11789657175540924
epoch£º346	 i:4 	 global-step:6924	 l-p:0.1095723882317543
epoch£º346	 i:5 	 global-step:6925	 l-p:0.11643768101930618
epoch£º346	 i:6 	 global-step:6926	 l-p:0.11397229880094528
epoch£º346	 i:7 	 global-step:6927	 l-p:0.12576207518577576
epoch£º346	 i:8 	 global-step:6928	 l-p:0.5447338223457336
epoch£º346	 i:9 	 global-step:6929	 l-p:0.1418350636959076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4574, 3.4574, 3.4574],
        [3.4574, 3.3930, 3.3090],
        [3.4574, 3.7790, 3.8395],
        [3.4574, 3.4526, 3.4570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): -0.09186490625143051 
model_pd.l_d.mean(): -22.904434204101562 
model_pd.lagr.mean(): -22.996299743652344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2305], device='cuda:0')), ('power', tensor([-23.1349], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:-0.09186490625143051
epoch£º347	 i:1 	 global-step:6941	 l-p:0.1270591765642166
epoch£º347	 i:2 	 global-step:6942	 l-p:0.13780593872070312
epoch£º347	 i:3 	 global-step:6943	 l-p:0.13198541104793549
epoch£º347	 i:4 	 global-step:6944	 l-p:0.11297011375427246
epoch£º347	 i:5 	 global-step:6945	 l-p:0.12542693316936493
epoch£º347	 i:6 	 global-step:6946	 l-p:0.1421341896057129
epoch£º347	 i:7 	 global-step:6947	 l-p:0.12360549718141556
epoch£º347	 i:8 	 global-step:6948	 l-p:0.12347052991390228
epoch£º347	 i:9 	 global-step:6949	 l-p:0.13551580905914307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2759, 3.2222, 3.2427],
        [3.2759, 3.2439, 3.2637],
        [3.2759, 3.2262, 3.2474],
        [3.2759, 3.2759, 3.2759]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.13686609268188477 
model_pd.l_d.mean(): -23.273653030395508 
model_pd.lagr.mean(): -23.13678741455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2379], device='cuda:0')), ('power', tensor([-23.5116], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.13686609268188477
epoch£º348	 i:1 	 global-step:6961	 l-p:0.14085912704467773
epoch£º348	 i:2 	 global-step:6962	 l-p:0.14458684623241425
epoch£º348	 i:3 	 global-step:6963	 l-p:0.14692211151123047
epoch£º348	 i:4 	 global-step:6964	 l-p:0.13987120985984802
epoch£º348	 i:5 	 global-step:6965	 l-p:0.12859629094600677
epoch£º348	 i:6 	 global-step:6966	 l-p:0.13456453382968903
epoch£º348	 i:7 	 global-step:6967	 l-p:0.12655073404312134
epoch£º348	 i:8 	 global-step:6968	 l-p:0.1342473328113556
epoch£º348	 i:9 	 global-step:6969	 l-p:0.1343870460987091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3717, 3.3717, 3.3717],
        [3.3717, 3.3700, 3.3716],
        [3.3717, 3.3696, 3.3715],
        [3.3717, 3.3408, 3.3600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.12035612016916275 
model_pd.l_d.mean(): -22.149063110351562 
model_pd.lagr.mean(): -22.02870750427246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2979], device='cuda:0')), ('power', tensor([-22.4469], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.12035612016916275
epoch£º349	 i:1 	 global-step:6981	 l-p:0.11944284290075302
epoch£º349	 i:2 	 global-step:6982	 l-p:0.1425388902425766
epoch£º349	 i:3 	 global-step:6983	 l-p:0.11954326182603836
epoch£º349	 i:4 	 global-step:6984	 l-p:0.3109227418899536
epoch£º349	 i:5 	 global-step:6985	 l-p:0.12654563784599304
epoch£º349	 i:6 	 global-step:6986	 l-p:0.07200232148170471
epoch£º349	 i:7 	 global-step:6987	 l-p:0.10377223789691925
epoch£º349	 i:8 	 global-step:6988	 l-p:0.10423879325389862
epoch£º349	 i:9 	 global-step:6989	 l-p:0.01967131532728672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6232, 3.6214, 3.6231],
        [3.6232, 3.8016, 3.7527],
        [3.6232, 3.7120, 3.6180],
        [3.6232, 3.8073, 3.7618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.12776130437850952 
model_pd.l_d.mean(): -23.22161102294922 
model_pd.lagr.mean(): -23.093849182128906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1217], device='cuda:0')), ('power', tensor([-23.3433], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.12776130437850952
epoch£º350	 i:1 	 global-step:7001	 l-p:0.10815087705850601
epoch£º350	 i:2 	 global-step:7002	 l-p:0.030625661835074425
epoch£º350	 i:3 	 global-step:7003	 l-p:0.11891969293355942
epoch£º350	 i:4 	 global-step:7004	 l-p:0.1336367130279541
epoch£º350	 i:5 	 global-step:7005	 l-p:0.12158721685409546
epoch£º350	 i:6 	 global-step:7006	 l-p:0.08799441158771515
epoch£º350	 i:7 	 global-step:7007	 l-p:0.09892505407333374
epoch£º350	 i:8 	 global-step:7008	 l-p:0.004320321138948202
epoch£º350	 i:9 	 global-step:7009	 l-p:0.10124364495277405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6162, 3.5742, 3.5921],
        [3.6162, 3.5627, 3.5755],
        [3.6162, 3.5942, 3.4840],
        [3.6162, 3.6035, 3.4911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.12931257486343384 
model_pd.l_d.mean(): -23.03434181213379 
model_pd.lagr.mean(): -22.905029296875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1561], device='cuda:0')), ('power', tensor([-23.1904], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.12931257486343384
epoch£º351	 i:1 	 global-step:7021	 l-p:0.12838764488697052
epoch£º351	 i:2 	 global-step:7022	 l-p:0.21027591824531555
epoch£º351	 i:3 	 global-step:7023	 l-p:0.11120980978012085
epoch£º351	 i:4 	 global-step:7024	 l-p:0.1166432723402977
epoch£º351	 i:5 	 global-step:7025	 l-p:0.10080546885728836
epoch£º351	 i:6 	 global-step:7026	 l-p:0.11663433909416199
epoch£º351	 i:7 	 global-step:7027	 l-p:0.08068576455116272
epoch£º351	 i:8 	 global-step:7028	 l-p:0.12470567226409912
epoch£º351	 i:9 	 global-step:7029	 l-p:0.06534324586391449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5532, 3.5128, 3.5323],
        [3.5532, 3.4902, 3.3967],
        [3.5532, 3.4891, 3.4967],
        [3.5532, 3.5475, 3.4289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.1304945945739746 
model_pd.l_d.mean(): -23.619661331176758 
model_pd.lagr.mean(): -23.489166259765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0739], device='cuda:0')), ('power', tensor([-23.6936], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.1304945945739746
epoch£º352	 i:1 	 global-step:7041	 l-p:0.12625379860401154
epoch£º352	 i:2 	 global-step:7042	 l-p:0.11437932401895523
epoch£º352	 i:3 	 global-step:7043	 l-p:0.12092173099517822
epoch£º352	 i:4 	 global-step:7044	 l-p:0.12549006938934326
epoch£º352	 i:5 	 global-step:7045	 l-p:0.11921046674251556
epoch£º352	 i:6 	 global-step:7046	 l-p:0.12390287965536118
epoch£º352	 i:7 	 global-step:7047	 l-p:0.08939273655414581
epoch£º352	 i:8 	 global-step:7048	 l-p:0.1279740333557129
epoch£º352	 i:9 	 global-step:7049	 l-p:0.11259300261735916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4181, 3.4086, 3.4167],
        [3.4181, 3.4147, 3.4178],
        [3.4181, 3.4150, 3.4179],
        [3.4181, 3.4180, 3.4181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.11856397241353989 
model_pd.l_d.mean(): -23.29999542236328 
model_pd.lagr.mean(): -23.18143081665039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1852], device='cuda:0')), ('power', tensor([-23.4852], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.11856397241353989
epoch£º353	 i:1 	 global-step:7061	 l-p:0.1250264197587967
epoch£º353	 i:2 	 global-step:7062	 l-p:0.10492540895938873
epoch£º353	 i:3 	 global-step:7063	 l-p:0.13635359704494476
epoch£º353	 i:4 	 global-step:7064	 l-p:0.13550543785095215
epoch£º353	 i:5 	 global-step:7065	 l-p:0.11411239951848984
epoch£º353	 i:6 	 global-step:7066	 l-p:0.11515367031097412
epoch£º353	 i:7 	 global-step:7067	 l-p:0.13267795741558075
epoch£º353	 i:8 	 global-step:7068	 l-p:0.14797168970108032
epoch£º353	 i:9 	 global-step:7069	 l-p:0.08180806785821915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3988, 3.3597, 3.3817],
        [3.3988, 3.3838, 3.2536],
        [3.3988, 3.6932, 3.7287],
        [3.3988, 3.3894, 3.3975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.12940222024917603 
model_pd.l_d.mean(): -23.50131607055664 
model_pd.lagr.mean(): -23.37191390991211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1482], device='cuda:0')), ('power', tensor([-23.6495], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.12940222024917603
epoch£º354	 i:1 	 global-step:7081	 l-p:0.12687990069389343
epoch£º354	 i:2 	 global-step:7082	 l-p:0.12567853927612305
epoch£º354	 i:3 	 global-step:7083	 l-p:0.13677486777305603
epoch£º354	 i:4 	 global-step:7084	 l-p:0.11860999464988708
epoch£º354	 i:5 	 global-step:7085	 l-p:0.1296704262495041
epoch£º354	 i:6 	 global-step:7086	 l-p:0.1222677007317543
epoch£º354	 i:7 	 global-step:7087	 l-p:0.13053368031978607
epoch£º354	 i:8 	 global-step:7088	 l-p:0.1295105218887329
epoch£º354	 i:9 	 global-step:7089	 l-p:0.1620408147573471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2820, 3.5360, 3.5468],
        [3.2820, 3.2552, 3.2741],
        [3.2820, 3.2765, 3.2815],
        [3.2820, 3.2800, 3.2819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.13118334114551544 
model_pd.l_d.mean(): -23.058666229248047 
model_pd.lagr.mean(): -22.92748260498047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2613], device='cuda:0')), ('power', tensor([-23.3199], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.13118334114551544
epoch£º355	 i:1 	 global-step:7101	 l-p:0.11536483466625214
epoch£º355	 i:2 	 global-step:7102	 l-p:0.1615370661020279
epoch£º355	 i:3 	 global-step:7103	 l-p:0.12422499060630798
epoch£º355	 i:4 	 global-step:7104	 l-p:0.13385215401649475
epoch£º355	 i:5 	 global-step:7105	 l-p:0.13850447535514832
epoch£º355	 i:6 	 global-step:7106	 l-p:0.1205848976969719
epoch£º355	 i:7 	 global-step:7107	 l-p:0.1295337826013565
epoch£º355	 i:8 	 global-step:7108	 l-p:0.1164129450917244
epoch£º355	 i:9 	 global-step:7109	 l-p:0.13289795815944672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4285, 3.3621, 3.3805],
        [3.4285, 3.5130, 3.4128],
        [3.4285, 3.4285, 3.4285],
        [3.4285, 3.3595, 3.3764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.12882749736309052 
model_pd.l_d.mean(): -22.859607696533203 
model_pd.lagr.mean(): -22.73077964782715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2034], device='cuda:0')), ('power', tensor([-23.0630], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.12882749736309052
epoch£º356	 i:1 	 global-step:7121	 l-p:0.10425043106079102
epoch£º356	 i:2 	 global-step:7122	 l-p:0.11324319243431091
epoch£º356	 i:3 	 global-step:7123	 l-p:0.13196402788162231
epoch£º356	 i:4 	 global-step:7124	 l-p:0.13915865123271942
epoch£º356	 i:5 	 global-step:7125	 l-p:0.13594253361225128
epoch£º356	 i:6 	 global-step:7126	 l-p:0.13375557959079742
epoch£º356	 i:7 	 global-step:7127	 l-p:0.1255347728729248
epoch£º356	 i:8 	 global-step:7128	 l-p:0.11597917228937149
epoch£º356	 i:9 	 global-step:7129	 l-p:0.13261260092258453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3396, 3.5763, 3.5693],
        [3.3396, 3.3366, 3.3394],
        [3.3396, 3.3252, 3.3370],
        [3.3396, 3.2459, 3.2462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.11091107875108719 
model_pd.l_d.mean(): -22.947643280029297 
model_pd.lagr.mean(): -22.836732864379883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2950], device='cuda:0')), ('power', tensor([-23.2427], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.11091107875108719
epoch£º357	 i:1 	 global-step:7141	 l-p:0.1468428075313568
epoch£º357	 i:2 	 global-step:7142	 l-p:0.1257992535829544
epoch£º357	 i:3 	 global-step:7143	 l-p:0.14178918302059174
epoch£º357	 i:4 	 global-step:7144	 l-p:0.13856925070285797
epoch£º357	 i:5 	 global-step:7145	 l-p:0.13301923871040344
epoch£º357	 i:6 	 global-step:7146	 l-p:0.13254933059215546
epoch£º357	 i:7 	 global-step:7147	 l-p:0.1314605325460434
epoch£º357	 i:8 	 global-step:7148	 l-p:0.13833408057689667
epoch£º357	 i:9 	 global-step:7149	 l-p:0.126872256398201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3870, 3.3857, 3.3869],
        [3.3870, 3.3870, 3.3870],
        [3.3870, 3.2768, 3.2343],
        [3.3870, 3.5328, 3.4640]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.14620527625083923 
model_pd.l_d.mean(): -23.393558502197266 
model_pd.lagr.mean(): -23.247352600097656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1903], device='cuda:0')), ('power', tensor([-23.5839], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.14620527625083923
epoch£º358	 i:1 	 global-step:7161	 l-p:0.10904490947723389
epoch£º358	 i:2 	 global-step:7162	 l-p:0.13847874104976654
epoch£º358	 i:3 	 global-step:7163	 l-p:0.1116371676325798
epoch£º358	 i:4 	 global-step:7164	 l-p:0.14308881759643555
epoch£º358	 i:5 	 global-step:7165	 l-p:0.10880020260810852
epoch£º358	 i:6 	 global-step:7166	 l-p:0.12976816296577454
epoch£º358	 i:7 	 global-step:7167	 l-p:0.11600513011217117
epoch£º358	 i:8 	 global-step:7168	 l-p:0.09585317224264145
epoch£º358	 i:9 	 global-step:7169	 l-p:0.12295076251029968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4132, 3.3769, 3.3997],
        [3.4132, 3.4132, 3.4132],
        [3.4132, 3.3082, 3.2880],
        [3.4132, 3.4132, 3.4132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.09925038367509842 
model_pd.l_d.mean(): -23.158483505249023 
model_pd.lagr.mean(): -23.059232711791992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2113], device='cuda:0')), ('power', tensor([-23.3697], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.09925038367509842
epoch£º359	 i:1 	 global-step:7181	 l-p:0.12482671439647675
epoch£º359	 i:2 	 global-step:7182	 l-p:0.11314134299755096
epoch£º359	 i:3 	 global-step:7183	 l-p:0.12148670107126236
epoch£º359	 i:4 	 global-step:7184	 l-p:0.14264841377735138
epoch£º359	 i:5 	 global-step:7185	 l-p:0.13307447731494904
epoch£º359	 i:6 	 global-step:7186	 l-p:0.13423101603984833
epoch£º359	 i:7 	 global-step:7187	 l-p:0.11080854386091232
epoch£º359	 i:8 	 global-step:7188	 l-p:0.13004997372627258
epoch£º359	 i:9 	 global-step:7189	 l-p:0.11447036266326904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3805, 3.2838, 3.2868],
        [3.3805, 3.2821, 3.2827],
        [3.3805, 3.3025, 3.3222],
        [3.3805, 3.3805, 3.3805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.1085650697350502 
model_pd.l_d.mean(): -23.043066024780273 
model_pd.lagr.mean(): -22.93450164794922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2580], device='cuda:0')), ('power', tensor([-23.3011], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.1085650697350502
epoch£º360	 i:1 	 global-step:7201	 l-p:0.14151765406131744
epoch£º360	 i:2 	 global-step:7202	 l-p:0.15041927993297577
epoch£º360	 i:3 	 global-step:7203	 l-p:0.1521977037191391
epoch£º360	 i:4 	 global-step:7204	 l-p:0.17307567596435547
epoch£º360	 i:5 	 global-step:7205	 l-p:0.13443009555339813
epoch£º360	 i:6 	 global-step:7206	 l-p:0.148588627576828
epoch£º360	 i:7 	 global-step:7207	 l-p:0.16096729040145874
epoch£º360	 i:8 	 global-step:7208	 l-p:0.13643217086791992
epoch£º360	 i:9 	 global-step:7209	 l-p:0.1681872010231018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2212, 3.2212, 3.2212],
        [3.2212, 3.2212, 3.2212],
        [3.2212, 3.2210, 3.2212],
        [3.2212, 3.2186, 3.2211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.1673348844051361 
model_pd.l_d.mean(): -23.422029495239258 
model_pd.lagr.mean(): -23.25469398498535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2752], device='cuda:0')), ('power', tensor([-23.6973], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.1673348844051361
epoch£º361	 i:1 	 global-step:7221	 l-p:0.1494663804769516
epoch£º361	 i:2 	 global-step:7222	 l-p:0.13707616925239563
epoch£º361	 i:3 	 global-step:7223	 l-p:0.14465342462062836
epoch£º361	 i:4 	 global-step:7224	 l-p:0.107086181640625
epoch£º361	 i:5 	 global-step:7225	 l-p:0.09621420502662659
epoch£º361	 i:6 	 global-step:7226	 l-p:0.12589560449123383
epoch£º361	 i:7 	 global-step:7227	 l-p:0.12990671396255493
epoch£º361	 i:8 	 global-step:7228	 l-p:0.12680265307426453
epoch£º361	 i:9 	 global-step:7229	 l-p:0.12116695195436478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5032, 3.4029, 3.3954],
        [3.5032, 3.5032, 3.5032],
        [3.5032, 3.5032, 3.5032],
        [3.5032, 3.5611, 3.4371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.12349411100149155 
model_pd.l_d.mean(): -23.225387573242188 
model_pd.lagr.mean(): -23.10189437866211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1785], device='cuda:0')), ('power', tensor([-23.4039], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.12349411100149155
epoch£º362	 i:1 	 global-step:7241	 l-p:0.12571150064468384
epoch£º362	 i:2 	 global-step:7242	 l-p:0.11627184599637985
epoch£º362	 i:3 	 global-step:7243	 l-p:0.1282224953174591
epoch£º362	 i:4 	 global-step:7244	 l-p:0.1188800111413002
epoch£º362	 i:5 	 global-step:7245	 l-p:0.09504280239343643
epoch£º362	 i:6 	 global-step:7246	 l-p:0.1375281810760498
epoch£º362	 i:7 	 global-step:7247	 l-p:0.1372571885585785
epoch£º362	 i:8 	 global-step:7248	 l-p:0.1274370551109314
epoch£º362	 i:9 	 global-step:7249	 l-p:0.13396289944648743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3883, 3.3143, 3.3395],
        [3.3883, 3.3872, 3.3882],
        [3.3883, 3.3857, 3.3881],
        [3.3883, 3.2677, 3.2410]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.13832293450832367 
model_pd.l_d.mean(): -23.49691390991211 
model_pd.lagr.mean(): -23.358591079711914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1763], device='cuda:0')), ('power', tensor([-23.6732], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.13832293450832367
epoch£º363	 i:1 	 global-step:7261	 l-p:0.1319471150636673
epoch£º363	 i:2 	 global-step:7262	 l-p:0.13859304785728455
epoch£º363	 i:3 	 global-step:7263	 l-p:0.11916109174489975
epoch£º363	 i:4 	 global-step:7264	 l-p:0.13732275366783142
epoch£º363	 i:5 	 global-step:7265	 l-p:0.1359366476535797
epoch£º363	 i:6 	 global-step:7266	 l-p:0.12048546224832535
epoch£º363	 i:7 	 global-step:7267	 l-p:0.1352103352546692
epoch£º363	 i:8 	 global-step:7268	 l-p:0.13013549149036407
epoch£º363	 i:9 	 global-step:7269	 l-p:0.13455431163311005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2761, 3.5115, 3.4937],
        [3.2761, 3.1968, 3.2246],
        [3.2761, 3.2759, 3.2761],
        [3.2761, 3.1379, 3.0966]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.14821456372737885 
model_pd.l_d.mean(): -22.512876510620117 
model_pd.lagr.mean(): -22.364662170410156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3052], device='cuda:0')), ('power', tensor([-22.8181], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.14821456372737885
epoch£º364	 i:1 	 global-step:7281	 l-p:0.1329619586467743
epoch£º364	 i:2 	 global-step:7282	 l-p:0.13716253638267517
epoch£º364	 i:3 	 global-step:7283	 l-p:0.11267124116420746
epoch£º364	 i:4 	 global-step:7284	 l-p:0.13276442885398865
epoch£º364	 i:5 	 global-step:7285	 l-p:0.15875177085399628
epoch£º364	 i:6 	 global-step:7286	 l-p:0.12280146032571793
epoch£º364	 i:7 	 global-step:7287	 l-p:0.1202678456902504
epoch£º364	 i:8 	 global-step:7288	 l-p:0.1109904870390892
epoch£º364	 i:9 	 global-step:7289	 l-p:0.12546253204345703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4648, 3.4648, 3.4648],
        [3.4648, 3.3437, 3.3160],
        [3.4648, 3.3592, 3.3607],
        [3.4648, 3.4540, 3.4633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.12987472116947174 
model_pd.l_d.mean(): -23.479310989379883 
model_pd.lagr.mean(): -23.349435806274414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1482], device='cuda:0')), ('power', tensor([-23.6276], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.12987472116947174
epoch£º365	 i:1 	 global-step:7301	 l-p:0.11519528925418854
epoch£º365	 i:2 	 global-step:7302	 l-p:0.12746085226535797
epoch£º365	 i:3 	 global-step:7303	 l-p:0.14934542775154114
epoch£º365	 i:4 	 global-step:7304	 l-p:0.12818509340286255
epoch£º365	 i:5 	 global-step:7305	 l-p:0.13619418442249298
epoch£º365	 i:6 	 global-step:7306	 l-p:0.14580292999744415
epoch£º365	 i:7 	 global-step:7307	 l-p:0.14130444824695587
epoch£º365	 i:8 	 global-step:7308	 l-p:0.17848743498325348
epoch£º365	 i:9 	 global-step:7309	 l-p:0.14122171700000763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2374, 3.2303, 3.2368],
        [3.2374, 3.3598, 3.2682],
        [3.2374, 3.4096, 3.3479],
        [3.2374, 3.2318, 3.2370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.11319870501756668 
model_pd.l_d.mean(): -22.03017234802246 
model_pd.lagr.mean(): -21.916973114013672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4265], device='cuda:0')), ('power', tensor([-22.4567], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.11319870501756668
epoch£º366	 i:1 	 global-step:7321	 l-p:0.1334664672613144
epoch£º366	 i:2 	 global-step:7322	 l-p:0.2037603259086609
epoch£º366	 i:3 	 global-step:7323	 l-p:0.13248126208782196
epoch£º366	 i:4 	 global-step:7324	 l-p:0.14332452416419983
epoch£º366	 i:5 	 global-step:7325	 l-p:0.13813495635986328
epoch£º366	 i:6 	 global-step:7326	 l-p:0.15267154574394226
epoch£º366	 i:7 	 global-step:7327	 l-p:0.121501125395298
epoch£º366	 i:8 	 global-step:7328	 l-p:0.15350185334682465
epoch£º366	 i:9 	 global-step:7329	 l-p:0.1199643462896347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[3.3941, 3.5723, 3.5079],
        [3.3941, 3.2678, 3.2519],
        [3.3941, 3.2915, 3.3061],
        [3.3941, 3.2552, 3.1315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.1217922568321228 
model_pd.l_d.mean(): -22.525056838989258 
model_pd.lagr.mean(): -22.40326499938965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2855], device='cuda:0')), ('power', tensor([-22.8106], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.1217922568321228
epoch£º367	 i:1 	 global-step:7341	 l-p:0.1364612579345703
epoch£º367	 i:2 	 global-step:7342	 l-p:0.14092925190925598
epoch£º367	 i:3 	 global-step:7343	 l-p:0.11958076059818268
epoch£º367	 i:4 	 global-step:7344	 l-p:0.1297934204339981
epoch£º367	 i:5 	 global-step:7345	 l-p:0.12366459518671036
epoch£º367	 i:6 	 global-step:7346	 l-p:0.14667406678199768
epoch£º367	 i:7 	 global-step:7347	 l-p:0.13154010474681854
epoch£º367	 i:8 	 global-step:7348	 l-p:0.10151584446430206
epoch£º367	 i:9 	 global-step:7349	 l-p:0.14012889564037323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3138, 3.3138, 3.3138],
        [3.3138, 3.3138, 3.3138],
        [3.3138, 3.3091, 3.3134],
        [3.3138, 3.1573, 3.0707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.15305760502815247 
model_pd.l_d.mean(): -23.241214752197266 
model_pd.lagr.mean(): -23.088157653808594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2661], device='cuda:0')), ('power', tensor([-23.5073], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.15305760502815247
epoch£º368	 i:1 	 global-step:7361	 l-p:0.1644563376903534
epoch£º368	 i:2 	 global-step:7362	 l-p:0.13695213198661804
epoch£º368	 i:3 	 global-step:7363	 l-p:0.14885735511779785
epoch£º368	 i:4 	 global-step:7364	 l-p:0.1282544732093811
epoch£º368	 i:5 	 global-step:7365	 l-p:0.14365975558757782
epoch£º368	 i:6 	 global-step:7366	 l-p:0.1525271236896515
epoch£º368	 i:7 	 global-step:7367	 l-p:0.11766039580106735
epoch£º368	 i:8 	 global-step:7368	 l-p:0.11316993832588196
epoch£º368	 i:9 	 global-step:7369	 l-p:0.1384391039609909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3000, 3.1472, 2.9986],
        [3.3000, 3.2966, 3.2998],
        [3.3000, 3.3000, 3.3000],
        [3.3000, 3.4409, 3.3531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.15419958531856537 
model_pd.l_d.mean(): -22.79479217529297 
model_pd.lagr.mean(): -22.640592575073242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3469], device='cuda:0')), ('power', tensor([-23.1417], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.15419958531856537
epoch£º369	 i:1 	 global-step:7381	 l-p:0.12415681779384613
epoch£º369	 i:2 	 global-step:7382	 l-p:0.14583802223205566
epoch£º369	 i:3 	 global-step:7383	 l-p:0.13377580046653748
epoch£º369	 i:4 	 global-step:7384	 l-p:0.12237556278705597
epoch£º369	 i:5 	 global-step:7385	 l-p:0.12095306813716888
epoch£º369	 i:6 	 global-step:7386	 l-p:0.1351461559534073
epoch£º369	 i:7 	 global-step:7387	 l-p:0.14246106147766113
epoch£º369	 i:8 	 global-step:7388	 l-p:0.16009502112865448
epoch£º369	 i:9 	 global-step:7389	 l-p:0.1287374496459961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3565, 3.1970, 3.0941],
        [3.3565, 3.3519, 3.3562],
        [3.3565, 3.3921, 3.2447],
        [3.3565, 3.2664, 3.2949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.127988800406456 
model_pd.l_d.mean(): -21.54556655883789 
model_pd.lagr.mean(): -21.417577743530273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3439], device='cuda:0')), ('power', tensor([-21.8895], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.127988800406456
epoch£º370	 i:1 	 global-step:7401	 l-p:0.13058558106422424
epoch£º370	 i:2 	 global-step:7402	 l-p:0.1405455619096756
epoch£º370	 i:3 	 global-step:7403	 l-p:0.11020619422197342
epoch£º370	 i:4 	 global-step:7404	 l-p:0.136680006980896
epoch£º370	 i:5 	 global-step:7405	 l-p:0.13224679231643677
epoch£º370	 i:6 	 global-step:7406	 l-p:0.14291046559810638
epoch£º370	 i:7 	 global-step:7407	 l-p:0.12166270613670349
epoch£º370	 i:8 	 global-step:7408	 l-p:0.1823136806488037
epoch£º370	 i:9 	 global-step:7409	 l-p:0.12031916528940201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2919, 3.2918, 3.2919],
        [3.2919, 3.2895, 3.2918],
        [3.2919, 3.4147, 3.3128],
        [3.2919, 3.2681, 3.2868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.16783294081687927 
model_pd.l_d.mean(): -23.197444915771484 
model_pd.lagr.mean(): -23.029611587524414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2754], device='cuda:0')), ('power', tensor([-23.4728], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.16783294081687927
epoch£º371	 i:1 	 global-step:7421	 l-p:0.13625074923038483
epoch£º371	 i:2 	 global-step:7422	 l-p:0.12394537031650543
epoch£º371	 i:3 	 global-step:7423	 l-p:0.1449822336435318
epoch£º371	 i:4 	 global-step:7424	 l-p:0.10732696205377579
epoch£º371	 i:5 	 global-step:7425	 l-p:0.16003009676933289
epoch£º371	 i:6 	 global-step:7426	 l-p:0.14441144466400146
epoch£º371	 i:7 	 global-step:7427	 l-p:0.14140653610229492
epoch£º371	 i:8 	 global-step:7428	 l-p:0.1625799536705017
epoch£º371	 i:9 	 global-step:7429	 l-p:0.1406347006559372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3636, 3.5847, 3.5391],
        [3.3636, 3.3636, 3.3636],
        [3.3636, 3.3611, 3.3635],
        [3.3636, 3.1974, 3.1073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.14474011957645416 
model_pd.l_d.mean(): -22.574607849121094 
model_pd.lagr.mean(): -22.429866790771484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2829], device='cuda:0')), ('power', tensor([-22.8575], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.14474011957645416
epoch£º372	 i:1 	 global-step:7441	 l-p:0.13314533233642578
epoch£º372	 i:2 	 global-step:7442	 l-p:0.11294558644294739
epoch£º372	 i:3 	 global-step:7443	 l-p:0.1189974918961525
epoch£º372	 i:4 	 global-step:7444	 l-p:0.11683162301778793
epoch£º372	 i:5 	 global-step:7445	 l-p:0.11565028131008148
epoch£º372	 i:6 	 global-step:7446	 l-p:0.15005871653556824
epoch£º372	 i:7 	 global-step:7447	 l-p:0.13750357925891876
epoch£º372	 i:8 	 global-step:7448	 l-p:0.13781030476093292
epoch£º372	 i:9 	 global-step:7449	 l-p:0.1531759798526764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3035, 3.4186, 3.3080],
        [3.3035, 3.2652, 3.2920],
        [3.3035, 3.3035, 3.3035],
        [3.3035, 3.1257, 2.9972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.12989245355129242 
model_pd.l_d.mean(): -22.292789459228516 
model_pd.lagr.mean(): -22.16289710998535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3285], device='cuda:0')), ('power', tensor([-22.6213], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.12989245355129242
epoch£º373	 i:1 	 global-step:7461	 l-p:0.15057966113090515
epoch£º373	 i:2 	 global-step:7462	 l-p:0.14939764142036438
epoch£º373	 i:3 	 global-step:7463	 l-p:0.13607114553451538
epoch£º373	 i:4 	 global-step:7464	 l-p:0.11291742324829102
epoch£º373	 i:5 	 global-step:7465	 l-p:0.12715785205364227
epoch£º373	 i:6 	 global-step:7466	 l-p:0.14300638437271118
epoch£º373	 i:7 	 global-step:7467	 l-p:0.1413319855928421
epoch£º373	 i:8 	 global-step:7468	 l-p:0.14286813139915466
epoch£º373	 i:9 	 global-step:7469	 l-p:0.12435876578092575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3727, 3.2037, 3.1366],
        [3.3727, 3.3158, 3.3488],
        [3.3727, 3.3309, 3.3591],
        [3.3727, 3.5450, 3.4643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.14353877305984497 
model_pd.l_d.mean(): -23.42753028869629 
model_pd.lagr.mean(): -23.28399085998535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2095], device='cuda:0')), ('power', tensor([-23.6371], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.14353877305984497
epoch£º374	 i:1 	 global-step:7481	 l-p:0.13318702578544617
epoch£º374	 i:2 	 global-step:7482	 l-p:0.133340522646904
epoch£º374	 i:3 	 global-step:7483	 l-p:0.12171691656112671
epoch£º374	 i:4 	 global-step:7484	 l-p:0.12522166967391968
epoch£º374	 i:5 	 global-step:7485	 l-p:0.1302647441625595
epoch£º374	 i:6 	 global-step:7486	 l-p:0.1567278951406479
epoch£º374	 i:7 	 global-step:7487	 l-p:0.1455545872449875
epoch£º374	 i:8 	 global-step:7488	 l-p:0.13769108057022095
epoch£º374	 i:9 	 global-step:7489	 l-p:0.1493205577135086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2573, 3.2496, 3.2566],
        [3.2573, 3.2573, 3.2573],
        [3.2573, 3.2482, 3.2563],
        [3.2573, 3.2571, 3.2573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.1686333864927292 
model_pd.l_d.mean(): -23.32529640197754 
model_pd.lagr.mean(): -23.15666389465332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2462], device='cuda:0')), ('power', tensor([-23.5715], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.1686333864927292
epoch£º375	 i:1 	 global-step:7501	 l-p:0.15443632006645203
epoch£º375	 i:2 	 global-step:7502	 l-p:0.14256495237350464
epoch£º375	 i:3 	 global-step:7503	 l-p:0.15800748765468597
epoch£º375	 i:4 	 global-step:7504	 l-p:0.12658248841762543
epoch£º375	 i:5 	 global-step:7505	 l-p:0.1655620038509369
epoch£º375	 i:6 	 global-step:7506	 l-p:0.1306396722793579
epoch£º375	 i:7 	 global-step:7507	 l-p:0.12176251411437988
epoch£º375	 i:8 	 global-step:7508	 l-p:0.14706647396087646
epoch£º375	 i:9 	 global-step:7509	 l-p:0.11318475753068924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3077, 3.2275, 3.2644],
        [3.3077, 3.1179, 3.0021],
        [3.3077, 3.3075, 3.3077],
        [3.3077, 3.1482, 3.1306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.11432389169931412 
model_pd.l_d.mean(): -22.77120018005371 
model_pd.lagr.mean(): -22.656875610351562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3069], device='cuda:0')), ('power', tensor([-23.0781], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.11432389169931412
epoch£º376	 i:1 	 global-step:7521	 l-p:0.15029765665531158
epoch£º376	 i:2 	 global-step:7522	 l-p:0.13625188171863556
epoch£º376	 i:3 	 global-step:7523	 l-p:0.12586069107055664
epoch£º376	 i:4 	 global-step:7524	 l-p:0.14772950112819672
epoch£º376	 i:5 	 global-step:7525	 l-p:0.14050456881523132
epoch£º376	 i:6 	 global-step:7526	 l-p:0.12100588530302048
epoch£º376	 i:7 	 global-step:7527	 l-p:0.10376458615064621
epoch£º376	 i:8 	 global-step:7528	 l-p:0.13320578634738922
epoch£º376	 i:9 	 global-step:7529	 l-p:0.15882688760757446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3896, 3.2253, 3.1927],
        [3.3896, 3.3121, 3.3485],
        [3.3896, 3.3248, 3.3603],
        [3.3896, 3.4943, 3.3684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.13431479036808014 
model_pd.l_d.mean(): -23.20512580871582 
model_pd.lagr.mean(): -23.070810317993164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2204], device='cuda:0')), ('power', tensor([-23.4255], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.13431479036808014
epoch£º377	 i:1 	 global-step:7541	 l-p:0.14058004319667816
epoch£º377	 i:2 	 global-step:7542	 l-p:0.14770200848579407
epoch£º377	 i:3 	 global-step:7543	 l-p:0.14860153198242188
epoch£º377	 i:4 	 global-step:7544	 l-p:0.12531031668186188
epoch£º377	 i:5 	 global-step:7545	 l-p:0.13269685208797455
epoch£º377	 i:6 	 global-step:7546	 l-p:0.1597178876399994
epoch£º377	 i:7 	 global-step:7547	 l-p:0.14832083880901337
epoch£º377	 i:8 	 global-step:7548	 l-p:0.4507365822792053
epoch£º377	 i:9 	 global-step:7549	 l-p:0.13956575095653534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2718, 3.0734, 2.9850],
        [3.2718, 3.2718, 3.2718],
        [3.2718, 3.1552, 3.1865],
        [3.2718, 3.2063, 3.2435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.15402169525623322 
model_pd.l_d.mean(): -22.97379493713379 
model_pd.lagr.mean(): -22.819772720336914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3346], device='cuda:0')), ('power', tensor([-23.3084], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.15402169525623322
epoch£º378	 i:1 	 global-step:7561	 l-p:0.18774688243865967
epoch£º378	 i:2 	 global-step:7562	 l-p:0.14208292961120605
epoch£º378	 i:3 	 global-step:7563	 l-p:0.12082073092460632
epoch£º378	 i:4 	 global-step:7564	 l-p:0.10588302463293076
epoch£º378	 i:5 	 global-step:7565	 l-p:0.12982942163944244
epoch£º378	 i:6 	 global-step:7566	 l-p:0.13320587575435638
epoch£º378	 i:7 	 global-step:7567	 l-p:0.12636105716228485
epoch£º378	 i:8 	 global-step:7568	 l-p:0.11819478124380112
epoch£º378	 i:9 	 global-step:7569	 l-p:0.13734011352062225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3510, 3.1956, 3.1927],
        [3.3510, 3.3510, 3.3510],
        [3.3510, 3.3314, 3.3477],
        [3.3510, 3.1770, 3.1449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.13874216377735138 
model_pd.l_d.mean(): -22.394319534301758 
model_pd.lagr.mean(): -22.255577087402344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3231], device='cuda:0')), ('power', tensor([-22.7174], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.13874216377735138
epoch£º379	 i:1 	 global-step:7581	 l-p:0.13284344971179962
epoch£º379	 i:2 	 global-step:7582	 l-p:0.15185818076133728
epoch£º379	 i:3 	 global-step:7583	 l-p:0.14887379109859467
epoch£º379	 i:4 	 global-step:7584	 l-p:0.17343685030937195
epoch£º379	 i:5 	 global-step:7585	 l-p:0.13027268648147583
epoch£º379	 i:6 	 global-step:7586	 l-p:0.16204044222831726
epoch£º379	 i:7 	 global-step:7587	 l-p:0.1300397515296936
epoch£º379	 i:8 	 global-step:7588	 l-p:0.12957419455051422
epoch£º379	 i:9 	 global-step:7589	 l-p:0.14040902256965637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3036, 3.1262, 2.9138],
        [3.3036, 3.2762, 3.2978],
        [3.3036, 3.3031, 3.3036],
        [3.3036, 3.2982, 3.3033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.13816124200820923 
model_pd.l_d.mean(): -23.271753311157227 
model_pd.lagr.mean(): -23.13359260559082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2537], device='cuda:0')), ('power', tensor([-23.5255], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.13816124200820923
epoch£º380	 i:1 	 global-step:7601	 l-p:0.16284091770648956
epoch£º380	 i:2 	 global-step:7602	 l-p:0.14613842964172363
epoch£º380	 i:3 	 global-step:7603	 l-p:0.1442512720823288
epoch£º380	 i:4 	 global-step:7604	 l-p:0.1283436417579651
epoch£º380	 i:5 	 global-step:7605	 l-p:0.12536689639091492
epoch£º380	 i:6 	 global-step:7606	 l-p:0.12966331839561462
epoch£º380	 i:7 	 global-step:7607	 l-p:0.15825849771499634
epoch£º380	 i:8 	 global-step:7608	 l-p:0.2061932384967804
epoch£º380	 i:9 	 global-step:7609	 l-p:0.14663776755332947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3283, 3.3282, 3.3283],
        [3.3283, 3.2837, 3.0798],
        [3.3283, 3.2487, 3.2887],
        [3.3283, 3.1774, 3.1885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.17556621134281158 
model_pd.l_d.mean(): -23.05471420288086 
model_pd.lagr.mean(): -22.879148483276367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3195], device='cuda:0')), ('power', tensor([-23.3742], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.17556621134281158
epoch£º381	 i:1 	 global-step:7621	 l-p:0.14032040536403656
epoch£º381	 i:2 	 global-step:7622	 l-p:0.11814982444047928
epoch£º381	 i:3 	 global-step:7623	 l-p:0.12026388198137283
epoch£º381	 i:4 	 global-step:7624	 l-p:0.12881158292293549
epoch£º381	 i:5 	 global-step:7625	 l-p:0.12553469836711884
epoch£º381	 i:6 	 global-step:7626	 l-p:0.12858319282531738
epoch£º381	 i:7 	 global-step:7627	 l-p:0.11964284628629684
epoch£º381	 i:8 	 global-step:7628	 l-p:0.14930787682533264
epoch£º381	 i:9 	 global-step:7629	 l-p:0.1316976696252823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4479, 3.3335, 3.3662],
        [3.4479, 3.4475, 3.4479],
        [3.4479, 3.4465, 3.4479],
        [3.4479, 3.3034, 3.3157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.13013944029808044 
model_pd.l_d.mean(): -23.569292068481445 
model_pd.lagr.mean(): -23.439151763916016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1347], device='cuda:0')), ('power', tensor([-23.7039], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.13013944029808044
epoch£º382	 i:1 	 global-step:7641	 l-p:0.11962155997753143
epoch£º382	 i:2 	 global-step:7642	 l-p:0.14749421179294586
epoch£º382	 i:3 	 global-step:7643	 l-p:0.13829877972602844
epoch£º382	 i:4 	 global-step:7644	 l-p:0.1486390233039856
epoch£º382	 i:5 	 global-step:7645	 l-p:0.12678979337215424
epoch£º382	 i:6 	 global-step:7646	 l-p:0.202890545129776
epoch£º382	 i:7 	 global-step:7647	 l-p:0.17113319039344788
epoch£º382	 i:8 	 global-step:7648	 l-p:0.1779911369085312
epoch£º382	 i:9 	 global-step:7649	 l-p:0.05920286476612091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2731, 3.2671, 3.2726],
        [3.2731, 3.0597, 2.9886],
        [3.2731, 3.2731, 3.2731],
        [3.2731, 3.1894, 3.2317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.1366806924343109 
model_pd.l_d.mean(): -23.175186157226562 
model_pd.lagr.mean(): -23.03850555419922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2900], device='cuda:0')), ('power', tensor([-23.4652], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.1366806924343109
epoch£º383	 i:1 	 global-step:7661	 l-p:0.1664220541715622
epoch£º383	 i:2 	 global-step:7662	 l-p:0.1325344294309616
epoch£º383	 i:3 	 global-step:7663	 l-p:0.12899428606033325
epoch£º383	 i:4 	 global-step:7664	 l-p:0.13862477242946625
epoch£º383	 i:5 	 global-step:7665	 l-p:0.13251665234565735
epoch£º383	 i:6 	 global-step:7666	 l-p:0.13054385781288147
epoch£º383	 i:7 	 global-step:7667	 l-p:0.13619385659694672
epoch£º383	 i:8 	 global-step:7668	 l-p:0.12034423649311066
epoch£º383	 i:9 	 global-step:7669	 l-p:0.1388467252254486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4131, 3.4069, 3.2111],
        [3.4131, 3.2388, 3.2261],
        [3.4131, 3.2620, 3.0320],
        [3.4131, 3.3576, 3.3931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.15232941508293152 
model_pd.l_d.mean(): -23.550649642944336 
model_pd.lagr.mean(): -23.3983211517334 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1942], device='cuda:0')), ('power', tensor([-23.7448], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.15232941508293152
epoch£º384	 i:1 	 global-step:7681	 l-p:0.1357921063899994
epoch£º384	 i:2 	 global-step:7682	 l-p:0.1276872754096985
epoch£º384	 i:3 	 global-step:7683	 l-p:0.14531485736370087
epoch£º384	 i:4 	 global-step:7684	 l-p:0.13763616979122162
epoch£º384	 i:5 	 global-step:7685	 l-p:0.14851631224155426
epoch£º384	 i:6 	 global-step:7686	 l-p:0.15484999120235443
epoch£º384	 i:7 	 global-step:7687	 l-p:0.17837628722190857
epoch£º384	 i:8 	 global-step:7688	 l-p:0.09417952597141266
epoch£º384	 i:9 	 global-step:7689	 l-p:0.13245700299739838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4615, 3.2564, 3.1736],
        [3.4615, 3.4575, 3.4612],
        [3.4615, 3.4614, 3.4615],
        [3.4615, 3.4412, 3.2373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.12050945311784744 
model_pd.l_d.mean(): -23.168075561523438 
model_pd.lagr.mean(): -23.047565460205078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2256], device='cuda:0')), ('power', tensor([-23.3937], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.12050945311784744
epoch£º385	 i:1 	 global-step:7701	 l-p:0.11244584619998932
epoch£º385	 i:2 	 global-step:7702	 l-p:0.13313327729701996
epoch£º385	 i:3 	 global-step:7703	 l-p:0.1357806921005249
epoch£º385	 i:4 	 global-step:7704	 l-p:0.14625661075115204
epoch£º385	 i:5 	 global-step:7705	 l-p:0.13268953561782837
epoch£º385	 i:6 	 global-step:7706	 l-p:0.16176661849021912
epoch£º385	 i:7 	 global-step:7707	 l-p:0.14411838352680206
epoch£º385	 i:8 	 global-step:7708	 l-p:0.04010551795363426
epoch£º385	 i:9 	 global-step:7709	 l-p:0.14268270134925842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3020, 3.2074, 3.2516],
        [3.3020, 3.1053, 2.8645],
        [3.3020, 3.3020, 3.3020],
        [3.3020, 3.2314, 3.2728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.13348935544490814 
model_pd.l_d.mean(): -23.12616729736328 
model_pd.lagr.mean(): -22.992677688598633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3187], device='cuda:0')), ('power', tensor([-23.4449], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.13348935544490814
epoch£º386	 i:1 	 global-step:7721	 l-p:0.15602801740169525
epoch£º386	 i:2 	 global-step:7722	 l-p:0.1416165828704834
epoch£º386	 i:3 	 global-step:7723	 l-p:0.30222517251968384
epoch£º386	 i:4 	 global-step:7724	 l-p:0.15120835602283478
epoch£º386	 i:5 	 global-step:7725	 l-p:0.15838779509067535
epoch£º386	 i:6 	 global-step:7726	 l-p:0.11923284083604813
epoch£º386	 i:7 	 global-step:7727	 l-p:0.13189737498760223
epoch£º386	 i:8 	 global-step:7728	 l-p:0.16451570391654968
epoch£º386	 i:9 	 global-step:7729	 l-p:0.12689310312271118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3628, 3.3085, 3.3446],
        [3.3628, 3.2723, 3.0380],
        [3.3628, 3.3618, 3.3628],
        [3.3628, 3.2657, 3.3096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.1411445289850235 
model_pd.l_d.mean(): -23.516592025756836 
model_pd.lagr.mean(): -23.37544822692871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2179], device='cuda:0')), ('power', tensor([-23.7345], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.1411445289850235
epoch£º387	 i:1 	 global-step:7741	 l-p:0.13448594510555267
epoch£º387	 i:2 	 global-step:7742	 l-p:0.1308550387620926
epoch£º387	 i:3 	 global-step:7743	 l-p:0.1622903048992157
epoch£º387	 i:4 	 global-step:7744	 l-p:0.12516199052333832
epoch£º387	 i:5 	 global-step:7745	 l-p:0.13919734954833984
epoch£º387	 i:6 	 global-step:7746	 l-p:0.11685934662818909
epoch£º387	 i:7 	 global-step:7747	 l-p:0.13400153815746307
epoch£º387	 i:8 	 global-step:7748	 l-p:0.1412268877029419
epoch£º387	 i:9 	 global-step:7749	 l-p:0.11846557259559631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3773, 3.3710, 3.3768],
        [3.3773, 3.3210, 3.3580],
        [3.3773, 3.3697, 3.3766],
        [3.3773, 3.1594, 3.0959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.10605847835540771 
model_pd.l_d.mean(): -22.6424560546875 
model_pd.lagr.mean(): -22.53639793395996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3660], device='cuda:0')), ('power', tensor([-23.0085], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.10605847835540771
epoch£º388	 i:1 	 global-step:7761	 l-p:0.1411992758512497
epoch£º388	 i:2 	 global-step:7762	 l-p:0.13941234350204468
epoch£º388	 i:3 	 global-step:7763	 l-p:0.13253623247146606
epoch£º388	 i:4 	 global-step:7764	 l-p:0.11811690777540207
epoch£º388	 i:5 	 global-step:7765	 l-p:0.13029254972934723
epoch£º388	 i:6 	 global-step:7766	 l-p:0.13560353219509125
epoch£º388	 i:7 	 global-step:7767	 l-p:0.13388949632644653
epoch£º388	 i:8 	 global-step:7768	 l-p:0.17163582146167755
epoch£º388	 i:9 	 global-step:7769	 l-p:0.13202215731143951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3182, 3.0676, 2.8837],
        [3.3182, 3.0989, 3.0522],
        [3.3182, 3.3178, 3.3182],
        [3.3182, 3.2224, 3.2684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.15361058712005615 
model_pd.l_d.mean(): -22.613859176635742 
model_pd.lagr.mean(): -22.460248947143555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3312], device='cuda:0')), ('power', tensor([-22.9451], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.15361058712005615
epoch£º389	 i:1 	 global-step:7781	 l-p:0.11318651586771011
epoch£º389	 i:2 	 global-step:7782	 l-p:0.14792145788669586
epoch£º389	 i:3 	 global-step:7783	 l-p:0.13172803819179535
epoch£º389	 i:4 	 global-step:7784	 l-p:0.13528291881084442
epoch£º389	 i:5 	 global-step:7785	 l-p:0.1327962577342987
epoch£º389	 i:6 	 global-step:7786	 l-p:1.2020151615142822
epoch£º389	 i:7 	 global-step:7787	 l-p:0.1672438532114029
epoch£º389	 i:8 	 global-step:7788	 l-p:0.15020185708999634
epoch£º389	 i:9 	 global-step:7789	 l-p:0.16335245966911316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3665, 3.3660, 3.3665],
        [3.3665, 3.1972, 3.2149],
        [3.3665, 3.3210, 3.0948],
        [3.3665, 3.3660, 3.3665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.16355399787425995 
model_pd.l_d.mean(): -23.498750686645508 
model_pd.lagr.mean(): -23.33519744873047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2170], device='cuda:0')), ('power', tensor([-23.7157], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.16355399787425995
epoch£º390	 i:1 	 global-step:7801	 l-p:0.15850752592086792
epoch£º390	 i:2 	 global-step:7802	 l-p:0.11999081075191498
epoch£º390	 i:3 	 global-step:7803	 l-p:0.12144406884908676
epoch£º390	 i:4 	 global-step:7804	 l-p:0.10686967521905899
epoch£º390	 i:5 	 global-step:7805	 l-p:0.09990045428276062
epoch£º390	 i:6 	 global-step:7806	 l-p:0.11609405279159546
epoch£º390	 i:7 	 global-step:7807	 l-p:0.1309983879327774
epoch£º390	 i:8 	 global-step:7808	 l-p:0.13856938481330872
epoch£º390	 i:9 	 global-step:7809	 l-p:0.12303725630044937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4526, 3.4353, 3.4501],
        [3.4526, 3.2223, 3.1355],
        [3.4526, 3.2375, 3.1888],
        [3.4526, 3.4448, 3.4519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.13972045481204987 
model_pd.l_d.mean(): -23.509340286254883 
model_pd.lagr.mean(): -23.369619369506836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1719], device='cuda:0')), ('power', tensor([-23.6812], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.13972045481204987
epoch£º391	 i:1 	 global-step:7821	 l-p:0.15579846501350403
epoch£º391	 i:2 	 global-step:7822	 l-p:0.1354188472032547
epoch£º391	 i:3 	 global-step:7823	 l-p:0.12871764600276947
epoch£º391	 i:4 	 global-step:7824	 l-p:0.09390250593423843
epoch£º391	 i:5 	 global-step:7825	 l-p:0.6575254797935486
epoch£º391	 i:6 	 global-step:7826	 l-p:0.19567686319351196
epoch£º391	 i:7 	 global-step:7827	 l-p:0.12466173619031906
epoch£º391	 i:8 	 global-step:7828	 l-p:0.13156813383102417
epoch£º391	 i:9 	 global-step:7829	 l-p:0.814610481262207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2252, 3.1323, 3.1810],
        [3.2252, 3.2247, 3.2252],
        [3.2252, 3.2787, 3.0919],
        [3.2252, 3.2252, 3.2252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.20685577392578125 
model_pd.l_d.mean(): -22.966772079467773 
model_pd.lagr.mean(): -22.759916305541992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3013], device='cuda:0')), ('power', tensor([-23.2680], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.20685577392578125
epoch£º392	 i:1 	 global-step:7841	 l-p:0.1555967926979065
epoch£º392	 i:2 	 global-step:7842	 l-p:0.14519773423671722
epoch£º392	 i:3 	 global-step:7843	 l-p:0.16139747202396393
epoch£º392	 i:4 	 global-step:7844	 l-p:0.1478406935930252
epoch£º392	 i:5 	 global-step:7845	 l-p:0.11305812746286392
epoch£º392	 i:6 	 global-step:7846	 l-p:0.13369402289390564
epoch£º392	 i:7 	 global-step:7847	 l-p:0.1345365047454834
epoch£º392	 i:8 	 global-step:7848	 l-p:0.16450020670890808
epoch£º392	 i:9 	 global-step:7849	 l-p:0.11538024991750717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4050, 3.4117, 3.2012],
        [3.4050, 3.4050, 3.4050],
        [3.4050, 3.1916, 2.9328],
        [3.4050, 3.4049, 3.4050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.11333480477333069 
model_pd.l_d.mean(): -23.26214027404785 
model_pd.lagr.mean(): -23.148805618286133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2614], device='cuda:0')), ('power', tensor([-23.5235], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.11333480477333069
epoch£º393	 i:1 	 global-step:7861	 l-p:0.15250661969184875
epoch£º393	 i:2 	 global-step:7862	 l-p:0.15226660668849945
epoch£º393	 i:3 	 global-step:7863	 l-p:0.12205366045236588
epoch£º393	 i:4 	 global-step:7864	 l-p:0.1205224096775055
epoch£º393	 i:5 	 global-step:7865	 l-p:0.13357029855251312
epoch£º393	 i:6 	 global-step:7866	 l-p:0.1590098887681961
epoch£º393	 i:7 	 global-step:7867	 l-p:0.14871110022068024
epoch£º393	 i:8 	 global-step:7868	 l-p:0.14074036478996277
epoch£º393	 i:9 	 global-step:7869	 l-p:0.21691982448101044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3799, 3.2815, 3.3297],
        [3.3799, 3.3791, 3.3799],
        [3.3799, 3.1563, 3.1202],
        [3.3799, 3.4209, 3.2244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.13756100833415985 
model_pd.l_d.mean(): -23.440340042114258 
model_pd.lagr.mean(): -23.302778244018555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2197], device='cuda:0')), ('power', tensor([-23.6600], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.13756100833415985
epoch£º394	 i:1 	 global-step:7881	 l-p:0.11660266667604446
epoch£º394	 i:2 	 global-step:7882	 l-p:0.18210986256599426
epoch£º394	 i:3 	 global-step:7883	 l-p:0.1554444432258606
epoch£º394	 i:4 	 global-step:7884	 l-p:0.14098310470581055
epoch£º394	 i:5 	 global-step:7885	 l-p:0.14246228337287903
epoch£º394	 i:6 	 global-step:7886	 l-p:0.13698236644268036
epoch£º394	 i:7 	 global-step:7887	 l-p:0.13169759511947632
epoch£º394	 i:8 	 global-step:7888	 l-p:0.1112649142742157
epoch£º394	 i:9 	 global-step:7889	 l-p:0.13790635764598846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4402, 3.4371, 3.4401],
        [3.4402, 3.3087, 3.3528],
        [3.4402, 3.4402, 3.4402],
        [3.4402, 3.3435, 3.3915]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.10579107701778412 
model_pd.l_d.mean(): -23.101224899291992 
model_pd.lagr.mean(): -22.995433807373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2554], device='cuda:0')), ('power', tensor([-23.3566], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.10579107701778412
epoch£º395	 i:1 	 global-step:7901	 l-p:0.12088797986507416
epoch£º395	 i:2 	 global-step:7902	 l-p:0.1385778784751892
epoch£º395	 i:3 	 global-step:7903	 l-p:0.1482618898153305
epoch£º395	 i:4 	 global-step:7904	 l-p:0.14243188500404358
epoch£º395	 i:5 	 global-step:7905	 l-p:0.18241797387599945
epoch£º395	 i:6 	 global-step:7906	 l-p:0.23431284725666046
epoch£º395	 i:7 	 global-step:7907	 l-p:0.11307897418737411
epoch£º395	 i:8 	 global-step:7908	 l-p:0.1177593395113945
epoch£º395	 i:9 	 global-step:7909	 l-p:0.1355944275856018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3101, 3.0600, 2.7869],
        [3.3101, 3.0848, 2.8034],
        [3.3101, 3.2684, 3.2999],
        [3.3101, 3.2852, 3.3059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.14315180480480194 
model_pd.l_d.mean(): -23.772197723388672 
model_pd.lagr.mean(): -23.629045486450195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1890], device='cuda:0')), ('power', tensor([-23.9611], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.14315180480480194
epoch£º396	 i:1 	 global-step:7921	 l-p:0.15643298625946045
epoch£º396	 i:2 	 global-step:7922	 l-p:0.1226511225104332
epoch£º396	 i:3 	 global-step:7923	 l-p:0.25822770595550537
epoch£º396	 i:4 	 global-step:7924	 l-p:0.1270635575056076
epoch£º396	 i:5 	 global-step:7925	 l-p:0.1375451683998108
epoch£º396	 i:6 	 global-step:7926	 l-p:0.14204755425453186
epoch£º396	 i:7 	 global-step:7927	 l-p:0.137558251619339
epoch£º396	 i:8 	 global-step:7928	 l-p:0.12712642550468445
epoch£º396	 i:9 	 global-step:7929	 l-p:0.1324695199728012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4212, 3.4572, 3.2501],
        [3.4212, 3.4098, 3.4201],
        [3.4212, 3.2937, 3.3416],
        [3.4212, 3.3054, 3.3550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.10146264731884003 
model_pd.l_d.mean(): -22.552515029907227 
model_pd.lagr.mean(): -22.451051712036133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3513], device='cuda:0')), ('power', tensor([-22.9038], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.10146264731884003
epoch£º397	 i:1 	 global-step:7941	 l-p:0.13179151713848114
epoch£º397	 i:2 	 global-step:7942	 l-p:0.14368820190429688
epoch£º397	 i:3 	 global-step:7943	 l-p:0.14928606152534485
epoch£º397	 i:4 	 global-step:7944	 l-p:0.17025130987167358
epoch£º397	 i:5 	 global-step:7945	 l-p:0.14732308685779572
epoch£º397	 i:6 	 global-step:7946	 l-p:0.11057250946760178
epoch£º397	 i:7 	 global-step:7947	 l-p:0.14041434228420258
epoch£º397	 i:8 	 global-step:7948	 l-p:0.1358494758605957
epoch£º397	 i:9 	 global-step:7949	 l-p:0.1540568321943283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3390, 3.0948, 2.8105],
        [3.3390, 3.3390, 3.3390],
        [3.3390, 3.3357, 3.3389],
        [3.3390, 3.0699, 2.8012]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.1571483165025711 
model_pd.l_d.mean(): -22.601402282714844 
model_pd.lagr.mean(): -22.44425392150879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3428], device='cuda:0')), ('power', tensor([-22.9442], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.1571483165025711
epoch£º398	 i:1 	 global-step:7961	 l-p:0.1166389137506485
epoch£º398	 i:2 	 global-step:7962	 l-p:0.16347047686576843
epoch£º398	 i:3 	 global-step:7963	 l-p:0.14771761000156403
epoch£º398	 i:4 	 global-step:7964	 l-p:0.15331359207630157
epoch£º398	 i:5 	 global-step:7965	 l-p:0.12571550905704498
epoch£º398	 i:6 	 global-step:7966	 l-p:0.13506010174751282
epoch£º398	 i:7 	 global-step:7967	 l-p:0.13855892419815063
epoch£º398	 i:8 	 global-step:7968	 l-p:0.13685347139835358
epoch£º398	 i:9 	 global-step:7969	 l-p:0.08094309270381927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2873, 3.2863, 3.2873],
        [3.2873, 3.1615, 3.2144],
        [3.2873, 2.9856, 2.7543],
        [3.2873, 3.0194, 2.9501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.1268022209405899 
model_pd.l_d.mean(): -23.104398727416992 
model_pd.lagr.mean(): -22.977596282958984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3156], device='cuda:0')), ('power', tensor([-23.4200], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.1268022209405899
epoch£º399	 i:1 	 global-step:7981	 l-p:0.22252574563026428
epoch£º399	 i:2 	 global-step:7982	 l-p:0.11411329358816147
epoch£º399	 i:3 	 global-step:7983	 l-p:0.22634291648864746
epoch£º399	 i:4 	 global-step:7984	 l-p:0.13369634747505188
epoch£º399	 i:5 	 global-step:7985	 l-p:0.13981597125530243
epoch£º399	 i:6 	 global-step:7986	 l-p:0.12184078991413116
epoch£º399	 i:7 	 global-step:7987	 l-p:0.12274728715419769
epoch£º399	 i:8 	 global-step:7988	 l-p:0.16421471536159515
epoch£º399	 i:9 	 global-step:7989	 l-p:0.1477425992488861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3965, 3.1114, 2.9728],
        [3.3965, 3.1174, 2.8587],
        [3.3965, 3.3600, 3.3886],
        [3.3965, 3.1161, 2.9951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.13773423433303833 
model_pd.l_d.mean(): -22.817913055419922 
model_pd.lagr.mean(): -22.680179595947266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2634], device='cuda:0')), ('power', tensor([-23.0813], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.13773423433303833
epoch£º400	 i:1 	 global-step:8001	 l-p:0.14502592384815216
epoch£º400	 i:2 	 global-step:8002	 l-p:0.15126895904541016
epoch£º400	 i:3 	 global-step:8003	 l-p:0.129364475607872
epoch£º400	 i:4 	 global-step:8004	 l-p:0.13006970286369324
epoch£º400	 i:5 	 global-step:8005	 l-p:0.24021278321743011
epoch£º400	 i:6 	 global-step:8006	 l-p:0.12731806933879852
epoch£º400	 i:7 	 global-step:8007	 l-p:0.13734053075313568
epoch£º400	 i:8 	 global-step:8008	 l-p:0.149763286113739
epoch£º400	 i:9 	 global-step:8009	 l-p:0.13018576800823212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4043, 3.1386, 2.8608],
        [3.4043, 3.4038, 3.4043],
        [3.4043, 3.1218, 2.8652],
        [3.4043, 3.2007, 2.9059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.1317959874868393 
model_pd.l_d.mean(): -22.711923599243164 
model_pd.lagr.mean(): -22.580127716064453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3665], device='cuda:0')), ('power', tensor([-23.0785], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.1317959874868393
epoch£º401	 i:1 	 global-step:8021	 l-p:0.1433582454919815
epoch£º401	 i:2 	 global-step:8022	 l-p:0.13483776152133942
epoch£º401	 i:3 	 global-step:8023	 l-p:0.13026945292949677
epoch£º401	 i:4 	 global-step:8024	 l-p:0.16292668879032135
epoch£º401	 i:5 	 global-step:8025	 l-p:-0.08961205929517746
epoch£º401	 i:6 	 global-step:8026	 l-p:0.14901591837406158
epoch£º401	 i:7 	 global-step:8027	 l-p:0.11698459088802338
epoch£º401	 i:8 	 global-step:8028	 l-p:0.1938781440258026
epoch£º401	 i:9 	 global-step:8029	 l-p:0.14918403327465057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3305, 3.3305, 3.3305],
        [3.3305, 3.3295, 3.3304],
        [3.3305, 3.2014, 3.2547],
        [3.3305, 3.3305, 3.3305]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.13022340834140778 
model_pd.l_d.mean(): -23.56199073791504 
model_pd.lagr.mean(): -23.431766510009766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2068], device='cuda:0')), ('power', tensor([-23.7688], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.13022340834140778
epoch£º402	 i:1 	 global-step:8041	 l-p:0.14051862061023712
epoch£º402	 i:2 	 global-step:8042	 l-p:0.1350974589586258
epoch£º402	 i:3 	 global-step:8043	 l-p:0.15654915571212769
epoch£º402	 i:4 	 global-step:8044	 l-p:-0.7514064908027649
epoch£º402	 i:5 	 global-step:8045	 l-p:0.1523364931344986
epoch£º402	 i:6 	 global-step:8046	 l-p:0.13723960518836975
epoch£º402	 i:7 	 global-step:8047	 l-p:0.12872321903705597
epoch£º402	 i:8 	 global-step:8048	 l-p:0.16980260610580444
epoch£º402	 i:9 	 global-step:8049	 l-p:0.14230376482009888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3885, 3.2823, 3.3354],
        [3.3885, 3.3884, 3.3885],
        [3.3885, 3.2918, 3.3439],
        [3.3885, 3.3528, 3.3809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.13677595555782318 
model_pd.l_d.mean(): -23.399606704711914 
model_pd.lagr.mean(): -23.26283073425293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2087], device='cuda:0')), ('power', tensor([-23.6083], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.13677595555782318
epoch£º403	 i:1 	 global-step:8061	 l-p:0.2364848256111145
epoch£º403	 i:2 	 global-step:8062	 l-p:0.12797755002975464
epoch£º403	 i:3 	 global-step:8063	 l-p:0.1262349933385849
epoch£º403	 i:4 	 global-step:8064	 l-p:0.14977893233299255
epoch£º403	 i:5 	 global-step:8065	 l-p:0.13792827725410461
epoch£º403	 i:6 	 global-step:8066	 l-p:0.12006066739559174
epoch£º403	 i:7 	 global-step:8067	 l-p:0.18017661571502686
epoch£º403	 i:8 	 global-step:8068	 l-p:0.11907252669334412
epoch£º403	 i:9 	 global-step:8069	 l-p:0.15667778253555298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3839, 3.0913, 2.8307],
        [3.3839, 3.2467, 2.9590],
        [3.3839, 3.3135, 3.3590],
        [3.3839, 3.3733, 3.3829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.14923807978630066 
model_pd.l_d.mean(): -22.507888793945312 
model_pd.lagr.mean(): -22.35865020751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3457], device='cuda:0')), ('power', tensor([-22.8536], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.14923807978630066
epoch£º404	 i:1 	 global-step:8081	 l-p:0.11680825799703598
epoch£º404	 i:2 	 global-step:8082	 l-p:0.15002413094043732
epoch£º404	 i:3 	 global-step:8083	 l-p:0.1293839067220688
epoch£º404	 i:4 	 global-step:8084	 l-p:0.12255828827619553
epoch£º404	 i:5 	 global-step:8085	 l-p:0.2204495221376419
epoch£º404	 i:6 	 global-step:8086	 l-p:0.1281130313873291
epoch£º404	 i:7 	 global-step:8087	 l-p:0.12518684566020966
epoch£º404	 i:8 	 global-step:8088	 l-p:0.15018415451049805
epoch£º404	 i:9 	 global-step:8089	 l-p:0.140926793217659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3853, 3.0957, 2.9781],
        [3.3853, 3.3853, 3.3853],
        [3.3853, 3.3853, 3.3853],
        [3.3853, 3.1570, 3.1519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.20703959465026855 
model_pd.l_d.mean(): -23.32180404663086 
model_pd.lagr.mean(): -23.114765167236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2571], device='cuda:0')), ('power', tensor([-23.5789], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.20703959465026855
epoch£º405	 i:1 	 global-step:8101	 l-p:0.15069672465324402
epoch£º405	 i:2 	 global-step:8102	 l-p:0.10501027852296829
epoch£º405	 i:3 	 global-step:8103	 l-p:0.1491495817899704
epoch£º405	 i:4 	 global-step:8104	 l-p:0.14718391001224518
epoch£º405	 i:5 	 global-step:8105	 l-p:0.15956269204616547
epoch£º405	 i:6 	 global-step:8106	 l-p:0.16165700554847717
epoch£º405	 i:7 	 global-step:8107	 l-p:0.15743854641914368
epoch£º405	 i:8 	 global-step:8108	 l-p:0.13666149973869324
epoch£º405	 i:9 	 global-step:8109	 l-p:0.14023703336715698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3380, 3.2053, 3.2596],
        [3.3380, 3.2934, 3.3271],
        [3.3380, 3.1071, 3.1047],
        [3.3380, 3.0435, 2.7607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.14873585104942322 
model_pd.l_d.mean(): -22.86255645751953 
model_pd.lagr.mean(): -22.713821411132812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3180], device='cuda:0')), ('power', tensor([-23.1806], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.14873585104942322
epoch£º406	 i:1 	 global-step:8121	 l-p:-0.0504651740193367
epoch£º406	 i:2 	 global-step:8122	 l-p:0.13753478229045868
epoch£º406	 i:3 	 global-step:8123	 l-p:0.13644294440746307
epoch£º406	 i:4 	 global-step:8124	 l-p:0.14680036902427673
epoch£º406	 i:5 	 global-step:8125	 l-p:0.1771053522825241
epoch£º406	 i:6 	 global-step:8126	 l-p:0.1638762205839157
epoch£º406	 i:7 	 global-step:8127	 l-p:0.14955289661884308
epoch£º406	 i:8 	 global-step:8128	 l-p:0.13090647757053375
epoch£º406	 i:9 	 global-step:8129	 l-p:0.13079452514648438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4040, 3.3907, 3.4026],
        [3.4040, 3.3921, 3.4029],
        [3.4040, 3.1009, 2.8559],
        [3.4040, 3.1092, 2.9788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.19946585595607758 
model_pd.l_d.mean(): -23.500526428222656 
model_pd.lagr.mean(): -23.30105972290039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2044], device='cuda:0')), ('power', tensor([-23.7050], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.19946585595607758
epoch£º407	 i:1 	 global-step:8141	 l-p:0.11725328117609024
epoch£º407	 i:2 	 global-step:8142	 l-p:0.131424218416214
epoch£º407	 i:3 	 global-step:8143	 l-p:0.13022606074810028
epoch£º407	 i:4 	 global-step:8144	 l-p:0.13788118958473206
epoch£º407	 i:5 	 global-step:8145	 l-p:0.13926126062870026
epoch£º407	 i:6 	 global-step:8146	 l-p:0.13534629344940186
epoch£º407	 i:7 	 global-step:8147	 l-p:0.128920316696167
epoch£º407	 i:8 	 global-step:8148	 l-p:0.11724795401096344
epoch£º407	 i:9 	 global-step:8149	 l-p:0.11996280401945114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4657, 3.3900, 3.4374],
        [3.4657, 3.4002, 3.4439],
        [3.4657, 3.2385, 3.2322],
        [3.4657, 3.1666, 2.9368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.14856041967868805 
model_pd.l_d.mean(): -23.48284339904785 
model_pd.lagr.mean(): -23.33428382873535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2107], device='cuda:0')), ('power', tensor([-23.6936], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.14856041967868805
epoch£º408	 i:1 	 global-step:8161	 l-p:0.13891607522964478
epoch£º408	 i:2 	 global-step:8162	 l-p:0.13044913113117218
epoch£º408	 i:3 	 global-step:8163	 l-p:0.13792556524276733
epoch£º408	 i:4 	 global-step:8164	 l-p:0.1624036431312561
epoch£º408	 i:5 	 global-step:8165	 l-p:0.168344184756279
epoch£º408	 i:6 	 global-step:8166	 l-p:0.1331002563238144
epoch£º408	 i:7 	 global-step:8167	 l-p:0.13410349190235138
epoch£º408	 i:8 	 global-step:8168	 l-p:0.14511477947235107
epoch£º408	 i:9 	 global-step:8169	 l-p:0.09007233381271362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2974, 3.2478, 3.2846],
        [3.2974, 3.2422, 3.2819],
        [3.2974, 3.2937, 3.2973],
        [3.2974, 3.2948, 3.2974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.14565598964691162 
model_pd.l_d.mean(): -23.51861000061035 
model_pd.lagr.mean(): -23.372953414916992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2367], device='cuda:0')), ('power', tensor([-23.7553], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.14565598964691162
epoch£º409	 i:1 	 global-step:8181	 l-p:0.1810358762741089
epoch£º409	 i:2 	 global-step:8182	 l-p:0.20569849014282227
epoch£º409	 i:3 	 global-step:8183	 l-p:0.11650366336107254
epoch£º409	 i:4 	 global-step:8184	 l-p:0.1538999378681183
epoch£º409	 i:5 	 global-step:8185	 l-p:0.134067103266716
epoch£º409	 i:6 	 global-step:8186	 l-p:0.15950533747673035
epoch£º409	 i:7 	 global-step:8187	 l-p:0.13716426491737366
epoch£º409	 i:8 	 global-step:8188	 l-p:0.14742721617221832
epoch£º409	 i:9 	 global-step:8189	 l-p:0.11696962267160416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4155, 3.3372, 3.3860],
        [3.4155, 3.2643, 3.3153],
        [3.4155, 3.4027, 3.4142],
        [3.4155, 3.4155, 3.4155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.1396695375442505 
model_pd.l_d.mean(): -23.393583297729492 
model_pd.lagr.mean(): -23.25391387939453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2340], device='cuda:0')), ('power', tensor([-23.6275], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.1396695375442505
epoch£º410	 i:1 	 global-step:8201	 l-p:0.13343094289302826
epoch£º410	 i:2 	 global-step:8202	 l-p:0.1757572442293167
epoch£º410	 i:3 	 global-step:8203	 l-p:0.1323452591896057
epoch£º410	 i:4 	 global-step:8204	 l-p:0.14214645326137543
epoch£º410	 i:5 	 global-step:8205	 l-p:0.12733784317970276
epoch£º410	 i:6 	 global-step:8206	 l-p:0.15109600126743317
epoch£º410	 i:7 	 global-step:8207	 l-p:0.12852083146572113
epoch£º410	 i:8 	 global-step:8208	 l-p:0.14422878623008728
epoch£º410	 i:9 	 global-step:8209	 l-p:0.17651015520095825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3811, 3.3811, 3.3811],
        [3.3811, 3.3797, 3.3811],
        [3.3811, 3.3808, 3.3811],
        [3.3811, 3.3786, 3.3810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.34859132766723633 
model_pd.l_d.mean(): -23.299457550048828 
model_pd.lagr.mean(): -22.95086669921875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2574], device='cuda:0')), ('power', tensor([-23.5569], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.34859132766723633
epoch£º411	 i:1 	 global-step:8221	 l-p:0.13788390159606934
epoch£º411	 i:2 	 global-step:8222	 l-p:0.14266589283943176
epoch£º411	 i:3 	 global-step:8223	 l-p:0.1534586101770401
epoch£º411	 i:4 	 global-step:8224	 l-p:0.13536033034324646
epoch£º411	 i:5 	 global-step:8225	 l-p:0.1320958435535431
epoch£º411	 i:6 	 global-step:8226	 l-p:0.1433621644973755
epoch£º411	 i:7 	 global-step:8227	 l-p:0.17049676179885864
epoch£º411	 i:8 	 global-step:8228	 l-p:0.12960590422153473
epoch£º411	 i:9 	 global-step:8229	 l-p:0.14210541546344757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3893, 3.3766, 3.3880],
        [3.3893, 3.4332, 3.2076],
        [3.3893, 3.3893, 3.3893],
        [3.3893, 3.2383, 3.2909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.1458776295185089 
model_pd.l_d.mean(): -22.517120361328125 
model_pd.lagr.mean(): -22.37124252319336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3139], device='cuda:0')), ('power', tensor([-22.8310], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.1458776295185089
epoch£º412	 i:1 	 global-step:8241	 l-p:0.14428235590457916
epoch£º412	 i:2 	 global-step:8242	 l-p:0.12816913425922394
epoch£º412	 i:3 	 global-step:8243	 l-p:0.13471285998821259
epoch£º412	 i:4 	 global-step:8244	 l-p:0.14431072771549225
epoch£º412	 i:5 	 global-step:8245	 l-p:0.13898023962974548
epoch£º412	 i:6 	 global-step:8246	 l-p:0.14691875874996185
epoch£º412	 i:7 	 global-step:8247	 l-p:0.7384275794029236
epoch£º412	 i:8 	 global-step:8248	 l-p:0.14242704212665558
epoch£º412	 i:9 	 global-step:8249	 l-p:0.15641741454601288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3788, 3.2626, 3.3192],
        [3.3788, 3.3788, 3.3788],
        [3.3788, 3.3797, 3.1328],
        [3.3788, 3.3788, 3.3788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.13646765053272247 
model_pd.l_d.mean(): -23.011638641357422 
model_pd.lagr.mean(): -22.875171661376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2868], device='cuda:0')), ('power', tensor([-23.2984], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.13646765053272247
epoch£º413	 i:1 	 global-step:8261	 l-p:0.13238678872585297
epoch£º413	 i:2 	 global-step:8262	 l-p:0.13077551126480103
epoch£º413	 i:3 	 global-step:8263	 l-p:0.14948596060276031
epoch£º413	 i:4 	 global-step:8264	 l-p:0.11299150437116623
epoch£º413	 i:5 	 global-step:8265	 l-p:0.130528524518013
epoch£º413	 i:6 	 global-step:8266	 l-p:0.14826041460037231
epoch£º413	 i:7 	 global-step:8267	 l-p:0.21518436074256897
epoch£º413	 i:8 	 global-step:8268	 l-p:0.1886642426252365
epoch£º413	 i:9 	 global-step:8269	 l-p:0.12427961081266403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4352, 3.4276, 3.4346],
        [3.4352, 3.1363, 2.8499],
        [3.4352, 3.4352, 3.4352],
        [3.4352, 3.3804, 3.4198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.14556457102298737 
model_pd.l_d.mean(): -22.7542781829834 
model_pd.lagr.mean(): -22.608713150024414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2797], device='cuda:0')), ('power', tensor([-23.0340], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.14556457102298737
epoch£º414	 i:1 	 global-step:8281	 l-p:0.13140831887722015
epoch£º414	 i:2 	 global-step:8282	 l-p:0.1509731560945511
epoch£º414	 i:3 	 global-step:8283	 l-p:0.1755712628364563
epoch£º414	 i:4 	 global-step:8284	 l-p:0.12350621819496155
epoch£º414	 i:5 	 global-step:8285	 l-p:0.11933406442403793
epoch£º414	 i:6 	 global-step:8286	 l-p:0.13912124931812286
epoch£º414	 i:7 	 global-step:8287	 l-p:0.12927697598934174
epoch£º414	 i:8 	 global-step:8288	 l-p:0.12012948840856552
epoch£º414	 i:9 	 global-step:8289	 l-p:0.13534173369407654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4116, 3.0970, 2.8255],
        [3.4116, 3.2773, 2.9768],
        [3.4116, 3.0888, 2.8574],
        [3.4116, 3.3809, 3.4060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.13210372626781464 
model_pd.l_d.mean(): -22.985109329223633 
model_pd.lagr.mean(): -22.85300636291504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3305], device='cuda:0')), ('power', tensor([-23.3156], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.13210372626781464
epoch£º415	 i:1 	 global-step:8301	 l-p:0.11950130015611649
epoch£º415	 i:2 	 global-step:8302	 l-p:0.13216161727905273
epoch£º415	 i:3 	 global-step:8303	 l-p:0.1763855367898941
epoch£º415	 i:4 	 global-step:8304	 l-p:0.13991092145442963
epoch£º415	 i:5 	 global-step:8305	 l-p:0.1995493322610855
epoch£º415	 i:6 	 global-step:8306	 l-p:0.17922145128250122
epoch£º415	 i:7 	 global-step:8307	 l-p:0.07618774473667145
epoch£º415	 i:8 	 global-step:8308	 l-p:0.14390742778778076
epoch£º415	 i:9 	 global-step:8309	 l-p:0.12838409841060638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3770, 3.3024, 3.3508],
        [3.3770, 3.3769, 3.3770],
        [3.3770, 3.0509, 2.7877],
        [3.3770, 3.3770, 3.3770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.15115897357463837 
model_pd.l_d.mean(): -23.519580841064453 
model_pd.lagr.mean(): -23.36842155456543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2448], device='cuda:0')), ('power', tensor([-23.7644], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.15115897357463837
epoch£º416	 i:1 	 global-step:8321	 l-p:0.15200212597846985
epoch£º416	 i:2 	 global-step:8322	 l-p:-0.30856794118881226
epoch£º416	 i:3 	 global-step:8323	 l-p:0.15750561654567719
epoch£º416	 i:4 	 global-step:8324	 l-p:0.118833027780056
epoch£º416	 i:5 	 global-step:8325	 l-p:0.13615049421787262
epoch£º416	 i:6 	 global-step:8326	 l-p:0.1336478739976883
epoch£º416	 i:7 	 global-step:8327	 l-p:0.15304240584373474
epoch£º416	 i:8 	 global-step:8328	 l-p:0.11737971752882004
epoch£º416	 i:9 	 global-step:8329	 l-p:0.12258819490671158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4657, 3.4476, 3.4634],
        [3.4657, 3.4266, 3.4572],
        [3.4657, 3.3492, 3.4057],
        [3.4657, 3.4657, 3.4657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.1449061781167984 
model_pd.l_d.mean(): -23.3333797454834 
model_pd.lagr.mean(): -23.188472747802734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2108], device='cuda:0')), ('power', tensor([-23.5442], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.1449061781167984
epoch£º417	 i:1 	 global-step:8341	 l-p:0.12937253713607788
epoch£º417	 i:2 	 global-step:8342	 l-p:0.12430017441511154
epoch£º417	 i:3 	 global-step:8343	 l-p:0.2072799801826477
epoch£º417	 i:4 	 global-step:8344	 l-p:0.1270332634449005
epoch£º417	 i:5 	 global-step:8345	 l-p:0.1450893133878708
epoch£º417	 i:6 	 global-step:8346	 l-p:0.17359277606010437
epoch£º417	 i:7 	 global-step:8347	 l-p:0.16366052627563477
epoch£º417	 i:8 	 global-step:8348	 l-p:0.18467193841934204
epoch£º417	 i:9 	 global-step:8349	 l-p:0.1313735693693161
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3397, 3.3394, 3.3397],
        [3.3397, 3.3395, 3.3397],
        [3.3397, 3.0394, 2.7222],
        [3.3397, 3.1433, 3.1826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.16570936143398285 
model_pd.l_d.mean(): -22.840917587280273 
model_pd.lagr.mean(): -22.675209045410156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3117], device='cuda:0')), ('power', tensor([-23.1526], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:0.16570936143398285
epoch£º418	 i:1 	 global-step:8361	 l-p:0.14918410778045654
epoch£º418	 i:2 	 global-step:8362	 l-p:0.06336554139852524
epoch£º418	 i:3 	 global-step:8363	 l-p:0.12760517001152039
epoch£º418	 i:4 	 global-step:8364	 l-p:0.1353752613067627
epoch£º418	 i:5 	 global-step:8365	 l-p:0.1541634500026703
epoch£º418	 i:6 	 global-step:8366	 l-p:0.11850481480360031
epoch£º418	 i:7 	 global-step:8367	 l-p:0.14735569059848785
epoch£º418	 i:8 	 global-step:8368	 l-p:0.13233435153961182
epoch£º418	 i:9 	 global-step:8369	 l-p:0.15822365880012512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3955, 3.2017, 3.2408],
        [3.3955, 3.0624, 2.8313],
        [3.3955, 3.0782, 2.9424],
        [3.3955, 3.3757, 3.3929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.32538700103759766 
model_pd.l_d.mean(): -23.2905216217041 
model_pd.lagr.mean(): -22.965133666992188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2819], device='cuda:0')), ('power', tensor([-23.5724], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.32538700103759766
epoch£º419	 i:1 	 global-step:8381	 l-p:0.18013456463813782
epoch£º419	 i:2 	 global-step:8382	 l-p:0.15750975906848907
epoch£º419	 i:3 	 global-step:8383	 l-p:0.1287110149860382
epoch£º419	 i:4 	 global-step:8384	 l-p:0.13889819383621216
epoch£º419	 i:5 	 global-step:8385	 l-p:0.14982756972312927
epoch£º419	 i:6 	 global-step:8386	 l-p:0.13134507834911346
epoch£º419	 i:7 	 global-step:8387	 l-p:0.12422443181276321
epoch£º419	 i:8 	 global-step:8388	 l-p:0.1347862035036087
epoch£º419	 i:9 	 global-step:8389	 l-p:0.13581162691116333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3236, 2.9984, 2.6919],
        [3.3236, 3.1485, 3.1996],
        [3.3236, 3.0728, 3.0704],
        [3.3236, 3.3226, 3.3235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.1542634218931198 
model_pd.l_d.mean(): -22.826417922973633 
model_pd.lagr.mean(): -22.672155380249023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3034], device='cuda:0')), ('power', tensor([-23.1298], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.1542634218931198
epoch£º420	 i:1 	 global-step:8401	 l-p:0.1349761188030243
epoch£º420	 i:2 	 global-step:8402	 l-p:0.13545547425746918
epoch£º420	 i:3 	 global-step:8403	 l-p:0.20778067409992218
epoch£º420	 i:4 	 global-step:8404	 l-p:0.1387845128774643
epoch£º420	 i:5 	 global-step:8405	 l-p:0.1730194240808487
epoch£º420	 i:6 	 global-step:8406	 l-p:0.21692393720149994
epoch£º420	 i:7 	 global-step:8407	 l-p:0.12435361742973328
epoch£º420	 i:8 	 global-step:8408	 l-p:-0.08498376607894897
epoch£º420	 i:9 	 global-step:8409	 l-p:0.15249471366405487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4186, 3.1530, 3.1240],
        [3.4186, 3.3462, 3.0604],
        [3.4186, 3.3307, 3.3838],
        [3.4186, 3.4051, 3.4172]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.188246488571167 
model_pd.l_d.mean(): -23.26809310913086 
model_pd.lagr.mean(): -23.07984733581543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2376], device='cuda:0')), ('power', tensor([-23.5057], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.188246488571167
epoch£º421	 i:1 	 global-step:8421	 l-p:0.12365901470184326
epoch£º421	 i:2 	 global-step:8422	 l-p:0.10817808657884598
epoch£º421	 i:3 	 global-step:8423	 l-p:0.12900729477405548
epoch£º421	 i:4 	 global-step:8424	 l-p:0.12996181845664978
epoch£º421	 i:5 	 global-step:8425	 l-p:0.16840754449367523
epoch£º421	 i:6 	 global-step:8426	 l-p:0.13933266699314117
epoch£º421	 i:7 	 global-step:8427	 l-p:0.13714465498924255
epoch£º421	 i:8 	 global-step:8428	 l-p:0.13750319182872772
epoch£º421	 i:9 	 global-step:8429	 l-p:0.15833617746829987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4067, 3.3726, 3.4002],
        [3.4067, 3.4067, 3.4067],
        [3.4067, 3.4052, 3.4067],
        [3.4067, 3.0703, 2.8632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.13207364082336426 
model_pd.l_d.mean(): -23.257734298706055 
model_pd.lagr.mean(): -23.125659942626953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2605], device='cuda:0')), ('power', tensor([-23.5182], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.13207364082336426
epoch£º422	 i:1 	 global-step:8441	 l-p:0.13616381585597992
epoch£º422	 i:2 	 global-step:8442	 l-p:0.12403999269008636
epoch£º422	 i:3 	 global-step:8443	 l-p:0.11408762633800507
epoch£º422	 i:4 	 global-step:8444	 l-p:0.16399024426937103
epoch£º422	 i:5 	 global-step:8445	 l-p:0.5765087604522705
epoch£º422	 i:6 	 global-step:8446	 l-p:116.33448791503906
epoch£º422	 i:7 	 global-step:8447	 l-p:0.2582556903362274
epoch£º422	 i:8 	 global-step:8448	 l-p:0.16273240745067596
epoch£º422	 i:9 	 global-step:8449	 l-p:0.13635459542274475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3406, 3.3405, 3.3406],
        [3.3406, 2.9922, 2.7342],
        [3.3406, 3.2994, 3.3317],
        [3.3406, 3.0143, 2.8885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.08600568771362305 
model_pd.l_d.mean(): -23.209911346435547 
model_pd.lagr.mean(): -23.123905181884766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3274], device='cuda:0')), ('power', tensor([-23.5373], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.08600568771362305
epoch£º423	 i:1 	 global-step:8461	 l-p:0.17599081993103027
epoch£º423	 i:2 	 global-step:8462	 l-p:0.1503022313117981
epoch£º423	 i:3 	 global-step:8463	 l-p:0.1460118144750595
epoch£º423	 i:4 	 global-step:8464	 l-p:0.14658862352371216
epoch£º423	 i:5 	 global-step:8465	 l-p:0.11908740550279617
epoch£º423	 i:6 	 global-step:8466	 l-p:0.12120267748832703
epoch£º423	 i:7 	 global-step:8467	 l-p:0.11870941519737244
epoch£º423	 i:8 	 global-step:8468	 l-p:0.1197904720902443
epoch£º423	 i:9 	 global-step:8469	 l-p:0.12280923873186111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5459, 3.3622, 3.4040],
        [3.5459, 3.5411, 3.5456],
        [3.5459, 3.2954, 2.9739],
        [3.5459, 3.5459, 3.5459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.11570785194635391 
model_pd.l_d.mean(): -23.18666648864746 
model_pd.lagr.mean(): -23.070959091186523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2405], device='cuda:0')), ('power', tensor([-23.4272], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.11570785194635391
epoch£º424	 i:1 	 global-step:8481	 l-p:0.13175353407859802
epoch£º424	 i:2 	 global-step:8482	 l-p:0.1432856172323227
epoch£º424	 i:3 	 global-step:8483	 l-p:0.14969293773174286
epoch£º424	 i:4 	 global-step:8484	 l-p:0.13181941211223602
epoch£º424	 i:5 	 global-step:8485	 l-p:0.1402694582939148
epoch£º424	 i:6 	 global-step:8486	 l-p:0.1360609531402588
epoch£º424	 i:7 	 global-step:8487	 l-p:0.15826983749866486
epoch£º424	 i:8 	 global-step:8488	 l-p:0.15470200777053833
epoch£º424	 i:9 	 global-step:8489	 l-p:0.1324128657579422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3488, 3.1787, 3.2334],
        [3.3488, 3.0661, 3.0289],
        [3.3488, 3.0172, 2.7052],
        [3.3488, 3.3488, 3.3488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.1650252491235733 
model_pd.l_d.mean(): -22.775304794311523 
model_pd.lagr.mean(): -22.610279083251953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2974], device='cuda:0')), ('power', tensor([-23.0727], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.1650252491235733
epoch£º425	 i:1 	 global-step:8501	 l-p:0.14152304828166962
epoch£º425	 i:2 	 global-step:8502	 l-p:0.15799878537654877
epoch£º425	 i:3 	 global-step:8503	 l-p:0.1512930989265442
epoch£º425	 i:4 	 global-step:8504	 l-p:0.17362159490585327
epoch£º425	 i:5 	 global-step:8505	 l-p:0.04383834823966026
epoch£º425	 i:6 	 global-step:8506	 l-p:0.12230440229177475
epoch£º425	 i:7 	 global-step:8507	 l-p:0.1379062682390213
epoch£º425	 i:8 	 global-step:8508	 l-p:0.15312814712524414
epoch£º425	 i:9 	 global-step:8509	 l-p:0.12899966537952423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4576, 3.4390, 3.4553],
        [3.4576, 3.4562, 3.4576],
        [3.4576, 3.4521, 3.4573],
        [3.4576, 3.2298, 2.8975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.17047728598117828 
model_pd.l_d.mean(): -22.458349227905273 
model_pd.lagr.mean(): -22.287872314453125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3300], device='cuda:0')), ('power', tensor([-22.7883], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.17047728598117828
epoch£º426	 i:1 	 global-step:8521	 l-p:0.14259426295757294
epoch£º426	 i:2 	 global-step:8522	 l-p:0.13405302166938782
epoch£º426	 i:3 	 global-step:8523	 l-p:0.1351071149110794
epoch£º426	 i:4 	 global-step:8524	 l-p:0.12339654564857483
epoch£º426	 i:5 	 global-step:8525	 l-p:0.12815342843532562
epoch£º426	 i:6 	 global-step:8526	 l-p:0.12441270798444748
epoch£º426	 i:7 	 global-step:8527	 l-p:0.1654435098171234
epoch£º426	 i:8 	 global-step:8528	 l-p:0.1412103772163391
epoch£º426	 i:9 	 global-step:8529	 l-p:0.11599170416593552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4004, 3.3874, 3.3991],
        [3.4004, 3.3947, 3.4001],
        [3.4004, 3.2770, 3.3372],
        [3.4004, 3.0569, 2.8678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.13975748419761658 
model_pd.l_d.mean(): -22.951675415039062 
model_pd.lagr.mean(): -22.811918258666992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2306], device='cuda:0')), ('power', tensor([-23.1823], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.13975748419761658
epoch£º427	 i:1 	 global-step:8541	 l-p:0.16462455689907074
epoch£º427	 i:2 	 global-step:8542	 l-p:0.13789068162441254
epoch£º427	 i:3 	 global-step:8543	 l-p:0.12009891122579575
epoch£º427	 i:4 	 global-step:8544	 l-p:0.126640185713768
epoch£º427	 i:5 	 global-step:8545	 l-p:0.13336987793445587
epoch£º427	 i:6 	 global-step:8546	 l-p:0.17016533017158508
epoch£º427	 i:7 	 global-step:8547	 l-p:0.11853790283203125
epoch£º427	 i:8 	 global-step:8548	 l-p:0.14619100093841553
epoch£º427	 i:9 	 global-step:8549	 l-p:0.13715313374996185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[3.3693, 3.1861, 2.8550],
        [3.3693, 3.0839, 3.0466],
        [3.3693, 3.1129, 2.7707],
        [3.3693, 3.0607, 2.9849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.14746122062206268 
model_pd.l_d.mean(): -23.487083435058594 
model_pd.lagr.mean(): -23.339622497558594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2409], device='cuda:0')), ('power', tensor([-23.7279], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.14746122062206268
epoch£º428	 i:1 	 global-step:8561	 l-p:0.1741338074207306
epoch£º428	 i:2 	 global-step:8562	 l-p:0.17683303356170654
epoch£º428	 i:3 	 global-step:8563	 l-p:0.1350836306810379
epoch£º428	 i:4 	 global-step:8564	 l-p:0.0678829550743103
epoch£º428	 i:5 	 global-step:8565	 l-p:0.15416570007801056
epoch£º428	 i:6 	 global-step:8566	 l-p:0.13730743527412415
epoch£º428	 i:7 	 global-step:8567	 l-p:0.1280871033668518
epoch£º428	 i:8 	 global-step:8568	 l-p:0.1524171382188797
epoch£º428	 i:9 	 global-step:8569	 l-p:0.14033514261245728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4180, 3.0901, 2.9641],
        [3.4180, 3.4178, 3.4180],
        [3.4180, 3.1062, 2.7770],
        [3.4180, 3.3896, 3.1116]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.09999486804008484 
model_pd.l_d.mean(): -22.67327308654785 
model_pd.lagr.mean(): -22.573278427124023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3605], device='cuda:0')), ('power', tensor([-23.0338], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.09999486804008484
epoch£º429	 i:1 	 global-step:8581	 l-p:0.13980405032634735
epoch£º429	 i:2 	 global-step:8582	 l-p:0.13441698253154755
epoch£º429	 i:3 	 global-step:8583	 l-p:0.140128031373024
epoch£º429	 i:4 	 global-step:8584	 l-p:0.1694394201040268
epoch£º429	 i:5 	 global-step:8585	 l-p:0.1556062549352646
epoch£º429	 i:6 	 global-step:8586	 l-p:0.15092012286186218
epoch£º429	 i:7 	 global-step:8587	 l-p:0.14914068579673767
epoch£º429	 i:8 	 global-step:8588	 l-p:0.15047825872898102
epoch£º429	 i:9 	 global-step:8589	 l-p:-0.14257685840129852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3941, 3.3205, 3.3698],
        [3.3941, 3.3521, 3.0663],
        [3.3941, 3.2287, 3.2862],
        [3.3941, 3.3912, 3.3939]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.17452165484428406 
model_pd.l_d.mean(): -22.64033317565918 
model_pd.lagr.mean(): -22.465810775756836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3847], device='cuda:0')), ('power', tensor([-23.0250], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.17452165484428406
epoch£º430	 i:1 	 global-step:8601	 l-p:0.13930168747901917
epoch£º430	 i:2 	 global-step:8602	 l-p:0.1368444263935089
epoch£º430	 i:3 	 global-step:8603	 l-p:0.13868123292922974
epoch£º430	 i:4 	 global-step:8604	 l-p:0.1346016228199005
epoch£º430	 i:5 	 global-step:8605	 l-p:0.12229783833026886
epoch£º430	 i:6 	 global-step:8606	 l-p:0.19226451218128204
epoch£º430	 i:7 	 global-step:8607	 l-p:0.18474198877811432
epoch£º430	 i:8 	 global-step:8608	 l-p:0.3433704972267151
epoch£º430	 i:9 	 global-step:8609	 l-p:0.1146172508597374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3193, 2.9627, 2.6487],
        [3.3193, 3.2587, 2.9625],
        [3.3193, 3.2925, 3.3151],
        [3.3193, 2.9883, 2.8872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.23863424360752106 
model_pd.l_d.mean(): -23.150606155395508 
model_pd.lagr.mean(): -22.911972045898438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3176], device='cuda:0')), ('power', tensor([-23.4682], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.23863424360752106
epoch£º431	 i:1 	 global-step:8621	 l-p:0.21255725622177124
epoch£º431	 i:2 	 global-step:8622	 l-p:0.13269710540771484
epoch£º431	 i:3 	 global-step:8623	 l-p:0.1702100783586502
epoch£º431	 i:4 	 global-step:8624	 l-p:0.14647939801216125
epoch£º431	 i:5 	 global-step:8625	 l-p:0.13685275614261627
epoch£º431	 i:6 	 global-step:8626	 l-p:0.12682794034481049
epoch£º431	 i:7 	 global-step:8627	 l-p:-0.9945374727249146
epoch£º431	 i:8 	 global-step:8628	 l-p:0.11858558654785156
epoch£º431	 i:9 	 global-step:8629	 l-p:0.144722118973732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3844, 3.2977, 2.9925],
        [3.3844, 3.3842, 3.3844],
        [3.3844, 3.3437, 3.3758],
        [3.3844, 3.3067, 3.3577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.008643359877169132 
model_pd.l_d.mean(): -23.195545196533203 
model_pd.lagr.mean(): -23.186901092529297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2920], device='cuda:0')), ('power', tensor([-23.4875], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.008643359877169132
epoch£º432	 i:1 	 global-step:8641	 l-p:0.1459253430366516
epoch£º432	 i:2 	 global-step:8642	 l-p:0.14785252511501312
epoch£º432	 i:3 	 global-step:8643	 l-p:0.17360183596611023
epoch£º432	 i:4 	 global-step:8644	 l-p:0.16073232889175415
epoch£º432	 i:5 	 global-step:8645	 l-p:0.12219490110874176
epoch£º432	 i:6 	 global-step:8646	 l-p:0.13501715660095215
epoch£º432	 i:7 	 global-step:8647	 l-p:0.166286438703537
epoch£º432	 i:8 	 global-step:8648	 l-p:0.16525112092494965
epoch£º432	 i:9 	 global-step:8649	 l-p:0.12682600319385529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4258, 3.1448, 3.1155],
        [3.4258, 3.1092, 2.7749],
        [3.4258, 3.4258, 3.4258],
        [3.4258, 3.3662, 3.4091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.13544660806655884 
model_pd.l_d.mean(): -23.145742416381836 
model_pd.lagr.mean(): -23.010295867919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2786], device='cuda:0')), ('power', tensor([-23.4244], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.13544660806655884
epoch£º433	 i:1 	 global-step:8661	 l-p:0.1323864758014679
epoch£º433	 i:2 	 global-step:8662	 l-p:0.12808530032634735
epoch£º433	 i:3 	 global-step:8663	 l-p:0.14080071449279785
epoch£º433	 i:4 	 global-step:8664	 l-p:0.5384907126426697
epoch£º433	 i:5 	 global-step:8665	 l-p:0.15786439180374146
epoch£º433	 i:6 	 global-step:8666	 l-p:0.15882651507854462
epoch£º433	 i:7 	 global-step:8667	 l-p:0.14809218049049377
epoch£º433	 i:8 	 global-step:8668	 l-p:0.13538646697998047
epoch£º433	 i:9 	 global-step:8669	 l-p:0.14082451164722443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[3.4305, 3.0889, 2.9376],
        [3.4305, 3.1357, 2.7918],
        [3.4305, 3.2117, 3.2454],
        [3.4305, 3.1526, 3.1283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.14617983996868134 
model_pd.l_d.mean(): -23.701013565063477 
model_pd.lagr.mean(): -23.554834365844727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1885], device='cuda:0')), ('power', tensor([-23.8895], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.14617983996868134
epoch£º434	 i:1 	 global-step:8681	 l-p:0.14378447830677032
epoch£º434	 i:2 	 global-step:8682	 l-p:0.11960981041193008
epoch£º434	 i:3 	 global-step:8683	 l-p:0.08507159352302551
epoch£º434	 i:4 	 global-step:8684	 l-p:0.14932627975940704
epoch£º434	 i:5 	 global-step:8685	 l-p:0.1829758733510971
epoch£º434	 i:6 	 global-step:8686	 l-p:0.12016893178224564
epoch£º434	 i:7 	 global-step:8687	 l-p:0.14798177778720856
epoch£º434	 i:8 	 global-step:8688	 l-p:0.14040499925613403
epoch£º434	 i:9 	 global-step:8689	 l-p:0.16414473950862885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3761, 3.3758, 3.3761],
        [3.3761, 3.3761, 3.3761],
        [3.3761, 3.3760, 3.3761],
        [3.3761, 3.2666, 2.9500]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.1452690064907074 
model_pd.l_d.mean(): -23.62460708618164 
model_pd.lagr.mean(): -23.479337692260742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1978], device='cuda:0')), ('power', tensor([-23.8224], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.1452690064907074
epoch£º435	 i:1 	 global-step:8701	 l-p:0.15691855549812317
epoch£º435	 i:2 	 global-step:8702	 l-p:0.04691010341048241
epoch£º435	 i:3 	 global-step:8703	 l-p:0.1260063499212265
epoch£º435	 i:4 	 global-step:8704	 l-p:0.12738050520420074
epoch£º435	 i:5 	 global-step:8705	 l-p:0.1530994325876236
epoch£º435	 i:6 	 global-step:8706	 l-p:0.1658308207988739
epoch£º435	 i:7 	 global-step:8707	 l-p:0.1451667994260788
epoch£º435	 i:8 	 global-step:8708	 l-p:0.13542237877845764
epoch£º435	 i:9 	 global-step:8709	 l-p:0.20353266596794128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3903, 3.2692, 2.9475],
        [3.3903, 3.1235, 2.7700],
        [3.3903, 3.3730, 3.3883],
        [3.3903, 3.0297, 2.7281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.07282667607069016 
model_pd.l_d.mean(): -23.404428482055664 
model_pd.lagr.mean(): -23.331602096557617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2849], device='cuda:0')), ('power', tensor([-23.6893], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.07282667607069016
epoch£º436	 i:1 	 global-step:8721	 l-p:0.15067267417907715
epoch£º436	 i:2 	 global-step:8722	 l-p:0.12573058903217316
epoch£º436	 i:3 	 global-step:8723	 l-p:0.13568414747714996
epoch£º436	 i:4 	 global-step:8724	 l-p:0.11693277955055237
epoch£º436	 i:5 	 global-step:8725	 l-p:0.1352788656949997
epoch£º436	 i:6 	 global-step:8726	 l-p:0.141800194978714
epoch£º436	 i:7 	 global-step:8727	 l-p:0.14805415272712708
epoch£º436	 i:8 	 global-step:8728	 l-p:0.13056707382202148
epoch£º436	 i:9 	 global-step:8729	 l-p:0.17118899524211884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3886, 3.3801, 3.3880],
        [3.3886, 3.3768, 3.0961],
        [3.3886, 3.0569, 2.7140],
        [3.3886, 3.0701, 2.7211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.15480664372444153 
model_pd.l_d.mean(): -22.73600196838379 
model_pd.lagr.mean(): -22.581195831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2716], device='cuda:0')), ('power', tensor([-23.0076], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.15480664372444153
epoch£º437	 i:1 	 global-step:8741	 l-p:0.1424437165260315
epoch£º437	 i:2 	 global-step:8742	 l-p:0.1456850916147232
epoch£º437	 i:3 	 global-step:8743	 l-p:0.15030008554458618
epoch£º437	 i:4 	 global-step:8744	 l-p:0.1601833701133728
epoch£º437	 i:5 	 global-step:8745	 l-p:0.3550604283809662
epoch£º437	 i:6 	 global-step:8746	 l-p:0.14522963762283325
epoch£º437	 i:7 	 global-step:8747	 l-p:0.13419245183467865
epoch£º437	 i:8 	 global-step:8748	 l-p:0.14630216360092163
epoch£º437	 i:9 	 global-step:8749	 l-p:0.11825232952833176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4478, 3.1653, 3.1407],
        [3.4478, 3.0840, 2.8256],
        [3.4478, 3.3872, 3.4309],
        [3.4478, 3.4479, 3.4478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.1345078945159912 
model_pd.l_d.mean(): -22.686573028564453 
model_pd.lagr.mean(): -22.552064895629883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2372], device='cuda:0')), ('power', tensor([-22.9238], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.1345078945159912
epoch£º438	 i:1 	 global-step:8761	 l-p:0.14212702214717865
epoch£º438	 i:2 	 global-step:8762	 l-p:0.18040147423744202
epoch£º438	 i:3 	 global-step:8763	 l-p:0.11632807552814484
epoch£º438	 i:4 	 global-step:8764	 l-p:0.06776095926761627
epoch£º438	 i:5 	 global-step:8765	 l-p:0.1524898260831833
epoch£º438	 i:6 	 global-step:8766	 l-p:0.2289247065782547
epoch£º438	 i:7 	 global-step:8767	 l-p:0.1383192241191864
epoch£º438	 i:8 	 global-step:8768	 l-p:0.17415829002857208
epoch£º438	 i:9 	 global-step:8769	 l-p:0.1370375156402588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3148, 2.9630, 2.8419],
        [3.3148, 2.9465, 2.7816],
        [3.3148, 2.9444, 2.7725],
        [3.3148, 3.3143, 3.3148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.11486069113016129 
model_pd.l_d.mean(): -23.656906127929688 
model_pd.lagr.mean(): -23.54204559326172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2253], device='cuda:0')), ('power', tensor([-23.8822], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.11486069113016129
epoch£º439	 i:1 	 global-step:8781	 l-p:0.1378318816423416
epoch£º439	 i:2 	 global-step:8782	 l-p:0.39343130588531494
epoch£º439	 i:3 	 global-step:8783	 l-p:0.13195334374904633
epoch£º439	 i:4 	 global-step:8784	 l-p:0.14603246748447418
epoch£º439	 i:5 	 global-step:8785	 l-p:0.30443838238716125
epoch£º439	 i:6 	 global-step:8786	 l-p:0.14778365194797516
epoch£º439	 i:7 	 global-step:8787	 l-p:0.1782112419605255
epoch£º439	 i:8 	 global-step:8788	 l-p:0.14680394530296326
epoch£º439	 i:9 	 global-step:8789	 l-p:0.13514670729637146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3600, 3.2980, 3.3427],
        [3.3600, 3.3111, 3.3485],
        [3.3600, 3.1240, 3.1553],
        [3.3600, 3.0054, 2.6651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.20167967677116394 
model_pd.l_d.mean(): -22.681184768676758 
model_pd.lagr.mean(): -22.47950553894043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2984], device='cuda:0')), ('power', tensor([-22.9796], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.20167967677116394
epoch£º440	 i:1 	 global-step:8801	 l-p:0.13484153151512146
epoch£º440	 i:2 	 global-step:8802	 l-p:0.05010531470179558
epoch£º440	 i:3 	 global-step:8803	 l-p:0.126053124666214
epoch£º440	 i:4 	 global-step:8804	 l-p:0.14570584893226624
epoch£º440	 i:5 	 global-step:8805	 l-p:0.13214345276355743
epoch£º440	 i:6 	 global-step:8806	 l-p:0.12965907156467438
epoch£º440	 i:7 	 global-step:8807	 l-p:0.15344227850437164
epoch£º440	 i:8 	 global-step:8808	 l-p:0.14548753201961517
epoch£º440	 i:9 	 global-step:8809	 l-p:0.15066838264465332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4337, 3.4162, 3.4317],
        [3.4337, 3.0907, 2.7522],
        [3.4337, 3.4319, 3.4337],
        [3.4337, 3.1079, 3.0224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.15983764827251434 
model_pd.l_d.mean(): -22.53087043762207 
model_pd.lagr.mean(): -22.37103271484375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3264], device='cuda:0')), ('power', tensor([-22.8572], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.15983764827251434
epoch£º441	 i:1 	 global-step:8821	 l-p:0.2501172423362732
epoch£º441	 i:2 	 global-step:8822	 l-p:0.13617828488349915
epoch£º441	 i:3 	 global-step:8823	 l-p:0.12636861205101013
epoch£º441	 i:4 	 global-step:8824	 l-p:0.1449487805366516
epoch£º441	 i:5 	 global-step:8825	 l-p:0.13454259932041168
epoch£º441	 i:6 	 global-step:8826	 l-p:0.13303802907466888
epoch£º441	 i:7 	 global-step:8827	 l-p:0.152534618973732
epoch£º441	 i:8 	 global-step:8828	 l-p:0.12253163754940033
epoch£º441	 i:9 	 global-step:8829	 l-p:0.13517144322395325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3672, 3.3638, 3.3671],
        [3.3672, 3.1156, 2.7527],
        [3.3672, 2.9832, 2.7269],
        [3.3672, 3.3202, 3.3566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.13705557584762573 
model_pd.l_d.mean(): -23.311527252197266 
model_pd.lagr.mean(): -23.174470901489258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2499], device='cuda:0')), ('power', tensor([-23.5615], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.13705557584762573
epoch£º442	 i:1 	 global-step:8841	 l-p:0.14587555825710297
epoch£º442	 i:2 	 global-step:8842	 l-p:0.39503738284111023
epoch£º442	 i:3 	 global-step:8843	 l-p:0.1819964349269867
epoch£º442	 i:4 	 global-step:8844	 l-p:0.17800848186016083
epoch£º442	 i:5 	 global-step:8845	 l-p:0.008060235530138016
epoch£º442	 i:6 	 global-step:8846	 l-p:0.159885436296463
epoch£º442	 i:7 	 global-step:8847	 l-p:0.1581326127052307
epoch£º442	 i:8 	 global-step:8848	 l-p:0.12184226512908936
epoch£º442	 i:9 	 global-step:8849	 l-p:0.16908599436283112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4085, 3.3159, 3.3732],
        [3.4085, 3.4069, 3.4084],
        [3.4085, 3.4063, 3.4084],
        [3.4085, 3.2815, 3.3457]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.12131395190954208 
model_pd.l_d.mean(): -23.14354705810547 
model_pd.lagr.mean(): -23.022233963012695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2382], device='cuda:0')), ('power', tensor([-23.3817], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.12131395190954208
epoch£º443	 i:1 	 global-step:8861	 l-p:0.13739237189292908
epoch£º443	 i:2 	 global-step:8862	 l-p:0.12884217500686646
epoch£º443	 i:3 	 global-step:8863	 l-p:0.15091527998447418
epoch£º443	 i:4 	 global-step:8864	 l-p:0.14624546468257904
epoch£º443	 i:5 	 global-step:8865	 l-p:0.13979379832744598
epoch£º443	 i:6 	 global-step:8866	 l-p:0.11962657421827316
epoch£º443	 i:7 	 global-step:8867	 l-p:0.14553231000900269
epoch£º443	 i:8 	 global-step:8868	 l-p:0.1332360953092575
epoch£º443	 i:9 	 global-step:8869	 l-p:-0.1047859862446785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4107, 3.0519, 2.7132],
        [3.4107, 3.2864, 3.3505],
        [3.4107, 3.1288, 3.1184],
        [3.4107, 3.0361, 2.7256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.011148176155984402 
model_pd.l_d.mean(): -22.425243377685547 
model_pd.lagr.mean(): -22.414094924926758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3397], device='cuda:0')), ('power', tensor([-22.7649], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:0.011148176155984402
epoch£º444	 i:1 	 global-step:8881	 l-p:0.14180772006511688
epoch£º444	 i:2 	 global-step:8882	 l-p:0.13873516023159027
epoch£º444	 i:3 	 global-step:8883	 l-p:0.12901490926742554
epoch£º444	 i:4 	 global-step:8884	 l-p:0.14345332980155945
epoch£º444	 i:5 	 global-step:8885	 l-p:0.13171374797821045
epoch£º444	 i:6 	 global-step:8886	 l-p:0.151070699095726
epoch£º444	 i:7 	 global-step:8887	 l-p:0.15339550375938416
epoch£º444	 i:8 	 global-step:8888	 l-p:0.1646823287010193
epoch£º444	 i:9 	 global-step:8889	 l-p:-0.44538626074790955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3060, 3.3051, 3.3060],
        [3.3060, 2.9731, 2.9064],
        [3.3060, 3.2826, 3.3028],
        [3.3060, 3.0976, 3.1510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.24170276522636414 
model_pd.l_d.mean(): -23.53558921813965 
model_pd.lagr.mean(): -23.293886184692383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2789], device='cuda:0')), ('power', tensor([-23.8145], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.24170276522636414
epoch£º445	 i:1 	 global-step:8901	 l-p:0.14410941302776337
epoch£º445	 i:2 	 global-step:8902	 l-p:0.38784223794937134
epoch£º445	 i:3 	 global-step:8903	 l-p:0.1277969479560852
epoch£º445	 i:4 	 global-step:8904	 l-p:0.13647565245628357
epoch£º445	 i:5 	 global-step:8905	 l-p:0.15823251008987427
epoch£º445	 i:6 	 global-step:8906	 l-p:0.1836072951555252
epoch£º445	 i:7 	 global-step:8907	 l-p:0.18684673309326172
epoch£º445	 i:8 	 global-step:8908	 l-p:0.1261310577392578
epoch£º445	 i:9 	 global-step:8909	 l-p:0.12686966359615326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[3.3963, 3.0709, 3.0045],
        [3.3963, 3.1928, 2.8350],
        [3.3963, 3.0259, 2.8577],
        [3.3963, 3.2082, 2.8543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.09230612218379974 
model_pd.l_d.mean(): -23.227783203125 
model_pd.lagr.mean(): -23.13547706604004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2865], device='cuda:0')), ('power', tensor([-23.5143], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.09230612218379974
epoch£º446	 i:1 	 global-step:8921	 l-p:0.1276858150959015
epoch£º446	 i:2 	 global-step:8922	 l-p:0.12349549680948257
epoch£º446	 i:3 	 global-step:8923	 l-p:0.15165863931179047
epoch£º446	 i:4 	 global-step:8924	 l-p:0.13739906251430511
epoch£º446	 i:5 	 global-step:8925	 l-p:0.16662372648715973
epoch£º446	 i:6 	 global-step:8926	 l-p:0.1428856998682022
epoch£º446	 i:7 	 global-step:8927	 l-p:0.13085466623306274
epoch£º446	 i:8 	 global-step:8928	 l-p:0.15434622764587402
epoch£º446	 i:9 	 global-step:8929	 l-p:0.1938895732164383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3958, 3.3851, 3.3950],
        [3.3958, 3.3025, 3.3606],
        [3.3958, 3.0481, 2.9423],
        [3.3958, 3.0442, 2.9301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.1543087661266327 
model_pd.l_d.mean(): -23.35270118713379 
model_pd.lagr.mean(): -23.198392868041992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2698], device='cuda:0')), ('power', tensor([-23.6225], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.1543087661266327
epoch£º447	 i:1 	 global-step:8941	 l-p:0.13520678877830505
epoch£º447	 i:2 	 global-step:8942	 l-p:0.13250024616718292
epoch£º447	 i:3 	 global-step:8943	 l-p:0.13071145117282867
epoch£º447	 i:4 	 global-step:8944	 l-p:0.16861112415790558
epoch£º447	 i:5 	 global-step:8945	 l-p:0.03129810094833374
epoch£º447	 i:6 	 global-step:8946	 l-p:0.12883146107196808
epoch£º447	 i:7 	 global-step:8947	 l-p:0.1400650143623352
epoch£º447	 i:8 	 global-step:8948	 l-p:0.14408913254737854
epoch£º447	 i:9 	 global-step:8949	 l-p:0.15627503395080566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4458, 3.3275, 2.9933],
        [3.4458, 3.2153, 2.8506],
        [3.4458, 3.3788, 3.4262],
        [3.4458, 3.4457, 3.4458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.12558461725711823 
model_pd.l_d.mean(): -22.965984344482422 
model_pd.lagr.mean(): -22.84039878845215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2959], device='cuda:0')), ('power', tensor([-23.2619], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.12558461725711823
epoch£º448	 i:1 	 global-step:8961	 l-p:0.13672493398189545
epoch£º448	 i:2 	 global-step:8962	 l-p:0.1386059820652008
epoch£º448	 i:3 	 global-step:8963	 l-p:0.13263820111751556
epoch£º448	 i:4 	 global-step:8964	 l-p:-0.06894700974225998
epoch£º448	 i:5 	 global-step:8965	 l-p:0.1993149071931839
epoch£º448	 i:6 	 global-step:8966	 l-p:0.13219447433948517
epoch£º448	 i:7 	 global-step:8967	 l-p:0.12193963676691055
epoch£º448	 i:8 	 global-step:8968	 l-p:0.16328665614128113
epoch£º448	 i:9 	 global-step:8969	 l-p:0.1359616070985794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3679, 3.3633, 3.3677],
        [3.3679, 3.1167, 3.1453],
        [3.3679, 3.2718, 3.3312],
        [3.3679, 3.2474, 3.3126]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.20063823461532593 
model_pd.l_d.mean(): -23.604515075683594 
model_pd.lagr.mean(): -23.40387725830078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2419], device='cuda:0')), ('power', tensor([-23.8464], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.20063823461532593
epoch£º449	 i:1 	 global-step:8981	 l-p:0.16747483611106873
epoch£º449	 i:2 	 global-step:8982	 l-p:0.16298790276050568
epoch£º449	 i:3 	 global-step:8983	 l-p:0.1473022848367691
epoch£º449	 i:4 	 global-step:8984	 l-p:0.14111073315143585
epoch£º449	 i:5 	 global-step:8985	 l-p:0.6265037655830383
epoch£º449	 i:6 	 global-step:8986	 l-p:0.13317440450191498
epoch£º449	 i:7 	 global-step:8987	 l-p:0.12447483092546463
epoch£º449	 i:8 	 global-step:8988	 l-p:0.14796558022499084
epoch£º449	 i:9 	 global-step:8989	 l-p:0.17334987223148346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4096, 3.4087, 3.4096],
        [3.4096, 3.4093, 3.4096],
        [3.4096, 3.4028, 3.4092],
        [3.4096, 3.0904, 2.7152]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.1574186533689499 
model_pd.l_d.mean(): -23.553913116455078 
model_pd.lagr.mean(): -23.396493911743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2419], device='cuda:0')), ('power', tensor([-23.7958], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.1574186533689499
epoch£º450	 i:1 	 global-step:9001	 l-p:0.1418014019727707
epoch£º450	 i:2 	 global-step:9002	 l-p:0.11783304810523987
epoch£º450	 i:3 	 global-step:9003	 l-p:0.12999241054058075
epoch£º450	 i:4 	 global-step:9004	 l-p:0.22668901085853577
epoch£º450	 i:5 	 global-step:9005	 l-p:0.13517215847969055
epoch£º450	 i:6 	 global-step:9006	 l-p:0.14814555644989014
epoch£º450	 i:7 	 global-step:9007	 l-p:0.14376667141914368
epoch£º450	 i:8 	 global-step:9008	 l-p:0.02357466146349907
epoch£º450	 i:9 	 global-step:9009	 l-p:0.11950592696666718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4324, 3.3798, 3.4197],
        [3.4324, 3.2962, 3.3629],
        [3.4324, 3.4322, 3.4324],
        [3.4324, 3.4318, 3.4324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.1257091611623764 
model_pd.l_d.mean(): -22.291744232177734 
model_pd.lagr.mean(): -22.166034698486328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3701], device='cuda:0')), ('power', tensor([-22.6618], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.1257091611623764
epoch£º451	 i:1 	 global-step:9021	 l-p:0.15307623147964478
epoch£º451	 i:2 	 global-step:9022	 l-p:0.13163144886493683
epoch£º451	 i:3 	 global-step:9023	 l-p:0.2393132597208023
epoch£º451	 i:4 	 global-step:9024	 l-p:0.15941807627677917
epoch£º451	 i:5 	 global-step:9025	 l-p:0.11585824936628342
epoch£º451	 i:6 	 global-step:9026	 l-p:0.1447209119796753
epoch£º451	 i:7 	 global-step:9027	 l-p:0.13180340826511383
epoch£º451	 i:8 	 global-step:9028	 l-p:0.1302613914012909
epoch£º451	 i:9 	 global-step:9029	 l-p:0.11711063235998154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4799, 3.3089, 3.3732],
        [3.4799, 3.4770, 3.4798],
        [3.4799, 3.4740, 3.4796],
        [3.4799, 3.4799, 3.4799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.13861994445323944 
model_pd.l_d.mean(): -23.145736694335938 
model_pd.lagr.mean(): -23.007116317749023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2627], device='cuda:0')), ('power', tensor([-23.4085], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.13861994445323944
epoch£º452	 i:1 	 global-step:9041	 l-p:0.17674346268177032
epoch£º452	 i:2 	 global-step:9042	 l-p:0.10653373599052429
epoch£º452	 i:3 	 global-step:9043	 l-p:0.13051258027553558
epoch£º452	 i:4 	 global-step:9044	 l-p:0.12473059445619583
epoch£º452	 i:5 	 global-step:9045	 l-p:0.2700716257095337
epoch£º452	 i:6 	 global-step:9046	 l-p:0.10512670129537582
epoch£º452	 i:7 	 global-step:9047	 l-p:0.20030397176742554
epoch£º452	 i:8 	 global-step:9048	 l-p:0.14439670741558075
epoch£º452	 i:9 	 global-step:9049	 l-p:0.1430826336145401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3423, 3.3392, 3.3422],
        [3.3423, 3.3423, 3.3423],
        [3.3423, 3.3376, 3.3421],
        [3.3423, 3.2482, 3.3078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.18286077678203583 
model_pd.l_d.mean(): -23.59401512145996 
model_pd.lagr.mean(): -23.41115379333496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2582], device='cuda:0')), ('power', tensor([-23.8522], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.18286077678203583
epoch£º453	 i:1 	 global-step:9061	 l-p:0.15302623808383942
epoch£º453	 i:2 	 global-step:9062	 l-p:0.23901966214179993
epoch£º453	 i:3 	 global-step:9063	 l-p:0.22063714265823364
epoch£º453	 i:4 	 global-step:9064	 l-p:0.13618220388889313
epoch£º453	 i:5 	 global-step:9065	 l-p:0.13550935685634613
epoch£º453	 i:6 	 global-step:9066	 l-p:0.15700572729110718
epoch£º453	 i:7 	 global-step:9067	 l-p:0.17481456696987152
epoch£º453	 i:8 	 global-step:9068	 l-p:0.017147107049822807
epoch£º453	 i:9 	 global-step:9069	 l-p:0.14553287625312805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4668, 3.2539, 3.3064],
        [3.4668, 3.0883, 2.7406],
        [3.4668, 3.4606, 3.4664],
        [3.4668, 3.4395, 3.4627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.1342127025127411 
model_pd.l_d.mean(): -23.3521785736084 
model_pd.lagr.mean(): -23.217966079711914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1888], device='cuda:0')), ('power', tensor([-23.5410], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.1342127025127411
epoch£º454	 i:1 	 global-step:9081	 l-p:0.17707493901252747
epoch£º454	 i:2 	 global-step:9082	 l-p:0.1428794413805008
epoch£º454	 i:3 	 global-step:9083	 l-p:0.14283724129199982
epoch£º454	 i:4 	 global-step:9084	 l-p:0.1546291708946228
epoch£º454	 i:5 	 global-step:9085	 l-p:0.12158592790365219
epoch£º454	 i:6 	 global-step:9086	 l-p:0.11831080913543701
epoch£º454	 i:7 	 global-step:9087	 l-p:0.03849257528781891
epoch£º454	 i:8 	 global-step:9088	 l-p:0.1651032269001007
epoch£º454	 i:9 	 global-step:9089	 l-p:0.13704214990139008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3787, 3.0326, 2.9600],
        [3.3787, 3.3776, 3.3787],
        [3.3787, 3.3784, 3.3787],
        [3.3787, 3.3788, 3.3788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.1268763393163681 
model_pd.l_d.mean(): -23.252777099609375 
model_pd.lagr.mean(): -23.125900268554688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2951], device='cuda:0')), ('power', tensor([-23.5479], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.1268763393163681
epoch£º455	 i:1 	 global-step:9101	 l-p:0.12699618935585022
epoch£º455	 i:2 	 global-step:9102	 l-p:0.1488875448703766
epoch£º455	 i:3 	 global-step:9103	 l-p:0.12979575991630554
epoch£º455	 i:4 	 global-step:9104	 l-p:-0.13118086755275726
epoch£º455	 i:5 	 global-step:9105	 l-p:0.18764427304267883
epoch£º455	 i:6 	 global-step:9106	 l-p:0.21681798994541168
epoch£º455	 i:7 	 global-step:9107	 l-p:0.19904185831546783
epoch£º455	 i:8 	 global-step:9108	 l-p:0.1479916125535965
epoch£º455	 i:9 	 global-step:9109	 l-p:0.1245720386505127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3666, 3.3666, 3.3666],
        [3.3666, 3.1916, 3.2593],
        [3.3666, 3.3654, 3.3666],
        [3.3666, 3.3604, 3.3662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.20718248188495636 
model_pd.l_d.mean(): -23.363426208496094 
model_pd.lagr.mean(): -23.1562442779541 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2555], device='cuda:0')), ('power', tensor([-23.6189], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.20718248188495636
epoch£º456	 i:1 	 global-step:9121	 l-p:0.1413460075855255
epoch£º456	 i:2 	 global-step:9122	 l-p:0.1272200345993042
epoch£º456	 i:3 	 global-step:9123	 l-p:0.159873828291893
epoch£º456	 i:4 	 global-step:9124	 l-p:0.1278204768896103
epoch£º456	 i:5 	 global-step:9125	 l-p:0.14377646148204803
epoch£º456	 i:6 	 global-step:9126	 l-p:0.14637242257595062
epoch£º456	 i:7 	 global-step:9127	 l-p:0.146562859416008
epoch£º456	 i:8 	 global-step:9128	 l-p:0.14114367961883545
epoch£º456	 i:9 	 global-step:9129	 l-p:0.1712515652179718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4384, 3.0785, 2.7004],
        [3.4384, 3.2459, 3.3085],
        [3.4384, 3.0310, 2.7508],
        [3.4384, 3.4384, 3.4384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.1535889059305191 
model_pd.l_d.mean(): -23.669036865234375 
model_pd.lagr.mean(): -23.51544761657715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2022], device='cuda:0')), ('power', tensor([-23.8712], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.1535889059305191
epoch£º457	 i:1 	 global-step:9141	 l-p:0.12291637808084488
epoch£º457	 i:2 	 global-step:9142	 l-p:0.13329967856407166
epoch£º457	 i:3 	 global-step:9143	 l-p:0.13628481328487396
epoch£º457	 i:4 	 global-step:9144	 l-p:0.1633726805448532
epoch£º457	 i:5 	 global-step:9145	 l-p:0.14457353949546814
epoch£º457	 i:6 	 global-step:9146	 l-p:0.17806415259838104
epoch£º457	 i:7 	 global-step:9147	 l-p:0.12171002477407455
epoch£º457	 i:8 	 global-step:9148	 l-p:0.16294296085834503
epoch£º457	 i:9 	 global-step:9149	 l-p:0.13504187762737274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4004, 3.4003, 3.4004],
        [3.4004, 3.2939, 3.3575],
        [3.4004, 3.4004, 3.4004],
        [3.4004, 3.4002, 3.4004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.20467670261859894 
model_pd.l_d.mean(): -23.59554672241211 
model_pd.lagr.mean(): -23.390869140625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2260], device='cuda:0')), ('power', tensor([-23.8216], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.20467670261859894
epoch£º458	 i:1 	 global-step:9161	 l-p:0.12353833764791489
epoch£º458	 i:2 	 global-step:9162	 l-p:0.1362631618976593
epoch£º458	 i:3 	 global-step:9163	 l-p:0.16356121003627777
epoch£º458	 i:4 	 global-step:9164	 l-p:0.1224188581109047
epoch£º458	 i:5 	 global-step:9165	 l-p:0.13576388359069824
epoch£º458	 i:6 	 global-step:9166	 l-p:-64.79793548583984
epoch£º458	 i:7 	 global-step:9167	 l-p:0.14995905756950378
epoch£º458	 i:8 	 global-step:9168	 l-p:0.1376347541809082
epoch£º458	 i:9 	 global-step:9169	 l-p:0.22815367579460144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3365, 3.3365, 3.3365],
        [3.3365, 2.9242, 2.5651],
        [3.3365, 3.2439, 2.8981],
        [3.3365, 3.3364, 3.3365]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.1551612913608551 
model_pd.l_d.mean(): -23.278423309326172 
model_pd.lagr.mean(): -23.123262405395508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2548], device='cuda:0')), ('power', tensor([-23.5332], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.1551612913608551
epoch£º459	 i:1 	 global-step:9181	 l-p:0.15852895379066467
epoch£º459	 i:2 	 global-step:9182	 l-p:0.12259670346975327
epoch£º459	 i:3 	 global-step:9183	 l-p:0.19681449234485626
epoch£º459	 i:4 	 global-step:9184	 l-p:0.48438793420791626
epoch£º459	 i:5 	 global-step:9185	 l-p:0.5600751042366028
epoch£º459	 i:6 	 global-step:9186	 l-p:0.1439771205186844
epoch£º459	 i:7 	 global-step:9187	 l-p:0.11752443015575409
epoch£º459	 i:8 	 global-step:9188	 l-p:0.15323324501514435
epoch£º459	 i:9 	 global-step:9189	 l-p:0.16862379014492035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3944, 3.3728, 3.3918],
        [3.3944, 3.0446, 2.9746],
        [3.3944, 3.2836, 2.9326],
        [3.3944, 2.9745, 2.6798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.1201275885105133 
model_pd.l_d.mean(): -22.31972312927246 
model_pd.lagr.mean(): -22.199596405029297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3779], device='cuda:0')), ('power', tensor([-22.6977], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.1201275885105133
epoch£º460	 i:1 	 global-step:9201	 l-p:0.13379086554050446
epoch£º460	 i:2 	 global-step:9202	 l-p:0.13138391077518463
epoch£º460	 i:3 	 global-step:9203	 l-p:0.21558114886283875
epoch£º460	 i:4 	 global-step:9204	 l-p:0.07679848372936249
epoch£º460	 i:5 	 global-step:9205	 l-p:0.14605771005153656
epoch£º460	 i:6 	 global-step:9206	 l-p:0.13819105923175812
epoch£º460	 i:7 	 global-step:9207	 l-p:0.1316133439540863
epoch£º460	 i:8 	 global-step:9208	 l-p:0.22206194698810577
epoch£º460	 i:9 	 global-step:9209	 l-p:0.19753265380859375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4013, 3.3969, 3.4011],
        [3.4013, 3.0600, 3.0045],
        [3.4013, 3.2881, 3.3538],
        [3.4013, 2.9915, 2.6420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.14161810278892517 
model_pd.l_d.mean(): -22.176177978515625 
model_pd.lagr.mean(): -22.03455924987793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3538], device='cuda:0')), ('power', tensor([-22.5300], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.14161810278892517
epoch£º461	 i:1 	 global-step:9221	 l-p:0.12585581839084625
epoch£º461	 i:2 	 global-step:9222	 l-p:0.15906815230846405
epoch£º461	 i:3 	 global-step:9223	 l-p:0.13773773610591888
epoch£º461	 i:4 	 global-step:9224	 l-p:0.1351948231458664
epoch£º461	 i:5 	 global-step:9225	 l-p:0.16361069679260254
epoch£º461	 i:6 	 global-step:9226	 l-p:0.14843501150608063
epoch£º461	 i:7 	 global-step:9227	 l-p:0.15313611924648285
epoch£º461	 i:8 	 global-step:9228	 l-p:0.1035756841301918
epoch£º461	 i:9 	 global-step:9229	 l-p:0.1701999008655548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4201, 3.0148, 2.8187],
        [3.4201, 3.0415, 2.9175],
        [3.4201, 3.3195, 3.3817],
        [3.4201, 3.3209, 3.3827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.10600103437900543 
model_pd.l_d.mean(): -23.419891357421875 
model_pd.lagr.mean(): -23.31389045715332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2573], device='cuda:0')), ('power', tensor([-23.6772], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.10600103437900543
epoch£º462	 i:1 	 global-step:9241	 l-p:0.14162403345108032
epoch£º462	 i:2 	 global-step:9242	 l-p:0.14166496694087982
epoch£º462	 i:3 	 global-step:9243	 l-p:0.14820629358291626
epoch£º462	 i:4 	 global-step:9244	 l-p:0.15790538489818573
epoch£º462	 i:5 	 global-step:9245	 l-p:0.11830544471740723
epoch£º462	 i:6 	 global-step:9246	 l-p:0.13525128364562988
epoch£º462	 i:7 	 global-step:9247	 l-p:0.14719797670841217
epoch£º462	 i:8 	 global-step:9248	 l-p:0.15062059462070465
epoch£º462	 i:9 	 global-step:9249	 l-p:0.13477493822574615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4564, 3.4530, 3.4563],
        [3.4564, 3.4317, 3.4531],
        [3.4564, 3.4344, 3.4537],
        [3.4564, 3.4561, 3.4564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.16170157492160797 
model_pd.l_d.mean(): -21.947948455810547 
model_pd.lagr.mean(): -21.78624725341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3896], device='cuda:0')), ('power', tensor([-22.3375], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.16170157492160797
epoch£º463	 i:1 	 global-step:9261	 l-p:0.14255960285663605
epoch£º463	 i:2 	 global-step:9262	 l-p:0.1336565911769867
epoch£º463	 i:3 	 global-step:9263	 l-p:0.16343903541564941
epoch£º463	 i:4 	 global-step:9264	 l-p:0.13315382599830627
epoch£º463	 i:5 	 global-step:9265	 l-p:0.12354505062103271
epoch£º463	 i:6 	 global-step:9266	 l-p:0.13073641061782837
epoch£º463	 i:7 	 global-step:9267	 l-p:0.14022183418273926
epoch£º463	 i:8 	 global-step:9268	 l-p:0.6062499284744263
epoch£º463	 i:9 	 global-step:9269	 l-p:0.13004711270332336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3109, 3.3027, 3.3104],
        [3.3109, 2.8729, 2.5308],
        [3.3109, 2.9058, 2.5135],
        [3.3109, 3.3109, 3.3109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.004446391947567463 
model_pd.l_d.mean(): -23.530845642089844 
model_pd.lagr.mean(): -23.526399612426758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2762], device='cuda:0')), ('power', tensor([-23.8070], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.004446391947567463
epoch£º464	 i:1 	 global-step:9281	 l-p:0.3572477400302887
epoch£º464	 i:2 	 global-step:9282	 l-p:0.11197932809591293
epoch£º464	 i:3 	 global-step:9283	 l-p:0.1588200330734253
epoch£º464	 i:4 	 global-step:9284	 l-p:0.13146045804023743
epoch£º464	 i:5 	 global-step:9285	 l-p:0.13846717774868011
epoch£º464	 i:6 	 global-step:9286	 l-p:0.280441015958786
epoch£º464	 i:7 	 global-step:9287	 l-p:0.15986455976963043
epoch£º464	 i:8 	 global-step:9288	 l-p:0.1765047013759613
epoch£º464	 i:9 	 global-step:9289	 l-p:0.14304079115390778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4076, 3.4076, 3.4076],
        [3.4076, 3.4044, 3.4075],
        [3.4076, 3.3594, 3.3972],
        [3.4076, 3.0934, 3.0809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.13389970362186432 
model_pd.l_d.mean(): -23.211166381835938 
model_pd.lagr.mean(): -23.077266693115234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3080], device='cuda:0')), ('power', tensor([-23.5191], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.13389970362186432
epoch£º465	 i:1 	 global-step:9301	 l-p:0.21412579715251923
epoch£º465	 i:2 	 global-step:9302	 l-p:0.1290363222360611
epoch£º465	 i:3 	 global-step:9303	 l-p:0.1310749650001526
epoch£º465	 i:4 	 global-step:9304	 l-p:0.12370464205741882
epoch£º465	 i:5 	 global-step:9305	 l-p:0.12406289577484131
epoch£º465	 i:6 	 global-step:9306	 l-p:0.09608973562717438
epoch£º465	 i:7 	 global-step:9307	 l-p:0.15490907430648804
epoch£º465	 i:8 	 global-step:9308	 l-p:0.1445227861404419
epoch£º465	 i:9 	 global-step:9309	 l-p:0.17815127968788147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3918, 3.1323, 3.1711],
        [3.3918, 3.0752, 3.0630],
        [3.3918, 3.3557, 3.3855],
        [3.3918, 2.9598, 2.6907]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.10933252424001694 
model_pd.l_d.mean(): -22.4681339263916 
model_pd.lagr.mean(): -22.358800888061523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3499], device='cuda:0')), ('power', tensor([-22.8181], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.10933252424001694
epoch£º466	 i:1 	 global-step:9321	 l-p:0.23137760162353516
epoch£º466	 i:2 	 global-step:9322	 l-p:0.13317933678627014
epoch£º466	 i:3 	 global-step:9323	 l-p:0.13235363364219666
epoch£º466	 i:4 	 global-step:9324	 l-p:0.1439664214849472
epoch£º466	 i:5 	 global-step:9325	 l-p:0.16275127232074738
epoch£º466	 i:6 	 global-step:9326	 l-p:0.17907850444316864
epoch£º466	 i:7 	 global-step:9327	 l-p:0.1573772430419922
epoch£º466	 i:8 	 global-step:9328	 l-p:0.17127414047718048
epoch£º466	 i:9 	 global-step:9329	 l-p:0.10279962420463562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4495, 3.4494, 3.4495],
        [3.4495, 3.0443, 2.6710],
        [3.4495, 3.3942, 3.4364],
        [3.4495, 3.1593, 3.1709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.12626521289348602 
model_pd.l_d.mean(): -23.32698631286621 
model_pd.lagr.mean(): -23.200721740722656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2482], device='cuda:0')), ('power', tensor([-23.5752], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.12626521289348602
epoch£º467	 i:1 	 global-step:9341	 l-p:0.1951339840888977
epoch£º467	 i:2 	 global-step:9342	 l-p:0.13679970800876617
epoch£º467	 i:3 	 global-step:9343	 l-p:-0.0017409372376278043
epoch£º467	 i:4 	 global-step:9344	 l-p:0.13198383152484894
epoch£º467	 i:5 	 global-step:9345	 l-p:0.14049437642097473
epoch£º467	 i:6 	 global-step:9346	 l-p:0.147069051861763
epoch£º467	 i:7 	 global-step:9347	 l-p:0.15971554815769196
epoch£º467	 i:8 	 global-step:9348	 l-p:0.11715400218963623
epoch£º467	 i:9 	 global-step:9349	 l-p:0.11528810113668442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4985, 3.4142, 3.4708],
        [3.4985, 3.1023, 2.7267],
        [3.4985, 3.3544, 3.4252],
        [3.4985, 3.3825, 3.0222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.16498705744743347 
model_pd.l_d.mean(): -23.481952667236328 
model_pd.lagr.mean(): -23.316965103149414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2202], device='cuda:0')), ('power', tensor([-23.7022], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.16498705744743347
epoch£º468	 i:1 	 global-step:9361	 l-p:0.10898307710886002
epoch£º468	 i:2 	 global-step:9362	 l-p:0.13496828079223633
epoch£º468	 i:3 	 global-step:9363	 l-p:0.18423707783222198
epoch£º468	 i:4 	 global-step:9364	 l-p:0.1625119000673294
epoch£º468	 i:5 	 global-step:9365	 l-p:0.1621766835451126
epoch£º468	 i:6 	 global-step:9366	 l-p:0.14498184621334076
epoch£º468	 i:7 	 global-step:9367	 l-p:0.117230623960495
epoch£º468	 i:8 	 global-step:9368	 l-p:0.22780227661132812
epoch£º468	 i:9 	 global-step:9369	 l-p:0.12282862514257431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4064, 3.2952, 3.3617],
        [3.4064, 3.4064, 3.4064],
        [3.4064, 3.3998, 3.4060],
        [3.4064, 3.4064, 3.4064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.08013627678155899 
model_pd.l_d.mean(): -23.4129581451416 
model_pd.lagr.mean(): -23.332822799682617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2114], device='cuda:0')), ('power', tensor([-23.6244], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.08013627678155899
epoch£º469	 i:1 	 global-step:9381	 l-p:0.2029455453157425
epoch£º469	 i:2 	 global-step:9382	 l-p:0.13144774734973907
epoch£º469	 i:3 	 global-step:9383	 l-p:0.1383858025074005
epoch£º469	 i:4 	 global-step:9384	 l-p:0.15806734561920166
epoch£º469	 i:5 	 global-step:9385	 l-p:0.15550866723060608
epoch£º469	 i:6 	 global-step:9386	 l-p:0.17426300048828125
epoch£º469	 i:7 	 global-step:9387	 l-p:0.13557425141334534
epoch£º469	 i:8 	 global-step:9388	 l-p:0.15443119406700134
epoch£º469	 i:9 	 global-step:9389	 l-p:0.11703905463218689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4823, 3.4817, 3.4823],
        [3.4823, 3.2146, 3.2468],
        [3.4823, 3.3416, 3.4130],
        [3.4823, 3.4819, 3.4823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.11902135610580444 
model_pd.l_d.mean(): -23.47279930114746 
model_pd.lagr.mean(): -23.353778839111328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1908], device='cuda:0')), ('power', tensor([-23.6636], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.11902135610580444
epoch£º470	 i:1 	 global-step:9401	 l-p:0.14693501591682434
epoch£º470	 i:2 	 global-step:9402	 l-p:0.16611140966415405
epoch£º470	 i:3 	 global-step:9403	 l-p:0.15093255043029785
epoch£º470	 i:4 	 global-step:9404	 l-p:0.13977226614952087
epoch£º470	 i:5 	 global-step:9405	 l-p:0.1281607449054718
epoch£º470	 i:6 	 global-step:9406	 l-p:0.14225657284259796
epoch£º470	 i:7 	 global-step:9407	 l-p:5.2640252113342285
epoch£º470	 i:8 	 global-step:9408	 l-p:0.13785722851753235
epoch£º470	 i:9 	 global-step:9409	 l-p:0.12559188902378082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4654, 3.4654, 3.4654],
        [3.4654, 3.4654, 3.4654],
        [3.4654, 3.4654, 3.4654],
        [3.4654, 3.4654, 3.4654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.13479579985141754 
model_pd.l_d.mean(): -23.142549514770508 
model_pd.lagr.mean(): -23.007753372192383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2793], device='cuda:0')), ('power', tensor([-23.4218], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.13479579985141754
epoch£º471	 i:1 	 global-step:9421	 l-p:0.13750897347927094
epoch£º471	 i:2 	 global-step:9422	 l-p:0.23087045550346375
epoch£º471	 i:3 	 global-step:9423	 l-p:0.1445043832063675
epoch£º471	 i:4 	 global-step:9424	 l-p:0.6975643634796143
epoch£º471	 i:5 	 global-step:9425	 l-p:0.14711995422840118
epoch£º471	 i:6 	 global-step:9426	 l-p:0.1399705410003662
epoch£º471	 i:7 	 global-step:9427	 l-p:0.1462520807981491
epoch£º471	 i:8 	 global-step:9428	 l-p:0.03716534376144409
epoch£º471	 i:9 	 global-step:9429	 l-p:0.15386728942394257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3245, 2.9149, 2.7815],
        [3.3245, 3.2743, 3.3137],
        [3.3245, 2.9770, 2.5546],
        [3.3245, 3.3245, 3.3245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.1428447663784027 
model_pd.l_d.mean(): -23.558351516723633 
model_pd.lagr.mean(): -23.41550636291504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2705], device='cuda:0')), ('power', tensor([-23.8288], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.1428447663784027
epoch£º472	 i:1 	 global-step:9441	 l-p:0.28458645939826965
epoch£º472	 i:2 	 global-step:9442	 l-p:0.14112401008605957
epoch£º472	 i:3 	 global-step:9443	 l-p:0.15186023712158203
epoch£º472	 i:4 	 global-step:9444	 l-p:0.15449158847332
epoch£º472	 i:5 	 global-step:9445	 l-p:0.13065212965011597
epoch£º472	 i:6 	 global-step:9446	 l-p:0.12732695043087006
epoch£º472	 i:7 	 global-step:9447	 l-p:-0.04439109191298485
epoch£º472	 i:8 	 global-step:9448	 l-p:0.159469872713089
epoch£º472	 i:9 	 global-step:9449	 l-p:0.14881914854049683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4498, 3.1134, 3.0838],
        [3.4498, 3.4497, 3.4498],
        [3.4498, 3.4497, 3.4498],
        [3.4498, 3.3976, 3.4381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.1577988564968109 
model_pd.l_d.mean(): -22.92075538635254 
model_pd.lagr.mean(): -22.762956619262695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3745], device='cuda:0')), ('power', tensor([-23.2953], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.1577988564968109
epoch£º473	 i:1 	 global-step:9461	 l-p:0.12261620908975601
epoch£º473	 i:2 	 global-step:9462	 l-p:0.2198033630847931
epoch£º473	 i:3 	 global-step:9463	 l-p:0.1336802840232849
epoch£º473	 i:4 	 global-step:9464	 l-p:0.11801853775978088
epoch£º473	 i:5 	 global-step:9465	 l-p:0.1508774757385254
epoch£º473	 i:6 	 global-step:9466	 l-p:0.11758124083280563
epoch£º473	 i:7 	 global-step:9467	 l-p:0.11441934108734131
epoch£º473	 i:8 	 global-step:9468	 l-p:0.1283581703901291
epoch£º473	 i:9 	 global-step:9469	 l-p:0.13423140347003937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5620, 3.5575, 3.5618],
        [3.5620, 3.5620, 3.5620],
        [3.5620, 3.5620, 3.5620],
        [3.5620, 3.4047, 3.4768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.10434917360544205 
model_pd.l_d.mean(): -23.247196197509766 
model_pd.lagr.mean(): -23.142847061157227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2285], device='cuda:0')), ('power', tensor([-23.4757], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.10434917360544205
epoch£º474	 i:1 	 global-step:9481	 l-p:0.1499764323234558
epoch£º474	 i:2 	 global-step:9482	 l-p:0.15143069624900818
epoch£º474	 i:3 	 global-step:9483	 l-p:0.14788953959941864
epoch£º474	 i:4 	 global-step:9484	 l-p:0.08467619121074677
epoch£º474	 i:5 	 global-step:9485	 l-p:0.17691108584403992
epoch£º474	 i:6 	 global-step:9486	 l-p:0.12197891622781754
epoch£º474	 i:7 	 global-step:9487	 l-p:0.146974116563797
epoch£º474	 i:8 	 global-step:9488	 l-p:0.1737203150987625
epoch£º474	 i:9 	 global-step:9489	 l-p:0.16774429380893707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3679, 3.1579, 3.2272],
        [3.3679, 3.0864, 3.1204],
        [3.3679, 3.3679, 3.3679],
        [3.3679, 3.3679, 3.3679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.14158236980438232 
model_pd.l_d.mean(): -23.175270080566406 
model_pd.lagr.mean(): -23.033687591552734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3414], device='cuda:0')), ('power', tensor([-23.5167], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.14158236980438232
epoch£º475	 i:1 	 global-step:9501	 l-p:0.15242137014865875
epoch£º475	 i:2 	 global-step:9502	 l-p:0.14472490549087524
epoch£º475	 i:3 	 global-step:9503	 l-p:0.01973011903464794
epoch£º475	 i:4 	 global-step:9504	 l-p:0.07010365277528763
epoch£º475	 i:5 	 global-step:9505	 l-p:0.08377024531364441
epoch£º475	 i:6 	 global-step:9506	 l-p:0.1594172567129135
epoch£º475	 i:7 	 global-step:9507	 l-p:0.16194015741348267
epoch£º475	 i:8 	 global-step:9508	 l-p:0.18135645985603333
epoch£º475	 i:9 	 global-step:9509	 l-p:0.08599591255187988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4474, 3.1301, 2.7082],
        [3.4474, 3.4474, 3.4474],
        [3.4474, 3.0940, 2.6720],
        [3.4474, 3.3143, 3.3865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.12685362994670868 
model_pd.l_d.mean(): -23.429094314575195 
model_pd.lagr.mean(): -23.3022403717041 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2291], device='cuda:0')), ('power', tensor([-23.6582], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.12685362994670868
epoch£º476	 i:1 	 global-step:9521	 l-p:0.135158970952034
epoch£º476	 i:2 	 global-step:9522	 l-p:0.1684911698102951
epoch£º476	 i:3 	 global-step:9523	 l-p:0.1407749503850937
epoch£º476	 i:4 	 global-step:9524	 l-p:0.1497180014848709
epoch£º476	 i:5 	 global-step:9525	 l-p:0.12847021222114563
epoch£º476	 i:6 	 global-step:9526	 l-p:0.11893852055072784
epoch£º476	 i:7 	 global-step:9527	 l-p:0.12958957254886627
epoch£º476	 i:8 	 global-step:9528	 l-p:0.18581581115722656
epoch£º476	 i:9 	 global-step:9529	 l-p:0.12968339025974274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4292, 3.4272, 3.4292],
        [3.4292, 2.9865, 2.6131],
        [3.4292, 3.3156, 3.3838],
        [3.4292, 3.0898, 3.0660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.13233302533626556 
model_pd.l_d.mean(): -22.692928314208984 
model_pd.lagr.mean(): -22.56059455871582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3120], device='cuda:0')), ('power', tensor([-23.0049], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.13233302533626556
epoch£º477	 i:1 	 global-step:9541	 l-p:0.14149664342403412
epoch£º477	 i:2 	 global-step:9542	 l-p:0.11448223143815994
epoch£º477	 i:3 	 global-step:9543	 l-p:0.14304715394973755
epoch£º477	 i:4 	 global-step:9544	 l-p:0.11671842634677887
epoch£º477	 i:5 	 global-step:9545	 l-p:0.12319352477788925
epoch£º477	 i:6 	 global-step:9546	 l-p:-0.14186996221542358
epoch£º477	 i:7 	 global-step:9547	 l-p:0.10505169630050659
epoch£º477	 i:8 	 global-step:9548	 l-p:0.16716599464416504
epoch£º477	 i:9 	 global-step:9549	 l-p:0.1641971319913864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3342, 3.3342, 3.3342],
        [3.3342, 3.0318, 2.6018],
        [3.3342, 3.3342, 3.3342],
        [3.3342, 2.9015, 2.7382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.022092293947935104 
model_pd.l_d.mean(): -22.343137741088867 
model_pd.lagr.mean(): -22.321044921875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4150], device='cuda:0')), ('power', tensor([-22.7582], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.022092293947935104
epoch£º478	 i:1 	 global-step:9561	 l-p:0.1522725522518158
epoch£º478	 i:2 	 global-step:9562	 l-p:0.24026839435100555
epoch£º478	 i:3 	 global-step:9563	 l-p:0.14591026306152344
epoch£º478	 i:4 	 global-step:9564	 l-p:0.10394840687513351
epoch£º478	 i:5 	 global-step:9565	 l-p:0.12737856805324554
epoch£º478	 i:6 	 global-step:9566	 l-p:0.1842678189277649
epoch£º478	 i:7 	 global-step:9567	 l-p:0.13425295054912567
epoch£º478	 i:8 	 global-step:9568	 l-p:0.11109040677547455
epoch£º478	 i:9 	 global-step:9569	 l-p:0.14238493144512177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5321, 3.2741, 2.8588],
        [3.5321, 3.4949, 3.5257],
        [3.5321, 3.1198, 2.9571],
        [3.5321, 3.3493, 3.4221]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.13933421671390533 
model_pd.l_d.mean(): -23.00577163696289 
model_pd.lagr.mean(): -22.866437911987305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2622], device='cuda:0')), ('power', tensor([-23.2680], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.13933421671390533
epoch£º479	 i:1 	 global-step:9581	 l-p:0.14598721265792847
epoch£º479	 i:2 	 global-step:9582	 l-p:0.1304580122232437
epoch£º479	 i:3 	 global-step:9583	 l-p:0.035326603800058365
epoch£º479	 i:4 	 global-step:9584	 l-p:0.13642464578151703
epoch£º479	 i:5 	 global-step:9585	 l-p:0.1350841224193573
epoch£º479	 i:6 	 global-step:9586	 l-p:0.21355323493480682
epoch£º479	 i:7 	 global-step:9587	 l-p:0.13901957869529724
epoch£º479	 i:8 	 global-step:9588	 l-p:0.14954344928264618
epoch£º479	 i:9 	 global-step:9589	 l-p:0.17296412587165833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4010, 3.4009, 3.4010],
        [3.4010, 3.1235, 3.1640],
        [3.4010, 3.4009, 3.4010],
        [3.4010, 3.2428, 3.3192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.2522244453430176 
model_pd.l_d.mean(): -22.70903968811035 
model_pd.lagr.mean(): -22.456815719604492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3008], device='cuda:0')), ('power', tensor([-23.0099], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.2522244453430176
epoch£º480	 i:1 	 global-step:9601	 l-p:0.13247336447238922
epoch£º480	 i:2 	 global-step:9602	 l-p:0.17564639449119568
epoch£º480	 i:3 	 global-step:9603	 l-p:0.10233812034130096
epoch£º480	 i:4 	 global-step:9604	 l-p:0.17425580322742462
epoch£º480	 i:5 	 global-step:9605	 l-p:0.14762112498283386
epoch£º480	 i:6 	 global-step:9606	 l-p:0.13311873376369476
epoch£º480	 i:7 	 global-step:9607	 l-p:0.1371067613363266
epoch£º480	 i:8 	 global-step:9608	 l-p:0.15769626200199127
epoch£º480	 i:9 	 global-step:9609	 l-p:0.15142008662223816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4857, 3.4016, 3.4594],
        [3.4857, 3.4856, 3.4857],
        [3.4857, 3.4762, 3.4850],
        [3.4857, 3.4856, 3.4857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.004163675010204315 
model_pd.l_d.mean(): -23.663652420043945 
model_pd.lagr.mean(): -23.659488677978516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1968], device='cuda:0')), ('power', tensor([-23.8605], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.004163675010204315
epoch£º481	 i:1 	 global-step:9621	 l-p:0.1271054893732071
epoch£º481	 i:2 	 global-step:9622	 l-p:0.13257871568202972
epoch£º481	 i:3 	 global-step:9623	 l-p:0.1349293291568756
epoch£º481	 i:4 	 global-step:9624	 l-p:0.17718623578548431
epoch£º481	 i:5 	 global-step:9625	 l-p:0.1325356662273407
epoch£º481	 i:6 	 global-step:9626	 l-p:0.11747535318136215
epoch£º481	 i:7 	 global-step:9627	 l-p:0.12222511321306229
epoch£º481	 i:8 	 global-step:9628	 l-p:0.18203327059745789
epoch£º481	 i:9 	 global-step:9629	 l-p:0.18380901217460632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4380, 3.3853, 3.4265],
        [3.4380, 3.3155, 3.3868],
        [3.4380, 3.0328, 2.9182],
        [3.4380, 2.9835, 2.6061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.1639476716518402 
model_pd.l_d.mean(): -23.69732666015625 
model_pd.lagr.mean(): -23.53337860107422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1879], device='cuda:0')), ('power', tensor([-23.8852], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.1639476716518402
epoch£º482	 i:1 	 global-step:9641	 l-p:0.19112828373908997
epoch£º482	 i:2 	 global-step:9642	 l-p:0.24555666744709015
epoch£º482	 i:3 	 global-step:9643	 l-p:0.14583145081996918
epoch£º482	 i:4 	 global-step:9644	 l-p:0.15920646488666534
epoch£º482	 i:5 	 global-step:9645	 l-p:0.11754017323255539
epoch£º482	 i:6 	 global-step:9646	 l-p:-0.04264409840106964
epoch£º482	 i:7 	 global-step:9647	 l-p:0.12708383798599243
epoch£º482	 i:8 	 global-step:9648	 l-p:0.1214471086859703
epoch£º482	 i:9 	 global-step:9649	 l-p:0.13565559685230255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5321, 3.5321, 3.5321],
        [3.5321, 3.0931, 2.7177],
        [3.5321, 3.5312, 3.5321],
        [3.5321, 3.5321, 3.5321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.12775090336799622 
model_pd.l_d.mean(): -22.28452491760254 
model_pd.lagr.mean(): -22.156774520874023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3172], device='cuda:0')), ('power', tensor([-22.6018], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.12775090336799622
epoch£º483	 i:1 	 global-step:9661	 l-p:0.13036233186721802
epoch£º483	 i:2 	 global-step:9662	 l-p:0.14268405735492706
epoch£º483	 i:3 	 global-step:9663	 l-p:0.11012450605630875
epoch£º483	 i:4 	 global-step:9664	 l-p:0.13144785165786743
epoch£º483	 i:5 	 global-step:9665	 l-p:0.1408626139163971
epoch£º483	 i:6 	 global-step:9666	 l-p:0.11855939775705338
epoch£º483	 i:7 	 global-step:9667	 l-p:0.29269227385520935
epoch£º483	 i:8 	 global-step:9668	 l-p:0.14472545683383942
epoch£º483	 i:9 	 global-step:9669	 l-p:0.17283889651298523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[3.4788, 3.0794, 2.6522],
        [3.4788, 3.0815, 2.9793],
        [3.4788, 3.0309, 2.8027],
        [3.4788, 3.0917, 2.6610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.12132192403078079 
model_pd.l_d.mean(): -22.885053634643555 
model_pd.lagr.mean(): -22.763731002807617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3387], device='cuda:0')), ('power', tensor([-23.2237], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.12132192403078079
epoch£º484	 i:1 	 global-step:9681	 l-p:0.14078624546527863
epoch£º484	 i:2 	 global-step:9682	 l-p:0.15894575417041779
epoch£º484	 i:3 	 global-step:9683	 l-p:0.12611503899097443
epoch£º484	 i:4 	 global-step:9684	 l-p:0.31246647238731384
epoch£º484	 i:5 	 global-step:9685	 l-p:0.2640993893146515
epoch£º484	 i:6 	 global-step:9686	 l-p:0.13143926858901978
epoch£º484	 i:7 	 global-step:9687	 l-p:0.13316816091537476
epoch£º484	 i:8 	 global-step:9688	 l-p:0.1449328511953354
epoch£º484	 i:9 	 global-step:9689	 l-p:0.13002881407737732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4077, 3.3295, 3.3850],
        [3.4077, 3.1816, 3.2503],
        [3.4077, 3.4077, 3.4077],
        [3.4077, 3.4077, 3.4077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.13666993379592896 
model_pd.l_d.mean(): -23.853342056274414 
model_pd.lagr.mean(): -23.716672897338867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1488], device='cuda:0')), ('power', tensor([-24.0022], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.13666993379592896
epoch£º485	 i:1 	 global-step:9701	 l-p:0.21252650022506714
epoch£º485	 i:2 	 global-step:9702	 l-p:0.12455784529447556
epoch£º485	 i:3 	 global-step:9703	 l-p:0.12986961007118225
epoch£º485	 i:4 	 global-step:9704	 l-p:1.5601272583007812
epoch£º485	 i:5 	 global-step:9705	 l-p:0.13312210142612457
epoch£º485	 i:6 	 global-step:9706	 l-p:0.16313214600086212
epoch£º485	 i:7 	 global-step:9707	 l-p:0.19533920288085938
epoch£º485	 i:8 	 global-step:9708	 l-p:0.133780837059021
epoch£º485	 i:9 	 global-step:9709	 l-p:0.15435799956321716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3017, 2.8220, 2.5658],
        [3.3017, 2.9660, 2.9706],
        [3.3017, 2.8537, 2.4210],
        [3.3017, 3.1508, 3.2290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.13735206425189972 
model_pd.l_d.mean(): -23.16602897644043 
model_pd.lagr.mean(): -23.028676986694336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3775], device='cuda:0')), ('power', tensor([-23.5435], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.13735206425189972
epoch£º486	 i:1 	 global-step:9721	 l-p:0.048125531524419785
epoch£º486	 i:2 	 global-step:9722	 l-p:0.13535284996032715
epoch£º486	 i:3 	 global-step:9723	 l-p:0.1637720763683319
epoch£º486	 i:4 	 global-step:9724	 l-p:0.3420160710811615
epoch£º486	 i:5 	 global-step:9725	 l-p:0.11952207237482071
epoch£º486	 i:6 	 global-step:9726	 l-p:0.14753258228302002
epoch£º486	 i:7 	 global-step:9727	 l-p:0.16778509318828583
epoch£º486	 i:8 	 global-step:9728	 l-p:0.1888473778963089
epoch£º486	 i:9 	 global-step:9729	 l-p:0.15978115797042847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5105, 3.1865, 3.1865],
        [3.5105, 3.5101, 3.5105],
        [3.5105, 3.5105, 3.5105],
        [3.5105, 3.0522, 2.6997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.13740889728069305 
model_pd.l_d.mean(): -23.19501304626465 
model_pd.lagr.mean(): -23.05760383605957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2442], device='cuda:0')), ('power', tensor([-23.4392], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.13740889728069305
epoch£º487	 i:1 	 global-step:9741	 l-p:0.1299978643655777
epoch£º487	 i:2 	 global-step:9742	 l-p:0.132782444357872
epoch£º487	 i:3 	 global-step:9743	 l-p:0.33492952585220337
epoch£º487	 i:4 	 global-step:9744	 l-p:0.13584771752357483
epoch£º487	 i:5 	 global-step:9745	 l-p:0.14433009922504425
epoch£º487	 i:6 	 global-step:9746	 l-p:0.13550645112991333
epoch£º487	 i:7 	 global-step:9747	 l-p:0.13965260982513428
epoch£º487	 i:8 	 global-step:9748	 l-p:0.13093270361423492
epoch£º487	 i:9 	 global-step:9749	 l-p:0.13273276388645172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3921, 3.3895, 3.3920],
        [3.3921, 2.9376, 2.7378],
        [3.3921, 3.3396, 3.3808],
        [3.3921, 3.0413, 2.5960]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.14084000885486603 
model_pd.l_d.mean(): -23.404285430908203 
model_pd.lagr.mean(): -23.263444900512695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2872], device='cuda:0')), ('power', tensor([-23.6915], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.14084000885486603
epoch£º488	 i:1 	 global-step:9761	 l-p:-0.29542824625968933
epoch£º488	 i:2 	 global-step:9762	 l-p:0.17522525787353516
epoch£º488	 i:3 	 global-step:9763	 l-p:0.12407863140106201
epoch£º488	 i:4 	 global-step:9764	 l-p:0.17277728021144867
epoch£º488	 i:5 	 global-step:9765	 l-p:-0.08461465686559677
epoch£º488	 i:6 	 global-step:9766	 l-p:0.046123333275318146
epoch£º488	 i:7 	 global-step:9767	 l-p:0.0908498466014862
epoch£º488	 i:8 	 global-step:9768	 l-p:0.0910269170999527
epoch£º488	 i:9 	 global-step:9769	 l-p:0.15594364702701569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4106, 3.3316, 3.3876],
        [3.4106, 3.4098, 3.4106],
        [3.4106, 3.3238, 3.3835],
        [3.4106, 3.4106, 3.4106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.410764217376709 
model_pd.l_d.mean(): -23.586517333984375 
model_pd.lagr.mean(): -23.175752639770508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2076], device='cuda:0')), ('power', tensor([-23.7941], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.410764217376709
epoch£º489	 i:1 	 global-step:9781	 l-p:0.1185218095779419
epoch£º489	 i:2 	 global-step:9782	 l-p:0.22885112464427948
epoch£º489	 i:3 	 global-step:9783	 l-p:0.15311408042907715
epoch£º489	 i:4 	 global-step:9784	 l-p:0.1321709156036377
epoch£º489	 i:5 	 global-step:9785	 l-p:0.13935650885105133
epoch£º489	 i:6 	 global-step:9786	 l-p:0.12943993508815765
epoch£º489	 i:7 	 global-step:9787	 l-p:0.13430237770080566
epoch£º489	 i:8 	 global-step:9788	 l-p:0.1506471037864685
epoch£º489	 i:9 	 global-step:9789	 l-p:0.09661971777677536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4463, 3.4463, 3.4463],
        [3.4463, 3.4452, 3.4463],
        [3.4463, 3.4446, 3.4463],
        [3.4463, 3.4463, 3.4463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.13853034377098083 
model_pd.l_d.mean(): -23.659814834594727 
model_pd.lagr.mean(): -23.521284103393555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1913], device='cuda:0')), ('power', tensor([-23.8512], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.13853034377098083
epoch£º490	 i:1 	 global-step:9801	 l-p:0.09581508487462997
epoch£º490	 i:2 	 global-step:9802	 l-p:0.1424250602722168
epoch£º490	 i:3 	 global-step:9803	 l-p:0.27839094400405884
epoch£º490	 i:4 	 global-step:9804	 l-p:0.18137115240097046
epoch£º490	 i:5 	 global-step:9805	 l-p:0.1818501204252243
epoch£º490	 i:6 	 global-step:9806	 l-p:0.20396113395690918
epoch£º490	 i:7 	 global-step:9807	 l-p:0.18567855656147003
epoch£º490	 i:8 	 global-step:9808	 l-p:0.1525508314371109
epoch£º490	 i:9 	 global-step:9809	 l-p:0.12658412754535675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5283, 3.2411, 2.8056],
        [3.5283, 3.2216, 3.2411],
        [3.5283, 3.4354, 3.4974],
        [3.5283, 3.1101, 2.6837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.157353937625885 
model_pd.l_d.mean(): -22.760780334472656 
model_pd.lagr.mean(): -22.603425979614258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2522], device='cuda:0')), ('power', tensor([-23.0130], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.157353937625885
epoch£º491	 i:1 	 global-step:9821	 l-p:0.1274637132883072
epoch£º491	 i:2 	 global-step:9822	 l-p:0.11717154830694199
epoch£º491	 i:3 	 global-step:9823	 l-p:0.13017414510250092
epoch£º491	 i:4 	 global-step:9824	 l-p:0.15056750178337097
epoch£º491	 i:5 	 global-step:9825	 l-p:0.14782974123954773
epoch£º491	 i:6 	 global-step:9826	 l-p:0.13500499725341797
epoch£º491	 i:7 	 global-step:9827	 l-p:-0.04499069228768349
epoch£º491	 i:8 	 global-step:9828	 l-p:0.1260799914598465
epoch£º491	 i:9 	 global-step:9829	 l-p:0.12882991135120392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4564, 3.4564, 3.4564],
        [3.4564, 3.0100, 2.8295],
        [3.4564, 3.4393, 3.4547],
        [3.4564, 3.4438, 3.4554]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.21498653292655945 
model_pd.l_d.mean(): -23.604557037353516 
model_pd.lagr.mean(): -23.389570236206055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2402], device='cuda:0')), ('power', tensor([-23.8448], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.21498653292655945
epoch£º492	 i:1 	 global-step:9841	 l-p:0.13988210260868073
epoch£º492	 i:2 	 global-step:9842	 l-p:0.09522516280412674
epoch£º492	 i:3 	 global-step:9843	 l-p:0.640568196773529
epoch£º492	 i:4 	 global-step:9844	 l-p:0.14308468997478485
epoch£º492	 i:5 	 global-step:9845	 l-p:0.5489249229431152
epoch£º492	 i:6 	 global-step:9846	 l-p:0.15692931413650513
epoch£º492	 i:7 	 global-step:9847	 l-p:0.13763128221035004
epoch£º492	 i:8 	 global-step:9848	 l-p:0.17390096187591553
epoch£º492	 i:9 	 global-step:9849	 l-p:0.15746527910232544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4436, 3.3180, 2.9192],
        [3.4436, 3.4431, 3.4436],
        [3.4436, 3.1547, 3.1954],
        [3.4436, 3.2091, 2.7777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.10405751317739487 
model_pd.l_d.mean(): -21.576221466064453 
model_pd.lagr.mean(): -21.472164154052734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4464], device='cuda:0')), ('power', tensor([-22.0226], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.10405751317739487
epoch£º493	 i:1 	 global-step:9861	 l-p:0.12121178209781647
epoch£º493	 i:2 	 global-step:9862	 l-p:0.14034008979797363
epoch£º493	 i:3 	 global-step:9863	 l-p:0.09303460270166397
epoch£º493	 i:4 	 global-step:9864	 l-p:0.13164976239204407
epoch£º493	 i:5 	 global-step:9865	 l-p:0.23195628821849823
epoch£º493	 i:6 	 global-step:9866	 l-p:0.11771995574235916
epoch£º493	 i:7 	 global-step:9867	 l-p:0.1377369612455368
epoch£º493	 i:8 	 global-step:9868	 l-p:0.1479807049036026
epoch£º493	 i:9 	 global-step:9869	 l-p:0.14991207420825958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4578, 3.0086, 2.8273],
        [3.4578, 3.2898, 2.8766],
        [3.4578, 3.2273, 3.2972],
        [3.4578, 3.4169, 3.4505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.12200456112623215 
model_pd.l_d.mean(): -22.020793914794922 
model_pd.lagr.mean(): -21.898788452148438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4224], device='cuda:0')), ('power', tensor([-22.4432], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.12200456112623215
epoch£º494	 i:1 	 global-step:9881	 l-p:0.15005379915237427
epoch£º494	 i:2 	 global-step:9882	 l-p:0.19221778213977814
epoch£º494	 i:3 	 global-step:9883	 l-p:-5.440427303314209
epoch£º494	 i:4 	 global-step:9884	 l-p:0.15142077207565308
epoch£º494	 i:5 	 global-step:9885	 l-p:0.18579840660095215
epoch£º494	 i:6 	 global-step:9886	 l-p:0.028233923017978668
epoch£º494	 i:7 	 global-step:9887	 l-p:0.15039494633674622
epoch£º494	 i:8 	 global-step:9888	 l-p:-0.03071867860853672
epoch£º494	 i:9 	 global-step:9889	 l-p:0.15896733105182648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4036, 3.1248, 3.1754],
        [3.4036, 3.4023, 3.4036],
        [3.4036, 3.0186, 2.5630],
        [3.4036, 3.3775, 3.4002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.14291660487651825 
model_pd.l_d.mean(): -23.436975479125977 
model_pd.lagr.mean(): -23.29405975341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2558], device='cuda:0')), ('power', tensor([-23.6928], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.14291660487651825
epoch£º495	 i:1 	 global-step:9901	 l-p:0.1403132826089859
epoch£º495	 i:2 	 global-step:9902	 l-p:0.15335622429847717
epoch£º495	 i:3 	 global-step:9903	 l-p:0.1414727121591568
epoch£º495	 i:4 	 global-step:9904	 l-p:0.16509471833705902
epoch£º495	 i:5 	 global-step:9905	 l-p:0.1415250301361084
epoch£º495	 i:6 	 global-step:9906	 l-p:-0.03790967911481857
epoch£º495	 i:7 	 global-step:9907	 l-p:0.13671860098838806
epoch£º495	 i:8 	 global-step:9908	 l-p:0.15670928359031677
epoch£º495	 i:9 	 global-step:9909	 l-p:0.12976205348968506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3351, 3.3351, 3.3351],
        [3.3351, 3.1747, 3.2557],
        [3.3351, 3.3351, 3.3351],
        [3.3351, 2.9314, 2.4700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.15235206484794617 
model_pd.l_d.mean(): -23.53550148010254 
model_pd.lagr.mean(): -23.383150100708008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2726], device='cuda:0')), ('power', tensor([-23.8081], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.15235206484794617
epoch£º496	 i:1 	 global-step:9921	 l-p:0.13946634531021118
epoch£º496	 i:2 	 global-step:9922	 l-p:0.09676294028759003
epoch£º496	 i:3 	 global-step:9923	 l-p:0.22434699535369873
epoch£º496	 i:4 	 global-step:9924	 l-p:0.26494675874710083
epoch£º496	 i:5 	 global-step:9925	 l-p:0.05403926223516464
epoch£º496	 i:6 	 global-step:9926	 l-p:0.13634732365608215
epoch£º496	 i:7 	 global-step:9927	 l-p:0.23244129121303558
epoch£º496	 i:8 	 global-step:9928	 l-p:0.15093253552913666
epoch£º496	 i:9 	 global-step:9929	 l-p:0.05089840665459633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5254, 3.0950, 2.9520],
        [3.5254, 3.5254, 3.5254],
        [3.5254, 3.5254, 3.5254],
        [3.5254, 3.5219, 3.5253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.12875762581825256 
model_pd.l_d.mean(): -23.32538414001465 
model_pd.lagr.mean(): -23.196626663208008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2536], device='cuda:0')), ('power', tensor([-23.5790], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.12875762581825256
epoch£º497	 i:1 	 global-step:9941	 l-p:0.16239535808563232
epoch£º497	 i:2 	 global-step:9942	 l-p:0.14209777116775513
epoch£º497	 i:3 	 global-step:9943	 l-p:0.14792370796203613
epoch£º497	 i:4 	 global-step:9944	 l-p:0.12316685169935226
epoch£º497	 i:5 	 global-step:9945	 l-p:0.13106344640254974
epoch£º497	 i:6 	 global-step:9946	 l-p:0.17795564234256744
epoch£º497	 i:7 	 global-step:9947	 l-p:0.14127454161643982
epoch£º497	 i:8 	 global-step:9948	 l-p:0.21710743010044098
epoch£º497	 i:9 	 global-step:9949	 l-p:0.1244310736656189
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4222, 3.3105, 3.3806],
        [3.4222, 3.0913, 2.6345],
        [3.4222, 2.9620, 2.5245],
        [3.4222, 3.4054, 3.4206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.17917220294475555 
model_pd.l_d.mean(): -23.37740707397461 
model_pd.lagr.mean(): -23.19823455810547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2740], device='cuda:0')), ('power', tensor([-23.6514], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.17917220294475555
epoch£º498	 i:1 	 global-step:9961	 l-p:0.32397711277008057
epoch£º498	 i:2 	 global-step:9962	 l-p:0.12152130156755447
epoch£º498	 i:3 	 global-step:9963	 l-p:0.13843081891536713
epoch£º498	 i:4 	 global-step:9964	 l-p:0.11550363153219223
epoch£º498	 i:5 	 global-step:9965	 l-p:0.12017707526683807
epoch£º498	 i:6 	 global-step:9966	 l-p:0.14228053390979767
epoch£º498	 i:7 	 global-step:9967	 l-p:0.16519466042518616
epoch£º498	 i:8 	 global-step:9968	 l-p:0.30901533365249634
epoch£º498	 i:9 	 global-step:9969	 l-p:0.19688780605793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4663, 3.4647, 3.4663],
        [3.4663, 3.1273, 2.6712],
        [3.4663, 3.4664, 3.4663],
        [3.4663, 3.4664, 3.4663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.1480037420988083 
model_pd.l_d.mean(): -23.754594802856445 
model_pd.lagr.mean(): -23.606590270996094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1500], device='cuda:0')), ('power', tensor([-23.9046], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.1480037420988083
epoch£º499	 i:1 	 global-step:9981	 l-p:0.07138698548078537
epoch£º499	 i:2 	 global-step:9982	 l-p:0.16126485168933868
epoch£º499	 i:3 	 global-step:9983	 l-p:0.1382327377796173
epoch£º499	 i:4 	 global-step:9984	 l-p:0.14910882711410522
epoch£º499	 i:5 	 global-step:9985	 l-p:0.2078663557767868
epoch£º499	 i:6 	 global-step:9986	 l-p:0.12269607931375504
epoch£º499	 i:7 	 global-step:9987	 l-p:0.16927780210971832
epoch£º499	 i:8 	 global-step:9988	 l-p:0.13365261256694794
epoch£º499	 i:9 	 global-step:9989	 l-p:0.12946926057338715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4678, 3.2550, 3.3318],
        [3.4678, 3.0315, 2.8948],
        [3.4678, 3.3564, 3.4264],
        [3.4678, 2.9919, 2.5795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.20557093620300293 
model_pd.l_d.mean(): -23.688119888305664 
model_pd.lagr.mean(): -23.4825496673584 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2135], device='cuda:0')), ('power', tensor([-23.9017], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.20557093620300293
epoch£º500	 i:1 	 global-step:10001	 l-p:0.13176801800727844
epoch£º500	 i:2 	 global-step:10002	 l-p:0.24763573706150055
epoch£º500	 i:3 	 global-step:10003	 l-p:0.12371736019849777
epoch£º500	 i:4 	 global-step:10004	 l-p:0.1216263398528099
epoch£º500	 i:5 	 global-step:10005	 l-p:0.1294884979724884
epoch£º500	 i:6 	 global-step:10006	 l-p:0.12501375377178192
epoch£º500	 i:7 	 global-step:10007	 l-p:0.14955468475818634
epoch£º500	 i:8 	 global-step:10008	 l-p:0.4267648160457611
epoch£º500	 i:9 	 global-step:10009	 l-p:0.14315181970596313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4108, 3.1412, 3.2002],
        [3.4108, 2.9590, 2.8053],
        [3.4108, 2.9374, 2.7313],
        [3.4108, 3.3478, 3.3958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): -0.06454971432685852 
model_pd.l_d.mean(): -23.13722801208496 
model_pd.lagr.mean(): -23.201778411865234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3097], device='cuda:0')), ('power', tensor([-23.4470], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:-0.06454971432685852
epoch£º501	 i:1 	 global-step:10021	 l-p:0.16629865765571594
epoch£º501	 i:2 	 global-step:10022	 l-p:-0.024683356285095215
epoch£º501	 i:3 	 global-step:10023	 l-p:0.11389490216970444
epoch£º501	 i:4 	 global-step:10024	 l-p:0.14475727081298828
epoch£º501	 i:5 	 global-step:10025	 l-p:0.35685718059539795
epoch£º501	 i:6 	 global-step:10026	 l-p:0.11922973394393921
epoch£º501	 i:7 	 global-step:10027	 l-p:0.1365930587053299
epoch£º501	 i:8 	 global-step:10028	 l-p:0.18523009121418
epoch£º501	 i:9 	 global-step:10029	 l-p:0.13748905062675476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3991, 2.9990, 2.9398],
        [3.3991, 3.3991, 3.3992],
        [3.3991, 3.0317, 2.5659],
        [3.3991, 3.3986, 3.3991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.14301930367946625 
model_pd.l_d.mean(): -22.866621017456055 
model_pd.lagr.mean(): -22.723602294921875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3150], device='cuda:0')), ('power', tensor([-23.1816], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.14301930367946625
epoch£º502	 i:1 	 global-step:10041	 l-p:0.2101723849773407
epoch£º502	 i:2 	 global-step:10042	 l-p:0.17139628529548645
epoch£º502	 i:3 	 global-step:10043	 l-p:0.1455298811197281
epoch£º502	 i:4 	 global-step:10044	 l-p:0.04743669927120209
epoch£º502	 i:5 	 global-step:10045	 l-p:0.14262370765209198
epoch£º502	 i:6 	 global-step:10046	 l-p:0.15468518435955048
epoch£º502	 i:7 	 global-step:10047	 l-p:0.5219624042510986
epoch£º502	 i:8 	 global-step:10048	 l-p:0.13438551127910614
epoch£º502	 i:9 	 global-step:10049	 l-p:0.10126428306102753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4669, 3.4667, 3.4669],
        [3.4669, 3.1026, 2.6389],
        [3.4669, 3.4669, 3.4669],
        [3.4669, 3.4124, 3.4552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.10914567857980728 
model_pd.l_d.mean(): -22.977266311645508 
model_pd.lagr.mean(): -22.868120193481445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2511], device='cuda:0')), ('power', tensor([-23.2283], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.10914567857980728
epoch£º503	 i:1 	 global-step:10061	 l-p:0.2005176842212677
epoch£º503	 i:2 	 global-step:10062	 l-p:0.14939464628696442
epoch£º503	 i:3 	 global-step:10063	 l-p:0.12485150992870331
epoch£º503	 i:4 	 global-step:10064	 l-p:0.12512768805027008
epoch£º503	 i:5 	 global-step:10065	 l-p:0.13698160648345947
epoch£º503	 i:6 	 global-step:10066	 l-p:0.13271982967853546
epoch£º503	 i:7 	 global-step:10067	 l-p:0.14758852124214172
epoch£º503	 i:8 	 global-step:10068	 l-p:0.1242096871137619
epoch£º503	 i:9 	 global-step:10069	 l-p:0.21939820051193237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4301, 3.4052, 3.4270],
        [3.4301, 2.9749, 2.8198],
        [3.4301, 3.3602, 3.4122],
        [3.4301, 3.1632, 3.2250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): -0.8000006675720215 
model_pd.l_d.mean(): -23.402666091918945 
model_pd.lagr.mean(): -24.202667236328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2779], device='cuda:0')), ('power', tensor([-23.6805], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:-0.8000006675720215
epoch£º504	 i:1 	 global-step:10081	 l-p:0.11217614263296127
epoch£º504	 i:2 	 global-step:10082	 l-p:0.15305982530117035
epoch£º504	 i:3 	 global-step:10083	 l-p:0.06030441075563431
epoch£º504	 i:4 	 global-step:10084	 l-p:0.16145221889019012
epoch£º504	 i:5 	 global-step:10085	 l-p:-0.04410075023770332
epoch£º504	 i:6 	 global-step:10086	 l-p:0.13738961517810822
epoch£º504	 i:7 	 global-step:10087	 l-p:0.028945021331310272
epoch£º504	 i:8 	 global-step:10088	 l-p:0.16906535625457764
epoch£º504	 i:9 	 global-step:10089	 l-p:0.1615324765443802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3235, 3.0012, 3.0350],
        [3.3235, 3.2006, 3.2757],
        [3.3235, 3.3135, 3.3228],
        [3.3235, 3.1503, 3.2342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.19493629038333893 
model_pd.l_d.mean(): -23.563854217529297 
model_pd.lagr.mean(): -23.36891746520996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2866], device='cuda:0')), ('power', tensor([-23.8504], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.19493629038333893
epoch£º505	 i:1 	 global-step:10101	 l-p:0.1430021971464157
epoch£º505	 i:2 	 global-step:10102	 l-p:0.08966637402772903
epoch£º505	 i:3 	 global-step:10103	 l-p:0.15528827905654907
epoch£º505	 i:4 	 global-step:10104	 l-p:0.16745194792747498
epoch£º505	 i:5 	 global-step:10105	 l-p:0.1342260092496872
epoch£º505	 i:6 	 global-step:10106	 l-p:0.15159626305103302
epoch£º505	 i:7 	 global-step:10107	 l-p:0.13760943710803986
epoch£º505	 i:8 	 global-step:10108	 l-p:-0.16651831567287445
epoch£º505	 i:9 	 global-step:10109	 l-p:0.13931505382061005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5997, 3.5751, 3.5967],
        [3.5997, 3.4789, 3.0742],
        [3.5997, 3.3792, 2.9434],
        [3.5997, 3.1433, 2.9435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.12146582454442978 
model_pd.l_d.mean(): -22.923377990722656 
model_pd.lagr.mean(): -22.801912307739258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2551], device='cuda:0')), ('power', tensor([-23.1785], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.12146582454442978
epoch£º506	 i:1 	 global-step:10121	 l-p:0.14437149465084076
epoch£º506	 i:2 	 global-step:10122	 l-p:0.13794396817684174
epoch£º506	 i:3 	 global-step:10123	 l-p:0.1656627207994461
epoch£º506	 i:4 	 global-step:10124	 l-p:0.15280048549175262
epoch£º506	 i:5 	 global-step:10125	 l-p:0.10661006718873978
epoch£º506	 i:6 	 global-step:10126	 l-p:0.1355406939983368
epoch£º506	 i:7 	 global-step:10127	 l-p:0.10758618265390396
epoch£º506	 i:8 	 global-step:10128	 l-p:0.12307488173246384
epoch£º506	 i:9 	 global-step:10129	 l-p:0.13932055234909058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4778, 3.3676, 3.4378],
        [3.4778, 3.1157, 3.1032],
        [3.4778, 3.0599, 2.9719],
        [3.4778, 3.2710, 2.8326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.1530386507511139 
model_pd.l_d.mean(): -23.413803100585938 
model_pd.lagr.mean(): -23.260765075683594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2625], device='cuda:0')), ('power', tensor([-23.6763], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.1530386507511139
epoch£º507	 i:1 	 global-step:10141	 l-p:0.14623402059078217
epoch£º507	 i:2 	 global-step:10142	 l-p:0.13863429427146912
epoch£º507	 i:3 	 global-step:10143	 l-p:-0.14537854492664337
epoch£º507	 i:4 	 global-step:10144	 l-p:0.15165665745735168
epoch£º507	 i:5 	 global-step:10145	 l-p:0.11059921234846115
epoch£º507	 i:6 	 global-step:10146	 l-p:0.08929742872714996
epoch£º507	 i:7 	 global-step:10147	 l-p:0.20805658400058746
epoch£º507	 i:8 	 global-step:10148	 l-p:0.8974049091339111
epoch£º507	 i:9 	 global-step:10149	 l-p:0.13088427484035492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4855, 3.4302, 3.4736],
        [3.4855, 3.4805, 3.4852],
        [3.4855, 3.4842, 3.4854],
        [3.4855, 2.9813, 2.6342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.07987046986818314 
model_pd.l_d.mean(): -22.817699432373047 
model_pd.lagr.mean(): -22.737829208374023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3098], device='cuda:0')), ('power', tensor([-23.1275], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.07987046986818314
epoch£º508	 i:1 	 global-step:10161	 l-p:0.13756272196769714
epoch£º508	 i:2 	 global-step:10162	 l-p:0.15442848205566406
epoch£º508	 i:3 	 global-step:10163	 l-p:0.1559312343597412
epoch£º508	 i:4 	 global-step:10164	 l-p:0.15487369894981384
epoch£º508	 i:5 	 global-step:10165	 l-p:0.13184396922588348
epoch£º508	 i:6 	 global-step:10166	 l-p:0.16531167924404144
epoch£º508	 i:7 	 global-step:10167	 l-p:0.11602487415075302
epoch£º508	 i:8 	 global-step:10168	 l-p:0.1569756120443344
epoch£º508	 i:9 	 global-step:10169	 l-p:0.14687588810920715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4516, 3.0064, 2.5410],
        [3.4516, 3.4506, 3.4516],
        [3.4516, 3.2896, 2.8628],
        [3.4516, 3.3543, 3.4199]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.2742424011230469 
model_pd.l_d.mean(): -23.099674224853516 
model_pd.lagr.mean(): -22.82543182373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2851], device='cuda:0')), ('power', tensor([-23.3848], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.2742424011230469
epoch£º509	 i:1 	 global-step:10181	 l-p:0.3120346665382385
epoch£º509	 i:2 	 global-step:10182	 l-p:0.13085024058818817
epoch£º509	 i:3 	 global-step:10183	 l-p:0.15599572658538818
epoch£º509	 i:4 	 global-step:10184	 l-p:0.09792159497737885
epoch£º509	 i:5 	 global-step:10185	 l-p:0.13290588557720184
epoch£º509	 i:6 	 global-step:10186	 l-p:0.12262808531522751
epoch£º509	 i:7 	 global-step:10187	 l-p:0.22113917768001556
epoch£º509	 i:8 	 global-step:10188	 l-p:0.112776979804039
epoch£º509	 i:9 	 global-step:10189	 l-p:0.1457555890083313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4760, 3.4729, 3.4759],
        [3.4760, 2.9930, 2.5529],
        [3.4760, 3.0074, 2.5547],
        [3.4760, 2.9670, 2.6094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.12296050786972046 
model_pd.l_d.mean(): -22.305587768554688 
model_pd.lagr.mean(): -22.182626724243164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2569], device='cuda:0')), ('power', tensor([-22.5625], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.12296050786972046
epoch£º510	 i:1 	 global-step:10201	 l-p:0.3335041105747223
epoch£º510	 i:2 	 global-step:10202	 l-p:0.1373136341571808
epoch£º510	 i:3 	 global-step:10203	 l-p:0.14908555150032043
epoch£º510	 i:4 	 global-step:10204	 l-p:0.13238976895809174
epoch£º510	 i:5 	 global-step:10205	 l-p:0.14150196313858032
epoch£º510	 i:6 	 global-step:10206	 l-p:0.14725609123706818
epoch£º510	 i:7 	 global-step:10207	 l-p:0.12642808258533478
epoch£º510	 i:8 	 global-step:10208	 l-p:0.15558375418186188
epoch£º510	 i:9 	 global-step:10209	 l-p:0.3467400372028351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3454, 3.1597, 3.2450],
        [3.3454, 2.8231, 2.3999],
        [3.3454, 3.2655, 3.3233],
        [3.3454, 3.3454, 3.3454]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.06617818027734756 
model_pd.l_d.mean(): -23.24091911315918 
model_pd.lagr.mean(): -23.174741744995117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3727], device='cuda:0')), ('power', tensor([-23.6136], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.06617818027734756
epoch£º511	 i:1 	 global-step:10221	 l-p:0.11033648997545242
epoch£º511	 i:2 	 global-step:10222	 l-p:0.128153994679451
epoch£º511	 i:3 	 global-step:10223	 l-p:0.14405034482479095
epoch£º511	 i:4 	 global-step:10224	 l-p:0.13704219460487366
epoch£º511	 i:5 	 global-step:10225	 l-p:0.1626967340707779
epoch£º511	 i:6 	 global-step:10226	 l-p:0.13100583851337433
epoch£º511	 i:7 	 global-step:10227	 l-p:0.14787670969963074
epoch£º511	 i:8 	 global-step:10228	 l-p:0.08326195180416107
epoch£º511	 i:9 	 global-step:10229	 l-p:0.1256142258644104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3998, 3.3738, 3.3965],
        [3.3998, 3.3010, 3.3676],
        [3.3998, 2.8810, 2.4698],
        [3.3998, 3.1222, 3.1847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.1534290909767151 
model_pd.l_d.mean(): -23.563642501831055 
model_pd.lagr.mean(): -23.410213470458984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2400], device='cuda:0')), ('power', tensor([-23.8036], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.1534290909767151
epoch£º512	 i:1 	 global-step:10241	 l-p:0.04649803042411804
epoch£º512	 i:2 	 global-step:10242	 l-p:0.15893900394439697
epoch£º512	 i:3 	 global-step:10243	 l-p:0.14044703543186188
epoch£º512	 i:4 	 global-step:10244	 l-p:0.15921100974082947
epoch£º512	 i:5 	 global-step:10245	 l-p:0.2351236492395401
epoch£º512	 i:6 	 global-step:10246	 l-p:0.12315233051776886
epoch£º512	 i:7 	 global-step:10247	 l-p:0.13655300438404083
epoch£º512	 i:8 	 global-step:10248	 l-p:0.4476073682308197
epoch£º512	 i:9 	 global-step:10249	 l-p:0.2311374992132187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4689, 3.1098, 3.1086],
        [3.4689, 3.0981, 2.6209],
        [3.4689, 3.1474, 3.1796],
        [3.4689, 3.4642, 3.4687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.20046867430210114 
model_pd.l_d.mean(): -23.430147171020508 
model_pd.lagr.mean(): -23.229679107666016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2558], device='cuda:0')), ('power', tensor([-23.6859], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.20046867430210114
epoch£º513	 i:1 	 global-step:10261	 l-p:0.19399672746658325
epoch£º513	 i:2 	 global-step:10262	 l-p:0.12567377090454102
epoch£º513	 i:3 	 global-step:10263	 l-p:0.16214008629322052
epoch£º513	 i:4 	 global-step:10264	 l-p:0.12976506352424622
epoch£º513	 i:5 	 global-step:10265	 l-p:0.149166077375412
epoch£º513	 i:6 	 global-step:10266	 l-p:-0.18058376014232635
epoch£º513	 i:7 	 global-step:10267	 l-p:0.12303794920444489
epoch£º513	 i:8 	 global-step:10268	 l-p:0.12406701594591141
epoch£º513	 i:9 	 global-step:10269	 l-p:0.13813385367393494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5492, 3.3255, 2.8774],
        [3.5492, 3.1843, 2.7112],
        [3.5492, 3.2615, 3.3132],
        [3.5492, 3.5488, 3.5492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.12678013741970062 
model_pd.l_d.mean(): -23.385290145874023 
model_pd.lagr.mean(): -23.25851058959961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1919], device='cuda:0')), ('power', tensor([-23.5772], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.12678013741970062
epoch£º514	 i:1 	 global-step:10281	 l-p:0.14204122126102448
epoch£º514	 i:2 	 global-step:10282	 l-p:0.15427252650260925
epoch£º514	 i:3 	 global-step:10283	 l-p:0.5088114738464355
epoch£º514	 i:4 	 global-step:10284	 l-p:0.1753484308719635
epoch£º514	 i:5 	 global-step:10285	 l-p:0.13410840928554535
epoch£º514	 i:6 	 global-step:10286	 l-p:0.1203337088227272
epoch£º514	 i:7 	 global-step:10287	 l-p:0.14736008644104004
epoch£º514	 i:8 	 global-step:10288	 l-p:0.11334061622619629
epoch£º514	 i:9 	 global-step:10289	 l-p:0.14534473419189453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3580, 2.9263, 2.4409],
        [3.3580, 2.9006, 2.4182],
        [3.3580, 3.2292, 3.3069],
        [3.3580, 2.8239, 2.4899]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): -0.09150151908397675 
model_pd.l_d.mean(): -22.583120346069336 
model_pd.lagr.mean(): -22.67462158203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4053], device='cuda:0')), ('power', tensor([-22.9884], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:-0.09150151908397675
epoch£º515	 i:1 	 global-step:10301	 l-p:0.1080784946680069
epoch£º515	 i:2 	 global-step:10302	 l-p:0.12506739795207977
epoch£º515	 i:3 	 global-step:10303	 l-p:0.17025518417358398
epoch£º515	 i:4 	 global-step:10304	 l-p:0.142457515001297
epoch£º515	 i:5 	 global-step:10305	 l-p:0.14241479337215424
epoch£º515	 i:6 	 global-step:10306	 l-p:-0.14137740433216095
epoch£º515	 i:7 	 global-step:10307	 l-p:0.1808372437953949
epoch£º515	 i:8 	 global-step:10308	 l-p:0.1863030642271042
epoch£º515	 i:9 	 global-step:10309	 l-p:0.07484672963619232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3953, 3.3745, 3.3931],
        [3.3953, 3.0754, 3.1160],
        [3.3953, 2.9682, 2.4824],
        [3.3953, 3.3563, 3.3889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.2980552315711975 
model_pd.l_d.mean(): -23.165781021118164 
model_pd.lagr.mean(): -22.867725372314453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3551], device='cuda:0')), ('power', tensor([-23.5208], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.2980552315711975
epoch£º516	 i:1 	 global-step:10321	 l-p:0.15303577482700348
epoch£º516	 i:2 	 global-step:10322	 l-p:0.13565662503242493
epoch£º516	 i:3 	 global-step:10323	 l-p:0.2533853054046631
epoch£º516	 i:4 	 global-step:10324	 l-p:0.23465082049369812
epoch£º516	 i:5 	 global-step:10325	 l-p:0.12741993367671967
epoch£º516	 i:6 	 global-step:10326	 l-p:0.16938510537147522
epoch£º516	 i:7 	 global-step:10327	 l-p:-0.004132990725338459
epoch£º516	 i:8 	 global-step:10328	 l-p:0.12412881851196289
epoch£º516	 i:9 	 global-step:10329	 l-p:0.1287768930196762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5957, 3.5798, 3.5942],
        [3.5957, 3.1348, 2.9648],
        [3.5957, 3.1550, 3.0283],
        [3.5957, 3.5956, 3.5957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.1359454244375229 
model_pd.l_d.mean(): -23.129013061523438 
model_pd.lagr.mean(): -22.993066787719727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2025], device='cuda:0')), ('power', tensor([-23.3315], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.1359454244375229
epoch£º517	 i:1 	 global-step:10341	 l-p:0.12234488129615784
epoch£º517	 i:2 	 global-step:10342	 l-p:0.13590668141841888
epoch£º517	 i:3 	 global-step:10343	 l-p:0.160152405500412
epoch£º517	 i:4 	 global-step:10344	 l-p:0.15908679366111755
epoch£º517	 i:5 	 global-step:10345	 l-p:0.12306690216064453
epoch£º517	 i:6 	 global-step:10346	 l-p:-2.2285542488098145
epoch£º517	 i:7 	 global-step:10347	 l-p:0.17455165088176727
epoch£º517	 i:8 	 global-step:10348	 l-p:0.05385114997625351
epoch£º517	 i:9 	 global-step:10349	 l-p:0.15503796935081482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[3.4180, 2.9221, 2.4578],
        [3.4180, 2.8982, 2.6195],
        [3.4180, 3.2073, 3.2918],
        [3.4180, 3.0408, 3.0308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.127014622092247 
model_pd.l_d.mean(): -23.37643051147461 
model_pd.lagr.mean(): -23.24941635131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3014], device='cuda:0')), ('power', tensor([-23.6778], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.127014622092247
epoch£º518	 i:1 	 global-step:10361	 l-p:0.055431727319955826
epoch£º518	 i:2 	 global-step:10362	 l-p:0.058865632861852646
epoch£º518	 i:3 	 global-step:10363	 l-p:0.13557103276252747
epoch£º518	 i:4 	 global-step:10364	 l-p:0.22551730275154114
epoch£º518	 i:5 	 global-step:10365	 l-p:0.15188482403755188
epoch£º518	 i:6 	 global-step:10366	 l-p:0.09201575815677643
epoch£º518	 i:7 	 global-step:10367	 l-p:0.13627922534942627
epoch£º518	 i:8 	 global-step:10368	 l-p:0.1302713304758072
epoch£º518	 i:9 	 global-step:10369	 l-p:0.1885037124156952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5176, 3.5114, 3.5173],
        [3.5176, 3.3236, 2.8785],
        [3.5176, 3.5108, 3.5172],
        [3.5176, 3.3479, 3.4320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.13571256399154663 
model_pd.l_d.mean(): -23.430749893188477 
model_pd.lagr.mean(): -23.2950382232666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2137], device='cuda:0')), ('power', tensor([-23.6445], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.13571256399154663
epoch£º519	 i:1 	 global-step:10381	 l-p:0.16906015574932098
epoch£º519	 i:2 	 global-step:10382	 l-p:0.11445058882236481
epoch£º519	 i:3 	 global-step:10383	 l-p:0.15925487875938416
epoch£º519	 i:4 	 global-step:10384	 l-p:0.8461190462112427
epoch£º519	 i:5 	 global-step:10385	 l-p:-0.6016814708709717
epoch£º519	 i:6 	 global-step:10386	 l-p:0.13211654126644135
epoch£º519	 i:7 	 global-step:10387	 l-p:0.11192792654037476
epoch£º519	 i:8 	 global-step:10388	 l-p:0.12510378658771515
epoch£º519	 i:9 	 global-step:10389	 l-p:0.13390250504016876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4095, 3.4095, 3.4095],
        [3.4095, 3.1046, 2.6297],
        [3.4095, 3.4095, 3.4095],
        [3.4095, 3.3931, 3.4080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.15181861817836761 
model_pd.l_d.mean(): -22.898237228393555 
model_pd.lagr.mean(): -22.746417999267578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2778], device='cuda:0')), ('power', tensor([-23.1761], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.15181861817836761
epoch£º520	 i:1 	 global-step:10401	 l-p:0.058353498578071594
epoch£º520	 i:2 	 global-step:10402	 l-p:0.12929309904575348
epoch£º520	 i:3 	 global-step:10403	 l-p:0.18474021553993225
epoch£º520	 i:4 	 global-step:10404	 l-p:0.12323255091905594
epoch£º520	 i:5 	 global-step:10405	 l-p:0.28381475806236267
epoch£º520	 i:6 	 global-step:10406	 l-p:0.1345020979642868
epoch£º520	 i:7 	 global-step:10407	 l-p:0.12829385697841644
epoch£º520	 i:8 	 global-step:10408	 l-p:0.13593080639839172
epoch£º520	 i:9 	 global-step:10409	 l-p:0.15279169380664825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5320, 3.1187, 3.0547],
        [3.5320, 3.4690, 3.5174],
        [3.5320, 3.3167, 2.8641],
        [3.5320, 3.1771, 3.1830]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.13560883700847626 
model_pd.l_d.mean(): -23.594524383544922 
model_pd.lagr.mean(): -23.45891571044922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1737], device='cuda:0')), ('power', tensor([-23.7683], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.13560883700847626
epoch£º521	 i:1 	 global-step:10421	 l-p:0.10808326303958893
epoch£º521	 i:2 	 global-step:10422	 l-p:0.2348790317773819
epoch£º521	 i:3 	 global-step:10423	 l-p:0.1452644169330597
epoch£º521	 i:4 	 global-step:10424	 l-p:0.20776274800300598
epoch£º521	 i:5 	 global-step:10425	 l-p:-0.07539492100477219
epoch£º521	 i:6 	 global-step:10426	 l-p:0.11259399354457855
epoch£º521	 i:7 	 global-step:10427	 l-p:0.14017659425735474
epoch£º521	 i:8 	 global-step:10428	 l-p:0.18259112536907196
epoch£º521	 i:9 	 global-step:10429	 l-p:0.15614762902259827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4434, 3.4355, 3.4429],
        [3.4434, 3.4433, 3.4434],
        [3.4434, 3.4248, 3.4415],
        [3.4434, 3.4224, 3.4411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): -0.895320475101471 
model_pd.l_d.mean(): -23.620458602905273 
model_pd.lagr.mean(): -24.515779495239258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2405], device='cuda:0')), ('power', tensor([-23.8610], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:-0.895320475101471
epoch£º522	 i:1 	 global-step:10441	 l-p:0.14400465786457062
epoch£º522	 i:2 	 global-step:10442	 l-p:0.19592183828353882
epoch£º522	 i:3 	 global-step:10443	 l-p:0.10909148305654526
epoch£º522	 i:4 	 global-step:10444	 l-p:0.1405208259820938
epoch£º522	 i:5 	 global-step:10445	 l-p:0.14508464932441711
epoch£º522	 i:6 	 global-step:10446	 l-p:0.1356745809316635
epoch£º522	 i:7 	 global-step:10447	 l-p:0.2074367105960846
epoch£º522	 i:8 	 global-step:10448	 l-p:0.12916074693202972
epoch£º522	 i:9 	 global-step:10449	 l-p:0.2853923738002777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4785, 3.4383, 3.4718],
        [3.4785, 3.4785, 3.4785],
        [3.4785, 3.2434, 2.7834],
        [3.4785, 3.4781, 3.4785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.2305227667093277 
model_pd.l_d.mean(): -22.076797485351562 
model_pd.lagr.mean(): -21.846275329589844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4098], device='cuda:0')), ('power', tensor([-22.4866], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.2305227667093277
epoch£º523	 i:1 	 global-step:10461	 l-p:0.17446893453598022
epoch£º523	 i:2 	 global-step:10462	 l-p:0.19062286615371704
epoch£º523	 i:3 	 global-step:10463	 l-p:0.1394893378019333
epoch£º523	 i:4 	 global-step:10464	 l-p:0.12582546472549438
epoch£º523	 i:5 	 global-step:10465	 l-p:0.15554386377334595
epoch£º523	 i:6 	 global-step:10466	 l-p:0.13947758078575134
epoch£º523	 i:7 	 global-step:10467	 l-p:0.13735532760620117
epoch£º523	 i:8 	 global-step:10468	 l-p:0.1410365104675293
epoch£º523	 i:9 	 global-step:10469	 l-p:0.10912995785474777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4624, 3.4381, 3.4595],
        [3.4624, 3.0387, 2.5478],
        [3.4624, 3.4617, 3.4624],
        [3.4624, 3.4393, 3.4598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.16855144500732422 
model_pd.l_d.mean(): -23.35398292541504 
model_pd.lagr.mean(): -23.18543243408203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2609], device='cuda:0')), ('power', tensor([-23.6149], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.16855144500732422
epoch£º524	 i:1 	 global-step:10481	 l-p:0.041639719158411026
epoch£º524	 i:2 	 global-step:10482	 l-p:0.3342970907688141
epoch£º524	 i:3 	 global-step:10483	 l-p:0.13617749512195587
epoch£º524	 i:4 	 global-step:10484	 l-p:0.13602875173091888
epoch£º524	 i:5 	 global-step:10485	 l-p:0.13574834167957306
epoch£º524	 i:6 	 global-step:10486	 l-p:0.12868523597717285
epoch£º524	 i:7 	 global-step:10487	 l-p:0.1290186643600464
epoch£º524	 i:8 	 global-step:10488	 l-p:0.1445411890745163
epoch£º524	 i:9 	 global-step:10489	 l-p:0.08566199988126755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4062, 3.4012, 3.4060],
        [3.4062, 3.4062, 3.4062],
        [3.4062, 3.3994, 3.4059],
        [3.4062, 3.0098, 2.9870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.16096462309360504 
model_pd.l_d.mean(): -22.997364044189453 
model_pd.lagr.mean(): -22.83639907836914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3200], device='cuda:0')), ('power', tensor([-23.3174], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.16096462309360504
epoch£º525	 i:1 	 global-step:10501	 l-p:0.1478520631790161
epoch£º525	 i:2 	 global-step:10502	 l-p:0.12519940733909607
epoch£º525	 i:3 	 global-step:10503	 l-p:0.18647414445877075
epoch£º525	 i:4 	 global-step:10504	 l-p:0.1244199350476265
epoch£º525	 i:5 	 global-step:10505	 l-p:0.12263931334018707
epoch£º525	 i:6 	 global-step:10506	 l-p:0.1605774313211441
epoch£º525	 i:7 	 global-step:10507	 l-p:0.14847484230995178
epoch£º525	 i:8 	 global-step:10508	 l-p:0.056812308728694916
epoch£º525	 i:9 	 global-step:10509	 l-p:0.25116026401519775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4408, 3.4168, 3.4380],
        [3.4408, 3.1177, 3.1605],
        [3.4408, 2.9150, 2.4658],
        [3.4408, 2.9432, 2.4659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.02948072925209999 
model_pd.l_d.mean(): -23.585765838623047 
model_pd.lagr.mean(): -23.556285858154297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2619], device='cuda:0')), ('power', tensor([-23.8476], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.02948072925209999
epoch£º526	 i:1 	 global-step:10521	 l-p:0.14335587620735168
epoch£º526	 i:2 	 global-step:10522	 l-p:0.1463884860277176
epoch£º526	 i:3 	 global-step:10523	 l-p:0.11940436065196991
epoch£º526	 i:4 	 global-step:10524	 l-p:0.17302939295768738
epoch£º526	 i:5 	 global-step:10525	 l-p:0.136699840426445
epoch£º526	 i:6 	 global-step:10526	 l-p:0.13452179729938507
epoch£º526	 i:7 	 global-step:10527	 l-p:0.13213969767093658
epoch£º526	 i:8 	 global-step:10528	 l-p:0.29024264216423035
epoch£º526	 i:9 	 global-step:10529	 l-p:0.13117918372154236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6018, 3.6018, 3.6018],
        [3.6018, 3.1378, 2.9857],
        [3.6018, 3.6018, 3.6018],
        [3.6018, 3.6004, 3.6018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.12634062767028809 
model_pd.l_d.mean(): -23.44837188720703 
model_pd.lagr.mean(): -23.322031021118164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1697], device='cuda:0')), ('power', tensor([-23.6181], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.12634062767028809
epoch£º527	 i:1 	 global-step:10541	 l-p:0.13410699367523193
epoch£º527	 i:2 	 global-step:10542	 l-p:0.13523846864700317
epoch£º527	 i:3 	 global-step:10543	 l-p:0.13704299926757812
epoch£º527	 i:4 	 global-step:10544	 l-p:0.1553649753332138
epoch£º527	 i:5 	 global-step:10545	 l-p:0.1342117339372635
epoch£º527	 i:6 	 global-step:10546	 l-p:0.14098884165287018
epoch£º527	 i:7 	 global-step:10547	 l-p:0.0371350459754467
epoch£º527	 i:8 	 global-step:10548	 l-p:0.13631556928157806
epoch£º527	 i:9 	 global-step:10549	 l-p:0.14949341118335724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3759, 3.2701, 3.3408],
        [3.3759, 3.3615, 3.3747],
        [3.3759, 2.9426, 2.4434],
        [3.3759, 2.9427, 2.4435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): -0.0720425471663475 
model_pd.l_d.mean(): -23.036972045898438 
model_pd.lagr.mean(): -23.1090145111084 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3567], device='cuda:0')), ('power', tensor([-23.3937], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:-0.0720425471663475
epoch£º528	 i:1 	 global-step:10561	 l-p:0.1308240443468094
epoch£º528	 i:2 	 global-step:10562	 l-p:0.1416616439819336
epoch£º528	 i:3 	 global-step:10563	 l-p:0.15836623311042786
epoch£º528	 i:4 	 global-step:10564	 l-p:0.1337333768606186
epoch£º528	 i:5 	 global-step:10565	 l-p:0.07975577563047409
epoch£º528	 i:6 	 global-step:10566	 l-p:0.1647857278585434
epoch£º528	 i:7 	 global-step:10567	 l-p:0.12901975214481354
epoch£º528	 i:8 	 global-step:10568	 l-p:0.13456231355667114
epoch£º528	 i:9 	 global-step:10569	 l-p:0.149931401014328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[3.4169, 2.9160, 2.7329],
        [3.4169, 2.8667, 2.5088],
        [3.4169, 3.2269, 3.3154],
        [3.4169, 2.9492, 2.4521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): -29.019779205322266 
model_pd.l_d.mean(): -23.598979949951172 
model_pd.lagr.mean(): -52.61875915527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2646], device='cuda:0')), ('power', tensor([-23.8636], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:-29.019779205322266
epoch£º529	 i:1 	 global-step:10581	 l-p:-0.0328243151307106
epoch£º529	 i:2 	 global-step:10582	 l-p:0.14068765938282013
epoch£º529	 i:3 	 global-step:10583	 l-p:0.130981907248497
epoch£º529	 i:4 	 global-step:10584	 l-p:0.1592591553926468
epoch£º529	 i:5 	 global-step:10585	 l-p:0.2726396918296814
epoch£º529	 i:6 	 global-step:10586	 l-p:0.13686618208885193
epoch£º529	 i:7 	 global-step:10587	 l-p:0.13935039937496185
epoch£º529	 i:8 	 global-step:10588	 l-p:0.12797650694847107
epoch£º529	 i:9 	 global-step:10589	 l-p:0.007274474948644638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4468, 3.4468, 3.4468],
        [3.4468, 3.0154, 2.9516],
        [3.4468, 3.4468, 3.4468],
        [3.4468, 2.9318, 2.4573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.1365128755569458 
model_pd.l_d.mean(): -23.372966766357422 
model_pd.lagr.mean(): -23.236454010009766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2828], device='cuda:0')), ('power', tensor([-23.6558], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.1365128755569458
epoch£º530	 i:1 	 global-step:10601	 l-p:0.22453409433364868
epoch£º530	 i:2 	 global-step:10602	 l-p:0.1285731941461563
epoch£º530	 i:3 	 global-step:10603	 l-p:0.04694029688835144
epoch£º530	 i:4 	 global-step:10604	 l-p:0.193022683262825
epoch£º530	 i:5 	 global-step:10605	 l-p:0.13847428560256958
epoch£º530	 i:6 	 global-step:10606	 l-p:0.1688343733549118
epoch£º530	 i:7 	 global-step:10607	 l-p:0.06553055346012115
epoch£º530	 i:8 	 global-step:10608	 l-p:0.13849975168704987
epoch£º530	 i:9 	 global-step:10609	 l-p:0.16515952348709106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5044, 3.4935, 3.5037],
        [3.5044, 3.5044, 3.5044],
        [3.5044, 3.5044, 3.5044],
        [3.5044, 3.5044, 3.5044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.14258314669132233 
model_pd.l_d.mean(): -22.792016983032227 
model_pd.lagr.mean(): -22.649433135986328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2556], device='cuda:0')), ('power', tensor([-23.0476], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.14258314669132233
epoch£º531	 i:1 	 global-step:10621	 l-p:0.1247897818684578
epoch£º531	 i:2 	 global-step:10622	 l-p:0.13080400228500366
epoch£º531	 i:3 	 global-step:10623	 l-p:0.13005074858665466
epoch£º531	 i:4 	 global-step:10624	 l-p:0.13868515193462372
epoch£º531	 i:5 	 global-step:10625	 l-p:0.15819184482097626
epoch£º531	 i:6 	 global-step:10626	 l-p:0.14093084633350372
epoch£º531	 i:7 	 global-step:10627	 l-p:0.1327478438615799
epoch£º531	 i:8 	 global-step:10628	 l-p:0.1260172575712204
epoch£º531	 i:9 	 global-step:10629	 l-p:0.13501188158988953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5682, 3.0360, 2.6379],
        [3.5682, 3.4594, 3.5307],
        [3.5682, 3.0517, 2.6027],
        [3.5682, 3.5622, 3.5679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.1579040288925171 
model_pd.l_d.mean(): -22.803312301635742 
model_pd.lagr.mean(): -22.645408630371094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2608], device='cuda:0')), ('power', tensor([-23.0641], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:0.1579040288925171
epoch£º532	 i:1 	 global-step:10641	 l-p:0.12414387613534927
epoch£º532	 i:2 	 global-step:10642	 l-p:0.17571498453617096
epoch£º532	 i:3 	 global-step:10643	 l-p:0.13150404393672943
epoch£º532	 i:4 	 global-step:10644	 l-p:0.16768699884414673
epoch£º532	 i:5 	 global-step:10645	 l-p:0.1397857517004013
epoch£º532	 i:6 	 global-step:10646	 l-p:0.12417858839035034
epoch£º532	 i:7 	 global-step:10647	 l-p:0.12391135096549988
epoch£º532	 i:8 	 global-step:10648	 l-p:0.10740048438310623
epoch£º532	 i:9 	 global-step:10649	 l-p:0.20506888628005981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5144, 3.0833, 2.5816],
        [3.5144, 3.5074, 3.5141],
        [3.5144, 3.4585, 3.5028],
        [3.5144, 3.1286, 3.1186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.13565705716609955 
model_pd.l_d.mean(): -23.018266677856445 
model_pd.lagr.mean(): -22.882610321044922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2555], device='cuda:0')), ('power', tensor([-23.2738], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.13565705716609955
epoch£º533	 i:1 	 global-step:10661	 l-p:0.12588441371917725
epoch£º533	 i:2 	 global-step:10662	 l-p:0.12533149123191833
epoch£º533	 i:3 	 global-step:10663	 l-p:0.16000160574913025
epoch£º533	 i:4 	 global-step:10664	 l-p:0.04601587355136871
epoch£º533	 i:5 	 global-step:10665	 l-p:0.15514136850833893
epoch£º533	 i:6 	 global-step:10666	 l-p:0.893155574798584
epoch£º533	 i:7 	 global-step:10667	 l-p:0.13706283271312714
epoch£º533	 i:8 	 global-step:10668	 l-p:0.163105309009552
epoch£º533	 i:9 	 global-step:10669	 l-p:0.0808994397521019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3983, 3.0183, 3.0252],
        [3.3983, 2.8546, 2.5829],
        [3.3983, 3.3805, 3.3966],
        [3.3983, 2.9471, 2.8692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.10570036619901657 
model_pd.l_d.mean(): -23.18008041381836 
model_pd.lagr.mean(): -23.07438087463379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3580], device='cuda:0')), ('power', tensor([-23.5381], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.10570036619901657
epoch£º534	 i:1 	 global-step:10681	 l-p:0.1560603678226471
epoch£º534	 i:2 	 global-step:10682	 l-p:0.13854913413524628
epoch£º534	 i:3 	 global-step:10683	 l-p:0.14113877713680267
epoch£º534	 i:4 	 global-step:10684	 l-p:0.12227623909711838
epoch£º534	 i:5 	 global-step:10685	 l-p:-0.04073698818683624
epoch£º534	 i:6 	 global-step:10686	 l-p:9.767122268676758
epoch£º534	 i:7 	 global-step:10687	 l-p:0.12794388830661774
epoch£º534	 i:8 	 global-step:10688	 l-p:0.16486896574497223
epoch£º534	 i:9 	 global-step:10689	 l-p:0.1691630482673645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4195, 3.4190, 3.4195],
        [3.4195, 3.3730, 3.4112],
        [3.4195, 3.4194, 3.4195],
        [3.4195, 3.3149, 3.3855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.18304981291294098 
model_pd.l_d.mean(): -23.346845626831055 
model_pd.lagr.mean(): -23.163795471191406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2505], device='cuda:0')), ('power', tensor([-23.5973], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:0.18304981291294098
epoch£º535	 i:1 	 global-step:10701	 l-p:0.14610476791858673
epoch£º535	 i:2 	 global-step:10702	 l-p:0.14041633903980255
epoch£º535	 i:3 	 global-step:10703	 l-p:0.14718657732009888
epoch£º535	 i:4 	 global-step:10704	 l-p:0.10364770889282227
epoch£º535	 i:5 	 global-step:10705	 l-p:0.12953568994998932
epoch£º535	 i:6 	 global-step:10706	 l-p:0.17673645913600922
epoch£º535	 i:7 	 global-step:10707	 l-p:0.14655183255672455
epoch£º535	 i:8 	 global-step:10708	 l-p:0.16837340593338013
epoch£º535	 i:9 	 global-step:10709	 l-p:0.12638653814792633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6479, 3.6246, 3.6452],
        [3.6479, 3.6366, 3.6471],
        [3.6479, 3.6266, 3.6456],
        [3.6479, 3.1594, 2.6843]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.12520533800125122 
model_pd.l_d.mean(): -22.86975860595703 
model_pd.lagr.mean(): -22.744552612304688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2436], device='cuda:0')), ('power', tensor([-23.1133], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.12520533800125122
epoch£º536	 i:1 	 global-step:10721	 l-p:0.15589937567710876
epoch£º536	 i:2 	 global-step:10722	 l-p:0.13015666604042053
epoch£º536	 i:3 	 global-step:10723	 l-p:0.1305966079235077
epoch£º536	 i:4 	 global-step:10724	 l-p:0.11927445232868195
epoch£º536	 i:5 	 global-step:10725	 l-p:0.1387280374765396
epoch£º536	 i:6 	 global-step:10726	 l-p:0.11953970044851303
epoch£º536	 i:7 	 global-step:10727	 l-p:0.13408932089805603
epoch£º536	 i:8 	 global-step:10728	 l-p:0.12677626311779022
epoch£º536	 i:9 	 global-step:10729	 l-p:0.17775768041610718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5018, 3.5018, 3.5018],
        [3.5018, 3.0921, 3.0617],
        [3.5018, 3.1087, 3.0970],
        [3.5018, 3.5019, 3.5018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.2156190425157547 
model_pd.l_d.mean(): -23.650657653808594 
model_pd.lagr.mean(): -23.435039520263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1496], device='cuda:0')), ('power', tensor([-23.8003], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.2156190425157547
epoch£º537	 i:1 	 global-step:10741	 l-p:0.13133245706558228
epoch£º537	 i:2 	 global-step:10742	 l-p:0.19883298873901367
epoch£º537	 i:3 	 global-step:10743	 l-p:0.05338354781270027
epoch£º537	 i:4 	 global-step:10744	 l-p:0.1494639813899994
epoch£º537	 i:5 	 global-step:10745	 l-p:0.14213360846042633
epoch£º537	 i:6 	 global-step:10746	 l-p:0.1446014791727066
epoch£º537	 i:7 	 global-step:10747	 l-p:0.11693677306175232
epoch£º537	 i:8 	 global-step:10748	 l-p:0.14851827919483185
epoch£º537	 i:9 	 global-step:10749	 l-p:-0.06176698952913284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4141, 3.3375, 3.3946],
        [3.4141, 3.4130, 3.4141],
        [3.4141, 3.4140, 3.4141],
        [3.4141, 3.2873, 3.3664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.15434572100639343 
model_pd.l_d.mean(): -23.598176956176758 
model_pd.lagr.mean(): -23.443830490112305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2428], device='cuda:0')), ('power', tensor([-23.8410], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.15434572100639343
epoch£º538	 i:1 	 global-step:10761	 l-p:0.0945468321442604
epoch£º538	 i:2 	 global-step:10762	 l-p:0.1593829244375229
epoch£º538	 i:3 	 global-step:10763	 l-p:0.12869690358638763
epoch£º538	 i:4 	 global-step:10764	 l-p:0.1691688448190689
epoch£º538	 i:5 	 global-step:10765	 l-p:0.09911216795444489
epoch£º538	 i:6 	 global-step:10766	 l-p:0.15559326112270355
epoch£º538	 i:7 	 global-step:10767	 l-p:0.0595242865383625
epoch£º538	 i:8 	 global-step:10768	 l-p:0.1838923543691635
epoch£º538	 i:9 	 global-step:10769	 l-p:0.11297255009412766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3890, 3.1667, 3.2571],
        [3.3890, 3.3890, 3.3890],
        [3.3890, 3.3597, 3.3852],
        [3.3890, 3.3889, 3.3890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.15757255256175995 
model_pd.l_d.mean(): -23.513586044311523 
model_pd.lagr.mean(): -23.356014251708984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2816], device='cuda:0')), ('power', tensor([-23.7952], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.15757255256175995
epoch£º539	 i:1 	 global-step:10781	 l-p:0.07299109548330307
epoch£º539	 i:2 	 global-step:10782	 l-p:0.14612866938114166
epoch£º539	 i:3 	 global-step:10783	 l-p:0.18117694556713104
epoch£º539	 i:4 	 global-step:10784	 l-p:0.1323881447315216
epoch£º539	 i:5 	 global-step:10785	 l-p:0.13172119855880737
epoch£º539	 i:6 	 global-step:10786	 l-p:0.11795459687709808
epoch£º539	 i:7 	 global-step:10787	 l-p:0.12052421271800995
epoch£º539	 i:8 	 global-step:10788	 l-p:0.15798461437225342
epoch£º539	 i:9 	 global-step:10789	 l-p:0.14749309420585632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5837, 3.0710, 2.8502],
        [3.5837, 3.5837, 3.5837],
        [3.5837, 3.2910, 3.3550],
        [3.5837, 3.0760, 2.5934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.13732942938804626 
model_pd.l_d.mean(): -22.712604522705078 
model_pd.lagr.mean(): -22.575275421142578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3328], device='cuda:0')), ('power', tensor([-23.0454], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.13732942938804626
epoch£º540	 i:1 	 global-step:10801	 l-p:0.0517243854701519
epoch£º540	 i:2 	 global-step:10802	 l-p:0.13461677730083466
epoch£º540	 i:3 	 global-step:10803	 l-p:0.1258060485124588
epoch£º540	 i:4 	 global-step:10804	 l-p:0.1614508181810379
epoch£º540	 i:5 	 global-step:10805	 l-p:0.1263444721698761
epoch£º540	 i:6 	 global-step:10806	 l-p:0.1312638372182846
epoch£º540	 i:7 	 global-step:10807	 l-p:0.13533170521259308
epoch£º540	 i:8 	 global-step:10808	 l-p:0.14358574151992798
epoch£º540	 i:9 	 global-step:10809	 l-p:0.14695902168750763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4210, 3.3355, 3.3974],
        [3.4210, 3.3537, 3.4055],
        [3.4210, 3.0812, 2.5826],
        [3.4210, 3.4210, 3.4210]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.12345302104949951 
model_pd.l_d.mean(): -22.656963348388672 
model_pd.lagr.mean(): -22.533510208129883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2956], device='cuda:0')), ('power', tensor([-22.9525], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.12345302104949951
epoch£º541	 i:1 	 global-step:10821	 l-p:0.15373794734477997
epoch£º541	 i:2 	 global-step:10822	 l-p:-0.14293424785137177
epoch£º541	 i:3 	 global-step:10823	 l-p:0.09048761427402496
epoch£º541	 i:4 	 global-step:10824	 l-p:0.16332006454467773
epoch£º541	 i:5 	 global-step:10825	 l-p:0.13604648411273956
epoch£º541	 i:6 	 global-step:10826	 l-p:0.10494831949472427
epoch£º541	 i:7 	 global-step:10827	 l-p:-17.260583877563477
epoch£º541	 i:8 	 global-step:10828	 l-p:0.1090487614274025
epoch£º541	 i:9 	 global-step:10829	 l-p:1.9632606506347656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5224, 3.5224, 3.5224],
        [3.5224, 3.0217, 2.8536],
        [3.5224, 3.3341, 2.8719],
        [3.5224, 2.9805, 2.7046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.12257417291402817 
model_pd.l_d.mean(): -22.25479507446289 
model_pd.lagr.mean(): -22.132221221923828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3211], device='cuda:0')), ('power', tensor([-22.5759], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.12257417291402817
epoch£º542	 i:1 	 global-step:10841	 l-p:0.13723041117191315
epoch£º542	 i:2 	 global-step:10842	 l-p:0.12558794021606445
epoch£º542	 i:3 	 global-step:10843	 l-p:0.016352210193872452
epoch£º542	 i:4 	 global-step:10844	 l-p:0.12761802971363068
epoch£º542	 i:5 	 global-step:10845	 l-p:0.12384576350450516
epoch£º542	 i:6 	 global-step:10846	 l-p:0.1553080677986145
epoch£º542	 i:7 	 global-step:10847	 l-p:0.20167438685894012
epoch£º542	 i:8 	 global-step:10848	 l-p:0.14502930641174316
epoch£º542	 i:9 	 global-step:10849	 l-p:0.12223309278488159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5421, 3.0050, 2.7432],
        [3.5421, 3.3151, 2.8394],
        [3.5421, 3.3559, 3.4459],
        [3.5421, 3.5392, 3.5420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.12592713534832 
model_pd.l_d.mean(): -23.19283676147461 
model_pd.lagr.mean(): -23.066909790039062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2468], device='cuda:0')), ('power', tensor([-23.4396], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.12592713534832
epoch£º543	 i:1 	 global-step:10861	 l-p:0.2873993515968323
epoch£º543	 i:2 	 global-step:10862	 l-p:0.18686723709106445
epoch£º543	 i:3 	 global-step:10863	 l-p:16.286569595336914
epoch£º543	 i:4 	 global-step:10864	 l-p:0.12376488745212555
epoch£º543	 i:5 	 global-step:10865	 l-p:0.11674797534942627
epoch£º543	 i:6 	 global-step:10866	 l-p:0.16837014257907867
epoch£º543	 i:7 	 global-step:10867	 l-p:0.17265832424163818
epoch£º543	 i:8 	 global-step:10868	 l-p:0.13688375055789948
epoch£º543	 i:9 	 global-step:10869	 l-p:0.14462928473949432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4658, 3.4634, 3.4658],
        [3.4658, 2.9022, 2.5811],
        [3.4658, 3.4658, 3.4658],
        [3.4658, 3.0672, 2.5536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.02093590423464775 
model_pd.l_d.mean(): -23.519390106201172 
model_pd.lagr.mean(): -23.498455047607422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2215], device='cuda:0')), ('power', tensor([-23.7409], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.02093590423464775
epoch£º544	 i:1 	 global-step:10881	 l-p:0.1354115605354309
epoch£º544	 i:2 	 global-step:10882	 l-p:0.17884601652622223
epoch£º544	 i:3 	 global-step:10883	 l-p:0.1004493311047554
epoch£º544	 i:4 	 global-step:10884	 l-p:0.1319916695356369
epoch£º544	 i:5 	 global-step:10885	 l-p:0.1914108544588089
epoch£º544	 i:6 	 global-step:10886	 l-p:0.14372581243515015
epoch£º544	 i:7 	 global-step:10887	 l-p:0.16259220242500305
epoch£º544	 i:8 	 global-step:10888	 l-p:0.16930440068244934
epoch£º544	 i:9 	 global-step:10889	 l-p:0.11951015144586563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6203, 3.5949, 3.6173],
        [3.6203, 3.5156, 3.5861],
        [3.6203, 3.2865, 3.3276],
        [3.6203, 3.2965, 3.3444]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.14163795113563538 
model_pd.l_d.mean(): -23.607812881469727 
model_pd.lagr.mean(): -23.466175079345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1669], device='cuda:0')), ('power', tensor([-23.7747], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.14163795113563538
epoch£º545	 i:1 	 global-step:10901	 l-p:0.12886996567249298
epoch£º545	 i:2 	 global-step:10902	 l-p:0.12728461623191833
epoch£º545	 i:3 	 global-step:10903	 l-p:0.6262556314468384
epoch£º545	 i:4 	 global-step:10904	 l-p:0.16362982988357544
epoch£º545	 i:5 	 global-step:10905	 l-p:0.13885630667209625
epoch£º545	 i:6 	 global-step:10906	 l-p:0.1393352895975113
epoch£º545	 i:7 	 global-step:10907	 l-p:0.1508713811635971
epoch£º545	 i:8 	 global-step:10908	 l-p:0.12304026633501053
epoch£º545	 i:9 	 global-step:10909	 l-p:0.11610999703407288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5172, 3.4566, 3.5043],
        [3.5172, 3.4593, 3.5052],
        [3.5172, 3.5169, 3.5172],
        [3.5172, 3.0190, 2.8686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.13468879461288452 
model_pd.l_d.mean(): -23.046350479125977 
model_pd.lagr.mean(): -22.91166114807129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1881], device='cuda:0')), ('power', tensor([-23.2344], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.13468879461288452
epoch£º546	 i:1 	 global-step:10921	 l-p:0.14664076268672943
epoch£º546	 i:2 	 global-step:10922	 l-p:0.14753639698028564
epoch£º546	 i:3 	 global-step:10923	 l-p:0.13513727486133575
epoch£º546	 i:4 	 global-step:10924	 l-p:0.09350572526454926
epoch£º546	 i:5 	 global-step:10925	 l-p:0.08448873460292816
epoch£º546	 i:6 	 global-step:10926	 l-p:0.17959345877170563
epoch£º546	 i:7 	 global-step:10927	 l-p:0.1850215047597885
epoch£º546	 i:8 	 global-step:10928	 l-p:0.14421594142913818
epoch£º546	 i:9 	 global-step:10929	 l-p:0.17824363708496094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4741, 3.2183, 3.3040],
        [3.4741, 3.1497, 2.6472],
        [3.4741, 3.4719, 3.4740],
        [3.4741, 3.4741, 3.4741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.1272474229335785 
model_pd.l_d.mean(): -23.49657440185547 
model_pd.lagr.mean(): -23.369327545166016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2422], device='cuda:0')), ('power', tensor([-23.7388], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.1272474229335785
epoch£º547	 i:1 	 global-step:10941	 l-p:0.28423234820365906
epoch£º547	 i:2 	 global-step:10942	 l-p:0.11488967388868332
epoch£º547	 i:3 	 global-step:10943	 l-p:0.15230470895767212
epoch£º547	 i:4 	 global-step:10944	 l-p:0.12331648916006088
epoch£º547	 i:5 	 global-step:10945	 l-p:0.12965606153011322
epoch£º547	 i:6 	 global-step:10946	 l-p:0.11886005103588104
epoch£º547	 i:7 	 global-step:10947	 l-p:0.11775227636098862
epoch£º547	 i:8 	 global-step:10948	 l-p:0.12359238415956497
epoch£º547	 i:9 	 global-step:10949	 l-p:0.14287739992141724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6494, 3.6489, 3.6494],
        [3.6494, 3.4280, 3.5164],
        [3.6494, 3.5789, 3.6324],
        [3.6494, 3.5441, 3.6150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.1423122137784958 
model_pd.l_d.mean(): -22.770069122314453 
model_pd.lagr.mean(): -22.627756118774414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2398], device='cuda:0')), ('power', tensor([-23.0099], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.1423122137784958
epoch£º548	 i:1 	 global-step:10961	 l-p:0.12557558715343475
epoch£º548	 i:2 	 global-step:10962	 l-p:0.07655198872089386
epoch£º548	 i:3 	 global-step:10963	 l-p:0.12047486007213593
epoch£º548	 i:4 	 global-step:10964	 l-p:0.14332669973373413
epoch£º548	 i:5 	 global-step:10965	 l-p:0.02381172589957714
epoch£º548	 i:6 	 global-step:10966	 l-p:0.13320757448673248
epoch£º548	 i:7 	 global-step:10967	 l-p:0.12845054268836975
epoch£º548	 i:8 	 global-step:10968	 l-p:0.12231609225273132
epoch£º548	 i:9 	 global-step:10969	 l-p:0.24431350827217102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4246, 2.9937, 2.4728],
        [3.4246, 3.4136, 3.4239],
        [3.4246, 3.4072, 3.4231],
        [3.4246, 3.0174, 3.0133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): -178.54083251953125 
model_pd.l_d.mean(): -23.517518997192383 
model_pd.lagr.mean(): -202.058349609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2367], device='cuda:0')), ('power', tensor([-23.7542], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:-178.54083251953125
epoch£º549	 i:1 	 global-step:10981	 l-p:0.1332225203514099
epoch£º549	 i:2 	 global-step:10982	 l-p:0.15227122604846954
epoch£º549	 i:3 	 global-step:10983	 l-p:0.05988207831978798
epoch£º549	 i:4 	 global-step:10984	 l-p:0.006723961792886257
epoch£º549	 i:5 	 global-step:10985	 l-p:0.11063583195209503
epoch£º549	 i:6 	 global-step:10986	 l-p:0.1788184642791748
epoch£º549	 i:7 	 global-step:10987	 l-p:0.13788796961307526
epoch£º549	 i:8 	 global-step:10988	 l-p:0.18287654221057892
epoch£º549	 i:9 	 global-step:10989	 l-p:0.13868694007396698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6031, 3.5281, 3.5844],
        [3.6031, 3.0969, 2.9223],
        [3.6031, 3.1594, 3.0938],
        [3.6031, 3.4769, 3.5560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.12687402963638306 
model_pd.l_d.mean(): -22.257429122924805 
model_pd.lagr.mean(): -22.13055419921875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3160], device='cuda:0')), ('power', tensor([-22.5735], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.12687402963638306
epoch£º550	 i:1 	 global-step:11001	 l-p:0.9303781390190125
epoch£º550	 i:2 	 global-step:11002	 l-p:0.13379834592342377
epoch£º550	 i:3 	 global-step:11003	 l-p:0.15694420039653778
epoch£º550	 i:4 	 global-step:11004	 l-p:0.11382868140935898
epoch£º550	 i:5 	 global-step:11005	 l-p:0.12748554348945618
epoch£º550	 i:6 	 global-step:11006	 l-p:0.1477384865283966
epoch£º550	 i:7 	 global-step:11007	 l-p:0.1277300864458084
epoch£º550	 i:8 	 global-step:11008	 l-p:0.12815053761005402
epoch£º550	 i:9 	 global-step:11009	 l-p:0.10787266492843628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5151, 3.2256, 3.3008],
        [3.5151, 2.9702, 2.4711],
        [3.5151, 3.2586, 2.7691],
        [3.5151, 3.5151, 3.5151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.1241280734539032 
model_pd.l_d.mean(): -22.023895263671875 
model_pd.lagr.mean(): -21.89976692199707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3552], device='cuda:0')), ('power', tensor([-22.3791], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.1241280734539032
epoch£º551	 i:1 	 global-step:11021	 l-p:0.13807432353496552
epoch£º551	 i:2 	 global-step:11022	 l-p:0.22421298921108246
epoch£º551	 i:3 	 global-step:11023	 l-p:0.15792876482009888
epoch£º551	 i:4 	 global-step:11024	 l-p:0.047629617154598236
epoch£º551	 i:5 	 global-step:11025	 l-p:0.15566377341747284
epoch£º551	 i:6 	 global-step:11026	 l-p:0.14071428775787354
epoch£º551	 i:7 	 global-step:11027	 l-p:0.12294934689998627
epoch£º551	 i:8 	 global-step:11028	 l-p:0.26357218623161316
epoch£º551	 i:9 	 global-step:11029	 l-p:0.1335972398519516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4665, 2.9389, 2.7564],
        [3.4665, 3.4664, 3.4665],
        [3.4665, 3.4665, 3.4665],
        [3.4665, 3.4665, 3.4665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.33415621519088745 
model_pd.l_d.mean(): -23.10556411743164 
model_pd.lagr.mean(): -22.771408081054688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2859], device='cuda:0')), ('power', tensor([-23.3914], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.33415621519088745
epoch£º552	 i:1 	 global-step:11041	 l-p:0.1635567992925644
epoch£º552	 i:2 	 global-step:11042	 l-p:0.284940242767334
epoch£º552	 i:3 	 global-step:11043	 l-p:0.10286689549684525
epoch£º552	 i:4 	 global-step:11044	 l-p:0.1307852417230606
epoch£º552	 i:5 	 global-step:11045	 l-p:0.1453041434288025
epoch£º552	 i:6 	 global-step:11046	 l-p:0.14406512677669525
epoch£º552	 i:7 	 global-step:11047	 l-p:0.13567113876342773
epoch£º552	 i:8 	 global-step:11048	 l-p:0.12311439961194992
epoch£º552	 i:9 	 global-step:11049	 l-p:0.12227833271026611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6414, 3.6414, 3.6414],
        [3.6414, 3.1275, 2.9359],
        [3.6414, 3.3940, 2.9027],
        [3.6414, 3.5408, 3.6100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.13857552409172058 
model_pd.l_d.mean(): -23.295839309692383 
model_pd.lagr.mean(): -23.157264709472656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2466], device='cuda:0')), ('power', tensor([-23.5425], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.13857552409172058
epoch£º553	 i:1 	 global-step:11061	 l-p:0.13477939367294312
epoch£º553	 i:2 	 global-step:11062	 l-p:0.07278654724359512
epoch£º553	 i:3 	 global-step:11063	 l-p:0.14284761250019073
epoch£º553	 i:4 	 global-step:11064	 l-p:0.12894095480442047
epoch£º553	 i:5 	 global-step:11065	 l-p:2.8845126628875732
epoch£º553	 i:6 	 global-step:11066	 l-p:0.1369750052690506
epoch£º553	 i:7 	 global-step:11067	 l-p:0.13185110688209534
epoch£º553	 i:8 	 global-step:11068	 l-p:0.06112203001976013
epoch£º553	 i:9 	 global-step:11069	 l-p:0.19150468707084656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4025, 3.3543, 3.3940],
        [3.4025, 3.4006, 3.4025],
        [3.4025, 3.3423, 3.3901],
        [3.4025, 2.8030, 2.4226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.0507504865527153 
model_pd.l_d.mean(): -23.171117782592773 
model_pd.lagr.mean(): -23.1203670501709 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2573], device='cuda:0')), ('power', tensor([-23.4285], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.0507504865527153
epoch£º554	 i:1 	 global-step:11081	 l-p:0.13974295556545258
epoch£º554	 i:2 	 global-step:11082	 l-p:0.09625802934169769
epoch£º554	 i:3 	 global-step:11083	 l-p:0.11716840416193008
epoch£º554	 i:4 	 global-step:11084	 l-p:0.14854420721530914
epoch£º554	 i:5 	 global-step:11085	 l-p:0.18275554478168488
epoch£º554	 i:6 	 global-step:11086	 l-p:0.12723271548748016
epoch£º554	 i:7 	 global-step:11087	 l-p:0.7375628352165222
epoch£º554	 i:8 	 global-step:11088	 l-p:0.1263105571269989
epoch£º554	 i:9 	 global-step:11089	 l-p:0.13765369355678558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5556, 3.5555, 3.5556],
        [3.5556, 3.3692, 3.4617],
        [3.5556, 3.5271, 3.5520],
        [3.5556, 3.3256, 3.4170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.1379920095205307 
model_pd.l_d.mean(): -23.417037963867188 
model_pd.lagr.mean(): -23.27904510498047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2027], device='cuda:0')), ('power', tensor([-23.6198], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.1379920095205307
epoch£º555	 i:1 	 global-step:11101	 l-p:0.1838853359222412
epoch£º555	 i:2 	 global-step:11102	 l-p:0.1087941899895668
epoch£º555	 i:3 	 global-step:11103	 l-p:0.12690284848213196
epoch£º555	 i:4 	 global-step:11104	 l-p:0.1945982575416565
epoch£º555	 i:5 	 global-step:11105	 l-p:0.15322285890579224
epoch£º555	 i:6 	 global-step:11106	 l-p:0.14604869484901428
epoch£º555	 i:7 	 global-step:11107	 l-p:0.3303951621055603
epoch£º555	 i:8 	 global-step:11108	 l-p:0.11433292180299759
epoch£º555	 i:9 	 global-step:11109	 l-p:0.127141073346138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6635, 3.1889, 3.0783],
        [3.6635, 3.6635, 3.6635],
        [3.6635, 3.1136, 2.6427],
        [3.6635, 3.6595, 3.6633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.14755047857761383 
model_pd.l_d.mean(): -23.51561737060547 
model_pd.lagr.mean(): -23.368066787719727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1698], device='cuda:0')), ('power', tensor([-23.6854], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.14755047857761383
epoch£º556	 i:1 	 global-step:11121	 l-p:0.13048776984214783
epoch£º556	 i:2 	 global-step:11122	 l-p:0.13735230267047882
epoch£º556	 i:3 	 global-step:11123	 l-p:0.1239861249923706
epoch£º556	 i:4 	 global-step:11124	 l-p:0.14612585306167603
epoch£º556	 i:5 	 global-step:11125	 l-p:0.16437622904777527
epoch£º556	 i:6 	 global-step:11126	 l-p:0.13479694724082947
epoch£º556	 i:7 	 global-step:11127	 l-p:0.14273762702941895
epoch£º556	 i:8 	 global-step:11128	 l-p:0.12827549874782562
epoch£º556	 i:9 	 global-step:11129	 l-p:0.16765350103378296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3909, 3.3678, 3.3885],
        [3.3909, 2.8066, 2.5203],
        [3.3909, 3.2468, 3.3336],
        [3.3909, 2.7926, 2.3092]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.06254377216100693 
model_pd.l_d.mean(): -23.21780776977539 
model_pd.lagr.mean(): -23.155263900756836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3767], device='cuda:0')), ('power', tensor([-23.5945], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.06254377216100693
epoch£º557	 i:1 	 global-step:11141	 l-p:0.15116465091705322
epoch£º557	 i:2 	 global-step:11142	 l-p:0.13589896261692047
epoch£º557	 i:3 	 global-step:11143	 l-p:0.1474733054637909
epoch£º557	 i:4 	 global-step:11144	 l-p:0.1495329886674881
epoch£º557	 i:5 	 global-step:11145	 l-p:0.14978623390197754
epoch£º557	 i:6 	 global-step:11146	 l-p:0.19187338650226593
epoch£º557	 i:7 	 global-step:11147	 l-p:0.13242195546627045
epoch£º557	 i:8 	 global-step:11148	 l-p:0.11757972091436386
epoch£º557	 i:9 	 global-step:11149	 l-p:0.12074007093906403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5506, 3.0789, 2.9942],
        [3.5506, 2.9697, 2.5112],
        [3.5506, 3.4849, 3.5360],
        [3.5506, 3.5357, 3.5494]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.2445998340845108 
model_pd.l_d.mean(): -23.105113983154297 
model_pd.lagr.mean(): -22.86051368713379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2399], device='cuda:0')), ('power', tensor([-23.3450], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.2445998340845108
epoch£º558	 i:1 	 global-step:11161	 l-p:0.17200133204460144
epoch£º558	 i:2 	 global-step:11162	 l-p:0.11709848046302795
epoch£º558	 i:3 	 global-step:11163	 l-p:0.12271317839622498
epoch£º558	 i:4 	 global-step:11164	 l-p:0.14884021878242493
epoch£º558	 i:5 	 global-step:11165	 l-p:0.12336593121290207
epoch£º558	 i:6 	 global-step:11166	 l-p:0.11898208409547806
epoch£º558	 i:7 	 global-step:11167	 l-p:0.1292060762643814
epoch£º558	 i:8 	 global-step:11168	 l-p:0.04893346503376961
epoch£º558	 i:9 	 global-step:11169	 l-p:0.13144531846046448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5833, 3.0013, 2.6219],
        [3.5833, 3.5636, 3.5814],
        [3.5833, 3.5833, 3.5833],
        [3.5833, 3.5819, 3.5833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.12035883963108063 
model_pd.l_d.mean(): -23.095600128173828 
model_pd.lagr.mean(): -22.97524070739746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3190], device='cuda:0')), ('power', tensor([-23.4146], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.12035883963108063
epoch£º559	 i:1 	 global-step:11181	 l-p:0.13694831728935242
epoch£º559	 i:2 	 global-step:11182	 l-p:0.12140313535928726
epoch£º559	 i:3 	 global-step:11183	 l-p:0.15020059049129486
epoch£º559	 i:4 	 global-step:11184	 l-p:0.1520535945892334
epoch£º559	 i:5 	 global-step:11185	 l-p:0.10751388221979141
epoch£º559	 i:6 	 global-step:11186	 l-p:0.13496309518814087
epoch£º559	 i:7 	 global-step:11187	 l-p:0.15425415337085724
epoch£º559	 i:8 	 global-step:11188	 l-p:-0.04238508269190788
epoch£º559	 i:9 	 global-step:11189	 l-p:0.12520894408226013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[3.4337, 3.1730, 2.6812],
        [3.4337, 2.8794, 2.6701],
        [3.4337, 3.1775, 2.6870],
        [3.4337, 2.8342, 2.3543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.12396646291017532 
model_pd.l_d.mean(): -22.359224319458008 
model_pd.lagr.mean(): -22.235258102416992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3568], device='cuda:0')), ('power', tensor([-22.7160], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.12396646291017532
epoch£º560	 i:1 	 global-step:11201	 l-p:0.210545614361763
epoch£º560	 i:2 	 global-step:11202	 l-p:0.058207638561725616
epoch£º560	 i:3 	 global-step:11203	 l-p:0.20470328629016876
epoch£º560	 i:4 	 global-step:11204	 l-p:0.15713229775428772
epoch£º560	 i:5 	 global-step:11205	 l-p:0.1168350800871849
epoch£º560	 i:6 	 global-step:11206	 l-p:0.14028331637382507
epoch£º560	 i:7 	 global-step:11207	 l-p:0.15036384761333466
epoch£º560	 i:8 	 global-step:11208	 l-p:0.15224707126617432
epoch£º560	 i:9 	 global-step:11209	 l-p:0.1274464875459671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6079, 3.6079, 3.6079],
        [3.6079, 3.0892, 2.9179],
        [3.6079, 3.5876, 3.6059],
        [3.6079, 3.0428, 2.5571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.16055157780647278 
model_pd.l_d.mean(): -22.97752571105957 
model_pd.lagr.mean(): -22.816974639892578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2100], device='cuda:0')), ('power', tensor([-23.1876], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.16055157780647278
epoch£º561	 i:1 	 global-step:11221	 l-p:0.1361481249332428
epoch£º561	 i:2 	 global-step:11222	 l-p:0.15068192780017853
epoch£º561	 i:3 	 global-step:11223	 l-p:0.12190230935811996
epoch£º561	 i:4 	 global-step:11224	 l-p:0.13727958500385284
epoch£º561	 i:5 	 global-step:11225	 l-p:0.13216175138950348
epoch£º561	 i:6 	 global-step:11226	 l-p:0.10170027613639832
epoch£º561	 i:7 	 global-step:11227	 l-p:0.15100206434726715
epoch£º561	 i:8 	 global-step:11228	 l-p:0.04827830195426941
epoch£º561	 i:9 	 global-step:11229	 l-p:0.38000616431236267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4786, 3.3889, 3.4539],
        [3.4786, 3.4774, 3.4786],
        [3.4786, 3.4785, 3.4786],
        [3.4786, 2.8821, 2.5320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.15520302951335907 
model_pd.l_d.mean(): -23.126649856567383 
model_pd.lagr.mean(): -22.971446990966797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2382], device='cuda:0')), ('power', tensor([-23.3649], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.15520302951335907
epoch£º562	 i:1 	 global-step:11241	 l-p:0.13989663124084473
epoch£º562	 i:2 	 global-step:11242	 l-p:0.1709861308336258
epoch£º562	 i:3 	 global-step:11243	 l-p:-0.05351874232292175
epoch£º562	 i:4 	 global-step:11244	 l-p:0.1134902834892273
epoch£º562	 i:5 	 global-step:11245	 l-p:0.1361309140920639
epoch£º562	 i:6 	 global-step:11246	 l-p:0.13908270001411438
epoch£º562	 i:7 	 global-step:11247	 l-p:0.1271929293870926
epoch£º562	 i:8 	 global-step:11248	 l-p:0.22154052555561066
epoch£º562	 i:9 	 global-step:11249	 l-p:0.2357860505580902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5694, 3.5540, 3.5682],
        [3.5694, 3.3184, 3.4088],
        [3.5694, 3.4011, 3.4926],
        [3.5694, 3.4952, 3.5515]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.14997874200344086 
model_pd.l_d.mean(): -23.523075103759766 
model_pd.lagr.mean(): -23.373096466064453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2153], device='cuda:0')), ('power', tensor([-23.7383], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.14997874200344086
epoch£º563	 i:1 	 global-step:11261	 l-p:0.131215438246727
epoch£º563	 i:2 	 global-step:11262	 l-p:0.1419331282377243
epoch£º563	 i:3 	 global-step:11263	 l-p:0.13679909706115723
epoch£º563	 i:4 	 global-step:11264	 l-p:0.1411251276731491
epoch£º563	 i:5 	 global-step:11265	 l-p:0.176502525806427
epoch£º563	 i:6 	 global-step:11266	 l-p:0.12962083518505096
epoch£º563	 i:7 	 global-step:11267	 l-p:0.13636459410190582
epoch£º563	 i:8 	 global-step:11268	 l-p:0.1555897742509842
epoch£º563	 i:9 	 global-step:11269	 l-p:0.11305619776248932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[3.5781, 3.0030, 2.6985],
        [3.5781, 2.9990, 2.5185],
        [3.5781, 3.1025, 3.0169],
        [3.5781, 3.2900, 3.3699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.14051981270313263 
model_pd.l_d.mean(): -22.961301803588867 
model_pd.lagr.mean(): -22.820781707763672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2203], device='cuda:0')), ('power', tensor([-23.1816], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.14051981270313263
epoch£º564	 i:1 	 global-step:11281	 l-p:0.14801454544067383
epoch£º564	 i:2 	 global-step:11282	 l-p:0.13284876942634583
epoch£º564	 i:3 	 global-step:11283	 l-p:0.13025717437267303
epoch£º564	 i:4 	 global-step:11284	 l-p:0.13050144910812378
epoch£º564	 i:5 	 global-step:11285	 l-p:-0.11621414124965668
epoch£º564	 i:6 	 global-step:11286	 l-p:0.13071222603321075
epoch£º564	 i:7 	 global-step:11287	 l-p:0.1326582282781601
epoch£º564	 i:8 	 global-step:11288	 l-p:0.12944911420345306
epoch£º564	 i:9 	 global-step:11289	 l-p:0.1146368607878685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3546, 3.3438, 3.3539],
        [3.3546, 3.0960, 3.1908],
        [3.3546, 3.0166, 2.5096],
        [3.3546, 3.0201, 2.5139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.23570118844509125 
model_pd.l_d.mean(): -23.276758193969727 
model_pd.lagr.mean(): -23.041057586669922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3478], device='cuda:0')), ('power', tensor([-23.6246], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.23570118844509125
epoch£º565	 i:1 	 global-step:11301	 l-p:0.14111454784870148
epoch£º565	 i:2 	 global-step:11302	 l-p:0.1737603396177292
epoch£º565	 i:3 	 global-step:11303	 l-p:0.14694522321224213
epoch£º565	 i:4 	 global-step:11304	 l-p:0.12743420898914337
epoch£º565	 i:5 	 global-step:11305	 l-p:0.1416274607181549
epoch£º565	 i:6 	 global-step:11306	 l-p:0.14465811848640442
epoch£º565	 i:7 	 global-step:11307	 l-p:0.14204050600528717
epoch£º565	 i:8 	 global-step:11308	 l-p:0.21318136155605316
epoch£º565	 i:9 	 global-step:11309	 l-p:0.14884929358959198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5826, 3.5539, 3.5791],
        [3.5826, 3.5791, 3.5825],
        [3.5826, 3.3420, 3.4349],
        [3.5826, 3.4697, 3.5453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.13918109238147736 
model_pd.l_d.mean(): -23.34621810913086 
model_pd.lagr.mean(): -23.2070369720459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2167], device='cuda:0')), ('power', tensor([-23.5629], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.13918109238147736
epoch£º566	 i:1 	 global-step:11321	 l-p:0.11431538313627243
epoch£º566	 i:2 	 global-step:11322	 l-p:0.14179599285125732
epoch£º566	 i:3 	 global-step:11323	 l-p:0.1353192925453186
epoch£º566	 i:4 	 global-step:11324	 l-p:0.13382136821746826
epoch£º566	 i:5 	 global-step:11325	 l-p:0.1100853681564331
epoch£º566	 i:6 	 global-step:11326	 l-p:0.13986535370349884
epoch£º566	 i:7 	 global-step:11327	 l-p:0.16399826109409332
epoch£º566	 i:8 	 global-step:11328	 l-p:0.1916630119085312
epoch£º566	 i:9 	 global-step:11329	 l-p:0.13454574346542358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[3.5448, 3.2749, 2.7710],
        [3.5448, 3.2298, 2.7138],
        [3.5448, 3.2270, 3.2968],
        [3.5448, 3.1081, 2.5684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.14049887657165527 
model_pd.l_d.mean(): -22.259689331054688 
model_pd.lagr.mean(): -22.119190216064453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3555], device='cuda:0')), ('power', tensor([-22.6152], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.14049887657165527
epoch£º567	 i:1 	 global-step:11341	 l-p:0.14329646527767181
epoch£º567	 i:2 	 global-step:11342	 l-p:0.16423553228378296
epoch£º567	 i:3 	 global-step:11343	 l-p:0.14083196222782135
epoch£º567	 i:4 	 global-step:11344	 l-p:-0.25335508584976196
epoch£º567	 i:5 	 global-step:11345	 l-p:0.13663695752620697
epoch£º567	 i:6 	 global-step:11346	 l-p:-0.09135158360004425
epoch£º567	 i:7 	 global-step:11347	 l-p:0.1544083207845688
epoch£º567	 i:8 	 global-step:11348	 l-p:0.14707306027412415
epoch£º567	 i:9 	 global-step:11349	 l-p:0.06474819034337997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4758, 2.9226, 2.7242],
        [3.4758, 3.4558, 3.4739],
        [3.4758, 2.9366, 2.3974],
        [3.4758, 2.9055, 2.6676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.11271590739488602 
model_pd.l_d.mean(): -23.258180618286133 
model_pd.lagr.mean(): -23.145463943481445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3175], device='cuda:0')), ('power', tensor([-23.5757], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.11271590739488602
epoch£º568	 i:1 	 global-step:11361	 l-p:0.16954360902309418
epoch£º568	 i:2 	 global-step:11362	 l-p:0.12355095893144608
epoch£º568	 i:3 	 global-step:11363	 l-p:0.12343260645866394
epoch£º568	 i:4 	 global-step:11364	 l-p:0.09788081794977188
epoch£º568	 i:5 	 global-step:11365	 l-p:0.13270825147628784
epoch£º568	 i:6 	 global-step:11366	 l-p:0.14724914729595184
epoch£º568	 i:7 	 global-step:11367	 l-p:-0.0386296845972538
epoch£º568	 i:8 	 global-step:11368	 l-p:0.09729337692260742
epoch£º568	 i:9 	 global-step:11369	 l-p:0.09898602962493896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4584, 3.4584, 3.4585],
        [3.4584, 3.3860, 3.4416],
        [3.4584, 3.1464, 2.6357],
        [3.4584, 3.4584, 3.4585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.1337265968322754 
model_pd.l_d.mean(): -22.915390014648438 
model_pd.lagr.mean(): -22.78166389465332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3032], device='cuda:0')), ('power', tensor([-23.2186], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.1337265968322754
epoch£º569	 i:1 	 global-step:11381	 l-p:0.2288951426744461
epoch£º569	 i:2 	 global-step:11382	 l-p:0.07628237456083298
epoch£º569	 i:3 	 global-step:11383	 l-p:0.1295507550239563
epoch£º569	 i:4 	 global-step:11384	 l-p:0.2651440501213074
epoch£º569	 i:5 	 global-step:11385	 l-p:0.1338900625705719
epoch£º569	 i:6 	 global-step:11386	 l-p:0.132634699344635
epoch£º569	 i:7 	 global-step:11387	 l-p:0.14581647515296936
epoch£º569	 i:8 	 global-step:11388	 l-p:0.1510014533996582
epoch£º569	 i:9 	 global-step:11389	 l-p:0.1271301954984665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5215, 3.4990, 3.5191],
        [3.5215, 3.5215, 3.5215],
        [3.5215, 2.9875, 2.8226],
        [3.5215, 3.0514, 2.5074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.13543809950351715 
model_pd.l_d.mean(): -23.194561004638672 
model_pd.lagr.mean(): -23.05912208557129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2336], device='cuda:0')), ('power', tensor([-23.4281], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.13543809950351715
epoch£º570	 i:1 	 global-step:11401	 l-p:0.14505314826965332
epoch£º570	 i:2 	 global-step:11402	 l-p:0.3033287823200226
epoch£º570	 i:3 	 global-step:11403	 l-p:0.021917376667261124
epoch£º570	 i:4 	 global-step:11404	 l-p:0.14789979159832
epoch£º570	 i:5 	 global-step:11405	 l-p:0.2014269232749939
epoch£º570	 i:6 	 global-step:11406	 l-p:0.16155675053596497
epoch£º570	 i:7 	 global-step:11407	 l-p:0.15535956621170044
epoch£º570	 i:8 	 global-step:11408	 l-p:0.11200012266635895
epoch£º570	 i:9 	 global-step:11409	 l-p:0.1840769499540329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4308, 2.9845, 2.9623],
        [3.4308, 3.4308, 3.4308],
        [3.4308, 3.3625, 3.4157],
        [3.4308, 3.4192, 3.4301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.1846126914024353 
model_pd.l_d.mean(): -22.788291931152344 
model_pd.lagr.mean(): -22.603679656982422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3100], device='cuda:0')), ('power', tensor([-23.0983], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.1846126914024353
epoch£º571	 i:1 	 global-step:11421	 l-p:0.24581927061080933
epoch£º571	 i:2 	 global-step:11422	 l-p:0.11410949379205704
epoch£º571	 i:3 	 global-step:11423	 l-p:0.3921572268009186
epoch£º571	 i:4 	 global-step:11424	 l-p:0.12425149977207184
epoch£º571	 i:5 	 global-step:11425	 l-p:0.11966617405414581
epoch£º571	 i:6 	 global-step:11426	 l-p:0.13025985658168793
epoch£º571	 i:7 	 global-step:11427	 l-p:0.13881491124629974
epoch£º571	 i:8 	 global-step:11428	 l-p:0.12449987232685089
epoch£º571	 i:9 	 global-step:11429	 l-p:0.11466782540082932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6564, 3.6544, 3.6564],
        [3.6564, 3.4522, 2.9614],
        [3.6564, 3.3824, 2.8705],
        [3.6564, 3.6295, 3.6533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.11806215345859528 
model_pd.l_d.mean(): -22.878952026367188 
model_pd.lagr.mean(): -22.760889053344727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2489], device='cuda:0')), ('power', tensor([-23.1278], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.11806215345859528
epoch£º572	 i:1 	 global-step:11441	 l-p:0.14232447743415833
epoch£º572	 i:2 	 global-step:11442	 l-p:0.14575716853141785
epoch£º572	 i:3 	 global-step:11443	 l-p:0.17061856389045715
epoch£º572	 i:4 	 global-step:11444	 l-p:0.12149965018033981
epoch£º572	 i:5 	 global-step:11445	 l-p:0.128611758351326
epoch£º572	 i:6 	 global-step:11446	 l-p:0.15093646943569183
epoch£º572	 i:7 	 global-step:11447	 l-p:0.2046678513288498
epoch£º572	 i:8 	 global-step:11448	 l-p:0.10200405865907669
epoch£º572	 i:9 	 global-step:11449	 l-p:0.17129932343959808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4218, 3.2909, 3.3746],
        [3.4218, 3.0349, 3.0705],
        [3.4218, 3.2355, 3.3326],
        [3.4218, 3.4212, 3.4218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.16452975571155548 
model_pd.l_d.mean(): -22.93568229675293 
model_pd.lagr.mean(): -22.77115249633789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2808], device='cuda:0')), ('power', tensor([-23.2165], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.16452975571155548
epoch£º573	 i:1 	 global-step:11461	 l-p:0.1584983617067337
epoch£º573	 i:2 	 global-step:11462	 l-p:0.04440203681588173
epoch£º573	 i:3 	 global-step:11463	 l-p:0.016417136415839195
epoch£º573	 i:4 	 global-step:11464	 l-p:-0.01731979288160801
epoch£º573	 i:5 	 global-step:11465	 l-p:0.14518369734287262
epoch£º573	 i:6 	 global-step:11466	 l-p:0.1552209109067917
epoch£º573	 i:7 	 global-step:11467	 l-p:0.1272873729467392
epoch£º573	 i:8 	 global-step:11468	 l-p:0.1367766410112381
epoch£º573	 i:9 	 global-step:11469	 l-p:0.09258060902357101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6754, 3.1682, 3.0306],
        [3.6754, 3.1142, 2.6047],
        [3.6754, 3.6754, 3.6754],
        [3.6754, 3.6671, 3.6749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.12608398497104645 
model_pd.l_d.mean(): -23.33305549621582 
model_pd.lagr.mean(): -23.206972122192383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1701], device='cuda:0')), ('power', tensor([-23.5031], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.12608398497104645
epoch£º574	 i:1 	 global-step:11481	 l-p:0.1274585872888565
epoch£º574	 i:2 	 global-step:11482	 l-p:0.12703993916511536
epoch£º574	 i:3 	 global-step:11483	 l-p:0.0930633544921875
epoch£º574	 i:4 	 global-step:11484	 l-p:0.14111392199993134
epoch£º574	 i:5 	 global-step:11485	 l-p:0.14398451149463654
epoch£º574	 i:6 	 global-step:11486	 l-p:0.21338103711605072
epoch£º574	 i:7 	 global-step:11487	 l-p:0.2206556797027588
epoch£º574	 i:8 	 global-step:11488	 l-p:0.1423729807138443
epoch£º574	 i:9 	 global-step:11489	 l-p:0.114779993891716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5555, 3.5297, 3.5526],
        [3.5555, 3.1942, 3.2415],
        [3.5555, 3.5555, 3.5555],
        [3.5555, 3.5262, 3.5519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.2163304090499878 
model_pd.l_d.mean(): -23.404687881469727 
model_pd.lagr.mean(): -23.188358306884766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2726], device='cuda:0')), ('power', tensor([-23.6773], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.2163304090499878
epoch£º575	 i:1 	 global-step:11501	 l-p:0.2573697566986084
epoch£º575	 i:2 	 global-step:11502	 l-p:0.21175025403499603
epoch£º575	 i:3 	 global-step:11503	 l-p:0.11197655647993088
epoch£º575	 i:4 	 global-step:11504	 l-p:0.14542703330516815
epoch£º575	 i:5 	 global-step:11505	 l-p:0.13001693785190582
epoch£º575	 i:6 	 global-step:11506	 l-p:0.1277300864458084
epoch£º575	 i:7 	 global-step:11507	 l-p:0.1449119746685028
epoch£º575	 i:8 	 global-step:11508	 l-p:0.1909925788640976
epoch£º575	 i:9 	 global-step:11509	 l-p:0.14774596691131592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5261, 3.5254, 3.5261],
        [3.5261, 3.5187, 3.5257],
        [3.5261, 3.1832, 2.6579],
        [3.5261, 2.9118, 2.5095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 2.407007932662964 
model_pd.l_d.mean(): -23.36870574951172 
model_pd.lagr.mean(): -20.961698532104492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2574], device='cuda:0')), ('power', tensor([-23.6261], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:2.407007932662964
epoch£º576	 i:1 	 global-step:11521	 l-p:0.14396876096725464
epoch£º576	 i:2 	 global-step:11522	 l-p:0.13996604084968567
epoch£º576	 i:3 	 global-step:11523	 l-p:0.146410271525383
epoch£º576	 i:4 	 global-step:11524	 l-p:0.12983369827270508
epoch£º576	 i:5 	 global-step:11525	 l-p:0.13465900719165802
epoch£º576	 i:6 	 global-step:11526	 l-p:0.11380036920309067
epoch£º576	 i:7 	 global-step:11527	 l-p:0.16454005241394043
epoch£º576	 i:8 	 global-step:11528	 l-p:0.9071500897407532
epoch£º576	 i:9 	 global-step:11529	 l-p:0.1504840850830078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5162, 3.2726, 3.3690],
        [3.5162, 3.5162, 3.5162],
        [3.5162, 2.9294, 2.4048],
        [3.5162, 3.5162, 3.5162]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): -0.2511325180530548 
model_pd.l_d.mean(): -23.612140655517578 
model_pd.lagr.mean(): -23.86327362060547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2263], device='cuda:0')), ('power', tensor([-23.8385], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:-0.2511325180530548
epoch£º577	 i:1 	 global-step:11541	 l-p:0.15766891837120056
epoch£º577	 i:2 	 global-step:11542	 l-p:0.11433517187833786
epoch£º577	 i:3 	 global-step:11543	 l-p:0.11346753686666489
epoch£º577	 i:4 	 global-step:11544	 l-p:0.13388261198997498
epoch£º577	 i:5 	 global-step:11545	 l-p:0.13355274498462677
epoch£º577	 i:6 	 global-step:11546	 l-p:0.16366347670555115
epoch£º577	 i:7 	 global-step:11547	 l-p:0.12204205244779587
epoch£º577	 i:8 	 global-step:11548	 l-p:0.12054257839918137
epoch£º577	 i:9 	 global-step:11549	 l-p:0.1545010209083557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5885, 3.5885, 3.5885],
        [3.5885, 3.0052, 2.4899],
        [3.5885, 3.1766, 3.1823],
        [3.5885, 3.0255, 2.7982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.160054013133049 
model_pd.l_d.mean(): -23.487642288208008 
model_pd.lagr.mean(): -23.32758903503418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1984], device='cuda:0')), ('power', tensor([-23.6861], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.160054013133049
epoch£º578	 i:1 	 global-step:11561	 l-p:0.2453652173280716
epoch£º578	 i:2 	 global-step:11562	 l-p:0.12694434821605682
epoch£º578	 i:3 	 global-step:11563	 l-p:0.10891548544168472
epoch£º578	 i:4 	 global-step:11564	 l-p:0.2134176641702652
epoch£º578	 i:5 	 global-step:11565	 l-p:0.2849789559841156
epoch£º578	 i:6 	 global-step:11566	 l-p:0.1324319690465927
epoch£º578	 i:7 	 global-step:11567	 l-p:0.1300777643918991
epoch£º578	 i:8 	 global-step:11568	 l-p:0.14304518699645996
epoch£º578	 i:9 	 global-step:11569	 l-p:0.18032130599021912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5358, 3.2474, 2.7340],
        [3.5358, 3.3616, 3.4565],
        [3.5358, 2.9769, 2.7753],
        [3.5358, 3.4123, 3.4930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.139828160405159 
model_pd.l_d.mean(): -23.45413589477539 
model_pd.lagr.mean(): -23.314308166503906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2276], device='cuda:0')), ('power', tensor([-23.6817], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.139828160405159
epoch£º579	 i:1 	 global-step:11581	 l-p:-0.09977805614471436
epoch£º579	 i:2 	 global-step:11582	 l-p:0.13019318878650665
epoch£º579	 i:3 	 global-step:11583	 l-p:-0.05767660588026047
epoch£º579	 i:4 	 global-step:11584	 l-p:-1.0382550954818726
epoch£º579	 i:5 	 global-step:11585	 l-p:0.1470450907945633
epoch£º579	 i:6 	 global-step:11586	 l-p:0.1590186506509781
epoch£º579	 i:7 	 global-step:11587	 l-p:0.10445329546928406
epoch£º579	 i:8 	 global-step:11588	 l-p:0.13940668106079102
epoch£º579	 i:9 	 global-step:11589	 l-p:0.1549820452928543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6010, 3.5568, 3.5938],
        [3.6010, 3.5889, 3.6002],
        [3.6010, 2.9910, 2.5727],
        [3.6010, 3.5443, 3.5900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.17572130262851715 
model_pd.l_d.mean(): -23.74213409423828 
model_pd.lagr.mean(): -23.5664119720459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1364], device='cuda:0')), ('power', tensor([-23.8785], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.17572130262851715
epoch£º580	 i:1 	 global-step:11601	 l-p:0.1361141949892044
epoch£º580	 i:2 	 global-step:11602	 l-p:0.1296134889125824
epoch£º580	 i:3 	 global-step:11603	 l-p:0.1912049949169159
epoch£º580	 i:4 	 global-step:11604	 l-p:0.1879214197397232
epoch£º580	 i:5 	 global-step:11605	 l-p:0.14438970386981964
epoch£º580	 i:6 	 global-step:11606	 l-p:0.1406138837337494
epoch£º580	 i:7 	 global-step:11607	 l-p:0.25363484025001526
epoch£º580	 i:8 	 global-step:11608	 l-p:0.09572948515415192
epoch£º580	 i:9 	 global-step:11609	 l-p:0.11960946023464203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6142, 3.6142, 3.6142],
        [3.6142, 3.1083, 2.5554],
        [3.6142, 3.0719, 2.8917],
        [3.6142, 3.3869, 3.4840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.13845781981945038 
model_pd.l_d.mean(): -23.31336784362793 
model_pd.lagr.mean(): -23.174909591674805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2311], device='cuda:0')), ('power', tensor([-23.5445], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.13845781981945038
epoch£º581	 i:1 	 global-step:11621	 l-p:0.1331043839454651
epoch£º581	 i:2 	 global-step:11622	 l-p:0.12268907576799393
epoch£º581	 i:3 	 global-step:11623	 l-p:0.12531442940235138
epoch£º581	 i:4 	 global-step:11624	 l-p:0.1496269702911377
epoch£º581	 i:5 	 global-step:11625	 l-p:0.19961445033550262
epoch£º581	 i:6 	 global-step:11626	 l-p:0.19977757334709167
epoch£º581	 i:7 	 global-step:11627	 l-p:0.13309676945209503
epoch£º581	 i:8 	 global-step:11628	 l-p:0.08744335919618607
epoch£º581	 i:9 	 global-step:11629	 l-p:0.1304537057876587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6055, 3.4932, 3.5693],
        [3.6055, 3.3037, 2.7811],
        [3.6055, 3.6047, 3.6055],
        [3.6055, 3.5827, 3.6031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.13010157644748688 
model_pd.l_d.mean(): -22.238094329833984 
model_pd.lagr.mean(): -22.10799217224121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3060], device='cuda:0')), ('power', tensor([-22.5441], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.13010157644748688
epoch£º582	 i:1 	 global-step:11641	 l-p:0.1858997493982315
epoch£º582	 i:2 	 global-step:11642	 l-p:0.11736652255058289
epoch£º582	 i:3 	 global-step:11643	 l-p:0.12998077273368835
epoch£º582	 i:4 	 global-step:11644	 l-p:0.12639079988002777
epoch£º582	 i:5 	 global-step:11645	 l-p:1.198307991027832
epoch£º582	 i:6 	 global-step:11646	 l-p:0.17559680342674255
epoch£º582	 i:7 	 global-step:11647	 l-p:0.04824546352028847
epoch£º582	 i:8 	 global-step:11648	 l-p:0.2069297879934311
epoch£º582	 i:9 	 global-step:11649	 l-p:0.13875912129878998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5522, 3.4773, 3.5346],
        [3.5522, 2.9803, 2.4367],
        [3.5522, 2.9913, 2.4430],
        [3.5522, 2.9670, 2.7100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.13137070834636688 
model_pd.l_d.mean(): -23.456680297851562 
model_pd.lagr.mean(): -23.32530975341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2584], device='cuda:0')), ('power', tensor([-23.7151], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.13137070834636688
epoch£º583	 i:1 	 global-step:11661	 l-p:0.13246791064739227
epoch£º583	 i:2 	 global-step:11662	 l-p:0.1776202768087387
epoch£º583	 i:3 	 global-step:11663	 l-p:0.2012951672077179
epoch£º583	 i:4 	 global-step:11664	 l-p:4.958091735839844
epoch£º583	 i:5 	 global-step:11665	 l-p:0.14086973667144775
epoch£º583	 i:6 	 global-step:11666	 l-p:0.11908946931362152
epoch£º583	 i:7 	 global-step:11667	 l-p:0.2139843851327896
epoch£º583	 i:8 	 global-step:11668	 l-p:0.17623399198055267
epoch£º583	 i:9 	 global-step:11669	 l-p:0.1252770721912384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6543, 3.6100, 3.6470],
        [3.6543, 3.6543, 3.6543],
        [3.6543, 3.5327, 3.6125],
        [3.6543, 3.5071, 3.5955]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): -0.27716943621635437 
model_pd.l_d.mean(): -23.53013801574707 
model_pd.lagr.mean(): -23.807308197021484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1624], device='cuda:0')), ('power', tensor([-23.6926], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:-0.27716943621635437
epoch£º584	 i:1 	 global-step:11681	 l-p:0.126304030418396
epoch£º584	 i:2 	 global-step:11682	 l-p:0.14427977800369263
epoch£º584	 i:3 	 global-step:11683	 l-p:0.13759005069732666
epoch£º584	 i:4 	 global-step:11684	 l-p:0.10216809064149857
epoch£º584	 i:5 	 global-step:11685	 l-p:0.13615329563617706
epoch£º584	 i:6 	 global-step:11686	 l-p:0.12274351716041565
epoch£º584	 i:7 	 global-step:11687	 l-p:0.11208610236644745
epoch£º584	 i:8 	 global-step:11688	 l-p:0.13858747482299805
epoch£º584	 i:9 	 global-step:11689	 l-p:0.10611721128225327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5965, 3.4243, 3.5190],
        [3.5965, 3.5810, 3.5953],
        [3.5965, 3.5933, 3.5964],
        [3.5965, 3.5761, 3.5946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.14826369285583496 
model_pd.l_d.mean(): -23.41872787475586 
model_pd.lagr.mean(): -23.270463943481445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1753], device='cuda:0')), ('power', tensor([-23.5940], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.14826369285583496
epoch£º585	 i:1 	 global-step:11701	 l-p:0.12726277112960815
epoch£º585	 i:2 	 global-step:11702	 l-p:0.23878951370716095
epoch£º585	 i:3 	 global-step:11703	 l-p:0.04722200334072113
epoch£º585	 i:4 	 global-step:11704	 l-p:0.1390589028596878
epoch£º585	 i:5 	 global-step:11705	 l-p:0.1464090645313263
epoch£º585	 i:6 	 global-step:11706	 l-p:0.1713784784078598
epoch£º585	 i:7 	 global-step:11707	 l-p:0.12899452447891235
epoch£º585	 i:8 	 global-step:11708	 l-p:0.15257064998149872
epoch£º585	 i:9 	 global-step:11709	 l-p:0.08053960651159286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5014, 3.5000, 3.5013],
        [3.5014, 3.1592, 2.6314],
        [3.5014, 3.1545, 3.2201],
        [3.5014, 3.4310, 3.4857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.14996524155139923 
model_pd.l_d.mean(): -23.318405151367188 
model_pd.lagr.mean(): -23.168439865112305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2550], device='cuda:0')), ('power', tensor([-23.5735], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.14996524155139923
epoch£º586	 i:1 	 global-step:11721	 l-p:0.1612723469734192
epoch£º586	 i:2 	 global-step:11722	 l-p:0.06479991972446442
epoch£º586	 i:3 	 global-step:11723	 l-p:0.16694578528404236
epoch£º586	 i:4 	 global-step:11724	 l-p:1.4309836626052856
epoch£º586	 i:5 	 global-step:11725	 l-p:0.11198332905769348
epoch£º586	 i:6 	 global-step:11726	 l-p:0.1525677591562271
epoch£º586	 i:7 	 global-step:11727	 l-p:0.12928247451782227
epoch£º586	 i:8 	 global-step:11728	 l-p:0.06890270113945007
epoch£º586	 i:9 	 global-step:11729	 l-p:0.13401080667972565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6225, 3.0716, 2.8860],
        [3.6225, 3.0436, 2.5080],
        [3.6225, 3.0196, 2.5133],
        [3.6225, 3.5020, 3.5818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.11683641374111176 
model_pd.l_d.mean(): -23.26513671875 
model_pd.lagr.mean(): -23.148300170898438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2429], device='cuda:0')), ('power', tensor([-23.5080], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.11683641374111176
epoch£º587	 i:1 	 global-step:11741	 l-p:0.17195937037467957
epoch£º587	 i:2 	 global-step:11742	 l-p:0.1923023760318756
epoch£º587	 i:3 	 global-step:11743	 l-p:0.12252309173345566
epoch£º587	 i:4 	 global-step:11744	 l-p:0.1291464865207672
epoch£º587	 i:5 	 global-step:11745	 l-p:0.1514565795660019
epoch£º587	 i:6 	 global-step:11746	 l-p:0.1521860510110855
epoch£º587	 i:7 	 global-step:11747	 l-p:-0.2211562693119049
epoch£º587	 i:8 	 global-step:11748	 l-p:0.121745765209198
epoch£º587	 i:9 	 global-step:11749	 l-p:0.14528295397758484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4897, 3.4897, 3.4897],
        [3.4897, 3.2930, 3.3928],
        [3.4897, 3.0139, 2.9694],
        [3.4897, 3.3235, 3.4183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.09264920651912689 
model_pd.l_d.mean(): -23.45037078857422 
model_pd.lagr.mean(): -23.35772132873535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2552], device='cuda:0')), ('power', tensor([-23.7055], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.09264920651912689
epoch£º588	 i:1 	 global-step:11761	 l-p:0.29869961738586426
epoch£º588	 i:2 	 global-step:11762	 l-p:0.023391855880618095
epoch£º588	 i:3 	 global-step:11763	 l-p:0.10646838694810867
epoch£º588	 i:4 	 global-step:11764	 l-p:0.22951728105545044
epoch£º588	 i:5 	 global-step:11765	 l-p:0.14756383001804352
epoch£º588	 i:6 	 global-step:11766	 l-p:0.1361003816127777
epoch£º588	 i:7 	 global-step:11767	 l-p:0.12616616487503052
epoch£º588	 i:8 	 global-step:11768	 l-p:0.13212229311466217
epoch£º588	 i:9 	 global-step:11769	 l-p:0.1782071888446808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5526, 3.5221, 3.5488],
        [3.5526, 3.5509, 3.5526],
        [3.5526, 3.4591, 3.5268],
        [3.5526, 3.5526, 3.5526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 1.0071181058883667 
model_pd.l_d.mean(): -23.287181854248047 
model_pd.lagr.mean(): -22.28006362915039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2674], device='cuda:0')), ('power', tensor([-23.5545], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:1.0071181058883667
epoch£º589	 i:1 	 global-step:11781	 l-p:0.12084922194480896
epoch£º589	 i:2 	 global-step:11782	 l-p:0.21866244077682495
epoch£º589	 i:3 	 global-step:11783	 l-p:0.1599903702735901
epoch£º589	 i:4 	 global-step:11784	 l-p:0.1312587708234787
epoch£º589	 i:5 	 global-step:11785	 l-p:0.13831055164337158
epoch£º589	 i:6 	 global-step:11786	 l-p:0.12706802785396576
epoch£º589	 i:7 	 global-step:11787	 l-p:0.13058117032051086
epoch£º589	 i:8 	 global-step:11788	 l-p:0.1308721899986267
epoch£º589	 i:9 	 global-step:11789	 l-p:1.2158805131912231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5419, 3.4680, 3.5249],
        [3.5419, 2.9102, 2.4311],
        [3.5419, 3.4164, 3.4987],
        [3.5419, 3.5212, 3.5399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.15102757513523102 
model_pd.l_d.mean(): -23.402339935302734 
model_pd.lagr.mean(): -23.251312255859375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2581], device='cuda:0')), ('power', tensor([-23.6604], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.15102757513523102
epoch£º590	 i:1 	 global-step:11801	 l-p:0.3294295370578766
epoch£º590	 i:2 	 global-step:11802	 l-p:0.12585888803005219
epoch£º590	 i:3 	 global-step:11803	 l-p:0.1412292867898941
epoch£º590	 i:4 	 global-step:11804	 l-p:0.15577395260334015
epoch£º590	 i:5 	 global-step:11805	 l-p:0.14734233915805817
epoch£º590	 i:6 	 global-step:11806	 l-p:0.1443025916814804
epoch£º590	 i:7 	 global-step:11807	 l-p:0.13255637884140015
epoch£º590	 i:8 	 global-step:11808	 l-p:0.05947626009583473
epoch£º590	 i:9 	 global-step:11809	 l-p:0.15764351189136505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4331, 2.8172, 2.2744],
        [3.4331, 2.8616, 2.6832],
        [3.4331, 3.4329, 3.4331],
        [3.4331, 3.4324, 3.4331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.13320958614349365 
model_pd.l_d.mean(): -22.456148147583008 
model_pd.lagr.mean(): -22.322938919067383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3357], device='cuda:0')), ('power', tensor([-22.7918], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.13320958614349365
epoch£º591	 i:1 	 global-step:11821	 l-p:0.16093528270721436
epoch£º591	 i:2 	 global-step:11822	 l-p:0.11689986288547516
epoch£º591	 i:3 	 global-step:11823	 l-p:0.19222676753997803
epoch£º591	 i:4 	 global-step:11824	 l-p:0.15266266465187073
epoch£º591	 i:5 	 global-step:11825	 l-p:0.16130191087722778
epoch£º591	 i:6 	 global-step:11826	 l-p:0.15616412460803986
epoch£º591	 i:7 	 global-step:11827	 l-p:0.14315347373485565
epoch£º591	 i:8 	 global-step:11828	 l-p:0.1298806220293045
epoch£º591	 i:9 	 global-step:11829	 l-p:0.12563678622245789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[3.6741, 3.2508, 3.2514],
        [3.6741, 3.2504, 3.2507],
        [3.6741, 3.4417, 2.9309],
        [3.6741, 3.2087, 3.1619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.12284868210554123 
model_pd.l_d.mean(): -23.24048614501953 
model_pd.lagr.mean(): -23.117637634277344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1675], device='cuda:0')), ('power', tensor([-23.4080], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.12284868210554123
epoch£º592	 i:1 	 global-step:11841	 l-p:0.15852531790733337
epoch£º592	 i:2 	 global-step:11842	 l-p:0.1233651265501976
epoch£º592	 i:3 	 global-step:11843	 l-p:0.12394990772008896
epoch£º592	 i:4 	 global-step:11844	 l-p:0.1728874146938324
epoch£º592	 i:5 	 global-step:11845	 l-p:0.07115637511014938
epoch£º592	 i:6 	 global-step:11846	 l-p:0.12995365262031555
epoch£º592	 i:7 	 global-step:11847	 l-p:0.1228717565536499
epoch£º592	 i:8 	 global-step:11848	 l-p:0.14786040782928467
epoch£º592	 i:9 	 global-step:11849	 l-p:0.20240585505962372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6060, 3.4473, 3.5401],
        [3.6060, 3.3495, 3.4469],
        [3.6060, 3.6059, 3.6060],
        [3.6060, 3.6060, 3.6060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.14526136219501495 
model_pd.l_d.mean(): -23.640602111816406 
model_pd.lagr.mean(): -23.49534034729004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1773], device='cuda:0')), ('power', tensor([-23.8179], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.14526136219501495
epoch£º593	 i:1 	 global-step:11861	 l-p:0.18192332983016968
epoch£º593	 i:2 	 global-step:11862	 l-p:0.15229131281375885
epoch£º593	 i:3 	 global-step:11863	 l-p:0.14649493992328644
epoch£º593	 i:4 	 global-step:11864	 l-p:0.13013386726379395
epoch£º593	 i:5 	 global-step:11865	 l-p:0.16254651546478271
epoch£º593	 i:6 	 global-step:11866	 l-p:0.11014477163553238
epoch£º593	 i:7 	 global-step:11867	 l-p:0.1147068589925766
epoch£º593	 i:8 	 global-step:11868	 l-p:0.13457198441028595
epoch£º593	 i:9 	 global-step:11869	 l-p:-0.4380606412887573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5539, 3.3163, 3.4172],
        [3.5539, 3.5538, 3.5539],
        [3.5539, 3.4185, 3.5047],
        [3.5539, 3.0532, 2.4894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.646728515625 
model_pd.l_d.mean(): -23.397499084472656 
model_pd.lagr.mean(): -22.750770568847656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2867], device='cuda:0')), ('power', tensor([-23.6842], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.646728515625
epoch£º594	 i:1 	 global-step:11881	 l-p:0.13071410357952118
epoch£º594	 i:2 	 global-step:11882	 l-p:1.957968831062317
epoch£º594	 i:3 	 global-step:11883	 l-p:0.12995308637619019
epoch£º594	 i:4 	 global-step:11884	 l-p:0.16185355186462402
epoch£º594	 i:5 	 global-step:11885	 l-p:0.24531032145023346
epoch£º594	 i:6 	 global-step:11886	 l-p:0.13650956749916077
epoch£º594	 i:7 	 global-step:11887	 l-p:0.14338278770446777
epoch£º594	 i:8 	 global-step:11888	 l-p:0.08281085640192032
epoch£º594	 i:9 	 global-step:11889	 l-p:0.12874916195869446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6045, 3.1173, 2.5516],
        [3.6045, 3.1729, 3.1746],
        [3.6045, 3.6044, 3.6045],
        [3.6045, 3.6012, 3.6044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.13180217146873474 
model_pd.l_d.mean(): -23.528249740600586 
model_pd.lagr.mean(): -23.396448135375977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1851], device='cuda:0')), ('power', tensor([-23.7134], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.13180217146873474
epoch£º595	 i:1 	 global-step:11901	 l-p:0.13494081795215607
epoch£º595	 i:2 	 global-step:11902	 l-p:0.14301186800003052
epoch£º595	 i:3 	 global-step:11903	 l-p:0.18358761072158813
epoch£º595	 i:4 	 global-step:11904	 l-p:0.24527190625667572
epoch£º595	 i:5 	 global-step:11905	 l-p:0.2767643928527832
epoch£º595	 i:6 	 global-step:11906	 l-p:0.18593867123126984
epoch£º595	 i:7 	 global-step:11907	 l-p:0.13673995435237885
epoch£º595	 i:8 	 global-step:11908	 l-p:0.11202940344810486
epoch£º595	 i:9 	 global-step:11909	 l-p:0.10309071093797684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7026, 3.6246, 3.6837],
        [3.7026, 3.7010, 3.7026],
        [3.7026, 3.0840, 2.6694],
        [3.7026, 3.7025, 3.7026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.23369674384593964 
model_pd.l_d.mean(): -23.452558517456055 
model_pd.lagr.mean(): -23.218862533569336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1452], device='cuda:0')), ('power', tensor([-23.5978], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.23369674384593964
epoch£º596	 i:1 	 global-step:11921	 l-p:0.11953885853290558
epoch£º596	 i:2 	 global-step:11922	 l-p:0.12316826730966568
epoch£º596	 i:3 	 global-step:11923	 l-p:0.11407463997602463
epoch£º596	 i:4 	 global-step:11924	 l-p:0.12601691484451294
epoch£º596	 i:5 	 global-step:11925	 l-p:0.15745201706886292
epoch£º596	 i:6 	 global-step:11926	 l-p:0.14902263879776
epoch£º596	 i:7 	 global-step:11927	 l-p:0.1432691514492035
epoch£º596	 i:8 	 global-step:11928	 l-p:0.13562384247779846
epoch£º596	 i:9 	 global-step:11929	 l-p:0.1236560046672821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5381, 3.1578, 3.2078],
        [3.5381, 3.4863, 3.5289],
        [3.5381, 3.5380, 3.5381],
        [3.5381, 3.5347, 3.5380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.02362070418894291 
model_pd.l_d.mean(): -23.421653747558594 
model_pd.lagr.mean(): -23.398033142089844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2140], device='cuda:0')), ('power', tensor([-23.6357], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.02362070418894291
epoch£º597	 i:1 	 global-step:11941	 l-p:0.13603727519512177
epoch£º597	 i:2 	 global-step:11942	 l-p:0.11092578619718552
epoch£º597	 i:3 	 global-step:11943	 l-p:0.16067729890346527
epoch£º597	 i:4 	 global-step:11944	 l-p:0.08168601989746094
epoch£º597	 i:5 	 global-step:11945	 l-p:0.19388529658317566
epoch£º597	 i:6 	 global-step:11946	 l-p:-0.30666249990463257
epoch£º597	 i:7 	 global-step:11947	 l-p:0.1537708044052124
epoch£º597	 i:8 	 global-step:11948	 l-p:0.12849220633506775
epoch£º597	 i:9 	 global-step:11949	 l-p:0.13259382545948029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5345, 3.5329, 3.5344],
        [3.5345, 2.9586, 2.3952],
        [3.5345, 3.2184, 3.3032],
        [3.5345, 3.1481, 2.6026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.11297306418418884 
model_pd.l_d.mean(): -22.7269287109375 
model_pd.lagr.mean(): -22.613956451416016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2954], device='cuda:0')), ('power', tensor([-23.0223], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.11297306418418884
epoch£º598	 i:1 	 global-step:11961	 l-p:0.33089208602905273
epoch£º598	 i:2 	 global-step:11962	 l-p:0.028210753574967384
epoch£º598	 i:3 	 global-step:11963	 l-p:0.13088154792785645
epoch£º598	 i:4 	 global-step:11964	 l-p:2.212604284286499
epoch£º598	 i:5 	 global-step:11965	 l-p:0.17152242362499237
epoch£º598	 i:6 	 global-step:11966	 l-p:0.12881310284137726
epoch£º598	 i:7 	 global-step:11967	 l-p:0.1332360953092575
epoch£º598	 i:8 	 global-step:11968	 l-p:0.1426396369934082
epoch£º598	 i:9 	 global-step:11969	 l-p:0.13080701231956482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5671, 3.5671, 3.5671],
        [3.5671, 3.2335, 3.3099],
        [3.5671, 3.0655, 2.4982],
        [3.5671, 3.5594, 3.5667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): -23.712018966674805 
model_pd.l_d.mean(): -23.053207397460938 
model_pd.lagr.mean(): -46.765228271484375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3181], device='cuda:0')), ('power', tensor([-23.3713], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:-23.712018966674805
epoch£º599	 i:1 	 global-step:11981	 l-p:0.14953073859214783
epoch£º599	 i:2 	 global-step:11982	 l-p:0.12407781183719635
epoch£º599	 i:3 	 global-step:11983	 l-p:0.10617171227931976
epoch£º599	 i:4 	 global-step:11984	 l-p:0.3800097703933716
epoch£º599	 i:5 	 global-step:11985	 l-p:-0.06682035326957703
epoch£º599	 i:6 	 global-step:11986	 l-p:0.13777105510234833
epoch£º599	 i:7 	 global-step:11987	 l-p:0.13619230687618256
epoch£º599	 i:8 	 global-step:11988	 l-p:0.24434050917625427
epoch£º599	 i:9 	 global-step:11989	 l-p:0.12681683897972107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4800, 2.8566, 2.3070],
        [3.4800, 3.4800, 3.4800],
        [3.4800, 3.4800, 3.4800],
        [3.4800, 3.4800, 3.4800]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.1374165564775467 
model_pd.l_d.mean(): -22.962106704711914 
model_pd.lagr.mean(): -22.824689865112305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2628], device='cuda:0')), ('power', tensor([-23.2250], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.1374165564775467
epoch£º600	 i:1 	 global-step:12001	 l-p:0.13903439044952393
epoch£º600	 i:2 	 global-step:12002	 l-p:0.14908942580223083
epoch£º600	 i:3 	 global-step:12003	 l-p:0.02694198489189148
epoch£º600	 i:4 	 global-step:12004	 l-p:-0.07597921043634415
epoch£º600	 i:5 	 global-step:12005	 l-p:0.11743862181901932
epoch£º600	 i:6 	 global-step:12006	 l-p:0.20667093992233276
epoch£º600	 i:7 	 global-step:12007	 l-p:0.10754026472568512
epoch£º600	 i:8 	 global-step:12008	 l-p:0.12693749368190765
epoch£º600	 i:9 	 global-step:12009	 l-p:0.13200892508029938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5013, 2.8512, 2.3449],
        [3.5013, 2.9122, 2.7039],
        [3.5013, 3.4588, 3.4948],
        [3.5013, 3.5000, 3.5013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.2086455523967743 
model_pd.l_d.mean(): -23.572458267211914 
model_pd.lagr.mean(): -23.363813400268555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2036], device='cuda:0')), ('power', tensor([-23.7761], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.2086455523967743
epoch£º601	 i:1 	 global-step:12021	 l-p:0.1288238912820816
epoch£º601	 i:2 	 global-step:12022	 l-p:0.12243664264678955
epoch£º601	 i:3 	 global-step:12023	 l-p:0.1447722464799881
epoch£º601	 i:4 	 global-step:12024	 l-p:0.13238640129566193
epoch£º601	 i:5 	 global-step:12025	 l-p:0.22213128209114075
epoch£º601	 i:6 	 global-step:12026	 l-p:0.15933670103549957
epoch£º601	 i:7 	 global-step:12027	 l-p:0.06268923729658127
epoch£º601	 i:8 	 global-step:12028	 l-p:0.1325230896472931
epoch£º601	 i:9 	 global-step:12029	 l-p:0.10690353810787201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5658, 3.5658, 3.5658],
        [3.5658, 2.9494, 2.4047],
        [3.5658, 3.5156, 3.5571],
        [3.5658, 2.9328, 2.4110]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.1881011575460434 
model_pd.l_d.mean(): -23.414884567260742 
model_pd.lagr.mean(): -23.226783752441406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2374], device='cuda:0')), ('power', tensor([-23.6523], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.1881011575460434
epoch£º602	 i:1 	 global-step:12041	 l-p:0.14049798250198364
epoch£º602	 i:2 	 global-step:12042	 l-p:0.1140839084982872
epoch£º602	 i:3 	 global-step:12043	 l-p:0.10766162723302841
epoch£º602	 i:4 	 global-step:12044	 l-p:0.1473986804485321
epoch£º602	 i:5 	 global-step:12045	 l-p:0.13958118855953217
epoch£º602	 i:6 	 global-step:12046	 l-p:0.15406005084514618
epoch£º602	 i:7 	 global-step:12047	 l-p:0.13609400391578674
epoch£º602	 i:8 	 global-step:12048	 l-p:0.13349899649620056
epoch£º602	 i:9 	 global-step:12049	 l-p:0.13642379641532898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6671, 3.3877, 3.4812],
        [3.6671, 3.6448, 3.6649],
        [3.6671, 3.6670, 3.6671],
        [3.6671, 3.4088, 3.5067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.14096756279468536 
model_pd.l_d.mean(): -23.604185104370117 
model_pd.lagr.mean(): -23.46321678161621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1308], device='cuda:0')), ('power', tensor([-23.7350], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.14096756279468536
epoch£º603	 i:1 	 global-step:12061	 l-p:0.13574735820293427
epoch£º603	 i:2 	 global-step:12062	 l-p:0.11870504170656204
epoch£º603	 i:3 	 global-step:12063	 l-p:0.12740081548690796
epoch£º603	 i:4 	 global-step:12064	 l-p:0.14805540442466736
epoch£º603	 i:5 	 global-step:12065	 l-p:0.24607674777507782
epoch£º603	 i:6 	 global-step:12066	 l-p:0.13961981236934662
epoch£º603	 i:7 	 global-step:12067	 l-p:0.2602008283138275
epoch£º603	 i:8 	 global-step:12068	 l-p:0.11337219923734665
epoch£º603	 i:9 	 global-step:12069	 l-p:0.12229127436876297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5668, 3.0831, 2.5163],
        [3.5668, 3.5668, 3.5668],
        [3.5668, 3.4240, 3.5132],
        [3.5668, 3.1010, 3.0757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.1408965140581131 
model_pd.l_d.mean(): -23.488113403320312 
model_pd.lagr.mean(): -23.347217559814453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2046], device='cuda:0')), ('power', tensor([-23.6927], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:0.1408965140581131
epoch£º604	 i:1 	 global-step:12081	 l-p:0.14962579309940338
epoch£º604	 i:2 	 global-step:12082	 l-p:-0.05613885819911957
epoch£º604	 i:3 	 global-step:12083	 l-p:0.26713627576828003
epoch£º604	 i:4 	 global-step:12084	 l-p:0.0745002031326294
epoch£º604	 i:5 	 global-step:12085	 l-p:0.11508242040872574
epoch£º604	 i:6 	 global-step:12086	 l-p:0.1809084415435791
epoch£º604	 i:7 	 global-step:12087	 l-p:0.07304202020168304
epoch£º604	 i:8 	 global-step:12088	 l-p:0.14600205421447754
epoch£º604	 i:9 	 global-step:12089	 l-p:0.12556348741054535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5413, 2.8920, 2.3970],
        [3.5413, 3.2913, 3.3932],
        [3.5413, 3.4222, 3.5026],
        [3.5413, 3.4453, 3.5149]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.11044411361217499 
model_pd.l_d.mean(): -23.44803237915039 
model_pd.lagr.mean(): -23.337587356567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2212], device='cuda:0')), ('power', tensor([-23.6693], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.11044411361217499
epoch£º605	 i:1 	 global-step:12101	 l-p:0.048564691096544266
epoch£º605	 i:2 	 global-step:12102	 l-p:0.14146341383457184
epoch£º605	 i:3 	 global-step:12103	 l-p:0.13421249389648438
epoch£º605	 i:4 	 global-step:12104	 l-p:-2.171825885772705
epoch£º605	 i:5 	 global-step:12105	 l-p:0.1522388607263565
epoch£º605	 i:6 	 global-step:12106	 l-p:0.11762281507253647
epoch£º605	 i:7 	 global-step:12107	 l-p:0.261220246553421
epoch£º605	 i:8 	 global-step:12108	 l-p:0.1354212462902069
epoch£º605	 i:9 	 global-step:12109	 l-p:-0.059877343475818634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4705, 3.4705, 3.4705],
        [3.4705, 2.9432, 2.3790],
        [3.4705, 2.9981, 2.9758],
        [3.4705, 3.4698, 3.4705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.10923774540424347 
model_pd.l_d.mean(): -23.633691787719727 
model_pd.lagr.mean(): -23.52445411682129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2308], device='cuda:0')), ('power', tensor([-23.8645], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.10923774540424347
epoch£º606	 i:1 	 global-step:12121	 l-p:0.15029941499233246
epoch£º606	 i:2 	 global-step:12122	 l-p:0.13830605149269104
epoch£º606	 i:3 	 global-step:12123	 l-p:0.08740612864494324
epoch£º606	 i:4 	 global-step:12124	 l-p:0.14253926277160645
epoch£º606	 i:5 	 global-step:12125	 l-p:-0.8446457386016846
epoch£º606	 i:6 	 global-step:12126	 l-p:0.14948561787605286
epoch£º606	 i:7 	 global-step:12127	 l-p:0.14572493731975555
epoch£º606	 i:8 	 global-step:12128	 l-p:0.13598771393299103
epoch£º606	 i:9 	 global-step:12129	 l-p:0.1814393550157547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5251, 3.5059, 3.5234],
        [3.5251, 3.0194, 2.9513],
        [3.5251, 3.5224, 3.5250],
        [3.5251, 3.0826, 3.0866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.14644755423069 
model_pd.l_d.mean(): -23.470792770385742 
model_pd.lagr.mean(): -23.324344635009766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2439], device='cuda:0')), ('power', tensor([-23.7147], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.14644755423069
epoch£º607	 i:1 	 global-step:12141	 l-p:0.13181085884571075
epoch£º607	 i:2 	 global-step:12142	 l-p:0.15549439191818237
epoch£º607	 i:3 	 global-step:12143	 l-p:0.13516385853290558
epoch£º607	 i:4 	 global-step:12144	 l-p:0.2051444947719574
epoch£º607	 i:5 	 global-step:12145	 l-p:0.013761095702648163
epoch£º607	 i:6 	 global-step:12146	 l-p:0.2082056999206543
epoch£º607	 i:7 	 global-step:12147	 l-p:0.11041649430990219
epoch£º607	 i:8 	 global-step:12148	 l-p:0.8485353589057922
epoch£º607	 i:9 	 global-step:12149	 l-p:0.13548479974269867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5996, 3.5968, 3.5996],
        [3.5996, 3.5883, 3.5989],
        [3.5996, 3.4638, 3.5506],
        [3.5996, 3.0874, 2.5151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.13632655143737793 
model_pd.l_d.mean(): -22.91745948791504 
model_pd.lagr.mean(): -22.7811336517334 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3037], device='cuda:0')), ('power', tensor([-23.2211], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.13632655143737793
epoch£º608	 i:1 	 global-step:12161	 l-p:0.1323184370994568
epoch£º608	 i:2 	 global-step:12162	 l-p:0.16220353543758392
epoch£º608	 i:3 	 global-step:12163	 l-p:0.13555379211902618
epoch£º608	 i:4 	 global-step:12164	 l-p:0.22091719508171082
epoch£º608	 i:5 	 global-step:12165	 l-p:0.18191711604595184
epoch£º608	 i:6 	 global-step:12166	 l-p:0.13176274299621582
epoch£º608	 i:7 	 global-step:12167	 l-p:0.1578071117401123
epoch£º608	 i:8 	 global-step:12168	 l-p:0.12856322526931763
epoch£º608	 i:9 	 global-step:12169	 l-p:0.06007229536771774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6454, 3.6454, 3.6454],
        [3.6454, 3.6096, 3.6404],
        [3.6454, 3.1436, 2.5700],
        [3.6454, 3.3925, 3.4924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.19326922297477722 
model_pd.l_d.mean(): -22.681068420410156 
model_pd.lagr.mean(): -22.4877986907959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2896], device='cuda:0')), ('power', tensor([-22.9706], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.19326922297477722
epoch£º609	 i:1 	 global-step:12181	 l-p:0.12989015877246857
epoch£º609	 i:2 	 global-step:12182	 l-p:0.11642253398895264
epoch£º609	 i:3 	 global-step:12183	 l-p:0.12268789112567902
epoch£º609	 i:4 	 global-step:12184	 l-p:0.026255207136273384
epoch£º609	 i:5 	 global-step:12185	 l-p:0.1312265396118164
epoch£º609	 i:6 	 global-step:12186	 l-p:0.12843771278858185
epoch£º609	 i:7 	 global-step:12187	 l-p:0.12576399743556976
epoch£º609	 i:8 	 global-step:12188	 l-p:0.16984190046787262
epoch£º609	 i:9 	 global-step:12189	 l-p:0.1843341439962387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6260, 3.3723, 3.4727],
        [3.6260, 3.6253, 3.6260],
        [3.6260, 3.6259, 3.6260],
        [3.6260, 3.5548, 3.6102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.16491124033927917 
model_pd.l_d.mean(): -23.38418197631836 
model_pd.lagr.mean(): -23.219270706176758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1946], device='cuda:0')), ('power', tensor([-23.5788], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.16491124033927917
epoch£º610	 i:1 	 global-step:12201	 l-p:0.13628938794136047
epoch£º610	 i:2 	 global-step:12202	 l-p:0.07085072249174118
epoch£º610	 i:3 	 global-step:12203	 l-p:0.1683172881603241
epoch£º610	 i:4 	 global-step:12204	 l-p:0.1429567039012909
epoch£º610	 i:5 	 global-step:12205	 l-p:0.1588210165500641
epoch£º610	 i:6 	 global-step:12206	 l-p:0.11690253019332886
epoch£º610	 i:7 	 global-step:12207	 l-p:0.14133000373840332
epoch£º610	 i:8 	 global-step:12208	 l-p:0.13902419805526733
epoch£º610	 i:9 	 global-step:12209	 l-p:0.13060052692890167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6714, 3.2018, 3.1644],
        [3.6714, 3.2184, 2.6485],
        [3.6714, 3.3811, 2.8475],
        [3.6714, 3.6693, 3.6713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.1333189308643341 
model_pd.l_d.mean(): -23.454694747924805 
model_pd.lagr.mean(): -23.321374893188477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1129], device='cuda:0')), ('power', tensor([-23.5676], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.1333189308643341
epoch£º611	 i:1 	 global-step:12221	 l-p:0.11448758840560913
epoch£º611	 i:2 	 global-step:12222	 l-p:0.07096254080533981
epoch£º611	 i:3 	 global-step:12223	 l-p:0.13645689189434052
epoch£º611	 i:4 	 global-step:12224	 l-p:0.12705640494823456
epoch£º611	 i:5 	 global-step:12225	 l-p:0.16405394673347473
epoch£º611	 i:6 	 global-step:12226	 l-p:0.1280840039253235
epoch£º611	 i:7 	 global-step:12227	 l-p:0.12485764920711517
epoch£º611	 i:8 	 global-step:12228	 l-p:0.7704354524612427
epoch£º611	 i:9 	 global-step:12229	 l-p:0.14903558790683746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5581, 2.9078, 2.4112],
        [3.5581, 3.5581, 3.5581],
        [3.5581, 3.1250, 3.1367],
        [3.5581, 3.5563, 3.5581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.14222124218940735 
model_pd.l_d.mean(): -22.883520126342773 
model_pd.lagr.mean(): -22.74129867553711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2632], device='cuda:0')), ('power', tensor([-23.1467], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.14222124218940735
epoch£º612	 i:1 	 global-step:12241	 l-p:0.13358643651008606
epoch£º612	 i:2 	 global-step:12242	 l-p:0.0672973170876503
epoch£º612	 i:3 	 global-step:12243	 l-p:0.13383160531520844
epoch£º612	 i:4 	 global-step:12244	 l-p:2.1721484661102295
epoch£º612	 i:5 	 global-step:12245	 l-p:0.1354074776172638
epoch£º612	 i:6 	 global-step:12246	 l-p:0.12496574223041534
epoch£º612	 i:7 	 global-step:12247	 l-p:0.12611882388591766
epoch£º612	 i:8 	 global-step:12248	 l-p:0.18785834312438965
epoch£º612	 i:9 	 global-step:12249	 l-p:-0.9319022297859192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4734, 3.4732, 3.4734],
        [3.4734, 3.4223, 3.4646],
        [3.4734, 3.4669, 3.4731],
        [3.4734, 3.3504, 3.4329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.1958022117614746 
model_pd.l_d.mean(): -23.557687759399414 
model_pd.lagr.mean(): -23.36188507080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2546], device='cuda:0')), ('power', tensor([-23.8123], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.1958022117614746
epoch£º613	 i:1 	 global-step:12261	 l-p:0.162925124168396
epoch£º613	 i:2 	 global-step:12262	 l-p:0.12420140951871872
epoch£º613	 i:3 	 global-step:12263	 l-p:0.18476475775241852
epoch£º613	 i:4 	 global-step:12264	 l-p:0.08897226303815842
epoch£º613	 i:5 	 global-step:12265	 l-p:0.12467975169420242
epoch£º613	 i:6 	 global-step:12266	 l-p:0.14243856072425842
epoch£º613	 i:7 	 global-step:12267	 l-p:0.34665167331695557
epoch£º613	 i:8 	 global-step:12268	 l-p:0.028563039377331734
epoch£º613	 i:9 	 global-step:12269	 l-p:0.10969023406505585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5803, 2.9292, 2.4950],
        [3.5803, 3.0531, 2.9503],
        [3.5803, 3.5735, 3.5800],
        [3.5803, 3.5770, 3.5802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.13134649395942688 
model_pd.l_d.mean(): -23.434438705444336 
model_pd.lagr.mean(): -23.30309295654297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2278], device='cuda:0')), ('power', tensor([-23.6622], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.13134649395942688
epoch£º614	 i:1 	 global-step:12281	 l-p:0.1385992169380188
epoch£º614	 i:2 	 global-step:12282	 l-p:0.2510811984539032
epoch£º614	 i:3 	 global-step:12283	 l-p:0.12579657137393951
epoch£º614	 i:4 	 global-step:12284	 l-p:0.12099166959524155
epoch£º614	 i:5 	 global-step:12285	 l-p:0.12971387803554535
epoch£º614	 i:6 	 global-step:12286	 l-p:0.14601098001003265
epoch£º614	 i:7 	 global-step:12287	 l-p:0.15386049449443817
epoch£º614	 i:8 	 global-step:12288	 l-p:0.26330506801605225
epoch£º614	 i:9 	 global-step:12289	 l-p:0.22561191022396088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5944, 3.0441, 2.9025],
        [3.5944, 3.0766, 2.9865],
        [3.5944, 2.9830, 2.4274],
        [3.5944, 3.5944, 3.5944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.12934401631355286 
model_pd.l_d.mean(): -22.630800247192383 
model_pd.lagr.mean(): -22.501455307006836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2598], device='cuda:0')), ('power', tensor([-22.8906], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.12934401631355286
epoch£º615	 i:1 	 global-step:12301	 l-p:0.14023616909980774
epoch£º615	 i:2 	 global-step:12302	 l-p:0.21528708934783936
epoch£º615	 i:3 	 global-step:12303	 l-p:0.20645268261432648
epoch£º615	 i:4 	 global-step:12304	 l-p:0.12918250262737274
epoch£º615	 i:5 	 global-step:12305	 l-p:0.12916260957717896
epoch£º615	 i:6 	 global-step:12306	 l-p:0.14040735363960266
epoch£º615	 i:7 	 global-step:12307	 l-p:0.14719274640083313
epoch£º615	 i:8 	 global-step:12308	 l-p:0.20682165026664734
epoch£º615	 i:9 	 global-step:12309	 l-p:-1.3730965852737427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5736, 3.5695, 3.5734],
        [3.5736, 3.5052, 3.5589],
        [3.5736, 3.0180, 2.4443],
        [3.5736, 2.9339, 2.5813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.12321418523788452 
model_pd.l_d.mean(): -22.396833419799805 
model_pd.lagr.mean(): -22.273618698120117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3684], device='cuda:0')), ('power', tensor([-22.7652], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.12321418523788452
epoch£º616	 i:1 	 global-step:12321	 l-p:0.13858215510845184
epoch£º616	 i:2 	 global-step:12322	 l-p:0.129574254155159
epoch£º616	 i:3 	 global-step:12323	 l-p:0.19197314977645874
epoch£º616	 i:4 	 global-step:12324	 l-p:0.011990413069725037
epoch£º616	 i:5 	 global-step:12325	 l-p:0.18813766539096832
epoch£º616	 i:6 	 global-step:12326	 l-p:0.1195477843284607
epoch£º616	 i:7 	 global-step:12327	 l-p:0.4069812595844269
epoch£º616	 i:8 	 global-step:12328	 l-p:0.23599080741405487
epoch£º616	 i:9 	 global-step:12329	 l-p:0.10141564905643463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6169, 3.4894, 3.5732],
        [3.6169, 3.2768, 3.3519],
        [3.6169, 3.3289, 2.7983],
        [3.6169, 3.4918, 3.5747]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.1257692277431488 
model_pd.l_d.mean(): -23.310094833374023 
model_pd.lagr.mean(): -23.184326171875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2126], device='cuda:0')), ('power', tensor([-23.5227], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.1257692277431488
epoch£º617	 i:1 	 global-step:12341	 l-p:0.14656807482242584
epoch£º617	 i:2 	 global-step:12342	 l-p:0.13594982028007507
epoch£º617	 i:3 	 global-step:12343	 l-p:0.15307168662548065
epoch£º617	 i:4 	 global-step:12344	 l-p:0.11752428114414215
epoch£º617	 i:5 	 global-step:12345	 l-p:0.192717045545578
epoch£º617	 i:6 	 global-step:12346	 l-p:0.14620691537857056
epoch£º617	 i:7 	 global-step:12347	 l-p:0.13984256982803345
epoch£º617	 i:8 	 global-step:12348	 l-p:0.11257163435220718
epoch£º617	 i:9 	 global-step:12349	 l-p:0.08067075908184052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6483, 3.1105, 2.9836],
        [3.6483, 3.5181, 3.6028],
        [3.6483, 3.3527, 2.8177],
        [3.6483, 3.2160, 3.2237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.06346933543682098 
model_pd.l_d.mean(): -23.320024490356445 
model_pd.lagr.mean(): -23.256555557250977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2014], device='cuda:0')), ('power', tensor([-23.5214], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.06346933543682098
epoch£º618	 i:1 	 global-step:12361	 l-p:0.14650841057300568
epoch£º618	 i:2 	 global-step:12362	 l-p:0.13999874889850616
epoch£º618	 i:3 	 global-step:12363	 l-p:0.14900466799736023
epoch£º618	 i:4 	 global-step:12364	 l-p:0.15803262591362
epoch£º618	 i:5 	 global-step:12365	 l-p:0.11781688779592514
epoch£º618	 i:6 	 global-step:12366	 l-p:0.1640860140323639
epoch£º618	 i:7 	 global-step:12367	 l-p:0.14063695073127747
epoch£º618	 i:8 	 global-step:12368	 l-p:0.11451438814401627
epoch£º618	 i:9 	 global-step:12369	 l-p:0.14591044187545776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6523, 3.5274, 3.6101],
        [3.6523, 3.3936, 3.4937],
        [3.6523, 3.3831, 3.4814],
        [3.6523, 3.0760, 2.5056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.16875934600830078 
model_pd.l_d.mean(): -23.376293182373047 
model_pd.lagr.mean(): -23.207534790039062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2351], device='cuda:0')), ('power', tensor([-23.6114], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.16875934600830078
epoch£º619	 i:1 	 global-step:12381	 l-p:0.04133728891611099
epoch£º619	 i:2 	 global-step:12382	 l-p:0.11254659295082092
epoch£º619	 i:3 	 global-step:12383	 l-p:0.11370643228292465
epoch£º619	 i:4 	 global-step:12384	 l-p:0.14535687863826752
epoch£º619	 i:5 	 global-step:12385	 l-p:0.18993650376796722
epoch£º619	 i:6 	 global-step:12386	 l-p:0.12358900159597397
epoch£º619	 i:7 	 global-step:12387	 l-p:0.1496955156326294
epoch£º619	 i:8 	 global-step:12388	 l-p:0.1538223773241043
epoch£º619	 i:9 	 global-step:12389	 l-p:0.1348041445016861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6037, 3.5571, 3.5961],
        [3.6037, 3.2697, 3.3488],
        [3.6037, 3.6037, 3.6037],
        [3.6037, 2.9613, 2.4434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.23927661776542664 
model_pd.l_d.mean(): -23.716554641723633 
model_pd.lagr.mean(): -23.477277755737305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1833], device='cuda:0')), ('power', tensor([-23.8999], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.23927661776542664
epoch£º620	 i:1 	 global-step:12401	 l-p:0.26406970620155334
epoch£º620	 i:2 	 global-step:12402	 l-p:0.12493372708559036
epoch£º620	 i:3 	 global-step:12403	 l-p:0.15201619267463684
epoch£º620	 i:4 	 global-step:12404	 l-p:0.12285146117210388
epoch£º620	 i:5 	 global-step:12405	 l-p:0.16545093059539795
epoch£º620	 i:6 	 global-step:12406	 l-p:0.13503749668598175
epoch£º620	 i:7 	 global-step:12407	 l-p:0.11147212982177734
epoch£º620	 i:8 	 global-step:12408	 l-p:0.1307605654001236
epoch£º620	 i:9 	 global-step:12409	 l-p:-0.243433877825737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5582, 2.9395, 2.3791],
        [3.5582, 3.5564, 3.5582],
        [3.5582, 3.5582, 3.5582],
        [3.5582, 3.5582, 3.5582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.1387961357831955 
model_pd.l_d.mean(): -23.69542121887207 
model_pd.lagr.mean(): -23.556625366210938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1567], device='cuda:0')), ('power', tensor([-23.8521], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.1387961357831955
epoch£º621	 i:1 	 global-step:12421	 l-p:-0.07655985653400421
epoch£º621	 i:2 	 global-step:12422	 l-p:0.2326091080904007
epoch£º621	 i:3 	 global-step:12423	 l-p:0.05021531879901886
epoch£º621	 i:4 	 global-step:12424	 l-p:0.1513303518295288
epoch£º621	 i:5 	 global-step:12425	 l-p:0.14264433085918427
epoch£º621	 i:6 	 global-step:12426	 l-p:0.10997285693883896
epoch£º621	 i:7 	 global-step:12427	 l-p:0.12626494467258453
epoch£º621	 i:8 	 global-step:12428	 l-p:0.14065642654895782
epoch£º621	 i:9 	 global-step:12429	 l-p:-0.18141323328018188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5645, 3.5645, 3.5645],
        [3.5645, 3.0917, 3.0664],
        [3.5645, 2.9058, 2.4688],
        [3.5645, 3.1889, 3.2476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.14805929362773895 
model_pd.l_d.mean(): -23.64337921142578 
model_pd.lagr.mean(): -23.495319366455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1866], device='cuda:0')), ('power', tensor([-23.8299], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.14805929362773895
epoch£º622	 i:1 	 global-step:12441	 l-p:0.1299831122159958
epoch£º622	 i:2 	 global-step:12442	 l-p:-0.018911942839622498
epoch£º622	 i:3 	 global-step:12443	 l-p:0.1862311065196991
epoch£º622	 i:4 	 global-step:12444	 l-p:0.1205647885799408
epoch£º622	 i:5 	 global-step:12445	 l-p:0.19338957965373993
epoch£º622	 i:6 	 global-step:12446	 l-p:0.13656210899353027
epoch£º622	 i:7 	 global-step:12447	 l-p:0.1407417207956314
epoch£º622	 i:8 	 global-step:12448	 l-p:0.3403998613357544
epoch£º622	 i:9 	 global-step:12449	 l-p:10.404899597167969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5860, 3.5156, 3.5707],
        [3.5860, 3.3285, 3.4305],
        [3.5860, 3.5731, 3.5851],
        [3.5860, 3.5857, 3.5860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.14000676572322845 
model_pd.l_d.mean(): -23.323848724365234 
model_pd.lagr.mean(): -23.183841705322266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1725], device='cuda:0')), ('power', tensor([-23.4964], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.14000676572322845
epoch£º623	 i:1 	 global-step:12461	 l-p:0.47330954670906067
epoch£º623	 i:2 	 global-step:12462	 l-p:0.14353294670581818
epoch£º623	 i:3 	 global-step:12463	 l-p:0.12298939377069473
epoch£º623	 i:4 	 global-step:12464	 l-p:0.20675063133239746
epoch£º623	 i:5 	 global-step:12465	 l-p:0.1570805162191391
epoch£º623	 i:6 	 global-step:12466	 l-p:0.1133192628622055
epoch£º623	 i:7 	 global-step:12467	 l-p:0.14711643755435944
epoch£º623	 i:8 	 global-step:12468	 l-p:0.1332840919494629
epoch£º623	 i:9 	 global-step:12469	 l-p:0.1952633410692215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6159, 3.5991, 3.6145],
        [3.6159, 3.2042, 2.6421],
        [3.6159, 3.6125, 3.6158],
        [3.6159, 3.6159, 3.6159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.14002928137779236 
model_pd.l_d.mean(): -23.44480323791504 
model_pd.lagr.mean(): -23.304773330688477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1982], device='cuda:0')), ('power', tensor([-23.6430], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.14002928137779236
epoch£º624	 i:1 	 global-step:12481	 l-p:0.12470468878746033
epoch£º624	 i:2 	 global-step:12482	 l-p:0.1372026950120926
epoch£º624	 i:3 	 global-step:12483	 l-p:0.2572573125362396
epoch£º624	 i:4 	 global-step:12484	 l-p:0.11422836780548096
epoch£º624	 i:5 	 global-step:12485	 l-p:0.12246762216091156
epoch£º624	 i:6 	 global-step:12486	 l-p:0.13062287867069244
epoch£º624	 i:7 	 global-step:12487	 l-p:0.15736804902553558
epoch£º624	 i:8 	 global-step:12488	 l-p:0.5250476002693176
epoch£º624	 i:9 	 global-step:12489	 l-p:0.9437509179115295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5900, 3.5900, 3.5900],
        [3.5900, 3.5650, 3.5873],
        [3.5900, 3.5876, 3.5899],
        [3.5900, 2.9918, 2.7655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.17939212918281555 
model_pd.l_d.mean(): -23.364524841308594 
model_pd.lagr.mean(): -23.18513298034668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2377], device='cuda:0')), ('power', tensor([-23.6022], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.17939212918281555
epoch£º625	 i:1 	 global-step:12501	 l-p:0.0734240710735321
epoch£º625	 i:2 	 global-step:12502	 l-p:0.15319395065307617
epoch£º625	 i:3 	 global-step:12503	 l-p:0.1334208995103836
epoch£º625	 i:4 	 global-step:12504	 l-p:0.18541602790355682
epoch£º625	 i:5 	 global-step:12505	 l-p:0.13461731374263763
epoch£º625	 i:6 	 global-step:12506	 l-p:0.19804739952087402
epoch£º625	 i:7 	 global-step:12507	 l-p:0.115817591547966
epoch£º625	 i:8 	 global-step:12508	 l-p:0.1262497901916504
epoch£º625	 i:9 	 global-step:12509	 l-p:0.16597019135951996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6392, 3.5523, 3.6170],
        [3.6392, 3.1650, 3.1322],
        [3.6392, 3.6392, 3.6392],
        [3.6392, 3.2472, 2.6873]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.13970425724983215 
model_pd.l_d.mean(): -23.64840316772461 
model_pd.lagr.mean(): -23.508699417114258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1344], device='cuda:0')), ('power', tensor([-23.7828], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.13970425724983215
epoch£º626	 i:1 	 global-step:12521	 l-p:0.16699092090129852
epoch£º626	 i:2 	 global-step:12522	 l-p:0.11304223537445068
epoch£º626	 i:3 	 global-step:12523	 l-p:0.1747090071439743
epoch£º626	 i:4 	 global-step:12524	 l-p:0.15144947171211243
epoch£º626	 i:5 	 global-step:12525	 l-p:0.10316110402345657
epoch£º626	 i:6 	 global-step:12526	 l-p:0.1365046501159668
epoch£º626	 i:7 	 global-step:12527	 l-p:0.1373409926891327
epoch£º626	 i:8 	 global-step:12528	 l-p:0.11585131287574768
epoch£º626	 i:9 	 global-step:12529	 l-p:0.1205914095044136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6894, 3.4793, 3.5815],
        [3.6894, 3.0985, 2.8647],
        [3.6894, 3.4971, 3.5977],
        [3.6894, 3.6242, 3.6759]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.134568452835083 
model_pd.l_d.mean(): -23.566036224365234 
model_pd.lagr.mean(): -23.431467056274414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1078], device='cuda:0')), ('power', tensor([-23.6739], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.134568452835083
epoch£º627	 i:1 	 global-step:12541	 l-p:0.13318616151809692
epoch£º627	 i:2 	 global-step:12542	 l-p:0.14057141542434692
epoch£º627	 i:3 	 global-step:12543	 l-p:0.05346238985657692
epoch£º627	 i:4 	 global-step:12544	 l-p:0.11539043486118317
epoch£º627	 i:5 	 global-step:12545	 l-p:0.17096583545207977
epoch£º627	 i:6 	 global-step:12546	 l-p:0.1340503990650177
epoch£º627	 i:7 	 global-step:12547	 l-p:0.23678012192249298
epoch£º627	 i:8 	 global-step:12548	 l-p:0.13715502619743347
epoch£º627	 i:9 	 global-step:12549	 l-p:0.1358730047941208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5904, 3.0553, 2.9476],
        [3.5904, 3.5897, 3.5904],
        [3.5904, 3.5887, 3.5904],
        [3.5904, 3.4514, 3.5399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.17548222839832306 
model_pd.l_d.mean(): -23.61102867126465 
model_pd.lagr.mean(): -23.435546875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1564], device='cuda:0')), ('power', tensor([-23.7675], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.17548222839832306
epoch£º628	 i:1 	 global-step:12561	 l-p:0.13111431896686554
epoch£º628	 i:2 	 global-step:12562	 l-p:0.15579181909561157
epoch£º628	 i:3 	 global-step:12563	 l-p:0.14872415363788605
epoch£º628	 i:4 	 global-step:12564	 l-p:0.020067185163497925
epoch£º628	 i:5 	 global-step:12565	 l-p:0.13766305148601532
epoch£º628	 i:6 	 global-step:12566	 l-p:0.13197225332260132
epoch£º628	 i:7 	 global-step:12567	 l-p:0.104311004281044
epoch£º628	 i:8 	 global-step:12568	 l-p:0.11886931955814362
epoch£º628	 i:9 	 global-step:12569	 l-p:0.004669284913688898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4831, 3.4813, 3.4830],
        [3.4831, 3.0029, 2.9798],
        [3.4831, 3.4741, 3.4826],
        [3.4831, 2.9306, 2.3591]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.14857405424118042 
model_pd.l_d.mean(): -23.832773208618164 
model_pd.lagr.mean(): -23.6841983795166 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1655], device='cuda:0')), ('power', tensor([-23.9983], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.14857405424118042
epoch£º629	 i:1 	 global-step:12581	 l-p:0.05365188792347908
epoch£º629	 i:2 	 global-step:12582	 l-p:0.13556432723999023
epoch£º629	 i:3 	 global-step:12583	 l-p:0.13393010199069977
epoch£º629	 i:4 	 global-step:12584	 l-p:0.17189179360866547
epoch£º629	 i:5 	 global-step:12585	 l-p:0.08280857652425766
epoch£º629	 i:6 	 global-step:12586	 l-p:0.073402538895607
epoch£º629	 i:7 	 global-step:12587	 l-p:0.12369636446237564
epoch£º629	 i:8 	 global-step:12588	 l-p:0.1828470379114151
epoch£º629	 i:9 	 global-step:12589	 l-p:1.7348215579986572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4879, 3.4878, 3.4879],
        [3.4879, 3.0081, 2.9854],
        [3.4879, 2.9208, 2.7789],
        [3.4879, 3.4879, 3.4879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.15688078105449677 
model_pd.l_d.mean(): -23.236600875854492 
model_pd.lagr.mean(): -23.07971954345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2853], device='cuda:0')), ('power', tensor([-23.5219], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.15688078105449677
epoch£º630	 i:1 	 global-step:12601	 l-p:-0.16052813827991486
epoch£º630	 i:2 	 global-step:12602	 l-p:0.18334798514842987
epoch£º630	 i:3 	 global-step:12603	 l-p:0.08573940396308899
epoch£º630	 i:4 	 global-step:12604	 l-p:0.1915319263935089
epoch£º630	 i:5 	 global-step:12605	 l-p:0.13716700673103333
epoch£º630	 i:6 	 global-step:12606	 l-p:-0.15633735060691833
epoch£º630	 i:7 	 global-step:12607	 l-p:0.12057359516620636
epoch£º630	 i:8 	 global-step:12608	 l-p:0.13790078461170197
epoch£º630	 i:9 	 global-step:12609	 l-p:0.3629930019378662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5962, 3.5628, 3.5919],
        [3.5962, 2.9411, 2.4389],
        [3.5962, 3.5962, 3.5962],
        [3.5962, 3.3082, 3.4052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.22329041361808777 
model_pd.l_d.mean(): -23.743209838867188 
model_pd.lagr.mean(): -23.519920349121094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1197], device='cuda:0')), ('power', tensor([-23.8629], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.22329041361808777
epoch£º631	 i:1 	 global-step:12621	 l-p:0.14626915752887726
epoch£º631	 i:2 	 global-step:12622	 l-p:0.15707863867282867
epoch£º631	 i:3 	 global-step:12623	 l-p:0.12177247554063797
epoch£º631	 i:4 	 global-step:12624	 l-p:0.1784191131591797
epoch£º631	 i:5 	 global-step:12625	 l-p:0.12737436592578888
epoch£º631	 i:6 	 global-step:12626	 l-p:0.11627046018838882
epoch£º631	 i:7 	 global-step:12627	 l-p:-0.09789811819791794
epoch£º631	 i:8 	 global-step:12628	 l-p:0.12252702564001083
epoch£º631	 i:9 	 global-step:12629	 l-p:0.14959406852722168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6923, 3.5797, 3.6573],
        [3.6923, 3.6587, 3.6879],
        [3.6923, 3.6821, 3.6916],
        [3.6923, 3.6842, 3.6918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.16107016801834106 
model_pd.l_d.mean(): -22.827919006347656 
model_pd.lagr.mean(): -22.66684913635254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1978], device='cuda:0')), ('power', tensor([-23.0257], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.16107016801834106
epoch£º632	 i:1 	 global-step:12641	 l-p:0.1210220605134964
epoch£º632	 i:2 	 global-step:12642	 l-p:0.12432512640953064
epoch£º632	 i:3 	 global-step:12643	 l-p:0.12788166105747223
epoch£º632	 i:4 	 global-step:12644	 l-p:-0.010103091597557068
epoch£º632	 i:5 	 global-step:12645	 l-p:0.12546920776367188
epoch£º632	 i:6 	 global-step:12646	 l-p:0.11340482532978058
epoch£º632	 i:7 	 global-step:12647	 l-p:0.20244060456752777
epoch£º632	 i:8 	 global-step:12648	 l-p:0.14182980358600616
epoch£º632	 i:9 	 global-step:12649	 l-p:0.1369318813085556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6203, 3.5918, 3.6170],
        [3.6203, 3.0390, 2.8481],
        [3.6203, 3.3764, 3.4799],
        [3.6203, 3.3442, 2.8143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.12609480321407318 
model_pd.l_d.mean(): -23.083999633789062 
model_pd.lagr.mean(): -22.957904815673828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2397], device='cuda:0')), ('power', tensor([-23.3237], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.12609480321407318
epoch£º633	 i:1 	 global-step:12661	 l-p:0.09523744136095047
epoch£º633	 i:2 	 global-step:12662	 l-p:0.15672199428081512
epoch£º633	 i:3 	 global-step:12663	 l-p:-0.41369864344596863
epoch£º633	 i:4 	 global-step:12664	 l-p:0.12281936407089233
epoch£º633	 i:5 	 global-step:12665	 l-p:0.1358032524585724
epoch£º633	 i:6 	 global-step:12666	 l-p:0.140072762966156
epoch£º633	 i:7 	 global-step:12667	 l-p:0.15202470123767853
epoch£º633	 i:8 	 global-step:12668	 l-p:0.07887080311775208
epoch£º633	 i:9 	 global-step:12669	 l-p:-0.6695761680603027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4955, 3.4955, 3.4955],
        [3.4955, 3.4879, 3.4951],
        [3.4955, 3.2691, 3.3752],
        [3.4955, 3.2929, 3.3973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.12681826949119568 
model_pd.l_d.mean(): -23.21024513244629 
model_pd.lagr.mean(): -23.08342742919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2773], device='cuda:0')), ('power', tensor([-23.4875], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.12681826949119568
epoch£º634	 i:1 	 global-step:12681	 l-p:0.1703803539276123
epoch£º634	 i:2 	 global-step:12682	 l-p:0.1635533720254898
epoch£º634	 i:3 	 global-step:12683	 l-p:0.17269039154052734
epoch£º634	 i:4 	 global-step:12684	 l-p:1.4654046297073364
epoch£º634	 i:5 	 global-step:12685	 l-p:0.14856268465518951
epoch£º634	 i:6 	 global-step:12686	 l-p:0.10343591123819351
epoch£º634	 i:7 	 global-step:12687	 l-p:-0.002791881561279297
epoch£º634	 i:8 	 global-step:12688	 l-p:0.10420586168766022
epoch£º634	 i:9 	 global-step:12689	 l-p:0.14796577394008636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5015, 3.1516, 3.2300],
        [3.5015, 3.1807, 3.2713],
        [3.5015, 3.4997, 3.5014],
        [3.5015, 2.8317, 2.4279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.15044738352298737 
model_pd.l_d.mean(): -23.585102081298828 
model_pd.lagr.mean(): -23.434654235839844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2516], device='cuda:0')), ('power', tensor([-23.8367], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.15044738352298737
epoch£º635	 i:1 	 global-step:12701	 l-p:0.139628067612648
epoch£º635	 i:2 	 global-step:12702	 l-p:0.10500340908765793
epoch£º635	 i:3 	 global-step:12703	 l-p:0.14536769688129425
epoch£º635	 i:4 	 global-step:12704	 l-p:0.3263952434062958
epoch£º635	 i:5 	 global-step:12705	 l-p:-0.2660183310508728
epoch£º635	 i:6 	 global-step:12706	 l-p:0.08613137900829315
epoch£º635	 i:7 	 global-step:12707	 l-p:0.12848439812660217
epoch£º635	 i:8 	 global-step:12708	 l-p:0.014411544427275658
epoch£º635	 i:9 	 global-step:12709	 l-p:0.16145284473896027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5865, 3.4592, 3.5436],
        [3.5865, 3.5507, 3.5816],
        [3.5865, 2.9269, 2.4226],
        [3.5865, 3.5863, 3.5865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.10238295048475266 
model_pd.l_d.mean(): -23.445537567138672 
model_pd.lagr.mean(): -23.343154907226562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1700], device='cuda:0')), ('power', tensor([-23.6155], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.10238295048475266
epoch£º636	 i:1 	 global-step:12721	 l-p:0.1209033951163292
epoch£º636	 i:2 	 global-step:12722	 l-p:0.23260430991649628
epoch£º636	 i:3 	 global-step:12723	 l-p:0.11734496057033539
epoch£º636	 i:4 	 global-step:12724	 l-p:0.20078079402446747
epoch£º636	 i:5 	 global-step:12725	 l-p:0.1932661235332489
epoch£º636	 i:6 	 global-step:12726	 l-p:0.12845315039157867
epoch£º636	 i:7 	 global-step:12727	 l-p:0.1397213190793991
epoch£º636	 i:8 	 global-step:12728	 l-p:0.14187096059322357
epoch£º636	 i:9 	 global-step:12729	 l-p:0.1422131508588791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6487, 3.2101, 3.2180],
        [3.6487, 3.3755, 2.8439],
        [3.6487, 3.3579, 3.4534],
        [3.6487, 3.6149, 3.6443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.13679832220077515 
model_pd.l_d.mean(): -22.8358097076416 
model_pd.lagr.mean(): -22.699010848999023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2212], device='cuda:0')), ('power', tensor([-23.0570], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.13679832220077515
epoch£º637	 i:1 	 global-step:12741	 l-p:0.11504122614860535
epoch£º637	 i:2 	 global-step:12742	 l-p:0.149039626121521
epoch£º637	 i:3 	 global-step:12743	 l-p:0.1324164867401123
epoch£º637	 i:4 	 global-step:12744	 l-p:0.11664178222417831
epoch£º637	 i:5 	 global-step:12745	 l-p:0.12395164370536804
epoch£º637	 i:6 	 global-step:12746	 l-p:-0.16190993785858154
epoch£º637	 i:7 	 global-step:12747	 l-p:0.22964797914028168
epoch£º637	 i:8 	 global-step:12748	 l-p:0.15627023577690125
epoch£º637	 i:9 	 global-step:12749	 l-p:0.12077786773443222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5407, 3.5401, 3.5406],
        [3.5407, 3.5407, 3.5407],
        [3.5407, 2.9967, 2.4192],
        [3.5407, 3.1293, 2.5719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.07663361728191376 
model_pd.l_d.mean(): -23.484895706176758 
model_pd.lagr.mean(): -23.408262252807617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3106], device='cuda:0')), ('power', tensor([-23.7955], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.07663361728191376
epoch£º638	 i:1 	 global-step:12761	 l-p:0.31705981492996216
epoch£º638	 i:2 	 global-step:12762	 l-p:0.1621696799993515
epoch£º638	 i:3 	 global-step:12763	 l-p:0.12473270297050476
epoch£º638	 i:4 	 global-step:12764	 l-p:0.13790977001190186
epoch£º638	 i:5 	 global-step:12765	 l-p:0.18501001596450806
epoch£º638	 i:6 	 global-step:12766	 l-p:0.12853582203388214
epoch£º638	 i:7 	 global-step:12767	 l-p:0.14008982479572296
epoch£º638	 i:8 	 global-step:12768	 l-p:0.03962944075465202
epoch£º638	 i:9 	 global-step:12769	 l-p:0.11912009119987488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5619, 3.1808, 2.6279],
        [3.5619, 3.5543, 3.5615],
        [3.5619, 3.0651, 3.0188],
        [3.5619, 3.1038, 3.1004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.09300557523965836 
model_pd.l_d.mean(): -22.16282844543457 
model_pd.lagr.mean(): -22.069822311401367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4171], device='cuda:0')), ('power', tensor([-22.5799], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.09300557523965836
epoch£º639	 i:1 	 global-step:12781	 l-p:0.1616504192352295
epoch£º639	 i:2 	 global-step:12782	 l-p:0.22446471452713013
epoch£º639	 i:3 	 global-step:12783	 l-p:0.1745014488697052
epoch£º639	 i:4 	 global-step:12784	 l-p:0.1409061849117279
epoch£º639	 i:5 	 global-step:12785	 l-p:5.22789192199707
epoch£º639	 i:6 	 global-step:12786	 l-p:0.13482941687107086
epoch£º639	 i:7 	 global-step:12787	 l-p:0.14309829473495483
epoch£º639	 i:8 	 global-step:12788	 l-p:0.15298879146575928
epoch£º639	 i:9 	 global-step:12789	 l-p:0.13311828672885895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5739, 3.0958, 3.0704],
        [3.5739, 2.9858, 2.7965],
        [3.5739, 3.5740, 3.5739],
        [3.5739, 3.5381, 3.5691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 2.3244502544403076 
model_pd.l_d.mean(): -23.221986770629883 
model_pd.lagr.mean(): -20.897537231445312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3161], device='cuda:0')), ('power', tensor([-23.5381], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:2.3244502544403076
epoch£º640	 i:1 	 global-step:12801	 l-p:0.14007756114006042
epoch£º640	 i:2 	 global-step:12802	 l-p:0.19568289816379547
epoch£º640	 i:3 	 global-step:12803	 l-p:0.1879916787147522
epoch£º640	 i:4 	 global-step:12804	 l-p:-1.6446053981781006
epoch£º640	 i:5 	 global-step:12805	 l-p:0.11929697543382645
epoch£º640	 i:6 	 global-step:12806	 l-p:0.10518050938844681
epoch£º640	 i:7 	 global-step:12807	 l-p:0.12170080095529556
epoch£º640	 i:8 	 global-step:12808	 l-p:0.2645004093647003
epoch£º640	 i:9 	 global-step:12809	 l-p:0.1316249668598175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6162, 3.6134, 3.6161],
        [3.6162, 2.9639, 2.4421],
        [3.6162, 3.6161, 3.6162],
        [3.6162, 3.4890, 3.5733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.1458105742931366 
model_pd.l_d.mean(): -23.57406997680664 
model_pd.lagr.mean(): -23.428258895874023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2116], device='cuda:0')), ('power', tensor([-23.7857], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.1458105742931366
epoch£º641	 i:1 	 global-step:12821	 l-p:0.16942831873893738
epoch£º641	 i:2 	 global-step:12822	 l-p:0.09947957843542099
epoch£º641	 i:3 	 global-step:12823	 l-p:0.17175503075122833
epoch£º641	 i:4 	 global-step:12824	 l-p:0.13327986001968384
epoch£º641	 i:5 	 global-step:12825	 l-p:0.14634931087493896
epoch£º641	 i:6 	 global-step:12826	 l-p:0.12897242605686188
epoch£º641	 i:7 	 global-step:12827	 l-p:0.15338550508022308
epoch£º641	 i:8 	 global-step:12828	 l-p:0.18512988090515137
epoch£º641	 i:9 	 global-step:12829	 l-p:0.12999944388866425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6230, 3.6226, 3.6230],
        [3.6230, 3.1682, 3.1638],
        [3.6230, 3.6230, 3.6230],
        [3.6230, 3.2753, 2.7249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.18163485825061798 
model_pd.l_d.mean(): -22.69203758239746 
model_pd.lagr.mean(): -22.51040267944336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3339], device='cuda:0')), ('power', tensor([-23.0260], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:0.18163485825061798
epoch£º642	 i:1 	 global-step:12841	 l-p:0.14473021030426025
epoch£º642	 i:2 	 global-step:12842	 l-p:0.24144577980041504
epoch£º642	 i:3 	 global-step:12843	 l-p:0.12948746979236603
epoch£º642	 i:4 	 global-step:12844	 l-p:0.15345554053783417
epoch£º642	 i:5 	 global-step:12845	 l-p:0.10817041248083115
epoch£º642	 i:6 	 global-step:12846	 l-p:0.12581323087215424
epoch£º642	 i:7 	 global-step:12847	 l-p:0.10771602392196655
epoch£º642	 i:8 	 global-step:12848	 l-p:0.3216058015823364
epoch£º642	 i:9 	 global-step:12849	 l-p:0.14121569693088531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5974, 3.4571, 3.5464],
        [3.5974, 3.5974, 3.5974],
        [3.5974, 2.9338, 2.5030],
        [3.5974, 3.5974, 3.5974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.14875741302967072 
model_pd.l_d.mean(): -22.569778442382812 
model_pd.lagr.mean(): -22.4210205078125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3069], device='cuda:0')), ('power', tensor([-22.8767], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.14875741302967072
epoch£º643	 i:1 	 global-step:12861	 l-p:0.11807237565517426
epoch£º643	 i:2 	 global-step:12862	 l-p:0.1290491819381714
epoch£º643	 i:3 	 global-step:12863	 l-p:0.19359901547431946
epoch£º643	 i:4 	 global-step:12864	 l-p:0.17032504081726074
epoch£º643	 i:5 	 global-step:12865	 l-p:-0.22752009332180023
epoch£º643	 i:6 	 global-step:12866	 l-p:-2.7235167026519775
epoch£º643	 i:7 	 global-step:12867	 l-p:-0.08576920628547668
epoch£º643	 i:8 	 global-step:12868	 l-p:0.11964654922485352
epoch£º643	 i:9 	 global-step:12869	 l-p:0.1331431120634079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[3.5825, 3.0433, 2.9391],
        [3.5825, 2.9161, 2.4849],
        [3.5825, 3.1179, 3.1078],
        [3.5825, 3.3670, 3.4720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): -6.955118656158447 
model_pd.l_d.mean(): -23.35455894470215 
model_pd.lagr.mean(): -30.309677124023438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2646], device='cuda:0')), ('power', tensor([-23.6191], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:-6.955118656158447
epoch£º644	 i:1 	 global-step:12881	 l-p:0.1481882482767105
epoch£º644	 i:2 	 global-step:12882	 l-p:0.13364273309707642
epoch£º644	 i:3 	 global-step:12883	 l-p:0.1259719431400299
epoch£º644	 i:4 	 global-step:12884	 l-p:0.15293513238430023
epoch£º644	 i:5 	 global-step:12885	 l-p:0.14122523367404938
epoch£º644	 i:6 	 global-step:12886	 l-p:0.13768692314624786
epoch£º644	 i:7 	 global-step:12887	 l-p:0.21011626720428467
epoch£º644	 i:8 	 global-step:12888	 l-p:0.015656283125281334
epoch£º644	 i:9 	 global-step:12889	 l-p:-0.05404381453990936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4904, 3.3672, 3.4504],
        [3.4904, 3.0688, 3.1049],
        [3.4904, 2.8185, 2.2830],
        [3.4904, 3.4839, 3.4901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.15631631016731262 
model_pd.l_d.mean(): -23.238771438598633 
model_pd.lagr.mean(): -23.082454681396484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3124], device='cuda:0')), ('power', tensor([-23.5512], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.15631631016731262
epoch£º645	 i:1 	 global-step:12901	 l-p:2.854769468307495
epoch£º645	 i:2 	 global-step:12902	 l-p:0.12323367595672607
epoch£º645	 i:3 	 global-step:12903	 l-p:0.11991073936223984
epoch£º645	 i:4 	 global-step:12904	 l-p:0.13267451524734497
epoch£º645	 i:5 	 global-step:12905	 l-p:0.027763595804572105
epoch£º645	 i:6 	 global-step:12906	 l-p:0.15738680958747864
epoch£º645	 i:7 	 global-step:12907	 l-p:0.06318830698728561
epoch£º645	 i:8 	 global-step:12908	 l-p:0.0959802195429802
epoch£º645	 i:9 	 global-step:12909	 l-p:0.13343743979930878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[3.5525, 2.8787, 2.3949],
        [3.5525, 3.1780, 2.6266],
        [3.5525, 3.2056, 2.6607],
        [3.5525, 3.1052, 3.1153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.056189462542533875 
model_pd.l_d.mean(): -22.81773567199707 
model_pd.lagr.mean(): -22.761547088623047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3319], device='cuda:0')), ('power', tensor([-23.1496], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.056189462542533875
epoch£º646	 i:1 	 global-step:12921	 l-p:0.16579307615756989
epoch£º646	 i:2 	 global-step:12922	 l-p:0.12805503606796265
epoch£º646	 i:3 	 global-step:12923	 l-p:0.137115940451622
epoch£º646	 i:4 	 global-step:12924	 l-p:0.15026408433914185
epoch£º646	 i:5 	 global-step:12925	 l-p:-0.1725698858499527
epoch£º646	 i:6 	 global-step:12926	 l-p:0.12397235631942749
epoch£º646	 i:7 	 global-step:12927	 l-p:0.1285095363855362
epoch£º646	 i:8 	 global-step:12928	 l-p:0.13737311959266663
epoch£º646	 i:9 	 global-step:12929	 l-p:0.47268351912498474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5815, 3.5815, 3.5815],
        [3.5815, 3.5809, 3.5815],
        [3.5815, 2.9146, 2.4921],
        [3.5815, 3.5808, 3.5815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): -0.16712680459022522 
model_pd.l_d.mean(): -23.228580474853516 
model_pd.lagr.mean(): -23.395708084106445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2948], device='cuda:0')), ('power', tensor([-23.5234], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:-0.16712680459022522
epoch£º647	 i:1 	 global-step:12941	 l-p:0.1295216679573059
epoch£º647	 i:2 	 global-step:12942	 l-p:0.12318351119756699
epoch£º647	 i:3 	 global-step:12943	 l-p:0.12366735190153122
epoch£º647	 i:4 	 global-step:12944	 l-p:-0.01367102563381195
epoch£º647	 i:5 	 global-step:12945	 l-p:-0.04279907047748566
epoch£º647	 i:6 	 global-step:12946	 l-p:0.16237832605838776
epoch£º647	 i:7 	 global-step:12947	 l-p:0.20386897027492523
epoch£º647	 i:8 	 global-step:12948	 l-p:0.1368885338306427
epoch£º647	 i:9 	 global-step:12949	 l-p:0.1453525573015213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5900, 2.9362, 2.3966],
        [3.5900, 3.5899, 3.5900],
        [3.5900, 3.5126, 3.5721],
        [3.5900, 3.5810, 3.5895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 7.781588554382324 
model_pd.l_d.mean(): -23.662616729736328 
model_pd.lagr.mean(): -15.881028175354004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1969], device='cuda:0')), ('power', tensor([-23.8595], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:7.781588554382324
epoch£º648	 i:1 	 global-step:12961	 l-p:0.10375228524208069
epoch£º648	 i:2 	 global-step:12962	 l-p:0.1384202539920807
epoch£º648	 i:3 	 global-step:12963	 l-p:0.12147559225559235
epoch£º648	 i:4 	 global-step:12964	 l-p:0.1504671275615692
epoch£º648	 i:5 	 global-step:12965	 l-p:0.1582878977060318
epoch£º648	 i:6 	 global-step:12966	 l-p:0.12818042933940887
epoch£º648	 i:7 	 global-step:12967	 l-p:0.15391795337200165
epoch£º648	 i:8 	 global-step:12968	 l-p:0.11208362132310867
epoch£º648	 i:9 	 global-step:12969	 l-p:0.2729630172252655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5908, 3.5149, 3.5735],
        [3.5908, 3.5849, 3.5905],
        [3.5908, 3.5863, 3.5906],
        [3.5908, 3.1253, 3.1150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.12047822773456573 
model_pd.l_d.mean(): -23.218238830566406 
model_pd.lagr.mean(): -23.097761154174805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2231], device='cuda:0')), ('power', tensor([-23.4414], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.12047822773456573
epoch£º649	 i:1 	 global-step:12981	 l-p:0.11917129158973694
epoch£º649	 i:2 	 global-step:12982	 l-p:-0.15557698905467987
epoch£º649	 i:3 	 global-step:12983	 l-p:0.14292624592781067
epoch£º649	 i:4 	 global-step:12984	 l-p:0.15077649056911469
epoch£º649	 i:5 	 global-step:12985	 l-p:0.09530939906835556
epoch£º649	 i:6 	 global-step:12986	 l-p:0.12936381995677948
epoch£º649	 i:7 	 global-step:12987	 l-p:0.13110317289829254
epoch£º649	 i:8 	 global-step:12988	 l-p:0.006993875373154879
epoch£º649	 i:9 	 global-step:12989	 l-p:0.15296220779418945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4866, 3.2817, 3.3873],
        [3.4866, 3.3866, 3.4590],
        [3.4866, 2.8182, 2.4578],
        [3.4866, 3.1343, 3.2147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 1.5042577981948853 
model_pd.l_d.mean(): -23.63351821899414 
model_pd.lagr.mean(): -22.129261016845703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2334], device='cuda:0')), ('power', tensor([-23.8669], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:1.5042577981948853
epoch£º650	 i:1 	 global-step:13001	 l-p:0.09232041239738464
epoch£º650	 i:2 	 global-step:13002	 l-p:0.16384898126125336
epoch£º650	 i:3 	 global-step:13003	 l-p:0.008989953435957432
epoch£º650	 i:4 	 global-step:13004	 l-p:0.14293032884597778
epoch£º650	 i:5 	 global-step:13005	 l-p:0.1482580602169037
epoch£º650	 i:6 	 global-step:13006	 l-p:0.1511978954076767
epoch£º650	 i:7 	 global-step:13007	 l-p:0.15843574702739716
epoch£º650	 i:8 	 global-step:13008	 l-p:0.15426400303840637
epoch£º650	 i:9 	 global-step:13009	 l-p:0.14573505520820618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4824, 3.4824, 3.4824],
        [3.4824, 3.0742, 2.5220],
        [3.4824, 3.4824, 3.4824],
        [3.4824, 3.0587, 3.0952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.16234011948108673 
model_pd.l_d.mean(): -23.388416290283203 
model_pd.lagr.mean(): -23.226076126098633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3018], device='cuda:0')), ('power', tensor([-23.6902], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.16234011948108673
epoch£º651	 i:1 	 global-step:13021	 l-p:0.10691644251346588
epoch£º651	 i:2 	 global-step:13022	 l-p:0.5337666273117065
epoch£º651	 i:3 	 global-step:13023	 l-p:0.10304516553878784
epoch£º651	 i:4 	 global-step:13024	 l-p:0.10620306432247162
epoch£º651	 i:5 	 global-step:13025	 l-p:0.13809418678283691
epoch£º651	 i:6 	 global-step:13026	 l-p:0.45773616433143616
epoch£º651	 i:7 	 global-step:13027	 l-p:0.14143186807632446
epoch£º651	 i:8 	 global-step:13028	 l-p:0.13648122549057007
epoch£º651	 i:9 	 global-step:13029	 l-p:0.15678957104682922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5914, 3.3456, 3.4512],
        [3.5914, 3.5681, 3.5890],
        [3.5914, 3.5914, 3.5914],
        [3.5914, 3.5888, 3.5913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.1552313268184662 
model_pd.l_d.mean(): -23.47677993774414 
model_pd.lagr.mean(): -23.321548461914062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1748], device='cuda:0')), ('power', tensor([-23.6516], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.1552313268184662
epoch£º652	 i:1 	 global-step:13041	 l-p:0.1314721256494522
epoch£º652	 i:2 	 global-step:13042	 l-p:0.15806475281715393
epoch£º652	 i:3 	 global-step:13043	 l-p:0.20831497013568878
epoch£º652	 i:4 	 global-step:13044	 l-p:0.19995194673538208
epoch£º652	 i:5 	 global-step:13045	 l-p:0.13830584287643433
epoch£º652	 i:6 	 global-step:13046	 l-p:0.12965364754199982
epoch£º652	 i:7 	 global-step:13047	 l-p:0.1313663125038147
epoch£º652	 i:8 	 global-step:13048	 l-p:0.07281380146741867
epoch£º652	 i:9 	 global-step:13049	 l-p:0.1532353013753891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6578, 3.0865, 2.5009],
        [3.6578, 2.9965, 2.5252],
        [3.6578, 3.0910, 2.5050],
        [3.6578, 3.5175, 3.6068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.13386046886444092 
model_pd.l_d.mean(): -23.246057510375977 
model_pd.lagr.mean(): -23.112197875976562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2073], device='cuda:0')), ('power', tensor([-23.4533], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.13386046886444092
epoch£º653	 i:1 	 global-step:13061	 l-p:0.11419738829135895
epoch£º653	 i:2 	 global-step:13062	 l-p:0.12748225033283234
epoch£º653	 i:3 	 global-step:13063	 l-p:0.12392957508563995
epoch£º653	 i:4 	 global-step:13064	 l-p:0.13313840329647064
epoch£º653	 i:5 	 global-step:13065	 l-p:0.15651220083236694
epoch£º653	 i:6 	 global-step:13066	 l-p:0.5140096545219421
epoch£º653	 i:7 	 global-step:13067	 l-p:0.3225019872188568
epoch£º653	 i:8 	 global-step:13068	 l-p:0.19841749966144562
epoch£º653	 i:9 	 global-step:13069	 l-p:0.1550365388393402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5883, 2.9895, 2.4079],
        [3.5883, 3.2088, 3.2708],
        [3.5883, 3.5883, 3.5883],
        [3.5883, 3.1440, 3.1565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 2.251842975616455 
model_pd.l_d.mean(): -22.98398208618164 
model_pd.lagr.mean(): -20.732139587402344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2667], device='cuda:0')), ('power', tensor([-23.2507], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:2.251842975616455
epoch£º654	 i:1 	 global-step:13081	 l-p:0.15816694498062134
epoch£º654	 i:2 	 global-step:13082	 l-p:0.137831911444664
epoch£º654	 i:3 	 global-step:13083	 l-p:0.16446109116077423
epoch£º654	 i:4 	 global-step:13084	 l-p:0.13343551754951477
epoch£º654	 i:5 	 global-step:13085	 l-p:0.15214014053344727
epoch£º654	 i:6 	 global-step:13086	 l-p:0.12442784756422043
epoch£º654	 i:7 	 global-step:13087	 l-p:0.1785508692264557
epoch£º654	 i:8 	 global-step:13088	 l-p:0.14623454213142395
epoch£º654	 i:9 	 global-step:13089	 l-p:0.12320484966039658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[3.6486, 3.0600, 2.8648],
        [3.6486, 3.0039, 2.6623],
        [3.6486, 3.1864, 3.1766],
        [3.6486, 3.3579, 3.4556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.17130465805530548 
model_pd.l_d.mean(): -23.516685485839844 
model_pd.lagr.mean(): -23.345380783081055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1884], device='cuda:0')), ('power', tensor([-23.7051], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.17130465805530548
epoch£º655	 i:1 	 global-step:13101	 l-p:0.19451451301574707
epoch£º655	 i:2 	 global-step:13102	 l-p:0.12841211259365082
epoch£º655	 i:3 	 global-step:13103	 l-p:0.14629526436328888
epoch£º655	 i:4 	 global-step:13104	 l-p:0.1346510797739029
epoch£º655	 i:5 	 global-step:13105	 l-p:0.12001754343509674
epoch£º655	 i:6 	 global-step:13106	 l-p:0.12360730767250061
epoch£º655	 i:7 	 global-step:13107	 l-p:0.19799134135246277
epoch£º655	 i:8 	 global-step:13108	 l-p:0.11716987192630768
epoch£º655	 i:9 	 global-step:13109	 l-p:0.12217611074447632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6042, 3.5628, 3.5981],
        [3.6042, 3.5376, 3.5905],
        [3.6042, 3.5746, 3.6008],
        [3.6042, 2.9333, 2.4748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.11739864200353622 
model_pd.l_d.mean(): -23.52018165588379 
model_pd.lagr.mean(): -23.402782440185547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1814], device='cuda:0')), ('power', tensor([-23.7015], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.11739864200353622
epoch£º656	 i:1 	 global-step:13121	 l-p:0.12611670792102814
epoch£º656	 i:2 	 global-step:13122	 l-p:0.15246306359767914
epoch£º656	 i:3 	 global-step:13123	 l-p:0.12737639248371124
epoch£º656	 i:4 	 global-step:13124	 l-p:0.13544847071170807
epoch£º656	 i:5 	 global-step:13125	 l-p:0.18096081912517548
epoch£º656	 i:6 	 global-step:13126	 l-p:0.13714353740215302
epoch£º656	 i:7 	 global-step:13127	 l-p:0.4172956645488739
epoch£º656	 i:8 	 global-step:13128	 l-p:0.07574886083602905
epoch£º656	 i:9 	 global-step:13129	 l-p:0.06083829700946808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[3.5538, 3.1032, 3.1135],
        [3.5538, 2.9721, 2.8106],
        [3.5538, 2.8939, 2.3423],
        [3.5538, 2.8741, 2.3917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.14305423200130463 
model_pd.l_d.mean(): -23.480337142944336 
model_pd.lagr.mean(): -23.337282180786133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2682], device='cuda:0')), ('power', tensor([-23.7486], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.14305423200130463
epoch£º657	 i:1 	 global-step:13141	 l-p:0.09086985141038895
epoch£º657	 i:2 	 global-step:13142	 l-p:0.151004821062088
epoch£º657	 i:3 	 global-step:13143	 l-p:0.13479562103748322
epoch£º657	 i:4 	 global-step:13144	 l-p:0.27520954608917236
epoch£º657	 i:5 	 global-step:13145	 l-p:0.036307286471128464
epoch£º657	 i:6 	 global-step:13146	 l-p:0.0567794106900692
epoch£º657	 i:7 	 global-step:13147	 l-p:0.11449725925922394
epoch£º657	 i:8 	 global-step:13148	 l-p:0.14322738349437714
epoch£º657	 i:9 	 global-step:13149	 l-p:0.1536259651184082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5805, 3.5804, 3.5805],
        [3.5805, 2.9745, 2.7611],
        [3.5805, 3.0874, 3.0513],
        [3.5805, 3.0603, 2.9902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): -0.3855649530887604 
model_pd.l_d.mean(): -23.2901668548584 
model_pd.lagr.mean(): -23.675731658935547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2431], device='cuda:0')), ('power', tensor([-23.5333], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:-0.3855649530887604
epoch£º658	 i:1 	 global-step:13161	 l-p:0.14267860352993011
epoch£º658	 i:2 	 global-step:13162	 l-p:0.14412528276443481
epoch£º658	 i:3 	 global-step:13163	 l-p:0.17221279442310333
epoch£º658	 i:4 	 global-step:13164	 l-p:0.1903972029685974
epoch£º658	 i:5 	 global-step:13165	 l-p:0.26158151030540466
epoch£º658	 i:6 	 global-step:13166	 l-p:0.1412041336297989
epoch£º658	 i:7 	 global-step:13167	 l-p:0.12810002267360687
epoch£º658	 i:8 	 global-step:13168	 l-p:0.14371584355831146
epoch£º658	 i:9 	 global-step:13169	 l-p:0.09863840788602829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6114, 2.9555, 2.4128],
        [3.6114, 3.6108, 3.6114],
        [3.6114, 3.0434, 2.8967],
        [3.6114, 3.6096, 3.6114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.12224647402763367 
model_pd.l_d.mean(): -23.44556999206543 
model_pd.lagr.mean(): -23.32332420349121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1631], device='cuda:0')), ('power', tensor([-23.6087], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.12224647402763367
epoch£º659	 i:1 	 global-step:13181	 l-p:0.2586228549480438
epoch£º659	 i:2 	 global-step:13182	 l-p:0.14296136796474457
epoch£º659	 i:3 	 global-step:13183	 l-p:0.10951562970876694
epoch£º659	 i:4 	 global-step:13184	 l-p:0.36817657947540283
epoch£º659	 i:5 	 global-step:13185	 l-p:0.12865589559078217
epoch£º659	 i:6 	 global-step:13186	 l-p:0.1562623828649521
epoch£º659	 i:7 	 global-step:13187	 l-p:0.1840989887714386
epoch£º659	 i:8 	 global-step:13188	 l-p:0.20140309631824493
epoch£º659	 i:9 	 global-step:13189	 l-p:0.12764737010002136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6402, 3.1080, 2.5205],
        [3.6402, 3.3800, 3.4841],
        [3.6402, 2.9967, 2.4426],
        [3.6402, 3.6400, 3.6402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.1840256303548813 
model_pd.l_d.mean(): -22.983570098876953 
model_pd.lagr.mean(): -22.799545288085938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3144], device='cuda:0')), ('power', tensor([-23.2980], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.1840256303548813
epoch£º660	 i:1 	 global-step:13201	 l-p:0.1250939667224884
epoch£º660	 i:2 	 global-step:13202	 l-p:0.1278577446937561
epoch£º660	 i:3 	 global-step:13203	 l-p:0.16333431005477905
epoch£º660	 i:4 	 global-step:13204	 l-p:0.21604694426059723
epoch£º660	 i:5 	 global-step:13205	 l-p:0.045652035623788834
epoch£º660	 i:6 	 global-step:13206	 l-p:0.14070142805576324
epoch£º660	 i:7 	 global-step:13207	 l-p:0.11398688703775406
epoch£º660	 i:8 	 global-step:13208	 l-p:0.11954238265752792
epoch£º660	 i:9 	 global-step:13209	 l-p:0.1310153752565384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6768, 3.6750, 3.6768],
        [3.6768, 3.4608, 3.5656],
        [3.6768, 3.6234, 3.6673],
        [3.6768, 3.3456, 3.4297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.12845134735107422 
model_pd.l_d.mean(): -23.139541625976562 
model_pd.lagr.mean(): -23.011089324951172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2060], device='cuda:0')), ('power', tensor([-23.3455], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.12845134735107422
epoch£º661	 i:1 	 global-step:13221	 l-p:0.12539882957935333
epoch£º661	 i:2 	 global-step:13222	 l-p:0.172734797000885
epoch£º661	 i:3 	 global-step:13223	 l-p:0.14545375108718872
epoch£º661	 i:4 	 global-step:13224	 l-p:0.16465561091899872
epoch£º661	 i:5 	 global-step:13225	 l-p:0.09983458369970322
epoch£º661	 i:6 	 global-step:13226	 l-p:0.1304146945476532
epoch£º661	 i:7 	 global-step:13227	 l-p:0.2162058800458908
epoch£º661	 i:8 	 global-step:13228	 l-p:0.13945354521274567
epoch£º661	 i:9 	 global-step:13229	 l-p:-0.17568226158618927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5893, 3.5893, 3.5893],
        [3.5893, 3.5893, 3.5893],
        [3.5893, 3.5455, 3.5826],
        [3.5893, 3.1691, 3.2040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.13096590340137482 
model_pd.l_d.mean(): -23.675825119018555 
model_pd.lagr.mean(): -23.544858932495117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1260], device='cuda:0')), ('power', tensor([-23.8018], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.13096590340137482
epoch£º662	 i:1 	 global-step:13241	 l-p:0.15532483160495758
epoch£º662	 i:2 	 global-step:13242	 l-p:0.1287727653980255
epoch£º662	 i:3 	 global-step:13243	 l-p:0.16291362047195435
epoch£º662	 i:4 	 global-step:13244	 l-p:0.365142822265625
epoch£º662	 i:5 	 global-step:13245	 l-p:0.11887187510728836
epoch£º662	 i:6 	 global-step:13246	 l-p:0.21837686002254486
epoch£º662	 i:7 	 global-step:13247	 l-p:0.09348756819963455
epoch£º662	 i:8 	 global-step:13248	 l-p:0.13041631877422333
epoch£º662	 i:9 	 global-step:13249	 l-p:0.16153712570667267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6705, 3.1020, 2.5133],
        [3.6705, 3.5460, 3.6294],
        [3.6705, 3.0979, 2.9344],
        [3.6705, 3.6451, 3.6678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.12218040972948074 
model_pd.l_d.mean(): -23.473054885864258 
model_pd.lagr.mean(): -23.350873947143555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1334], device='cuda:0')), ('power', tensor([-23.6064], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.12218040972948074
epoch£º663	 i:1 	 global-step:13261	 l-p:0.10985439270734787
epoch£º663	 i:2 	 global-step:13262	 l-p:0.139358788728714
epoch£º663	 i:3 	 global-step:13263	 l-p:0.14813105762004852
epoch£º663	 i:4 	 global-step:13264	 l-p:0.18555405735969543
epoch£º663	 i:5 	 global-step:13265	 l-p:0.11906370520591736
epoch£º663	 i:6 	 global-step:13266	 l-p:0.13177497684955597
epoch£º663	 i:7 	 global-step:13267	 l-p:0.15420430898666382
epoch£º663	 i:8 	 global-step:13268	 l-p:0.17993620038032532
epoch£º663	 i:9 	 global-step:13269	 l-p:0.1952947974205017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6366, 3.6366, 3.6366],
        [3.6366, 3.0346, 2.8205],
        [3.6366, 3.6332, 3.6365],
        [3.6366, 3.2433, 3.2956]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.169754296541214 
model_pd.l_d.mean(): -23.381017684936523 
model_pd.lagr.mean(): -23.21126365661621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2082], device='cuda:0')), ('power', tensor([-23.5892], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.169754296541214
epoch£º664	 i:1 	 global-step:13281	 l-p:0.14092473685741425
epoch£º664	 i:2 	 global-step:13282	 l-p:0.12069805711507797
epoch£º664	 i:3 	 global-step:13283	 l-p:0.14985017478466034
epoch£º664	 i:4 	 global-step:13284	 l-p:0.12367265671491623
epoch£º664	 i:5 	 global-step:13285	 l-p:0.12467984855175018
epoch£º664	 i:6 	 global-step:13286	 l-p:0.13876873254776
epoch£º664	 i:7 	 global-step:13287	 l-p:0.090105801820755
epoch£º664	 i:8 	 global-step:13288	 l-p:0.3529732823371887
epoch£º664	 i:9 	 global-step:13289	 l-p:0.37484222650527954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6059, 3.6014, 3.6058],
        [3.6059, 3.6013, 3.6058],
        [3.6059, 3.0090, 2.8130],
        [3.6059, 3.4513, 3.5459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.45330801606178284 
model_pd.l_d.mean(): -23.432218551635742 
model_pd.lagr.mean(): -22.978910446166992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2552], device='cuda:0')), ('power', tensor([-23.6874], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.45330801606178284
epoch£º665	 i:1 	 global-step:13301	 l-p:0.13300500810146332
epoch£º665	 i:2 	 global-step:13302	 l-p:0.15110792219638824
epoch£º665	 i:3 	 global-step:13303	 l-p:0.21842177212238312
epoch£º665	 i:4 	 global-step:13304	 l-p:0.261611670255661
epoch£º665	 i:5 	 global-step:13305	 l-p:0.12668482959270477
epoch£º665	 i:6 	 global-step:13306	 l-p:0.12600597739219666
epoch£º665	 i:7 	 global-step:13307	 l-p:0.1318102478981018
epoch£º665	 i:8 	 global-step:13308	 l-p:0.14182856678962708
epoch£º665	 i:9 	 global-step:13309	 l-p:0.12022968381643295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5939, 3.5939, 3.5939],
        [3.5939, 2.9198, 2.4096],
        [3.5939, 2.9235, 2.3984],
        [3.5939, 3.2175, 3.2832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.12256738543510437 
model_pd.l_d.mean(): -22.49495506286621 
model_pd.lagr.mean(): -22.372386932373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3078], device='cuda:0')), ('power', tensor([-22.8027], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.12256738543510437
epoch£º666	 i:1 	 global-step:13321	 l-p:0.2271721065044403
epoch£º666	 i:2 	 global-step:13322	 l-p:-0.34712913632392883
epoch£º666	 i:3 	 global-step:13323	 l-p:0.13263149559497833
epoch£º666	 i:4 	 global-step:13324	 l-p:-0.020882392302155495
epoch£º666	 i:5 	 global-step:13325	 l-p:0.13611158728599548
epoch£º666	 i:6 	 global-step:13326	 l-p:0.17016875743865967
epoch£º666	 i:7 	 global-step:13327	 l-p:0.03949226438999176
epoch£º666	 i:8 	 global-step:13328	 l-p:0.1208704337477684
epoch£º666	 i:9 	 global-step:13329	 l-p:0.1564529538154602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[3.5724, 3.2280, 2.6798],
        [3.5724, 3.0611, 3.0069],
        [3.5724, 3.0631, 2.4819],
        [3.5724, 2.9544, 2.7233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.20346182584762573 
model_pd.l_d.mean(): -23.367883682250977 
model_pd.lagr.mean(): -23.16442108154297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2876], device='cuda:0')), ('power', tensor([-23.6555], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.20346182584762573
epoch£º667	 i:1 	 global-step:13341	 l-p:0.11225175857543945
epoch£º667	 i:2 	 global-step:13342	 l-p:0.13186080753803253
epoch£º667	 i:3 	 global-step:13343	 l-p:0.07476569712162018
epoch£º667	 i:4 	 global-step:13344	 l-p:0.14842675626277924
epoch£º667	 i:5 	 global-step:13345	 l-p:0.1558740735054016
epoch£º667	 i:6 	 global-step:13346	 l-p:0.07346916943788528
epoch£º667	 i:7 	 global-step:13347	 l-p:0.08464918285608292
epoch£º667	 i:8 	 global-step:13348	 l-p:0.12798476219177246
epoch£º667	 i:9 	 global-step:13349	 l-p:0.35833755135536194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5345, 3.4749, 3.5233],
        [3.5345, 3.5345, 3.5345],
        [3.5345, 3.0735, 3.0785],
        [3.5345, 2.8467, 2.3573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.11015979200601578 
model_pd.l_d.mean(): -23.13764762878418 
model_pd.lagr.mean(): -23.027488708496094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2872], device='cuda:0')), ('power', tensor([-23.4249], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.11015979200601578
epoch£º668	 i:1 	 global-step:13361	 l-p:0.13847017288208008
epoch£º668	 i:2 	 global-step:13362	 l-p:0.23324055969715118
epoch£º668	 i:3 	 global-step:13363	 l-p:0.15197046101093292
epoch£º668	 i:4 	 global-step:13364	 l-p:-0.4459574520587921
epoch£º668	 i:5 	 global-step:13365	 l-p:0.05061289668083191
epoch£º668	 i:6 	 global-step:13366	 l-p:0.10364946722984314
epoch£º668	 i:7 	 global-step:13367	 l-p:0.1313343197107315
epoch£º668	 i:8 	 global-step:13368	 l-p:0.0876438096165657
epoch£º668	 i:9 	 global-step:13369	 l-p:0.15946218371391296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5399, 3.1823, 3.2613],
        [3.5399, 3.5397, 3.5399],
        [3.5399, 3.5399, 3.5399],
        [3.5399, 2.9045, 2.6443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.13561776280403137 
model_pd.l_d.mean(): -23.2152042388916 
model_pd.lagr.mean(): -23.079586029052734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1732], device='cuda:0')), ('power', tensor([-23.3884], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.13561776280403137
epoch£º669	 i:1 	 global-step:13381	 l-p:0.14975625276565552
epoch£º669	 i:2 	 global-step:13382	 l-p:0.08794595301151276
epoch£º669	 i:3 	 global-step:13383	 l-p:0.0903991311788559
epoch£º669	 i:4 	 global-step:13384	 l-p:0.1519729346036911
epoch£º669	 i:5 	 global-step:13385	 l-p:-0.07124601304531097
epoch£º669	 i:6 	 global-step:13386	 l-p:0.14824186265468597
epoch£º669	 i:7 	 global-step:13387	 l-p:0.2717752456665039
epoch£º669	 i:8 	 global-step:13388	 l-p:0.15168963372707367
epoch£º669	 i:9 	 global-step:13389	 l-p:0.05901770293712616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5568, 3.5568, 3.5568],
        [3.5568, 3.4101, 3.5026],
        [3.5568, 3.5539, 3.5567],
        [3.5568, 3.5568, 3.5568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.11021191626787186 
model_pd.l_d.mean(): -23.57142448425293 
model_pd.lagr.mean(): -23.461212158203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2554], device='cuda:0')), ('power', tensor([-23.8269], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.11021191626787186
epoch£º670	 i:1 	 global-step:13401	 l-p:0.11193536221981049
epoch£º670	 i:2 	 global-step:13402	 l-p:0.13392552733421326
epoch£º670	 i:3 	 global-step:13403	 l-p:0.12517984211444855
epoch£º670	 i:4 	 global-step:13404	 l-p:0.18417099118232727
epoch£º670	 i:5 	 global-step:13405	 l-p:0.18469874560832977
epoch£º670	 i:6 	 global-step:13406	 l-p:1.6155283451080322
epoch£º670	 i:7 	 global-step:13407	 l-p:0.7679411172866821
epoch£º670	 i:8 	 global-step:13408	 l-p:0.1312919557094574
epoch£º670	 i:9 	 global-step:13409	 l-p:0.13053691387176514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6411, 3.6201, 3.6391],
        [3.6411, 3.6340, 3.6407],
        [3.6411, 3.1573, 3.1310],
        [3.6411, 3.6338, 3.6407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.13210996985435486 
model_pd.l_d.mean(): -23.59345245361328 
model_pd.lagr.mean(): -23.461341857910156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1474], device='cuda:0')), ('power', tensor([-23.7409], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.13210996985435486
epoch£º671	 i:1 	 global-step:13421	 l-p:0.15394605696201324
epoch£º671	 i:2 	 global-step:13422	 l-p:0.15203778445720673
epoch£º671	 i:3 	 global-step:13423	 l-p:0.16890861093997955
epoch£º671	 i:4 	 global-step:13424	 l-p:0.1387488842010498
epoch£º671	 i:5 	 global-step:13425	 l-p:0.15480026602745056
epoch£º671	 i:6 	 global-step:13426	 l-p:0.11985422670841217
epoch£º671	 i:7 	 global-step:13427	 l-p:-3.801952362060547
epoch£º671	 i:8 	 global-step:13428	 l-p:0.12726791203022003
epoch£º671	 i:9 	 global-step:13429	 l-p:0.13250550627708435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7274, 3.6860, 3.7213],
        [3.7274, 3.0675, 2.6201],
        [3.7274, 3.7274, 3.7274],
        [3.7274, 3.2834, 3.2911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.12486549466848373 
model_pd.l_d.mean(): -23.19602394104004 
model_pd.lagr.mean(): -23.07115936279297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2037], device='cuda:0')), ('power', tensor([-23.3997], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.12486549466848373
epoch£º672	 i:1 	 global-step:13441	 l-p:0.12542635202407837
epoch£º672	 i:2 	 global-step:13442	 l-p:0.12226202338933945
epoch£º672	 i:3 	 global-step:13443	 l-p:0.18049930036067963
epoch£º672	 i:4 	 global-step:13444	 l-p:0.13389363884925842
epoch£º672	 i:5 	 global-step:13445	 l-p:0.044476814568042755
epoch£º672	 i:6 	 global-step:13446	 l-p:0.14454494416713715
epoch£º672	 i:7 	 global-step:13447	 l-p:0.11569640040397644
epoch£º672	 i:8 	 global-step:13448	 l-p:0.10946781188249588
epoch£º672	 i:9 	 global-step:13449	 l-p:0.18393082916736603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6332, 3.6327, 3.6332],
        [3.6332, 3.6332, 3.6332],
        [3.6332, 2.9899, 2.4216],
        [3.6332, 3.6128, 3.6313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.12489835172891617 
model_pd.l_d.mean(): -23.29503631591797 
model_pd.lagr.mean(): -23.170137405395508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1819], device='cuda:0')), ('power', tensor([-23.4769], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.12489835172891617
epoch£º673	 i:1 	 global-step:13461	 l-p:0.13235515356063843
epoch£º673	 i:2 	 global-step:13462	 l-p:0.14409734308719635
epoch£º673	 i:3 	 global-step:13463	 l-p:0.15747758746147156
epoch£º673	 i:4 	 global-step:13464	 l-p:0.2974713444709778
epoch£º673	 i:5 	 global-step:13465	 l-p:0.14717207849025726
epoch£º673	 i:6 	 global-step:13466	 l-p:0.23289234936237335
epoch£º673	 i:7 	 global-step:13467	 l-p:0.06863810122013092
epoch£º673	 i:8 	 global-step:13468	 l-p:-0.123333640396595
epoch£º673	 i:9 	 global-step:13469	 l-p:0.07554931193590164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5131, 3.2715, 3.3802],
        [3.5131, 3.3057, 3.4126],
        [3.5131, 3.1783, 3.2689],
        [3.5131, 2.9276, 2.7771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.15150652825832367 
model_pd.l_d.mean(): -22.92550277709961 
model_pd.lagr.mean(): -22.773996353149414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3489], device='cuda:0')), ('power', tensor([-23.2744], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.15150652825832367
epoch£º674	 i:1 	 global-step:13481	 l-p:0.13204370439052582
epoch£º674	 i:2 	 global-step:13482	 l-p:0.21625961363315582
epoch£º674	 i:3 	 global-step:13483	 l-p:0.1284146010875702
epoch£º674	 i:4 	 global-step:13484	 l-p:0.1924954354763031
epoch£º674	 i:5 	 global-step:13485	 l-p:-2.4582433700561523
epoch£º674	 i:6 	 global-step:13486	 l-p:0.16575969755649567
epoch£º674	 i:7 	 global-step:13487	 l-p:0.1447560042142868
epoch£º674	 i:8 	 global-step:13488	 l-p:0.03680971637368202
epoch£º674	 i:9 	 global-step:13489	 l-p:0.13716718554496765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5736, 3.1819, 3.2415],
        [3.5736, 3.0322, 2.9423],
        [3.5736, 3.3724, 3.4780],
        [3.5736, 3.4230, 3.5169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): -0.06395933032035828 
model_pd.l_d.mean(): -23.257442474365234 
model_pd.lagr.mean(): -23.321401596069336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2192], device='cuda:0')), ('power', tensor([-23.4766], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:-0.06395933032035828
epoch£º675	 i:1 	 global-step:13501	 l-p:-0.010998415760695934
epoch£º675	 i:2 	 global-step:13502	 l-p:0.1746131330728531
epoch£º675	 i:3 	 global-step:13503	 l-p:0.18436454236507416
epoch£º675	 i:4 	 global-step:13504	 l-p:0.13708026707172394
epoch£º675	 i:5 	 global-step:13505	 l-p:0.13853389024734497
epoch£º675	 i:6 	 global-step:13506	 l-p:0.08827662467956543
epoch£º675	 i:7 	 global-step:13507	 l-p:0.12064676731824875
epoch£º675	 i:8 	 global-step:13508	 l-p:0.1409377008676529
epoch£º675	 i:9 	 global-step:13509	 l-p:0.1983153522014618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6477, 3.6476, 3.6477],
        [3.6477, 3.0143, 2.7364],
        [3.6477, 3.6242, 3.6454],
        [3.6477, 3.6400, 3.6473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.1622353494167328 
model_pd.l_d.mean(): -23.400156021118164 
model_pd.lagr.mean(): -23.2379207611084 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1642], device='cuda:0')), ('power', tensor([-23.5643], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.1622353494167328
epoch£º676	 i:1 	 global-step:13521	 l-p:0.1414118856191635
epoch£º676	 i:2 	 global-step:13522	 l-p:0.14573143422603607
epoch£º676	 i:3 	 global-step:13523	 l-p:0.1199650764465332
epoch£º676	 i:4 	 global-step:13524	 l-p:0.12864065170288086
epoch£º676	 i:5 	 global-step:13525	 l-p:0.1608496755361557
epoch£º676	 i:6 	 global-step:13526	 l-p:0.135465607047081
epoch£º676	 i:7 	 global-step:13527	 l-p:0.3620437681674957
epoch£º676	 i:8 	 global-step:13528	 l-p:0.0954374298453331
epoch£º676	 i:9 	 global-step:13529	 l-p:0.12235575169324875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6172, 3.6172, 3.6172],
        [3.6172, 3.4079, 3.5140],
        [3.6172, 3.1465, 3.1386],
        [3.6172, 3.5851, 3.6132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.224162757396698 
model_pd.l_d.mean(): -23.513967514038086 
model_pd.lagr.mean(): -23.289804458618164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1924], device='cuda:0')), ('power', tensor([-23.7064], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.224162757396698
epoch£º677	 i:1 	 global-step:13541	 l-p:0.13001662492752075
epoch£º677	 i:2 	 global-step:13542	 l-p:0.2584284245967865
epoch£º677	 i:3 	 global-step:13543	 l-p:0.1282573789358139
epoch£º677	 i:4 	 global-step:13544	 l-p:0.14144450426101685
epoch£º677	 i:5 	 global-step:13545	 l-p:0.12466592341661453
epoch£º677	 i:6 	 global-step:13546	 l-p:0.1651124656200409
epoch£º677	 i:7 	 global-step:13547	 l-p:0.1427621841430664
epoch£º677	 i:8 	 global-step:13548	 l-p:0.25914910435676575
epoch£º677	 i:9 	 global-step:13549	 l-p:0.027468206360936165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5780, 3.5157, 3.5659],
        [3.5780, 3.5749, 3.5779],
        [3.5780, 3.5234, 3.5683],
        [3.5780, 3.4989, 3.5597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): 0.1967175453901291 
model_pd.l_d.mean(): -23.108638763427734 
model_pd.lagr.mean(): -22.91192054748535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3146], device='cuda:0')), ('power', tensor([-23.4233], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:0.1967175453901291
epoch£º678	 i:1 	 global-step:13561	 l-p:-0.5969456434249878
epoch£º678	 i:2 	 global-step:13562	 l-p:-5.415680885314941
epoch£º678	 i:3 	 global-step:13563	 l-p:0.4254729449748993
epoch£º678	 i:4 	 global-step:13564	 l-p:0.12257415801286697
epoch£º678	 i:5 	 global-step:13565	 l-p:0.11841315776109695
epoch£º678	 i:6 	 global-step:13566	 l-p:0.0943136215209961
epoch£º678	 i:7 	 global-step:13567	 l-p:0.1576555073261261
epoch£º678	 i:8 	 global-step:13568	 l-p:0.1331256777048111
epoch£º678	 i:9 	 global-step:13569	 l-p:0.11968034505844116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6557, 3.6556, 3.6557],
        [3.6557, 3.0040, 2.4437],
        [3.6557, 3.6035, 3.6467],
        [3.6557, 3.5560, 3.6282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.08584649115800858 
model_pd.l_d.mean(): -23.393009185791016 
model_pd.lagr.mean(): -23.30716323852539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1910], device='cuda:0')), ('power', tensor([-23.5840], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.08584649115800858
epoch£º679	 i:1 	 global-step:13581	 l-p:0.13685621321201324
epoch£º679	 i:2 	 global-step:13582	 l-p:0.15308214724063873
epoch£º679	 i:3 	 global-step:13583	 l-p:0.15811865031719208
epoch£º679	 i:4 	 global-step:13584	 l-p:0.19128353893756866
epoch£º679	 i:5 	 global-step:13585	 l-p:0.10530305653810501
epoch£º679	 i:6 	 global-step:13586	 l-p:0.4047674834728241
epoch£º679	 i:7 	 global-step:13587	 l-p:0.2626952528953552
epoch£º679	 i:8 	 global-step:13588	 l-p:0.37743228673934937
epoch£º679	 i:9 	 global-step:13589	 l-p:0.10976099967956543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6124, 3.5993, 3.6115],
        [3.6124, 3.6077, 3.6122],
        [3.6124, 3.5378, 3.5959],
        [3.6124, 3.0781, 2.4881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.1508307158946991 
model_pd.l_d.mean(): -23.741451263427734 
model_pd.lagr.mean(): -23.590620040893555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1547], device='cuda:0')), ('power', tensor([-23.8961], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.1508307158946991
epoch£º680	 i:1 	 global-step:13601	 l-p:0.278188019990921
epoch£º680	 i:2 	 global-step:13602	 l-p:0.1810804307460785
epoch£º680	 i:3 	 global-step:13603	 l-p:0.13429759442806244
epoch£º680	 i:4 	 global-step:13604	 l-p:0.594102680683136
epoch£º680	 i:5 	 global-step:13605	 l-p:0.10525182634592056
epoch£º680	 i:6 	 global-step:13606	 l-p:0.12087234109640121
epoch£º680	 i:7 	 global-step:13607	 l-p:0.15268924832344055
epoch£º680	 i:8 	 global-step:13608	 l-p:0.11544258892536163
epoch£º680	 i:9 	 global-step:13609	 l-p:0.11050489544868469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6403, 3.2157, 3.2493],
        [3.6403, 3.2370, 2.6666],
        [3.6403, 3.6386, 3.6403],
        [3.6403, 3.6385, 3.6403]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.1422976702451706 
model_pd.l_d.mean(): -23.353801727294922 
model_pd.lagr.mean(): -23.211503982543945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1849], device='cuda:0')), ('power', tensor([-23.5387], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.1422976702451706
epoch£º681	 i:1 	 global-step:13621	 l-p:0.10515698045492172
epoch£º681	 i:2 	 global-step:13622	 l-p:0.13074040412902832
epoch£º681	 i:3 	 global-step:13623	 l-p:0.13059106469154358
epoch£º681	 i:4 	 global-step:13624	 l-p:0.12961560487747192
epoch£º681	 i:5 	 global-step:13625	 l-p:-2.504035711288452
epoch£º681	 i:6 	 global-step:13626	 l-p:0.14133809506893158
epoch£º681	 i:7 	 global-step:13627	 l-p:0.1444949209690094
epoch£º681	 i:8 	 global-step:13628	 l-p:0.14626209437847137
epoch£º681	 i:9 	 global-step:13629	 l-p:0.21656516194343567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5747, 3.3724, 3.4786],
        [3.5747, 3.3058, 3.4125],
        [3.5747, 3.5700, 3.5746],
        [3.5747, 3.5452, 3.5713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.1303742229938507 
model_pd.l_d.mean(): -23.500843048095703 
model_pd.lagr.mean(): -23.370468139648438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2213], device='cuda:0')), ('power', tensor([-23.7221], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.1303742229938507
epoch£º682	 i:1 	 global-step:13641	 l-p:0.05981796607375145
epoch£º682	 i:2 	 global-step:13642	 l-p:0.10885940492153168
epoch£º682	 i:3 	 global-step:13643	 l-p:0.09107325226068497
epoch£º682	 i:4 	 global-step:13644	 l-p:0.13896523416042328
epoch£º682	 i:5 	 global-step:13645	 l-p:0.12530946731567383
epoch£º682	 i:6 	 global-step:13646	 l-p:0.13653993606567383
epoch£º682	 i:7 	 global-step:13647	 l-p:0.13894407451152802
epoch£º682	 i:8 	 global-step:13648	 l-p:0.37275248765945435
epoch£º682	 i:9 	 global-step:13649	 l-p:0.14892946183681488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4902, 3.2338, 3.3433],
        [3.4902, 3.4180, 3.4747],
        [3.4902, 3.4902, 3.4902],
        [3.4902, 3.1488, 3.2395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.14664608240127563 
model_pd.l_d.mean(): -23.737817764282227 
model_pd.lagr.mean(): -23.591171264648438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2381], device='cuda:0')), ('power', tensor([-23.9759], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.14664608240127563
epoch£º683	 i:1 	 global-step:13661	 l-p:0.22333167493343353
epoch£º683	 i:2 	 global-step:13662	 l-p:0.099703349173069
epoch£º683	 i:3 	 global-step:13663	 l-p:0.1116868108510971
epoch£º683	 i:4 	 global-step:13664	 l-p:0.1015220433473587
epoch£º683	 i:5 	 global-step:13665	 l-p:0.12241147458553314
epoch£º683	 i:6 	 global-step:13666	 l-p:0.08049216866493225
epoch£º683	 i:7 	 global-step:13667	 l-p:0.11828218400478363
epoch£º683	 i:8 	 global-step:13668	 l-p:0.14829978346824646
epoch£º683	 i:9 	 global-step:13669	 l-p:0.21937869489192963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5390, 2.8439, 2.4015],
        [3.5390, 3.5390, 3.5390],
        [3.5390, 3.0170, 2.4343],
        [3.5390, 3.5347, 3.5389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.08513758331537247 
model_pd.l_d.mean(): -23.373092651367188 
model_pd.lagr.mean(): -23.287954330444336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2456], device='cuda:0')), ('power', tensor([-23.6187], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.08513758331537247
epoch£º684	 i:1 	 global-step:13681	 l-p:1.4607229232788086
epoch£º684	 i:2 	 global-step:13682	 l-p:0.12605799734592438
epoch£º684	 i:3 	 global-step:13683	 l-p:0.13968250155448914
epoch£º684	 i:4 	 global-step:13684	 l-p:0.15475882589817047
epoch£º684	 i:5 	 global-step:13685	 l-p:0.14036419987678528
epoch£º684	 i:6 	 global-step:13686	 l-p:0.118689626455307
epoch£º684	 i:7 	 global-step:13687	 l-p:0.061011724174022675
epoch£º684	 i:8 	 global-step:13688	 l-p:0.1215980276465416
epoch£º684	 i:9 	 global-step:13689	 l-p:0.17246146500110626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6063, 3.4442, 3.5418],
        [3.6063, 3.1867, 2.6151],
        [3.6063, 3.0165, 2.8511],
        [3.6063, 3.6061, 3.6063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.14099808037281036 
model_pd.l_d.mean(): -23.542062759399414 
model_pd.lagr.mean(): -23.401063919067383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2147], device='cuda:0')), ('power', tensor([-23.7568], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.14099808037281036
epoch£º685	 i:1 	 global-step:13701	 l-p:-1.5465261936187744
epoch£º685	 i:2 	 global-step:13702	 l-p:0.1738060861825943
epoch£º685	 i:3 	 global-step:13703	 l-p:0.15271751582622528
epoch£º685	 i:4 	 global-step:13704	 l-p:0.09876566380262375
epoch£º685	 i:5 	 global-step:13705	 l-p:0.20497888326644897
epoch£º685	 i:6 	 global-step:13706	 l-p:0.17087224125862122
epoch£º685	 i:7 	 global-step:13707	 l-p:0.12223082035779953
epoch£º685	 i:8 	 global-step:13708	 l-p:0.15264666080474854
epoch£º685	 i:9 	 global-step:13709	 l-p:0.11346246302127838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6770, 3.6763, 3.6770],
        [3.6770, 3.6759, 3.6769],
        [3.6770, 3.6160, 3.6653],
        [3.6770, 3.1734, 2.5805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.1386614441871643 
model_pd.l_d.mean(): -22.564449310302734 
model_pd.lagr.mean(): -22.4257869720459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3234], device='cuda:0')), ('power', tensor([-22.8879], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.1386614441871643
epoch£º686	 i:1 	 global-step:13721	 l-p:0.06173418462276459
epoch£º686	 i:2 	 global-step:13722	 l-p:0.13861842453479767
epoch£º686	 i:3 	 global-step:13723	 l-p:0.13263259828090668
epoch£º686	 i:4 	 global-step:13724	 l-p:0.1529429703950882
epoch£º686	 i:5 	 global-step:13725	 l-p:0.14650113880634308
epoch£º686	 i:6 	 global-step:13726	 l-p:0.13911864161491394
epoch£º686	 i:7 	 global-step:13727	 l-p:0.16380427777767181
epoch£º686	 i:8 	 global-step:13728	 l-p:0.12617766857147217
epoch£º686	 i:9 	 global-step:13729	 l-p:0.1323034018278122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[3.6825, 3.1546, 2.5584],
        [3.6825, 3.0244, 2.6737],
        [3.6825, 3.2113, 3.2014],
        [3.6825, 3.0844, 2.4899]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.05933058261871338 
model_pd.l_d.mean(): -22.99739646911621 
model_pd.lagr.mean(): -22.938066482543945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2945], device='cuda:0')), ('power', tensor([-23.2919], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.05933058261871338
epoch£º687	 i:1 	 global-step:13741	 l-p:0.1480099856853485
epoch£º687	 i:2 	 global-step:13742	 l-p:0.1265239417552948
epoch£º687	 i:3 	 global-step:13743	 l-p:0.1287369281053543
epoch£º687	 i:4 	 global-step:13744	 l-p:0.13793328404426575
epoch£º687	 i:5 	 global-step:13745	 l-p:0.10045009106397629
epoch£º687	 i:6 	 global-step:13746	 l-p:0.26356083154678345
epoch£º687	 i:7 	 global-step:13747	 l-p:0.258024662733078
epoch£º687	 i:8 	 global-step:13748	 l-p:0.139374241232872
epoch£º687	 i:9 	 global-step:13749	 l-p:-0.13900794088840485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5942, 3.5942, 3.5942],
        [3.5942, 2.9549, 2.6917],
        [3.5942, 3.2354, 3.3154],
        [3.5942, 3.5941, 3.5942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 6.397888660430908 
model_pd.l_d.mean(): -22.8553409576416 
model_pd.lagr.mean(): -16.45745277404785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3001], device='cuda:0')), ('power', tensor([-23.1554], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:6.397888660430908
epoch£º688	 i:1 	 global-step:13761	 l-p:0.1217699944972992
epoch£º688	 i:2 	 global-step:13762	 l-p:0.1412235051393509
epoch£º688	 i:3 	 global-step:13763	 l-p:0.16293582320213318
epoch£º688	 i:4 	 global-step:13764	 l-p:0.14605838060379028
epoch£º688	 i:5 	 global-step:13765	 l-p:0.5738934874534607
epoch£º688	 i:6 	 global-step:13766	 l-p:0.1701829731464386
epoch£º688	 i:7 	 global-step:13767	 l-p:0.13223077356815338
epoch£º688	 i:8 	 global-step:13768	 l-p:0.12648987770080566
epoch£º688	 i:9 	 global-step:13769	 l-p:0.11472836136817932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5250, 3.3897, 3.4788],
        [3.5250, 3.5110, 3.5240],
        [3.5250, 3.5249, 3.5250],
        [3.5250, 3.5250, 3.5250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.16228586435317993 
model_pd.l_d.mean(): -23.59520149230957 
model_pd.lagr.mean(): -23.43291473388672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2250], device='cuda:0')), ('power', tensor([-23.8202], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.16228586435317993
epoch£º689	 i:1 	 global-step:13781	 l-p:0.13767844438552856
epoch£º689	 i:2 	 global-step:13782	 l-p:0.12492414563894272
epoch£º689	 i:3 	 global-step:13783	 l-p:0.11643973737955093
epoch£º689	 i:4 	 global-step:13784	 l-p:0.07213570177555084
epoch£º689	 i:5 	 global-step:13785	 l-p:0.11043170839548111
epoch£º689	 i:6 	 global-step:13786	 l-p:-0.04753623530268669
epoch£º689	 i:7 	 global-step:13787	 l-p:0.21575668454170227
epoch£º689	 i:8 	 global-step:13788	 l-p:0.15589427947998047
epoch£º689	 i:9 	 global-step:13789	 l-p:0.1267971247434616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5517, 3.5515, 3.5517],
        [3.5517, 3.1523, 2.5901],
        [3.5517, 3.5506, 3.5517],
        [3.5517, 3.5517, 3.5517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.1376798301935196 
model_pd.l_d.mean(): -23.56476402282715 
model_pd.lagr.mean(): -23.42708396911621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2402], device='cuda:0')), ('power', tensor([-23.8050], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.1376798301935196
epoch£º690	 i:1 	 global-step:13801	 l-p:0.14757023751735687
epoch£º690	 i:2 	 global-step:13802	 l-p:0.12352226674556732
epoch£º690	 i:3 	 global-step:13803	 l-p:0.10534217953681946
epoch£º690	 i:4 	 global-step:13804	 l-p:0.12772974371910095
epoch£º690	 i:5 	 global-step:13805	 l-p:0.2074887454509735
epoch£º690	 i:6 	 global-step:13806	 l-p:0.1685943603515625
epoch£º690	 i:7 	 global-step:13807	 l-p:-0.9696429371833801
epoch£º690	 i:8 	 global-step:13808	 l-p:0.1358770728111267
epoch£º690	 i:9 	 global-step:13809	 l-p:0.1329948902130127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5467, 3.5466, 3.5467],
        [3.5467, 3.3233, 3.4326],
        [3.5467, 2.8483, 2.3294],
        [3.5467, 3.5423, 3.5465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.1965668946504593 
model_pd.l_d.mean(): -22.933034896850586 
model_pd.lagr.mean(): -22.736467361450195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2906], device='cuda:0')), ('power', tensor([-23.2236], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.1965668946504593
epoch£º691	 i:1 	 global-step:13821	 l-p:0.03387214243412018
epoch£º691	 i:2 	 global-step:13822	 l-p:0.1498989760875702
epoch£º691	 i:3 	 global-step:13823	 l-p:0.1507488191127777
epoch£º691	 i:4 	 global-step:13824	 l-p:0.16271089017391205
epoch£º691	 i:5 	 global-step:13825	 l-p:0.26918795704841614
epoch£º691	 i:6 	 global-step:13826	 l-p:0.16128307580947876
epoch£º691	 i:7 	 global-step:13827	 l-p:-0.3169914186000824
epoch£º691	 i:8 	 global-step:13828	 l-p:0.07551835477352142
epoch£º691	 i:9 	 global-step:13829	 l-p:0.1185465008020401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5770, 3.5764, 3.5770],
        [3.5770, 3.5759, 3.5770],
        [3.5770, 3.4475, 3.5341],
        [3.5770, 3.5770, 3.5770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.16589710116386414 
model_pd.l_d.mean(): -23.669971466064453 
model_pd.lagr.mean(): -23.504074096679688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1520], device='cuda:0')), ('power', tensor([-23.8219], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.16589710116386414
epoch£º692	 i:1 	 global-step:13841	 l-p:-0.15619689226150513
epoch£º692	 i:2 	 global-step:13842	 l-p:-0.6685398817062378
epoch£º692	 i:3 	 global-step:13843	 l-p:0.12886323034763336
epoch£º692	 i:4 	 global-step:13844	 l-p:0.1580183506011963
epoch£º692	 i:5 	 global-step:13845	 l-p:0.19650660455226898
epoch£º692	 i:6 	 global-step:13846	 l-p:0.13308200240135193
epoch£º692	 i:7 	 global-step:13847	 l-p:0.11399824172258377
epoch£º692	 i:8 	 global-step:13848	 l-p:0.1304679811000824
epoch£º692	 i:9 	 global-step:13849	 l-p:0.09024740010499954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6852, 3.6781, 3.6849],
        [3.6852, 3.1091, 2.5096],
        [3.6852, 3.2834, 2.7074],
        [3.6852, 3.3130, 2.7440]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.12795890867710114 
model_pd.l_d.mean(): -22.908859252929688 
model_pd.lagr.mean(): -22.780900955200195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3230], device='cuda:0')), ('power', tensor([-23.2319], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.12795890867710114
epoch£º693	 i:1 	 global-step:13861	 l-p:0.09006594121456146
epoch£º693	 i:2 	 global-step:13862	 l-p:0.13243991136550903
epoch£º693	 i:3 	 global-step:13863	 l-p:0.13404014706611633
epoch£º693	 i:4 	 global-step:13864	 l-p:0.12575139105319977
epoch£º693	 i:5 	 global-step:13865	 l-p:0.12603284418582916
epoch£º693	 i:6 	 global-step:13866	 l-p:0.1642780601978302
epoch£º693	 i:7 	 global-step:13867	 l-p:0.1486906260251999
epoch£º693	 i:8 	 global-step:13868	 l-p:0.19214420020580292
epoch£º693	 i:9 	 global-step:13869	 l-p:0.15325406193733215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6403, 3.5109, 3.5973],
        [3.6403, 3.0090, 2.7600],
        [3.6403, 3.6402, 3.6403],
        [3.6403, 3.6403, 3.6403]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.1414889693260193 
model_pd.l_d.mean(): -22.6572322845459 
model_pd.lagr.mean(): -22.515743255615234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2852], device='cuda:0')), ('power', tensor([-22.9424], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.1414889693260193
epoch£º694	 i:1 	 global-step:13881	 l-p:0.18515875935554504
epoch£º694	 i:2 	 global-step:13882	 l-p:0.18435218930244446
epoch£º694	 i:3 	 global-step:13883	 l-p:0.12048900127410889
epoch£º694	 i:4 	 global-step:13884	 l-p:0.08453847467899323
epoch£º694	 i:5 	 global-step:13885	 l-p:0.13041234016418457
epoch£º694	 i:6 	 global-step:13886	 l-p:0.12334897369146347
epoch£º694	 i:7 	 global-step:13887	 l-p:0.14792245626449585
epoch£º694	 i:8 	 global-step:13888	 l-p:0.12194987386465073
epoch£º694	 i:9 	 global-step:13889	 l-p:0.1461438238620758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6997, 3.1601, 3.0678],
        [3.6997, 3.0220, 2.5791],
        [3.6997, 3.0208, 2.5621],
        [3.6997, 3.6992, 3.6997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.11752715706825256 
model_pd.l_d.mean(): -22.88986587524414 
model_pd.lagr.mean(): -22.7723388671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2118], device='cuda:0')), ('power', tensor([-23.1017], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.11752715706825256
epoch£º695	 i:1 	 global-step:13901	 l-p:0.13224323093891144
epoch£º695	 i:2 	 global-step:13902	 l-p:0.16196227073669434
epoch£º695	 i:3 	 global-step:13903	 l-p:0.03961484506726265
epoch£º695	 i:4 	 global-step:13904	 l-p:0.15283311903476715
epoch£º695	 i:5 	 global-step:13905	 l-p:0.11386897414922714
epoch£º695	 i:6 	 global-step:13906	 l-p:0.11828359216451645
epoch£º695	 i:7 	 global-step:13907	 l-p:0.17046929895877838
epoch£º695	 i:8 	 global-step:13908	 l-p:0.1277657449245453
epoch£º695	 i:9 	 global-step:13909	 l-p:0.17147257924079895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6698, 3.6528, 3.6684],
        [3.6698, 2.9870, 2.5434],
        [3.6698, 3.5816, 3.6478],
        [3.6698, 3.2465, 2.6666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.13943810760974884 
model_pd.l_d.mean(): -23.639598846435547 
model_pd.lagr.mean(): -23.500160217285156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1509], device='cuda:0')), ('power', tensor([-23.7905], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.13943810760974884
epoch£º696	 i:1 	 global-step:13921	 l-p:0.0765683650970459
epoch£º696	 i:2 	 global-step:13922	 l-p:0.1400829702615738
epoch£º696	 i:3 	 global-step:13923	 l-p:0.11344359070062637
epoch£º696	 i:4 	 global-step:13924	 l-p:0.1314375251531601
epoch£º696	 i:5 	 global-step:13925	 l-p:0.2814982235431671
epoch£º696	 i:6 	 global-step:13926	 l-p:0.14612674713134766
epoch£º696	 i:7 	 global-step:13927	 l-p:0.37471532821655273
epoch£º696	 i:8 	 global-step:13928	 l-p:0.13217011094093323
epoch£º696	 i:9 	 global-step:13929	 l-p:0.2473737895488739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6253, 3.6253, 3.6253],
        [3.6253, 3.6252, 3.6253],
        [3.6253, 3.0245, 2.4274],
        [3.6253, 3.6236, 3.6252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.12566125392913818 
model_pd.l_d.mean(): -23.204612731933594 
model_pd.lagr.mean(): -23.078950881958008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1746], device='cuda:0')), ('power', tensor([-23.3792], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.12566125392913818
epoch£º697	 i:1 	 global-step:13941	 l-p:0.14435461163520813
epoch£º697	 i:2 	 global-step:13942	 l-p:0.10752017796039581
epoch£º697	 i:3 	 global-step:13943	 l-p:0.15552887320518494
epoch£º697	 i:4 	 global-step:13944	 l-p:0.18945476412773132
epoch£º697	 i:5 	 global-step:13945	 l-p:0.13278283178806305
epoch£º697	 i:6 	 global-step:13946	 l-p:0.13546022772789001
epoch£º697	 i:7 	 global-step:13947	 l-p:-0.05075754597783089
epoch£º697	 i:8 	 global-step:13948	 l-p:0.12892773747444153
epoch£º697	 i:9 	 global-step:13949	 l-p:0.0843539759516716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5662, 3.4940, 3.5508],
        [3.5662, 3.5614, 3.5660],
        [3.5662, 3.1327, 3.1695],
        [3.5662, 2.9742, 2.8215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.13663142919540405 
model_pd.l_d.mean(): -23.51675796508789 
model_pd.lagr.mean(): -23.380126953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2077], device='cuda:0')), ('power', tensor([-23.7244], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.13663142919540405
epoch£º698	 i:1 	 global-step:13961	 l-p:0.08282095938920975
epoch£º698	 i:2 	 global-step:13962	 l-p:0.2743590176105499
epoch£º698	 i:3 	 global-step:13963	 l-p:0.1306341290473938
epoch£º698	 i:4 	 global-step:13964	 l-p:0.1273685097694397
epoch£º698	 i:5 	 global-step:13965	 l-p:0.14246027171611786
epoch£º698	 i:6 	 global-step:13966	 l-p:0.1907098889350891
epoch£º698	 i:7 	 global-step:13967	 l-p:0.07726895064115524
epoch£º698	 i:8 	 global-step:13968	 l-p:0.2571609914302826
epoch£º698	 i:9 	 global-step:13969	 l-p:0.1102890595793724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5856, 3.5721, 3.5847],
        [3.5856, 3.4566, 3.5431],
        [3.5856, 2.9715, 2.7758],
        [3.5856, 3.5856, 3.5856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): -0.05726541206240654 
model_pd.l_d.mean(): -23.441049575805664 
model_pd.lagr.mean(): -23.498315811157227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2351], device='cuda:0')), ('power', tensor([-23.6761], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:-0.05726541206240654
epoch£º699	 i:1 	 global-step:13981	 l-p:0.12039954960346222
epoch£º699	 i:2 	 global-step:13982	 l-p:0.13424719870090485
epoch£º699	 i:3 	 global-step:13983	 l-p:-0.011011562310159206
epoch£º699	 i:4 	 global-step:13984	 l-p:0.14611472189426422
epoch£º699	 i:5 	 global-step:13985	 l-p:0.20052529871463776
epoch£º699	 i:6 	 global-step:13986	 l-p:0.6596172451972961
epoch£º699	 i:7 	 global-step:13987	 l-p:0.13961820304393768
epoch£º699	 i:8 	 global-step:13988	 l-p:0.10769064724445343
epoch£º699	 i:9 	 global-step:13989	 l-p:0.12625478208065033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6486, 3.6390, 3.6480],
        [3.6486, 3.1164, 3.0440],
        [3.6486, 3.1888, 3.1982],
        [3.6486, 3.5164, 3.6040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.08430713415145874 
model_pd.l_d.mean(): -23.41475486755371 
model_pd.lagr.mean(): -23.330448150634766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1587], device='cuda:0')), ('power', tensor([-23.5735], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.08430713415145874
epoch£º700	 i:1 	 global-step:14001	 l-p:0.1744316816329956
epoch£º700	 i:2 	 global-step:14002	 l-p:0.12482946366071701
epoch£º700	 i:3 	 global-step:14003	 l-p:0.11823645979166031
epoch£º700	 i:4 	 global-step:14004	 l-p:0.15340638160705566
epoch£º700	 i:5 	 global-step:14005	 l-p:0.13828662037849426
epoch£º700	 i:6 	 global-step:14006	 l-p:0.15220369398593903
epoch£º700	 i:7 	 global-step:14007	 l-p:0.1602257639169693
epoch£º700	 i:8 	 global-step:14008	 l-p:0.15829654037952423
epoch£º700	 i:9 	 global-step:14009	 l-p:0.138672336935997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6657, 3.6518, 3.6648],
        [3.6657, 3.5027, 3.6008],
        [3.6657, 3.6657, 3.6657],
        [3.6657, 3.2898, 3.3609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.1765037328004837 
model_pd.l_d.mean(): -23.216039657592773 
model_pd.lagr.mean(): -23.039535522460938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2154], device='cuda:0')), ('power', tensor([-23.4314], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.1765037328004837
epoch£º701	 i:1 	 global-step:14021	 l-p:0.1328500658273697
epoch£º701	 i:2 	 global-step:14022	 l-p:0.08856609463691711
epoch£º701	 i:3 	 global-step:14023	 l-p:0.12645262479782104
epoch£º701	 i:4 	 global-step:14024	 l-p:0.20440417528152466
epoch£º701	 i:5 	 global-step:14025	 l-p:0.15445579588413239
epoch£º701	 i:6 	 global-step:14026	 l-p:0.12423719465732574
epoch£º701	 i:7 	 global-step:14027	 l-p:0.23137924075126648
epoch£º701	 i:8 	 global-step:14028	 l-p:0.12539801001548767
epoch£º701	 i:9 	 global-step:14029	 l-p:0.13256768882274628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6168, 3.4878, 3.5743],
        [3.6168, 3.0525, 2.4545],
        [3.6168, 3.4919, 3.5767],
        [3.6168, 3.6168, 3.6168]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.10386161506175995 
model_pd.l_d.mean(): -23.3394718170166 
model_pd.lagr.mean(): -23.235610961914062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2293], device='cuda:0')), ('power', tensor([-23.5688], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.10386161506175995
epoch£º702	 i:1 	 global-step:14041	 l-p:0.14784874022006989
epoch£º702	 i:2 	 global-step:14042	 l-p:0.18703505396842957
epoch£º702	 i:3 	 global-step:14043	 l-p:-0.28649061918258667
epoch£º702	 i:4 	 global-step:14044	 l-p:0.13405124843120575
epoch£º702	 i:5 	 global-step:14045	 l-p:0.13945387303829193
epoch£º702	 i:6 	 global-step:14046	 l-p:0.2782559096813202
epoch£º702	 i:7 	 global-step:14047	 l-p:0.15790538489818573
epoch£º702	 i:8 	 global-step:14048	 l-p:0.09595060348510742
epoch£º702	 i:9 	 global-step:14049	 l-p:0.12255951017141342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5587, 3.5532, 3.5585],
        [3.5587, 3.0291, 2.4403],
        [3.5587, 3.5345, 3.5563],
        [3.5587, 3.0455, 2.4591]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.08576475083827972 
model_pd.l_d.mean(): -23.466461181640625 
model_pd.lagr.mean(): -23.38069725036621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2665], device='cuda:0')), ('power', tensor([-23.7330], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.08576475083827972
epoch£º703	 i:1 	 global-step:14061	 l-p:0.07301618903875351
epoch£º703	 i:2 	 global-step:14062	 l-p:1.0892126560211182
epoch£º703	 i:3 	 global-step:14063	 l-p:0.13671115040779114
epoch£º703	 i:4 	 global-step:14064	 l-p:0.16084200143814087
epoch£º703	 i:5 	 global-step:14065	 l-p:0.15929895639419556
epoch£º703	 i:6 	 global-step:14066	 l-p:0.0760725811123848
epoch£º703	 i:7 	 global-step:14067	 l-p:0.22165672481060028
epoch£º703	 i:8 	 global-step:14068	 l-p:0.15166890621185303
epoch£º703	 i:9 	 global-step:14069	 l-p:0.14957770705223083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5833, 3.5833, 3.5833],
        [3.5833, 3.4937, 3.5610],
        [3.5833, 3.4164, 3.5164],
        [3.5833, 2.8892, 2.4846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.1342514306306839 
model_pd.l_d.mean(): -23.813804626464844 
model_pd.lagr.mean(): -23.679553985595703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1151], device='cuda:0')), ('power', tensor([-23.9289], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.1342514306306839
epoch£º704	 i:1 	 global-step:14081	 l-p:0.1691233068704605
epoch£º704	 i:2 	 global-step:14082	 l-p:0.14938129484653473
epoch£º704	 i:3 	 global-step:14083	 l-p:0.17139549553394318
epoch£º704	 i:4 	 global-step:14084	 l-p:0.13397224247455597
epoch£º704	 i:5 	 global-step:14085	 l-p:0.1914757937192917
epoch£º704	 i:6 	 global-step:14086	 l-p:-0.1096985787153244
epoch£º704	 i:7 	 global-step:14087	 l-p:0.13949303328990936
epoch£º704	 i:8 	 global-step:14088	 l-p:-0.11153207719326019
epoch£º704	 i:9 	 global-step:14089	 l-p:0.11321621388196945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6275, 3.6275, 3.6276],
        [3.6275, 3.6190, 3.6271],
        [3.6275, 2.9346, 2.5069],
        [3.6275, 3.0162, 2.4165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 0.1855744570493698 
model_pd.l_d.mean(): -23.28078269958496 
model_pd.lagr.mean(): -23.0952091217041 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2831], device='cuda:0')), ('power', tensor([-23.5639], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:0.1855744570493698
epoch£º705	 i:1 	 global-step:14101	 l-p:0.1395598202943802
epoch£º705	 i:2 	 global-step:14102	 l-p:0.20781157910823822
epoch£º705	 i:3 	 global-step:14103	 l-p:0.12950381636619568
epoch£º705	 i:4 	 global-step:14104	 l-p:0.16930429637432098
epoch£º705	 i:5 	 global-step:14105	 l-p:0.10915422439575195
epoch£º705	 i:6 	 global-step:14106	 l-p:0.12147491425275803
epoch£º705	 i:7 	 global-step:14107	 l-p:0.12814025580883026
epoch£º705	 i:8 	 global-step:14108	 l-p:0.1694011688232422
epoch£º705	 i:9 	 global-step:14109	 l-p:0.06024723872542381
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6938, 3.5412, 3.6362],
        [3.6938, 3.1541, 2.5511],
        [3.6938, 3.6219, 3.6784],
        [3.6938, 3.0987, 2.9254]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.1232353076338768 
model_pd.l_d.mean(): -23.45781707763672 
model_pd.lagr.mean(): -23.33458137512207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1927], device='cuda:0')), ('power', tensor([-23.6505], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.1232353076338768
epoch£º706	 i:1 	 global-step:14121	 l-p:0.12060783058404922
epoch£º706	 i:2 	 global-step:14122	 l-p:0.13622866570949554
epoch£º706	 i:3 	 global-step:14123	 l-p:0.15208765864372253
epoch£º706	 i:4 	 global-step:14124	 l-p:0.17362144589424133
epoch£º706	 i:5 	 global-step:14125	 l-p:0.13839662075042725
epoch£º706	 i:6 	 global-step:14126	 l-p:0.06821519881486893
epoch£º706	 i:7 	 global-step:14127	 l-p:0.16829384863376617
epoch£º706	 i:8 	 global-step:14128	 l-p:0.12650524079799652
epoch£º706	 i:9 	 global-step:14129	 l-p:0.1615670919418335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6924, 3.6918, 3.6924],
        [3.6924, 3.6923, 3.6924],
        [3.6924, 3.6923, 3.6924],
        [3.6924, 3.4694, 3.5779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.14629952609539032 
model_pd.l_d.mean(): -23.687564849853516 
model_pd.lagr.mean(): -23.5412654876709 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1329], device='cuda:0')), ('power', tensor([-23.8205], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.14629952609539032
epoch£º707	 i:1 	 global-step:14141	 l-p:0.14564277231693268
epoch£º707	 i:2 	 global-step:14142	 l-p:0.13463005423545837
epoch£º707	 i:3 	 global-step:14143	 l-p:0.15510888397693634
epoch£º707	 i:4 	 global-step:14144	 l-p:0.11989123374223709
epoch£º707	 i:5 	 global-step:14145	 l-p:0.13048617541790009
epoch£º707	 i:6 	 global-step:14146	 l-p:0.15850543975830078
epoch£º707	 i:7 	 global-step:14147	 l-p:0.12006651610136032
epoch£º707	 i:8 	 global-step:14148	 l-p:0.21380500495433807
epoch£º707	 i:9 	 global-step:14149	 l-p:0.08313710987567902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6502, 3.5015, 3.5955],
        [3.6502, 3.2713, 3.3431],
        [3.6502, 2.9779, 2.4055],
        [3.6502, 3.4997, 3.5943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.12446028739213943 
model_pd.l_d.mean(): -23.388540267944336 
model_pd.lagr.mean(): -23.264080047607422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2252], device='cuda:0')), ('power', tensor([-23.6138], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.12446028739213943
epoch£º708	 i:1 	 global-step:14161	 l-p:0.1401490718126297
epoch£º708	 i:2 	 global-step:14162	 l-p:0.1339775025844574
epoch£º708	 i:3 	 global-step:14163	 l-p:0.23163919150829315
epoch£º708	 i:4 	 global-step:14164	 l-p:0.0984470546245575
epoch£º708	 i:5 	 global-step:14165	 l-p:0.2917971611022949
epoch£º708	 i:6 	 global-step:14166	 l-p:0.13625651597976685
epoch£º708	 i:7 	 global-step:14167	 l-p:0.13363632559776306
epoch£º708	 i:8 	 global-step:14168	 l-p:0.12587274610996246
epoch£º708	 i:9 	 global-step:14169	 l-p:0.1359887421131134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6479, 3.1361, 3.0945],
        [3.6479, 3.5737, 3.6318],
        [3.6479, 3.2997, 2.7374],
        [3.6479, 2.9578, 2.5404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.14939522743225098 
model_pd.l_d.mean(): -23.269573211669922 
model_pd.lagr.mean(): -23.12017822265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2085], device='cuda:0')), ('power', tensor([-23.4781], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.14939522743225098
epoch£º709	 i:1 	 global-step:14181	 l-p:0.1404798924922943
epoch£º709	 i:2 	 global-step:14182	 l-p:0.14106951653957367
epoch£º709	 i:3 	 global-step:14183	 l-p:-0.06175806745886803
epoch£º709	 i:4 	 global-step:14184	 l-p:0.14198356866836548
epoch£º709	 i:5 	 global-step:14185	 l-p:0.596404492855072
epoch£º709	 i:6 	 global-step:14186	 l-p:0.09532810002565384
epoch£º709	 i:7 	 global-step:14187	 l-p:0.1095140352845192
epoch£º709	 i:8 	 global-step:14188	 l-p:0.1181384027004242
epoch£º709	 i:9 	 global-step:14189	 l-p:0.15968091785907745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5535, 3.5199, 3.5493],
        [3.5535, 3.0416, 3.0100],
        [3.5535, 3.1557, 3.2217],
        [3.5535, 3.4994, 3.5442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.11794469505548477 
model_pd.l_d.mean(): -23.460811614990234 
model_pd.lagr.mean(): -23.342866897583008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1900], device='cuda:0')), ('power', tensor([-23.6508], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.11794469505548477
epoch£º710	 i:1 	 global-step:14201	 l-p:0.12548303604125977
epoch£º710	 i:2 	 global-step:14202	 l-p:0.13123667240142822
epoch£º710	 i:3 	 global-step:14203	 l-p:0.6737533211708069
epoch£º710	 i:4 	 global-step:14204	 l-p:0.16030974686145782
epoch£º710	 i:5 	 global-step:14205	 l-p:0.059862375259399414
epoch£º710	 i:6 	 global-step:14206	 l-p:0.20466606318950653
epoch£º710	 i:7 	 global-step:14207	 l-p:0.1531282365322113
epoch£º710	 i:8 	 global-step:14208	 l-p:0.1511896550655365
epoch£º710	 i:9 	 global-step:14209	 l-p:0.11614236235618591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5105, 3.4892, 3.5085],
        [3.5105, 2.8618, 2.2712],
        [3.5105, 2.8252, 2.2453],
        [3.5105, 3.5105, 3.5105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.1448773294687271 
model_pd.l_d.mean(): -23.016727447509766 
model_pd.lagr.mean(): -22.871850967407227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3311], device='cuda:0')), ('power', tensor([-23.3478], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.1448773294687271
epoch£º711	 i:1 	 global-step:14221	 l-p:0.1688791662454605
epoch£º711	 i:2 	 global-step:14222	 l-p:0.22336813807487488
epoch£º711	 i:3 	 global-step:14223	 l-p:0.1458316445350647
epoch£º711	 i:4 	 global-step:14224	 l-p:0.13880200684070587
epoch£º711	 i:5 	 global-step:14225	 l-p:0.14839322865009308
epoch£º711	 i:6 	 global-step:14226	 l-p:-0.6347358822822571
epoch£º711	 i:7 	 global-step:14227	 l-p:0.12284358590841293
epoch£º711	 i:8 	 global-step:14228	 l-p:0.2938903570175171
epoch£º711	 i:9 	 global-step:14229	 l-p:0.11609088629484177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6430, 2.9582, 2.3965],
        [3.6430, 3.0304, 2.4278],
        [3.6430, 3.4937, 3.5881],
        [3.6430, 3.1126, 2.5134]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 0.15619570016860962 
model_pd.l_d.mean(): -23.3405704498291 
model_pd.lagr.mean(): -23.18437385559082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3000], device='cuda:0')), ('power', tensor([-23.6406], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:0.15619570016860962
epoch£º712	 i:1 	 global-step:14241	 l-p:0.11566247791051865
epoch£º712	 i:2 	 global-step:14242	 l-p:0.13185235857963562
epoch£º712	 i:3 	 global-step:14243	 l-p:0.3281974196434021
epoch£º712	 i:4 	 global-step:14244	 l-p:0.10906295478343964
epoch£º712	 i:5 	 global-step:14245	 l-p:0.12602044641971588
epoch£º712	 i:6 	 global-step:14246	 l-p:0.139452263712883
epoch£º712	 i:7 	 global-step:14247	 l-p:0.11213354766368866
epoch£º712	 i:8 	 global-step:14248	 l-p:0.14820098876953125
epoch£º712	 i:9 	 global-step:14249	 l-p:0.12583129107952118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5687, 3.5552, 3.5678],
        [3.5687, 2.9423, 2.7404],
        [3.5687, 3.5687, 3.5687],
        [3.5687, 2.8703, 2.4785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 1.2218278646469116 
model_pd.l_d.mean(): -23.713762283325195 
model_pd.lagr.mean(): -22.491933822631836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1480], device='cuda:0')), ('power', tensor([-23.8617], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:1.2218278646469116
epoch£º713	 i:1 	 global-step:14261	 l-p:0.1556892991065979
epoch£º713	 i:2 	 global-step:14262	 l-p:0.14102663099765778
epoch£º713	 i:3 	 global-step:14263	 l-p:0.12884311378002167
epoch£º713	 i:4 	 global-step:14264	 l-p:0.15954791009426117
epoch£º713	 i:5 	 global-step:14265	 l-p:0.146045982837677
epoch£º713	 i:6 	 global-step:14266	 l-p:0.12562839686870575
epoch£º713	 i:7 	 global-step:14267	 l-p:0.11997541040182114
epoch£º713	 i:8 	 global-step:14268	 l-p:-0.14368949830532074
epoch£º713	 i:9 	 global-step:14269	 l-p:0.18286477029323578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5017, 3.5017, 3.5017],
        [3.5017, 2.7900, 2.2313],
        [3.5017, 2.7849, 2.3534],
        [3.5017, 3.4937, 3.5013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.16083547472953796 
model_pd.l_d.mean(): -23.58309555053711 
model_pd.lagr.mean(): -23.422260284423828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2533], device='cuda:0')), ('power', tensor([-23.8363], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.16083547472953796
epoch£º714	 i:1 	 global-step:14281	 l-p:0.17534534633159637
epoch£º714	 i:2 	 global-step:14282	 l-p:0.13677017390727997
epoch£º714	 i:3 	 global-step:14283	 l-p:0.14775951206684113
epoch£º714	 i:4 	 global-step:14284	 l-p:0.13381771743297577
epoch£º714	 i:5 	 global-step:14285	 l-p:0.1466539055109024
epoch£º714	 i:6 	 global-step:14286	 l-p:-0.07755062729120255
epoch£º714	 i:7 	 global-step:14287	 l-p:0.21045228838920593
epoch£º714	 i:8 	 global-step:14288	 l-p:0.12351885437965393
epoch£º714	 i:9 	 global-step:14289	 l-p:-0.004361200146377087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5951, 3.5720, 3.5929],
        [3.5951, 3.1663, 2.5895],
        [3.5951, 2.9744, 2.7803],
        [3.5951, 3.5951, 3.5951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): -0.07065138965845108 
model_pd.l_d.mean(): -23.393619537353516 
model_pd.lagr.mean(): -23.464271545410156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2256], device='cuda:0')), ('power', tensor([-23.6192], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:-0.07065138965845108
epoch£º715	 i:1 	 global-step:14301	 l-p:0.1428418755531311
epoch£º715	 i:2 	 global-step:14302	 l-p:0.1704808920621872
epoch£º715	 i:3 	 global-step:14303	 l-p:0.13468040525913239
epoch£º715	 i:4 	 global-step:14304	 l-p:0.13920877873897552
epoch£º715	 i:5 	 global-step:14305	 l-p:0.12735699117183685
epoch£º715	 i:6 	 global-step:14306	 l-p:0.12097956985235214
epoch£º715	 i:7 	 global-step:14307	 l-p:0.20801258087158203
epoch£º715	 i:8 	 global-step:14308	 l-p:0.15463151037693024
epoch£º715	 i:9 	 global-step:14309	 l-p:0.20190395414829254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6651, 2.9686, 2.4411],
        [3.6651, 3.6603, 3.6649],
        [3.6651, 3.6651, 3.6651],
        [3.6651, 3.2421, 2.6592]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.1506923884153366 
model_pd.l_d.mean(): -23.362728118896484 
model_pd.lagr.mean(): -23.2120361328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2244], device='cuda:0')), ('power', tensor([-23.5872], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.1506923884153366
epoch£º716	 i:1 	 global-step:14321	 l-p:0.12128808349370956
epoch£º716	 i:2 	 global-step:14322	 l-p:0.1272377222776413
epoch£º716	 i:3 	 global-step:14323	 l-p:0.12381614744663239
epoch£º716	 i:4 	 global-step:14324	 l-p:0.14115947484970093
epoch£º716	 i:5 	 global-step:14325	 l-p:0.2916470766067505
epoch£º716	 i:6 	 global-step:14326	 l-p:0.10389451682567596
epoch£º716	 i:7 	 global-step:14327	 l-p:0.14036113023757935
epoch£º716	 i:8 	 global-step:14328	 l-p:0.40065038204193115
epoch£º716	 i:9 	 global-step:14329	 l-p:-0.3790959119796753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6222, 3.4134, 3.5223],
        [3.6222, 3.3493, 3.4588],
        [3.6222, 3.6121, 3.6216],
        [3.6222, 3.0259, 2.4221]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.13642196357250214 
model_pd.l_d.mean(): -23.679121017456055 
model_pd.lagr.mean(): -23.542699813842773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1478], device='cuda:0')), ('power', tensor([-23.8269], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.13642196357250214
epoch£º717	 i:1 	 global-step:14341	 l-p:0.10476836562156677
epoch£º717	 i:2 	 global-step:14342	 l-p:0.14804530143737793
epoch£º717	 i:3 	 global-step:14343	 l-p:-0.4956243932247162
epoch£º717	 i:4 	 global-step:14344	 l-p:0.21780315041542053
epoch£º717	 i:5 	 global-step:14345	 l-p:0.0514945313334465
epoch£º717	 i:6 	 global-step:14346	 l-p:0.13789381086826324
epoch£º717	 i:7 	 global-step:14347	 l-p:0.14153215289115906
epoch£º717	 i:8 	 global-step:14348	 l-p:0.1351204812526703
epoch£º717	 i:9 	 global-step:14349	 l-p:0.11581912636756897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[3.6006, 3.1239, 3.1291],
        [3.6006, 3.2327, 3.3151],
        [3.6006, 3.1968, 2.6245],
        [3.6006, 3.3041, 3.4105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.12994319200515747 
model_pd.l_d.mean(): -23.69719696044922 
model_pd.lagr.mean(): -23.56725311279297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1510], device='cuda:0')), ('power', tensor([-23.8482], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.12994319200515747
epoch£º718	 i:1 	 global-step:14361	 l-p:0.1753522902727127
epoch£º718	 i:2 	 global-step:14362	 l-p:0.06830369681119919
epoch£º718	 i:3 	 global-step:14363	 l-p:0.1258247345685959
epoch£º718	 i:4 	 global-step:14364	 l-p:0.04028578847646713
epoch£º718	 i:5 	 global-step:14365	 l-p:0.2315281480550766
epoch£º718	 i:6 	 global-step:14366	 l-p:0.12171722948551178
epoch£º718	 i:7 	 global-step:14367	 l-p:0.15710629522800446
epoch£º718	 i:8 	 global-step:14368	 l-p:0.170677050948143
epoch£º718	 i:9 	 global-step:14369	 l-p:-0.09484296292066574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6126, 3.2905, 3.3904],
        [3.6126, 2.9108, 2.3602],
        [3.6126, 2.9923, 2.3892],
        [3.6126, 3.5692, 3.6062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.1397782415151596 
model_pd.l_d.mean(): -22.374473571777344 
model_pd.lagr.mean(): -22.234695434570312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3664], device='cuda:0')), ('power', tensor([-22.7409], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.1397782415151596
epoch£º719	 i:1 	 global-step:14381	 l-p:-1.0417938232421875
epoch£º719	 i:2 	 global-step:14382	 l-p:0.38585364818573
epoch£º719	 i:3 	 global-step:14383	 l-p:0.14867593348026276
epoch£º719	 i:4 	 global-step:14384	 l-p:0.1313353329896927
epoch£º719	 i:5 	 global-step:14385	 l-p:0.1357078105211258
epoch£º719	 i:6 	 global-step:14386	 l-p:0.12749263644218445
epoch£º719	 i:7 	 global-step:14387	 l-p:0.13130487501621246
epoch£º719	 i:8 	 global-step:14388	 l-p:0.12323558330535889
epoch£º719	 i:9 	 global-step:14389	 l-p:0.12525664269924164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6731, 3.5332, 3.6244],
        [3.6731, 3.2001, 3.2034],
        [3.6731, 2.9787, 2.5579],
        [3.6731, 3.1051, 2.9915]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.14958500862121582 
model_pd.l_d.mean(): -23.378503799438477 
model_pd.lagr.mean(): -23.228918075561523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1943], device='cuda:0')), ('power', tensor([-23.5728], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.14958500862121582
epoch£º720	 i:1 	 global-step:14401	 l-p:0.18224383890628815
epoch£º720	 i:2 	 global-step:14402	 l-p:0.16544391214847565
epoch£º720	 i:3 	 global-step:14403	 l-p:0.2351568043231964
epoch£º720	 i:4 	 global-step:14404	 l-p:0.12143862247467041
epoch£º720	 i:5 	 global-step:14405	 l-p:0.12570837140083313
epoch£º720	 i:6 	 global-step:14406	 l-p:0.13776075839996338
epoch£º720	 i:7 	 global-step:14407	 l-p:0.13499385118484497
epoch£º720	 i:8 	 global-step:14408	 l-p:0.13899561762809753
epoch£º720	 i:9 	 global-step:14409	 l-p:0.4537830650806427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[3.6153, 2.9046, 2.4085],
        [3.6153, 3.2141, 2.6405],
        [3.6153, 3.0185, 2.8683],
        [3.6153, 2.9656, 2.7121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.10342557728290558 
model_pd.l_d.mean(): -22.95516014099121 
model_pd.lagr.mean(): -22.851734161376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2735], device='cuda:0')), ('power', tensor([-23.2287], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.10342557728290558
epoch£º721	 i:1 	 global-step:14421	 l-p:0.14972837269306183
epoch£º721	 i:2 	 global-step:14422	 l-p:-0.4301455616950989
epoch£º721	 i:3 	 global-step:14423	 l-p:0.12432811409235
epoch£º721	 i:4 	 global-step:14424	 l-p:0.17472341656684875
epoch£º721	 i:5 	 global-step:14425	 l-p:0.16430938243865967
epoch£º721	 i:6 	 global-step:14426	 l-p:0.14356327056884766
epoch£º721	 i:7 	 global-step:14427	 l-p:0.3277986943721771
epoch£º721	 i:8 	 global-step:14428	 l-p:0.12383069843053818
epoch£º721	 i:9 	 global-step:14429	 l-p:0.13134999573230743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6570, 3.6570, 3.6570],
        [3.6570, 3.6523, 3.6569],
        [3.6570, 3.6570, 3.6570],
        [3.6570, 3.5646, 3.6336]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.11698884516954422 
model_pd.l_d.mean(): -23.278255462646484 
model_pd.lagr.mean(): -23.161266326904297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2289], device='cuda:0')), ('power', tensor([-23.5072], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.11698884516954422
epoch£º722	 i:1 	 global-step:14441	 l-p:0.19598689675331116
epoch£º722	 i:2 	 global-step:14442	 l-p:0.19786953926086426
epoch£º722	 i:3 	 global-step:14443	 l-p:0.12654736638069153
epoch£º722	 i:4 	 global-step:14444	 l-p:0.12170090526342392
epoch£º722	 i:5 	 global-step:14445	 l-p:0.1329805850982666
epoch£º722	 i:6 	 global-step:14446	 l-p:0.15988615155220032
epoch£º722	 i:7 	 global-step:14447	 l-p:0.12468869239091873
epoch£º722	 i:8 	 global-step:14448	 l-p:0.14845091104507446
epoch£º722	 i:9 	 global-step:14449	 l-p:0.1371401995420456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6586, 3.5303, 3.6169],
        [3.6586, 2.9647, 2.4077],
        [3.6586, 3.6582, 3.6586],
        [3.6586, 3.6586, 3.6586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.1834985762834549 
model_pd.l_d.mean(): -23.470378875732422 
model_pd.lagr.mean(): -23.286880493164062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2029], device='cuda:0')), ('power', tensor([-23.6733], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.1834985762834549
epoch£º723	 i:1 	 global-step:14461	 l-p:0.1303243637084961
epoch£º723	 i:2 	 global-step:14462	 l-p:0.14030082523822784
epoch£º723	 i:3 	 global-step:14463	 l-p:0.12571048736572266
epoch£º723	 i:4 	 global-step:14464	 l-p:0.14213760197162628
epoch£º723	 i:5 	 global-step:14465	 l-p:0.1474757194519043
epoch£º723	 i:6 	 global-step:14466	 l-p:0.1708955466747284
epoch£º723	 i:7 	 global-step:14467	 l-p:0.12467418611049652
epoch£º723	 i:8 	 global-step:14468	 l-p:0.16567176580429077
epoch£º723	 i:9 	 global-step:14469	 l-p:-0.10223869234323502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5453, 3.5401, 3.5451],
        [3.5453, 2.8423, 2.2681],
        [3.5453, 3.5225, 3.5431],
        [3.5453, 3.0189, 2.9786]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.0928463265299797 
model_pd.l_d.mean(): -23.064586639404297 
model_pd.lagr.mean(): -22.97174072265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2745], device='cuda:0')), ('power', tensor([-23.3391], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.0928463265299797
epoch£º724	 i:1 	 global-step:14481	 l-p:0.22836115956306458
epoch£º724	 i:2 	 global-step:14482	 l-p:0.1555502712726593
epoch£º724	 i:3 	 global-step:14483	 l-p:0.1333686262369156
epoch£º724	 i:4 	 global-step:14484	 l-p:0.11394169181585312
epoch£º724	 i:5 	 global-step:14485	 l-p:0.1462867259979248
epoch£º724	 i:6 	 global-step:14486	 l-p:0.12130153179168701
epoch£º724	 i:7 	 global-step:14487	 l-p:-0.03429711237549782
epoch£º724	 i:8 	 global-step:14488	 l-p:0.17444775998592377
epoch£º724	 i:9 	 global-step:14489	 l-p:0.04272026941180229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5534, 3.5453, 3.5530],
        [3.5534, 2.8307, 2.3333],
        [3.5534, 2.8386, 2.4116],
        [3.5534, 3.2763, 3.3874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.13214100897312164 
model_pd.l_d.mean(): -23.496559143066406 
model_pd.lagr.mean(): -23.364418029785156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1789], device='cuda:0')), ('power', tensor([-23.6755], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.13214100897312164
epoch£º725	 i:1 	 global-step:14501	 l-p:0.12231021374464035
epoch£º725	 i:2 	 global-step:14502	 l-p:0.13165625929832458
epoch£º725	 i:3 	 global-step:14503	 l-p:0.5157225131988525
epoch£º725	 i:4 	 global-step:14504	 l-p:0.12617173790931702
epoch£º725	 i:5 	 global-step:14505	 l-p:0.13546080887317657
epoch£º725	 i:6 	 global-step:14506	 l-p:0.23779529333114624
epoch£º725	 i:7 	 global-step:14507	 l-p:-0.03917861729860306
epoch£º725	 i:8 	 global-step:14508	 l-p:0.08090284466743469
epoch£º725	 i:9 	 global-step:14509	 l-p:0.12477405369281769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[3.5876, 3.3277, 3.4397],
        [3.5876, 2.9718, 2.7969],
        [3.5876, 2.8988, 2.3145],
        [3.5876, 3.0422, 2.4444]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.2856554687023163 
model_pd.l_d.mean(): -22.686426162719727 
model_pd.lagr.mean(): -22.40077018737793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3218], device='cuda:0')), ('power', tensor([-23.0083], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.2856554687023163
epoch£º726	 i:1 	 global-step:14521	 l-p:-0.052800681442022324
epoch£º726	 i:2 	 global-step:14522	 l-p:0.1396133452653885
epoch£º726	 i:3 	 global-step:14523	 l-p:0.14171376824378967
epoch£º726	 i:4 	 global-step:14524	 l-p:0.17199067771434784
epoch£º726	 i:5 	 global-step:14525	 l-p:0.127084881067276
epoch£º726	 i:6 	 global-step:14526	 l-p:0.13537991046905518
epoch£º726	 i:7 	 global-step:14527	 l-p:0.25114747881889343
epoch£º726	 i:8 	 global-step:14528	 l-p:0.1403859704732895
epoch£º726	 i:9 	 global-step:14529	 l-p:0.11995773017406464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6265, 3.6265, 3.6265],
        [3.6265, 3.0472, 2.4418],
        [3.6265, 3.2284, 2.6537],
        [3.6265, 3.6265, 3.6265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.14319449663162231 
model_pd.l_d.mean(): -23.50429916381836 
model_pd.lagr.mean(): -23.36110496520996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2251], device='cuda:0')), ('power', tensor([-23.7294], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.14319449663162231
epoch£º727	 i:1 	 global-step:14541	 l-p:0.12077342718839645
epoch£º727	 i:2 	 global-step:14542	 l-p:0.19219844043254852
epoch£º727	 i:3 	 global-step:14543	 l-p:0.043049659579992294
epoch£º727	 i:4 	 global-step:14544	 l-p:0.234476238489151
epoch£º727	 i:5 	 global-step:14545	 l-p:0.15526168048381805
epoch£º727	 i:6 	 global-step:14546	 l-p:0.10108838975429535
epoch£º727	 i:7 	 global-step:14547	 l-p:-0.15574626624584198
epoch£º727	 i:8 	 global-step:14548	 l-p:0.13792191445827484
epoch£º727	 i:9 	 global-step:14549	 l-p:0.14027315378189087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6344, 3.6087, 3.6318],
        [3.6344, 3.6330, 3.6344],
        [3.6344, 2.9300, 2.5067],
        [3.6344, 3.2600, 2.6898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.1612989753484726 
model_pd.l_d.mean(): -23.43647575378418 
model_pd.lagr.mean(): -23.275177001953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2027], device='cuda:0')), ('power', tensor([-23.6392], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.1612989753484726
epoch£º728	 i:1 	 global-step:14561	 l-p:0.15141625702381134
epoch£º728	 i:2 	 global-step:14562	 l-p:0.11827658116817474
epoch£º728	 i:3 	 global-step:14563	 l-p:0.12557493150234222
epoch£º728	 i:4 	 global-step:14564	 l-p:0.1700051724910736
epoch£º728	 i:5 	 global-step:14565	 l-p:0.13032321631908417
epoch£º728	 i:6 	 global-step:14566	 l-p:0.1347108632326126
epoch£º728	 i:7 	 global-step:14567	 l-p:0.12905330955982208
epoch£º728	 i:8 	 global-step:14568	 l-p:0.16818314790725708
epoch£º728	 i:9 	 global-step:14569	 l-p:0.1619480550289154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6896, 3.3977, 3.5042],
        [3.6896, 3.5415, 3.6358],
        [3.6896, 3.3246, 3.4069],
        [3.6896, 3.6896, 3.6896]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.14380644261837006 
model_pd.l_d.mean(): -22.766956329345703 
model_pd.lagr.mean(): -22.623149871826172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3391], device='cuda:0')), ('power', tensor([-23.1061], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:0.14380644261837006
epoch£º729	 i:1 	 global-step:14581	 l-p:0.16926726698875427
epoch£º729	 i:2 	 global-step:14582	 l-p:0.13832494616508484
epoch£º729	 i:3 	 global-step:14583	 l-p:0.12553925812244415
epoch£º729	 i:4 	 global-step:14584	 l-p:0.12026750296354294
epoch£º729	 i:5 	 global-step:14585	 l-p:0.16142994165420532
epoch£º729	 i:6 	 global-step:14586	 l-p:0.12142675369977951
epoch£º729	 i:7 	 global-step:14587	 l-p:0.18110224604606628
epoch£º729	 i:8 	 global-step:14588	 l-p:0.12755054235458374
epoch£º729	 i:9 	 global-step:14589	 l-p:0.04818591848015785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[3.7068, 3.0182, 2.4545],
        [3.7068, 3.1796, 2.5715],
        [3.7068, 3.0088, 2.5744],
        [3.7068, 3.0063, 2.5520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.1555549055337906 
model_pd.l_d.mean(): -22.764772415161133 
model_pd.lagr.mean(): -22.609216690063477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3002], device='cuda:0')), ('power', tensor([-23.0650], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.1555549055337906
epoch£º730	 i:1 	 global-step:14601	 l-p:0.12052042782306671
epoch£º730	 i:2 	 global-step:14602	 l-p:0.15662223100662231
epoch£º730	 i:3 	 global-step:14603	 l-p:0.14662402868270874
epoch£º730	 i:4 	 global-step:14604	 l-p:0.11742838472127914
epoch£º730	 i:5 	 global-step:14605	 l-p:0.1148042157292366
epoch£º730	 i:6 	 global-step:14606	 l-p:0.044555291533470154
epoch£º730	 i:7 	 global-step:14607	 l-p:0.12624453008174896
epoch£º730	 i:8 	 global-step:14608	 l-p:0.14538273215293884
epoch£º730	 i:9 	 global-step:14609	 l-p:0.17396987974643707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6856, 3.2397, 2.6476],
        [3.6856, 3.2840, 3.3469],
        [3.6856, 3.0656, 2.8692],
        [3.6856, 3.6650, 3.6838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.10611151158809662 
model_pd.l_d.mean(): -23.304874420166016 
model_pd.lagr.mean(): -23.198762893676758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2282], device='cuda:0')), ('power', tensor([-23.5331], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.10611151158809662
epoch£º731	 i:1 	 global-step:14621	 l-p:0.1874576210975647
epoch£º731	 i:2 	 global-step:14622	 l-p:0.1339980512857437
epoch£º731	 i:3 	 global-step:14623	 l-p:0.2042733132839203
epoch£º731	 i:4 	 global-step:14624	 l-p:0.12484521418809891
epoch£º731	 i:5 	 global-step:14625	 l-p:0.16767176985740662
epoch£º731	 i:6 	 global-step:14626	 l-p:0.1409057229757309
epoch£º731	 i:7 	 global-step:14627	 l-p:0.11787398904561996
epoch£º731	 i:8 	 global-step:14628	 l-p:0.15243978798389435
epoch£º731	 i:9 	 global-step:14629	 l-p:0.19710475206375122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6592, 2.9710, 2.3901],
        [3.6592, 3.6548, 3.6590],
        [3.6592, 3.6592, 3.6592],
        [3.6592, 3.6523, 3.6589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.12773260474205017 
model_pd.l_d.mean(): -22.86941909790039 
model_pd.lagr.mean(): -22.74168586730957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2666], device='cuda:0')), ('power', tensor([-23.1360], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:0.12773260474205017
epoch£º732	 i:1 	 global-step:14641	 l-p:0.1399753838777542
epoch£º732	 i:2 	 global-step:14642	 l-p:0.11399001628160477
epoch£º732	 i:3 	 global-step:14643	 l-p:0.15111775696277618
epoch£º732	 i:4 	 global-step:14644	 l-p:0.12164773046970367
epoch£º732	 i:5 	 global-step:14645	 l-p:0.1434253454208374
epoch£º732	 i:6 	 global-step:14646	 l-p:0.5177266001701355
epoch£º732	 i:7 	 global-step:14647	 l-p:1.9039819240570068
epoch£º732	 i:8 	 global-step:14648	 l-p:0.13107946515083313
epoch£º732	 i:9 	 global-step:14649	 l-p:0.12219943106174469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6314, 3.6314, 3.6314],
        [3.6314, 3.6086, 3.6292],
        [3.6314, 3.4746, 3.5724],
        [3.6314, 3.5679, 3.6192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.12797734141349792 
model_pd.l_d.mean(): -23.09327507019043 
model_pd.lagr.mean(): -22.96529769897461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2634], device='cuda:0')), ('power', tensor([-23.3567], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.12797734141349792
epoch£º733	 i:1 	 global-step:14661	 l-p:0.1496633142232895
epoch£º733	 i:2 	 global-step:14662	 l-p:0.1536983698606491
epoch£º733	 i:3 	 global-step:14663	 l-p:-0.28814008831977844
epoch£º733	 i:4 	 global-step:14664	 l-p:1.456190586090088
epoch£º733	 i:5 	 global-step:14665	 l-p:0.1422502100467682
epoch£º733	 i:6 	 global-step:14666	 l-p:0.20276568830013275
epoch£º733	 i:7 	 global-step:14667	 l-p:0.08833970874547958
epoch£º733	 i:8 	 global-step:14668	 l-p:0.14702235162258148
epoch£º733	 i:9 	 global-step:14669	 l-p:0.5208105444908142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6455, 3.6432, 3.6454],
        [3.6455, 3.0433, 2.4334],
        [3.6455, 3.1892, 2.5987],
        [3.6455, 2.9532, 2.5904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.14164821803569794 
model_pd.l_d.mean(): -23.42046546936035 
model_pd.lagr.mean(): -23.278818130493164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1981], device='cuda:0')), ('power', tensor([-23.6185], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.14164821803569794
epoch£º734	 i:1 	 global-step:14681	 l-p:0.21173232793807983
epoch£º734	 i:2 	 global-step:14682	 l-p:0.15448875725269318
epoch£º734	 i:3 	 global-step:14683	 l-p:0.14206920564174652
epoch£º734	 i:4 	 global-step:14684	 l-p:0.3594089448451996
epoch£º734	 i:5 	 global-step:14685	 l-p:0.12713536620140076
epoch£º734	 i:6 	 global-step:14686	 l-p:0.181141197681427
epoch£º734	 i:7 	 global-step:14687	 l-p:0.07863409072160721
epoch£º734	 i:8 	 global-step:14688	 l-p:0.1291758269071579
epoch£º734	 i:9 	 global-step:14689	 l-p:0.14205463230609894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6541, 3.0772, 2.9630],
        [3.6541, 3.4863, 3.5874],
        [3.6541, 2.9512, 2.3897],
        [3.6541, 2.9433, 2.4093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.13398608565330505 
model_pd.l_d.mean(): -23.18004608154297 
model_pd.lagr.mean(): -23.04606056213379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2128], device='cuda:0')), ('power', tensor([-23.3928], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.13398608565330505
epoch£º735	 i:1 	 global-step:14701	 l-p:0.14516282081604004
epoch£º735	 i:2 	 global-step:14702	 l-p:0.5106454491615295
epoch£º735	 i:3 	 global-step:14703	 l-p:0.12521590292453766
epoch£º735	 i:4 	 global-step:14704	 l-p:0.14168216288089752
epoch£º735	 i:5 	 global-step:14705	 l-p:1.6087760925292969
epoch£º735	 i:6 	 global-step:14706	 l-p:0.2068018615245819
epoch£º735	 i:7 	 global-step:14707	 l-p:0.13480359315872192
epoch£º735	 i:8 	 global-step:14708	 l-p:-0.03141570836305618
epoch£º735	 i:9 	 global-step:14709	 l-p:0.1599031537771225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6102, 3.1499, 2.5624],
        [3.6102, 3.4776, 3.5665],
        [3.6102, 2.9712, 2.7572],
        [3.6102, 2.9801, 2.7835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.14486148953437805 
model_pd.l_d.mean(): -23.479684829711914 
model_pd.lagr.mean(): -23.334823608398438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2580], device='cuda:0')), ('power', tensor([-23.7377], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.14486148953437805
epoch£º736	 i:1 	 global-step:14721	 l-p:0.20128676295280457
epoch£º736	 i:2 	 global-step:14722	 l-p:0.15648230910301208
epoch£º736	 i:3 	 global-step:14723	 l-p:0.1357487589120865
epoch£º736	 i:4 	 global-step:14724	 l-p:0.20882458984851837
epoch£º736	 i:5 	 global-step:14725	 l-p:0.1274808794260025
epoch£º736	 i:6 	 global-step:14726	 l-p:0.13927194476127625
epoch£º736	 i:7 	 global-step:14727	 l-p:0.10770343244075775
epoch£º736	 i:8 	 global-step:14728	 l-p:0.07518476247787476
epoch£º736	 i:9 	 global-step:14729	 l-p:0.1490527242422104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5489, 3.4835, 3.5362],
        [3.5489, 3.1383, 2.5679],
        [3.5489, 2.8224, 2.2739],
        [3.5489, 3.0908, 2.5108]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.1262936145067215 
model_pd.l_d.mean(): -23.5987491607666 
model_pd.lagr.mean(): -23.472455978393555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1823], device='cuda:0')), ('power', tensor([-23.7810], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.1262936145067215
epoch£º737	 i:1 	 global-step:14741	 l-p:0.17637822031974792
epoch£º737	 i:2 	 global-step:14742	 l-p:-0.8504952192306519
epoch£º737	 i:3 	 global-step:14743	 l-p:0.08755935728549957
epoch£º737	 i:4 	 global-step:14744	 l-p:0.14156872034072876
epoch£º737	 i:5 	 global-step:14745	 l-p:0.1281077116727829
epoch£º737	 i:6 	 global-step:14746	 l-p:0.09404107183218002
epoch£º737	 i:7 	 global-step:14747	 l-p:0.12362930923700333
epoch£º737	 i:8 	 global-step:14748	 l-p:0.13025067746639252
epoch£º737	 i:9 	 global-step:14749	 l-p:0.49750933051109314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5869, 3.5634, 3.5846],
        [3.5869, 3.5821, 3.5868],
        [3.5869, 3.5855, 3.5869],
        [3.5869, 3.5869, 3.5869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.05490872636437416 
model_pd.l_d.mean(): -23.33911895751953 
model_pd.lagr.mean(): -23.284210205078125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2534], device='cuda:0')), ('power', tensor([-23.5925], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.05490872636437416
epoch£º738	 i:1 	 global-step:14761	 l-p:0.12705127894878387
epoch£º738	 i:2 	 global-step:14762	 l-p:0.1915607452392578
epoch£º738	 i:3 	 global-step:14763	 l-p:0.1353224366903305
epoch£º738	 i:4 	 global-step:14764	 l-p:0.1792045384645462
epoch£º738	 i:5 	 global-step:14765	 l-p:0.13828259706497192
epoch£º738	 i:6 	 global-step:14766	 l-p:0.16663424670696259
epoch£º738	 i:7 	 global-step:14767	 l-p:0.14805306494235992
epoch£º738	 i:8 	 global-step:14768	 l-p:0.13052022457122803
epoch£º738	 i:9 	 global-step:14769	 l-p:0.06763262301683426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6048, 3.6048, 3.6048],
        [3.6048, 3.6048, 3.6048],
        [3.6048, 2.9000, 2.3206],
        [3.6048, 3.3242, 3.4356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.13332322239875793 
model_pd.l_d.mean(): -23.02599334716797 
model_pd.lagr.mean(): -22.892669677734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3028], device='cuda:0')), ('power', tensor([-23.3288], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.13332322239875793
epoch£º739	 i:1 	 global-step:14781	 l-p:0.12925349175930023
epoch£º739	 i:2 	 global-step:14782	 l-p:0.08002596348524094
epoch£º739	 i:3 	 global-step:14783	 l-p:0.09023921936750412
epoch£º739	 i:4 	 global-step:14784	 l-p:0.12669238448143005
epoch£º739	 i:5 	 global-step:14785	 l-p:0.14966344833374023
epoch£º739	 i:6 	 global-step:14786	 l-p:0.18223701417446136
epoch£º739	 i:7 	 global-step:14787	 l-p:0.13460539281368256
epoch£º739	 i:8 	 global-step:14788	 l-p:0.18074294924736023
epoch£º739	 i:9 	 global-step:14789	 l-p:-0.10898113250732422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5732, 3.5732, 3.5732],
        [3.5732, 3.5516, 3.5712],
        [3.5732, 3.0558, 3.0307],
        [3.5732, 2.8688, 2.5069]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.132146954536438 
model_pd.l_d.mean(): -22.733203887939453 
model_pd.lagr.mean(): -22.601057052612305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3223], device='cuda:0')), ('power', tensor([-23.0555], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.132146954536438
epoch£º740	 i:1 	 global-step:14801	 l-p:0.19468095898628235
epoch£º740	 i:2 	 global-step:14802	 l-p:0.09584581851959229
epoch£º740	 i:3 	 global-step:14803	 l-p:0.1979629546403885
epoch£º740	 i:4 	 global-step:14804	 l-p:0.13406431674957275
epoch£º740	 i:5 	 global-step:14805	 l-p:0.0955478772521019
epoch£º740	 i:6 	 global-step:14806	 l-p:0.142015740275383
epoch£º740	 i:7 	 global-step:14807	 l-p:-1.3261994123458862
epoch£º740	 i:8 	 global-step:14808	 l-p:0.09954517334699631
epoch£º740	 i:9 	 global-step:14809	 l-p:0.15100762248039246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5936, 2.8679, 2.3966],
        [3.5936, 3.1733, 2.5952],
        [3.5936, 2.8667, 2.3814],
        [3.5936, 3.5936, 3.5936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.11025101691484451 
model_pd.l_d.mean(): -23.394622802734375 
model_pd.lagr.mean(): -23.284372329711914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2605], device='cuda:0')), ('power', tensor([-23.6552], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.11025101691484451
epoch£º741	 i:1 	 global-step:14821	 l-p:0.14923836290836334
epoch£º741	 i:2 	 global-step:14822	 l-p:0.04741046205163002
epoch£º741	 i:3 	 global-step:14823	 l-p:0.14249219000339508
epoch£º741	 i:4 	 global-step:14824	 l-p:0.15108336508274078
epoch£º741	 i:5 	 global-step:14825	 l-p:0.01174547616392374
epoch£º741	 i:6 	 global-step:14826	 l-p:0.13487808406352997
epoch£º741	 i:7 	 global-step:14827	 l-p:0.13308696448802948
epoch£º741	 i:8 	 global-step:14828	 l-p:0.2040078490972519
epoch£º741	 i:9 	 global-step:14829	 l-p:0.14385829865932465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6270, 3.2186, 2.6394],
        [3.6270, 3.3624, 3.4752],
        [3.6270, 3.6194, 3.6267],
        [3.6270, 3.0569, 2.9610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.10098212212324142 
model_pd.l_d.mean(): -23.070388793945312 
model_pd.lagr.mean(): -22.969406127929688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3069], device='cuda:0')), ('power', tensor([-23.3773], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.10098212212324142
epoch£º742	 i:1 	 global-step:14841	 l-p:0.16484452784061432
epoch£º742	 i:2 	 global-step:14842	 l-p:-0.28929537534713745
epoch£º742	 i:3 	 global-step:14843	 l-p:0.11437918245792389
epoch£º742	 i:4 	 global-step:14844	 l-p:0.16136640310287476
epoch£º742	 i:5 	 global-step:14845	 l-p:0.1330031007528305
epoch£º742	 i:6 	 global-step:14846	 l-p:0.13693015277385712
epoch£º742	 i:7 	 global-step:14847	 l-p:-0.06538782268762589
epoch£º742	 i:8 	 global-step:14848	 l-p:-0.012335147708654404
epoch£º742	 i:9 	 global-step:14849	 l-p:0.1256662756204605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6092, 3.6091, 3.6092],
        [3.6092, 3.4800, 3.5677],
        [3.6092, 2.9115, 2.3204],
        [3.6092, 2.8852, 2.4255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): -0.029833916574716568 
model_pd.l_d.mean(): -23.097566604614258 
model_pd.lagr.mean(): -23.12740135192871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2824], device='cuda:0')), ('power', tensor([-23.3800], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:-0.029833916574716568
epoch£º743	 i:1 	 global-step:14861	 l-p:0.13038977980613708
epoch£º743	 i:2 	 global-step:14862	 l-p:0.053528107702732086
epoch£º743	 i:3 	 global-step:14863	 l-p:0.24439753592014313
epoch£º743	 i:4 	 global-step:14864	 l-p:0.04614177718758583
epoch£º743	 i:5 	 global-step:14865	 l-p:0.12659230828285217
epoch£º743	 i:6 	 global-step:14866	 l-p:0.15127640962600708
epoch£º743	 i:7 	 global-step:14867	 l-p:0.13385437428951263
epoch£º743	 i:8 	 global-step:14868	 l-p:0.12308289110660553
epoch£º743	 i:9 	 global-step:14869	 l-p:0.1417299509048462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5959, 3.4238, 3.5269],
        [3.5959, 3.3097, 3.4214],
        [3.5959, 3.4268, 3.5291],
        [3.5959, 3.5209, 3.5798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.1317998766899109 
model_pd.l_d.mean(): -23.717199325561523 
model_pd.lagr.mean(): -23.585399627685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1288], device='cuda:0')), ('power', tensor([-23.8460], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.1317998766899109
epoch£º744	 i:1 	 global-step:14881	 l-p:0.13174624741077423
epoch£º744	 i:2 	 global-step:14882	 l-p:0.027064785361289978
epoch£º744	 i:3 	 global-step:14883	 l-p:0.06996527314186096
epoch£º744	 i:4 	 global-step:14884	 l-p:0.170823335647583
epoch£º744	 i:5 	 global-step:14885	 l-p:0.598354160785675
epoch£º744	 i:6 	 global-step:14886	 l-p:0.1462748646736145
epoch£º744	 i:7 	 global-step:14887	 l-p:0.13445799052715302
epoch£º744	 i:8 	 global-step:14888	 l-p:0.14830699563026428
epoch£º744	 i:9 	 global-step:14889	 l-p:0.1750604808330536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5345, 3.4697, 3.5221],
        [3.5345, 3.5344, 3.5345],
        [3.5345, 3.1133, 2.5413],
        [3.5345, 2.7945, 2.2793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.11850055307149887 
model_pd.l_d.mean(): -22.998477935791016 
model_pd.lagr.mean(): -22.87997817993164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3269], device='cuda:0')), ('power', tensor([-23.3254], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.11850055307149887
epoch£º745	 i:1 	 global-step:14901	 l-p:-0.02601577155292034
epoch£º745	 i:2 	 global-step:14902	 l-p:0.17642207443714142
epoch£º745	 i:3 	 global-step:14903	 l-p:0.1954517364501953
epoch£º745	 i:4 	 global-step:14904	 l-p:0.13308286666870117
epoch£º745	 i:5 	 global-step:14905	 l-p:0.11481990665197372
epoch£º745	 i:6 	 global-step:14906	 l-p:0.11560840159654617
epoch£º745	 i:7 	 global-step:14907	 l-p:-0.014598369598388672
epoch£º745	 i:8 	 global-step:14908	 l-p:0.1526121348142624
epoch£º745	 i:9 	 global-step:14909	 l-p:0.2396117001771927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6593, 3.6468, 3.6585],
        [3.6593, 3.6410, 3.6578],
        [3.6593, 3.0166, 2.4037],
        [3.6593, 3.6593, 3.6593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.2596780061721802 
model_pd.l_d.mean(): -22.737594604492188 
model_pd.lagr.mean(): -22.477916717529297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3023], device='cuda:0')), ('power', tensor([-23.0399], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.2596780061721802
epoch£º746	 i:1 	 global-step:14921	 l-p:0.13844919204711914
epoch£º746	 i:2 	 global-step:14922	 l-p:0.1195368617773056
epoch£º746	 i:3 	 global-step:14923	 l-p:0.1486515998840332
epoch£º746	 i:4 	 global-step:14924	 l-p:0.11653893440961838
epoch£º746	 i:5 	 global-step:14925	 l-p:0.12470502406358719
epoch£º746	 i:6 	 global-step:14926	 l-p:0.14616341888904572
epoch£º746	 i:7 	 global-step:14927	 l-p:0.1612696647644043
epoch£º746	 i:8 	 global-step:14928	 l-p:0.16467735171318054
epoch£º746	 i:9 	 global-step:14929	 l-p:0.11653818190097809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6906, 3.5856, 3.6617],
        [3.6906, 3.6161, 3.6746],
        [3.6906, 3.0261, 2.4178],
        [3.6906, 3.0372, 2.4253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.1474083811044693 
model_pd.l_d.mean(): -23.149852752685547 
model_pd.lagr.mean(): -23.002445220947266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2501], device='cuda:0')), ('power', tensor([-23.4000], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.1474083811044693
epoch£º747	 i:1 	 global-step:14941	 l-p:0.13880512118339539
epoch£º747	 i:2 	 global-step:14942	 l-p:0.09566862136125565
epoch£º747	 i:3 	 global-step:14943	 l-p:0.17991456389427185
epoch£º747	 i:4 	 global-step:14944	 l-p:0.12843134999275208
epoch£º747	 i:5 	 global-step:14945	 l-p:0.25778359174728394
epoch£º747	 i:6 	 global-step:14946	 l-p:0.13757355511188507
epoch£º747	 i:7 	 global-step:14947	 l-p:0.17290744185447693
epoch£º747	 i:8 	 global-step:14948	 l-p:0.13426126539707184
epoch£º747	 i:9 	 global-step:14949	 l-p:0.13105064630508423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6771, 3.2445, 3.2922],
        [3.6771, 3.2789, 2.6958],
        [3.6771, 3.4521, 3.5643],
        [3.6771, 3.1984, 3.2077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.20032721757888794 
model_pd.l_d.mean(): -23.59996223449707 
model_pd.lagr.mean(): -23.399635314941406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1574], device='cuda:0')), ('power', tensor([-23.7573], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.20032721757888794
epoch£º748	 i:1 	 global-step:14961	 l-p:0.1283060610294342
epoch£º748	 i:2 	 global-step:14962	 l-p:0.1381533145904541
epoch£º748	 i:3 	 global-step:14963	 l-p:0.16103969514369965
epoch£º748	 i:4 	 global-step:14964	 l-p:0.13305611908435822
epoch£º748	 i:5 	 global-step:14965	 l-p:0.20882177352905273
epoch£º748	 i:6 	 global-step:14966	 l-p:0.16034726798534393
epoch£º748	 i:7 	 global-step:14967	 l-p:0.12330692261457443
epoch£º748	 i:8 	 global-step:14968	 l-p:0.11763286590576172
epoch£º748	 i:9 	 global-step:14969	 l-p:0.15846170485019684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6689, 3.2385, 2.6487],
        [3.6689, 3.6247, 3.6624],
        [3.6689, 3.6166, 3.6602],
        [3.6689, 2.9848, 2.6663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.20420172810554504 
model_pd.l_d.mean(): -23.202117919921875 
model_pd.lagr.mean(): -22.997915267944336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2563], device='cuda:0')), ('power', tensor([-23.4584], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:0.20420172810554504
epoch£º749	 i:1 	 global-step:14981	 l-p:0.12138503789901733
epoch£º749	 i:2 	 global-step:14982	 l-p:0.12156388908624649
epoch£º749	 i:3 	 global-step:14983	 l-p:0.12412463128566742
epoch£º749	 i:4 	 global-step:14984	 l-p:0.2786525785923004
epoch£º749	 i:5 	 global-step:14985	 l-p:0.1621253490447998
epoch£º749	 i:6 	 global-step:14986	 l-p:0.13929064571857452
epoch£º749	 i:7 	 global-step:14987	 l-p:0.1197095438838005
epoch£º749	 i:8 	 global-step:14988	 l-p:0.13405480980873108
epoch£º749	 i:9 	 global-step:14989	 l-p:0.2448137253522873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6578, 2.9461, 2.3743],
        [3.6578, 3.1934, 2.5975],
        [3.6578, 3.6495, 3.6574],
        [3.6578, 3.6476, 3.6572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.11258486658334732 
model_pd.l_d.mean(): -23.39105224609375 
model_pd.lagr.mean(): -23.278467178344727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2000], device='cuda:0')), ('power', tensor([-23.5910], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.11258486658334732
epoch£º750	 i:1 	 global-step:15001	 l-p:0.14385588467121124
epoch£º750	 i:2 	 global-step:15002	 l-p:0.15185295045375824
epoch£º750	 i:3 	 global-step:15003	 l-p:0.1766238808631897
epoch£º750	 i:4 	 global-step:15004	 l-p:0.38588467240333557
epoch£º750	 i:5 	 global-step:15005	 l-p:0.1301800161600113
epoch£º750	 i:6 	 global-step:15006	 l-p:0.12521055340766907
epoch£º750	 i:7 	 global-step:15007	 l-p:0.11999576538801193
epoch£º750	 i:8 	 global-step:15008	 l-p:0.06983797252178192
epoch£º750	 i:9 	 global-step:15009	 l-p:0.12954482436180115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5859, 3.3514, 3.4658],
        [3.5859, 2.8687, 2.2862],
        [3.5859, 3.5859, 3.5859],
        [3.5859, 3.2074, 3.2927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.2517229914665222 
model_pd.l_d.mean(): -23.616859436035156 
model_pd.lagr.mean(): -23.365137100219727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2359], device='cuda:0')), ('power', tensor([-23.8528], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.2517229914665222
epoch£º751	 i:1 	 global-step:15021	 l-p:0.12923958897590637
epoch£º751	 i:2 	 global-step:15022	 l-p:0.10929691046476364
epoch£º751	 i:3 	 global-step:15023	 l-p:0.15751871466636658
epoch£º751	 i:4 	 global-step:15024	 l-p:0.19906318187713623
epoch£º751	 i:5 	 global-step:15025	 l-p:0.10119421035051346
epoch£º751	 i:6 	 global-step:15026	 l-p:0.14678137004375458
epoch£º751	 i:7 	 global-step:15027	 l-p:0.13089583814144135
epoch£º751	 i:8 	 global-step:15028	 l-p:0.14336813986301422
epoch£º751	 i:9 	 global-step:15029	 l-p:0.06672528386116028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5386, 3.5386, 3.5386],
        [3.5386, 3.4583, 3.5207],
        [3.5386, 3.2691, 3.3844],
        [3.5386, 3.5304, 3.5382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.9519492983818054 
model_pd.l_d.mean(): -22.860780715942383 
model_pd.lagr.mean(): -21.908830642700195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3347], device='cuda:0')), ('power', tensor([-23.1955], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.9519492983818054
epoch£º752	 i:1 	 global-step:15041	 l-p:0.03129351884126663
epoch£º752	 i:2 	 global-step:15042	 l-p:0.07026881724596024
epoch£º752	 i:3 	 global-step:15043	 l-p:0.03624895587563515
epoch£º752	 i:4 	 global-step:15044	 l-p:0.15214984118938446
epoch£º752	 i:5 	 global-step:15045	 l-p:0.1572217345237732
epoch£º752	 i:6 	 global-step:15046	 l-p:0.12045091390609741
epoch£º752	 i:7 	 global-step:15047	 l-p:0.13722868263721466
epoch£º752	 i:8 	 global-step:15048	 l-p:0.13257454335689545
epoch£º752	 i:9 	 global-step:15049	 l-p:0.11964315176010132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5814, 3.5777, 3.5813],
        [3.5814, 3.5813, 3.5814],
        [3.5814, 3.2993, 3.4128],
        [3.5814, 3.0526, 3.0208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.13943158090114594 
model_pd.l_d.mean(): -23.579572677612305 
model_pd.lagr.mean(): -23.440141677856445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2504], device='cuda:0')), ('power', tensor([-23.8299], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.13943158090114594
epoch£º753	 i:1 	 global-step:15061	 l-p:0.14564161002635956
epoch£º753	 i:2 	 global-step:15062	 l-p:0.10923562943935394
epoch£º753	 i:3 	 global-step:15063	 l-p:0.08470649272203445
epoch£º753	 i:4 	 global-step:15064	 l-p:0.14518176019191742
epoch£º753	 i:5 	 global-step:15065	 l-p:-0.9810841679573059
epoch£º753	 i:6 	 global-step:15066	 l-p:0.1584397256374359
epoch£º753	 i:7 	 global-step:15067	 l-p:0.1317579746246338
epoch£º753	 i:8 	 global-step:15068	 l-p:0.12767282128334045
epoch£º753	 i:9 	 global-step:15069	 l-p:0.1308615803718567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5414, 3.5311, 3.5409],
        [3.5414, 3.5336, 3.5411],
        [3.5414, 2.8985, 2.2933],
        [3.5414, 3.5412, 3.5415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.10896743834018707 
model_pd.l_d.mean(): -23.495100021362305 
model_pd.lagr.mean(): -23.386133193969727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2282], device='cuda:0')), ('power', tensor([-23.7233], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.10896743834018707
epoch£º754	 i:1 	 global-step:15081	 l-p:-0.01683497428894043
epoch£º754	 i:2 	 global-step:15082	 l-p:0.32114672660827637
epoch£º754	 i:3 	 global-step:15083	 l-p:0.068030945956707
epoch£º754	 i:4 	 global-step:15084	 l-p:0.1667177975177765
epoch£º754	 i:5 	 global-step:15085	 l-p:0.12337222695350647
epoch£º754	 i:6 	 global-step:15086	 l-p:0.12537486851215363
epoch£º754	 i:7 	 global-step:15087	 l-p:0.14000333845615387
epoch£º754	 i:8 	 global-step:15088	 l-p:0.13421641290187836
epoch£º754	 i:9 	 global-step:15089	 l-p:0.09020427614450455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5629, 3.2073, 3.3039],
        [3.5629, 3.4805, 3.5442],
        [3.5629, 2.8296, 2.2629],
        [3.5629, 2.9977, 2.9249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.08805962651968002 
model_pd.l_d.mean(): -23.230152130126953 
model_pd.lagr.mean(): -23.142091751098633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2601], device='cuda:0')), ('power', tensor([-23.4903], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.08805962651968002
epoch£º755	 i:1 	 global-step:15101	 l-p:0.03798533231019974
epoch£º755	 i:2 	 global-step:15102	 l-p:0.13698725402355194
epoch£º755	 i:3 	 global-step:15103	 l-p:0.10642295330762863
epoch£º755	 i:4 	 global-step:15104	 l-p:0.14363940060138702
epoch£º755	 i:5 	 global-step:15105	 l-p:0.2096685767173767
epoch£º755	 i:6 	 global-step:15106	 l-p:0.3091632127761841
epoch£º755	 i:7 	 global-step:15107	 l-p:0.12720192968845367
epoch£º755	 i:8 	 global-step:15108	 l-p:0.10016381740570068
epoch£º755	 i:9 	 global-step:15109	 l-p:0.14943596720695496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5986, 2.8645, 2.3188],
        [3.5986, 3.5986, 3.5986],
        [3.5986, 2.9199, 2.6443],
        [3.5986, 3.0207, 2.9267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.20338988304138184 
model_pd.l_d.mean(): -23.55571746826172 
model_pd.lagr.mean(): -23.352327346801758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1927], device='cuda:0')), ('power', tensor([-23.7484], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.20338988304138184
epoch£º756	 i:1 	 global-step:15121	 l-p:0.1433449536561966
epoch£º756	 i:2 	 global-step:15122	 l-p:0.14662137627601624
epoch£º756	 i:3 	 global-step:15123	 l-p:0.11988474428653717
epoch£º756	 i:4 	 global-step:15124	 l-p:0.1529192179441452
epoch£º756	 i:5 	 global-step:15125	 l-p:0.10240316390991211
epoch£º756	 i:6 	 global-step:15126	 l-p:-2.870567798614502
epoch£º756	 i:7 	 global-step:15127	 l-p:0.12854339182376862
epoch£º756	 i:8 	 global-step:15128	 l-p:0.0667755976319313
epoch£º756	 i:9 	 global-step:15129	 l-p:0.14280778169631958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5855, 2.8547, 2.4223],
        [3.5855, 2.8549, 2.2868],
        [3.5855, 3.3028, 3.4166],
        [3.5855, 3.5855, 3.5855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.1391112506389618 
model_pd.l_d.mean(): -22.396249771118164 
model_pd.lagr.mean(): -22.257139205932617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3217], device='cuda:0')), ('power', tensor([-22.7180], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.1391112506389618
epoch£º757	 i:1 	 global-step:15141	 l-p:0.14632612466812134
epoch£º757	 i:2 	 global-step:15142	 l-p:0.12429545074701309
epoch£º757	 i:3 	 global-step:15143	 l-p:0.12143857032060623
epoch£º757	 i:4 	 global-step:15144	 l-p:0.1438330113887787
epoch£º757	 i:5 	 global-step:15145	 l-p:0.1356373131275177
epoch£º757	 i:6 	 global-step:15146	 l-p:-0.1421898752450943
epoch£º757	 i:7 	 global-step:15147	 l-p:0.35753121972084045
epoch£º757	 i:8 	 global-step:15148	 l-p:0.06465399265289307
epoch£º757	 i:9 	 global-step:15149	 l-p:0.1441546529531479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5599, 3.1294, 3.1880],
        [3.5599, 3.2032, 3.3002],
        [3.5599, 3.4163, 3.5106],
        [3.5599, 2.9627, 2.8478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.25205719470977783 
model_pd.l_d.mean(): -23.65917205810547 
model_pd.lagr.mean(): -23.407114028930664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1894], device='cuda:0')), ('power', tensor([-23.8485], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.25205719470977783
epoch£º758	 i:1 	 global-step:15161	 l-p:0.13962224125862122
epoch£º758	 i:2 	 global-step:15162	 l-p:0.15423263609409332
epoch£º758	 i:3 	 global-step:15163	 l-p:-0.3424301743507385
epoch£º758	 i:4 	 global-step:15164	 l-p:0.17182490229606628
epoch£º758	 i:5 	 global-step:15165	 l-p:0.10892944037914276
epoch£º758	 i:6 	 global-step:15166	 l-p:0.11541322618722916
epoch£º758	 i:7 	 global-step:15167	 l-p:0.044953636825084686
epoch£º758	 i:8 	 global-step:15168	 l-p:0.26394474506378174
epoch£º758	 i:9 	 global-step:15169	 l-p:0.13023100793361664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5876, 3.5876, 3.5876],
        [3.5876, 3.5876, 3.5876],
        [3.5876, 2.8509, 2.2935],
        [3.5876, 3.5876, 3.5876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.2289241999387741 
model_pd.l_d.mean(): -23.074827194213867 
model_pd.lagr.mean(): -22.845903396606445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2223], device='cuda:0')), ('power', tensor([-23.2972], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.2289241999387741
epoch£º759	 i:1 	 global-step:15181	 l-p:0.10621348023414612
epoch£º759	 i:2 	 global-step:15182	 l-p:0.06620430946350098
epoch£º759	 i:3 	 global-step:15183	 l-p:0.14154154062271118
epoch£º759	 i:4 	 global-step:15184	 l-p:0.11803286522626877
epoch£º759	 i:5 	 global-step:15185	 l-p:0.1512231081724167
epoch£º759	 i:6 	 global-step:15186	 l-p:0.2007274180650711
epoch£º759	 i:7 	 global-step:15187	 l-p:0.13152432441711426
epoch£º759	 i:8 	 global-step:15188	 l-p:0.14825449883937836
epoch£º759	 i:9 	 global-step:15189	 l-p:0.031020058318972588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6293, 3.5657, 3.6173],
        [3.6293, 3.2025, 3.2608],
        [3.6293, 2.9556, 2.6907],
        [3.6293, 3.4901, 3.5825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): -0.030092019587755203 
model_pd.l_d.mean(): -22.765050888061523 
model_pd.lagr.mean(): -22.795143127441406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3353], device='cuda:0')), ('power', tensor([-23.1004], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:-0.030092019587755203
epoch£º760	 i:1 	 global-step:15201	 l-p:0.4766763150691986
epoch£º760	 i:2 	 global-step:15202	 l-p:0.14180928468704224
epoch£º760	 i:3 	 global-step:15203	 l-p:0.13288383185863495
epoch£º760	 i:4 	 global-step:15204	 l-p:0.2074444741010666
epoch£º760	 i:5 	 global-step:15205	 l-p:0.12033940851688385
epoch£º760	 i:6 	 global-step:15206	 l-p:0.14136017858982086
epoch£º760	 i:7 	 global-step:15207	 l-p:0.12417726963758469
epoch£º760	 i:8 	 global-step:15208	 l-p:0.13171765208244324
epoch£º760	 i:9 	 global-step:15209	 l-p:0.07507208734750748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7023, 3.6706, 3.6986],
        [3.7023, 2.9834, 2.5463],
        [3.7023, 3.2145, 3.2187],
        [3.7023, 3.0092, 2.4075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.17248092591762543 
model_pd.l_d.mean(): -22.259244918823242 
model_pd.lagr.mean(): -22.086763381958008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3667], device='cuda:0')), ('power', tensor([-22.6259], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.17248092591762543
epoch£º761	 i:1 	 global-step:15221	 l-p:0.17752380669116974
epoch£º761	 i:2 	 global-step:15222	 l-p:0.11419262737035751
epoch£º761	 i:3 	 global-step:15223	 l-p:0.1275317519903183
epoch£º761	 i:4 	 global-step:15224	 l-p:0.12139133363962173
epoch£º761	 i:5 	 global-step:15225	 l-p:0.1225028783082962
epoch£º761	 i:6 	 global-step:15226	 l-p:0.13563954830169678
epoch£º761	 i:7 	 global-step:15227	 l-p:0.22622497379779816
epoch£º761	 i:8 	 global-step:15228	 l-p:0.13508456945419312
epoch£º761	 i:9 	 global-step:15229	 l-p:0.12095746397972107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6536, 3.2051, 3.2468],
        [3.6536, 3.1677, 3.1778],
        [3.6536, 2.9470, 2.5881],
        [3.6536, 3.6536, 3.6536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.1262272149324417 
model_pd.l_d.mean(): -23.606374740600586 
model_pd.lagr.mean(): -23.480148315429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1867], device='cuda:0')), ('power', tensor([-23.7931], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.1262272149324417
epoch£º762	 i:1 	 global-step:15241	 l-p:0.15642723441123962
epoch£º762	 i:2 	 global-step:15242	 l-p:0.16580432653427124
epoch£º762	 i:3 	 global-step:15243	 l-p:0.11429989337921143
epoch£º762	 i:4 	 global-step:15244	 l-p:0.12574996054172516
epoch£º762	 i:5 	 global-step:15245	 l-p:0.08154638111591339
epoch£º762	 i:6 	 global-step:15246	 l-p:-0.16029632091522217
epoch£º762	 i:7 	 global-step:15247	 l-p:0.14538869261741638
epoch£º762	 i:8 	 global-step:15248	 l-p:0.2091154009103775
epoch£º762	 i:9 	 global-step:15249	 l-p:0.13712334632873535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5719, 2.9340, 2.7551],
        [3.5719, 3.5618, 3.5714],
        [3.5719, 3.4911, 3.5539],
        [3.5719, 3.0022, 2.3976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.09895135462284088 
model_pd.l_d.mean(): -22.84102439880371 
model_pd.lagr.mean(): -22.74207305908203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2890], device='cuda:0')), ('power', tensor([-23.1300], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.09895135462284088
epoch£º763	 i:1 	 global-step:15261	 l-p:0.15120938420295715
epoch£º763	 i:2 	 global-step:15262	 l-p:-1.1609420776367188
epoch£º763	 i:3 	 global-step:15263	 l-p:0.24812068045139313
epoch£º763	 i:4 	 global-step:15264	 l-p:0.13535688817501068
epoch£º763	 i:5 	 global-step:15265	 l-p:0.09812837094068527
epoch£º763	 i:6 	 global-step:15266	 l-p:0.6295952796936035
epoch£º763	 i:7 	 global-step:15267	 l-p:0.2134186327457428
epoch£º763	 i:8 	 global-step:15268	 l-p:0.12856358289718628
epoch£º763	 i:9 	 global-step:15269	 l-p:0.11551973223686218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7185, 3.5678, 3.6642],
        [3.7185, 3.7116, 3.7182],
        [3.7185, 3.0168, 2.6494],
        [3.7185, 3.2312, 3.2355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.18131257593631744 
model_pd.l_d.mean(): -23.630319595336914 
model_pd.lagr.mean(): -23.449007034301758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1487], device='cuda:0')), ('power', tensor([-23.7790], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.18131257593631744
epoch£º764	 i:1 	 global-step:15281	 l-p:0.1286274492740631
epoch£º764	 i:2 	 global-step:15282	 l-p:0.12743884325027466
epoch£º764	 i:3 	 global-step:15283	 l-p:0.14725719392299652
epoch£º764	 i:4 	 global-step:15284	 l-p:0.014380712062120438
epoch£º764	 i:5 	 global-step:15285	 l-p:0.15214359760284424
epoch£º764	 i:6 	 global-step:15286	 l-p:0.11082736402750015
epoch£º764	 i:7 	 global-step:15287	 l-p:0.13572844862937927
epoch£º764	 i:8 	 global-step:15288	 l-p:0.1230875551700592
epoch£º764	 i:9 	 global-step:15289	 l-p:0.12372541427612305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7256, 3.5127, 3.6243],
        [3.7256, 3.7145, 3.7250],
        [3.7256, 3.7254, 3.7256],
        [3.7256, 3.4817, 3.5958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.1547689437866211 
model_pd.l_d.mean(): -22.317317962646484 
model_pd.lagr.mean(): -22.162548065185547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3137], device='cuda:0')), ('power', tensor([-22.6310], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.1547689437866211
epoch£º765	 i:1 	 global-step:15301	 l-p:0.13569816946983337
epoch£º765	 i:2 	 global-step:15302	 l-p:0.14412039518356323
epoch£º765	 i:3 	 global-step:15303	 l-p:0.11583054065704346
epoch£º765	 i:4 	 global-step:15304	 l-p:0.14846886694431305
epoch£º765	 i:5 	 global-step:15305	 l-p:0.12989455461502075
epoch£º765	 i:6 	 global-step:15306	 l-p:-0.9233598709106445
epoch£º765	 i:7 	 global-step:15307	 l-p:0.12802527844905853
epoch£º765	 i:8 	 global-step:15308	 l-p:1.0655933618545532
epoch£º765	 i:9 	 global-step:15309	 l-p:0.07091403007507324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5784, 3.0951, 2.5030],
        [3.5784, 2.9504, 2.7907],
        [3.5784, 3.4842, 3.5550],
        [3.5784, 2.9371, 2.3249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.13104401528835297 
model_pd.l_d.mean(): -23.5284366607666 
model_pd.lagr.mean(): -23.39739227294922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2301], device='cuda:0')), ('power', tensor([-23.7585], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.13104401528835297
epoch£º766	 i:1 	 global-step:15321	 l-p:0.13431933522224426
epoch£º766	 i:2 	 global-step:15322	 l-p:0.15621326863765717
epoch£º766	 i:3 	 global-step:15323	 l-p:1.988134741783142
epoch£º766	 i:4 	 global-step:15324	 l-p:0.1621948629617691
epoch£º766	 i:5 	 global-step:15325	 l-p:0.13756172358989716
epoch£º766	 i:6 	 global-step:15326	 l-p:0.11752375215291977
epoch£º766	 i:7 	 global-step:15327	 l-p:0.12704205513000488
epoch£º766	 i:8 	 global-step:15328	 l-p:0.10430177301168442
epoch£º766	 i:9 	 global-step:15329	 l-p:0.14603891968727112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5222, 3.3954, 3.4830],
        [3.5222, 3.5221, 3.5222],
        [3.5222, 3.5222, 3.5223],
        [3.5222, 2.9542, 2.3560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.13042421638965607 
model_pd.l_d.mean(): -23.70153045654297 
model_pd.lagr.mean(): -23.57110595703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1989], device='cuda:0')), ('power', tensor([-23.9004], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.13042421638965607
epoch£º767	 i:1 	 global-step:15341	 l-p:0.08190233260393143
epoch£º767	 i:2 	 global-step:15342	 l-p:0.1262865960597992
epoch£º767	 i:3 	 global-step:15343	 l-p:0.13678815960884094
epoch£º767	 i:4 	 global-step:15344	 l-p:0.14612676203250885
epoch£º767	 i:5 	 global-step:15345	 l-p:0.1551247388124466
epoch£º767	 i:6 	 global-step:15346	 l-p:0.22270086407661438
epoch£º767	 i:7 	 global-step:15347	 l-p:0.21070607006549835
epoch£º767	 i:8 	 global-step:15348	 l-p:0.16531898081302643
epoch£º767	 i:9 	 global-step:15349	 l-p:0.06052470579743385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5097, 2.8657, 2.6920],
        [3.5097, 3.1101, 2.5426],
        [3.5097, 3.4147, 3.4861],
        [3.5097, 2.7658, 2.1840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.1809300184249878 
model_pd.l_d.mean(): -22.935922622680664 
model_pd.lagr.mean(): -22.754993438720703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3236], device='cuda:0')), ('power', tensor([-23.2595], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.1809300184249878
epoch£º768	 i:1 	 global-step:15361	 l-p:0.1321040689945221
epoch£º768	 i:2 	 global-step:15362	 l-p:0.14181694388389587
epoch£º768	 i:3 	 global-step:15363	 l-p:0.09001734852790833
epoch£º768	 i:4 	 global-step:15364	 l-p:0.10588368773460388
epoch£º768	 i:5 	 global-step:15365	 l-p:0.42009052634239197
epoch£º768	 i:6 	 global-step:15366	 l-p:0.13500840961933136
epoch£º768	 i:7 	 global-step:15367	 l-p:0.16424137353897095
epoch£º768	 i:8 	 global-step:15368	 l-p:0.13294726610183716
epoch£º768	 i:9 	 global-step:15369	 l-p:0.1096130833029747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6280, 2.9374, 2.6434],
        [3.6280, 2.9256, 2.3185],
        [3.6280, 2.9217, 2.3167],
        [3.6280, 3.6013, 3.6252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.126760795712471 
model_pd.l_d.mean(): -23.3416805267334 
model_pd.lagr.mean(): -23.214920043945312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2382], device='cuda:0')), ('power', tensor([-23.5799], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.126760795712471
epoch£º769	 i:1 	 global-step:15381	 l-p:0.14695625007152557
epoch£º769	 i:2 	 global-step:15382	 l-p:0.010335538536310196
epoch£º769	 i:3 	 global-step:15383	 l-p:0.1510021835565567
epoch£º769	 i:4 	 global-step:15384	 l-p:0.13819146156311035
epoch£º769	 i:5 	 global-step:15385	 l-p:0.30917853116989136
epoch£º769	 i:6 	 global-step:15386	 l-p:0.057205211371183395
epoch£º769	 i:7 	 global-step:15387	 l-p:0.16500899195671082
epoch£º769	 i:8 	 global-step:15388	 l-p:0.14144182205200195
epoch£º769	 i:9 	 global-step:15389	 l-p:0.12293305993080139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6675, 3.6675, 3.6675],
        [3.6675, 2.9612, 2.3584],
        [3.6675, 2.9636, 2.6218],
        [3.6675, 3.4324, 3.5474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): 0.29038307070732117 
model_pd.l_d.mean(): -22.173044204711914 
model_pd.lagr.mean(): -21.882661819458008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3635], device='cuda:0')), ('power', tensor([-22.5366], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:0.29038307070732117
epoch£º770	 i:1 	 global-step:15401	 l-p:0.1295812726020813
epoch£º770	 i:2 	 global-step:15402	 l-p:0.15091624855995178
epoch£º770	 i:3 	 global-step:15403	 l-p:0.12147491425275803
epoch£º770	 i:4 	 global-step:15404	 l-p:0.24050317704677582
epoch£º770	 i:5 	 global-step:15405	 l-p:0.09877723455429077
epoch£º770	 i:6 	 global-step:15406	 l-p:0.226384237408638
epoch£º770	 i:7 	 global-step:15407	 l-p:0.13677571713924408
epoch£º770	 i:8 	 global-step:15408	 l-p:0.10007725656032562
epoch£º770	 i:9 	 global-step:15409	 l-p:0.17898593842983246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6710, 3.6709, 3.6710],
        [3.6710, 3.5884, 3.6523],
        [3.6710, 3.6623, 3.6706],
        [3.6710, 2.9979, 2.7357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.11907381564378738 
model_pd.l_d.mean(): -23.601524353027344 
model_pd.lagr.mean(): -23.482450485229492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1192], device='cuda:0')), ('power', tensor([-23.7207], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.11907381564378738
epoch£º771	 i:1 	 global-step:15421	 l-p:0.08509352803230286
epoch£º771	 i:2 	 global-step:15422	 l-p:0.24707695841789246
epoch£º771	 i:3 	 global-step:15423	 l-p:0.12438489496707916
epoch£º771	 i:4 	 global-step:15424	 l-p:0.1979968249797821
epoch£º771	 i:5 	 global-step:15425	 l-p:0.15616481006145477
epoch£º771	 i:6 	 global-step:15426	 l-p:0.24112533032894135
epoch£º771	 i:7 	 global-step:15427	 l-p:0.12720359861850739
epoch£º771	 i:8 	 global-step:15428	 l-p:0.20318695902824402
epoch£º771	 i:9 	 global-step:15429	 l-p:0.1253710687160492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7039, 3.6242, 3.6862],
        [3.7039, 3.5608, 3.6546],
        [3.7039, 3.2346, 2.6296],
        [3.7039, 3.6214, 3.6852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.14043918251991272 
model_pd.l_d.mean(): -23.61945343017578 
model_pd.lagr.mean(): -23.479013442993164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1359], device='cuda:0')), ('power', tensor([-23.7553], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.14043918251991272
epoch£º772	 i:1 	 global-step:15441	 l-p:0.0909644290804863
epoch£º772	 i:2 	 global-step:15442	 l-p:0.15139590203762054
epoch£º772	 i:3 	 global-step:15443	 l-p:0.13078713417053223
epoch£º772	 i:4 	 global-step:15444	 l-p:0.13664016127586365
epoch£º772	 i:5 	 global-step:15445	 l-p:0.15160003304481506
epoch£º772	 i:6 	 global-step:15446	 l-p:0.10873205214738846
epoch£º772	 i:7 	 global-step:15447	 l-p:0.13391822576522827
epoch£º772	 i:8 	 global-step:15448	 l-p:0.22263163328170776
epoch£º772	 i:9 	 global-step:15449	 l-p:0.12440238893032074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6996, 3.6996, 3.6996],
        [3.6996, 3.6273, 3.6847],
        [3.6996, 3.1367, 3.0613],
        [3.6996, 3.1502, 3.0921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.13197465240955353 
model_pd.l_d.mean(): -23.774158477783203 
model_pd.lagr.mean(): -23.642183303833008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1018], device='cuda:0')), ('power', tensor([-23.8760], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.13197465240955353
epoch£º773	 i:1 	 global-step:15461	 l-p:0.12524746358394623
epoch£º773	 i:2 	 global-step:15462	 l-p:0.11468131095170975
epoch£º773	 i:3 	 global-step:15463	 l-p:0.23792733252048492
epoch£º773	 i:4 	 global-step:15464	 l-p:0.1408637911081314
epoch£º773	 i:5 	 global-step:15465	 l-p:0.14048238098621368
epoch£º773	 i:6 	 global-step:15466	 l-p:-2.7312393188476562
epoch£º773	 i:7 	 global-step:15467	 l-p:0.13452109694480896
epoch£º773	 i:8 	 global-step:15468	 l-p:0.11993125826120377
epoch£º773	 i:9 	 global-step:15469	 l-p:0.06145060062408447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6122, 3.4103, 3.5218],
        [3.6122, 3.5338, 3.5951],
        [3.6122, 2.9838, 2.8233],
        [3.6122, 3.5751, 3.6074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.32796722650527954 
model_pd.l_d.mean(): -23.66072654724121 
model_pd.lagr.mean(): -23.332759857177734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1964], device='cuda:0')), ('power', tensor([-23.8571], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.32796722650527954
epoch£º774	 i:1 	 global-step:15481	 l-p:0.08537337929010391
epoch£º774	 i:2 	 global-step:15482	 l-p:0.13303668797016144
epoch£º774	 i:3 	 global-step:15483	 l-p:0.1650654524564743
epoch£º774	 i:4 	 global-step:15484	 l-p:0.1425943225622177
epoch£º774	 i:5 	 global-step:15485	 l-p:-0.0019263362046331167
epoch£º774	 i:6 	 global-step:15486	 l-p:0.14403213560581207
epoch£º774	 i:7 	 global-step:15487	 l-p:0.17875801026821136
epoch£º774	 i:8 	 global-step:15488	 l-p:0.07422448694705963
epoch£º774	 i:9 	 global-step:15489	 l-p:0.12371156364679337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6292, 3.0389, 2.9358],
        [3.6292, 3.1176, 3.1100],
        [3.6292, 3.6291, 3.6292],
        [3.6292, 2.9940, 2.3749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.1281396448612213 
model_pd.l_d.mean(): -22.464826583862305 
model_pd.lagr.mean(): -22.336687088012695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4143], device='cuda:0')), ('power', tensor([-22.8791], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.1281396448612213
epoch£º775	 i:1 	 global-step:15501	 l-p:0.14716699719429016
epoch£º775	 i:2 	 global-step:15502	 l-p:-0.10023755580186844
epoch£º775	 i:3 	 global-step:15503	 l-p:0.16236445307731628
epoch£º775	 i:4 	 global-step:15504	 l-p:0.3024655878543854
epoch£º775	 i:5 	 global-step:15505	 l-p:0.10213092714548111
epoch£º775	 i:6 	 global-step:15506	 l-p:0.21737784147262573
epoch£º775	 i:7 	 global-step:15507	 l-p:0.13662023842334747
epoch£º775	 i:8 	 global-step:15508	 l-p:0.13491950929164886
epoch£º775	 i:9 	 global-step:15509	 l-p:0.128548726439476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[3.7324, 3.3549, 3.4402],
        [3.7324, 3.2049, 3.1707],
        [3.7324, 3.2276, 3.2183],
        [3.7324, 3.2884, 2.6852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.16006067395210266 
model_pd.l_d.mean(): -23.56223487854004 
model_pd.lagr.mean(): -23.40217399597168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1778], device='cuda:0')), ('power', tensor([-23.7400], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.16006067395210266
epoch£º776	 i:1 	 global-step:15521	 l-p:0.11972665786743164
epoch£º776	 i:2 	 global-step:15522	 l-p:0.12357381731271744
epoch£º776	 i:3 	 global-step:15523	 l-p:0.12647674977779388
epoch£º776	 i:4 	 global-step:15524	 l-p:0.11840417236089706
epoch£º776	 i:5 	 global-step:15525	 l-p:0.16153083741664886
epoch£º776	 i:6 	 global-step:15526	 l-p:0.12592747807502747
epoch£º776	 i:7 	 global-step:15527	 l-p:0.15082550048828125
epoch£º776	 i:8 	 global-step:15528	 l-p:0.12870214879512787
epoch£º776	 i:9 	 global-step:15529	 l-p:0.18202660977840424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6828, 3.1983, 3.2129],
        [3.6828, 3.5152, 3.6178],
        [3.6828, 3.1587, 3.1336],
        [3.6828, 3.0341, 2.8255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.11381600797176361 
model_pd.l_d.mean(): -23.534666061401367 
model_pd.lagr.mean(): -23.42085075378418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1734], device='cuda:0')), ('power', tensor([-23.7080], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.11381600797176361
epoch£º777	 i:1 	 global-step:15541	 l-p:0.13564109802246094
epoch£º777	 i:2 	 global-step:15542	 l-p:0.12603765726089478
epoch£º777	 i:3 	 global-step:15543	 l-p:0.13748301565647125
epoch£º777	 i:4 	 global-step:15544	 l-p:0.14766645431518555
epoch£º777	 i:5 	 global-step:15545	 l-p:-0.18842124938964844
epoch£º777	 i:6 	 global-step:15546	 l-p:0.030559677630662918
epoch£º777	 i:7 	 global-step:15547	 l-p:0.14902332425117493
epoch£º777	 i:8 	 global-step:15548	 l-p:0.17244035005569458
epoch£º777	 i:9 	 global-step:15549	 l-p:0.1393183320760727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6211, 3.6193, 3.6210],
        [3.6211, 3.4653, 3.5645],
        [3.6211, 3.5888, 3.6173],
        [3.6211, 3.6083, 3.6202]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.263024240732193 
model_pd.l_d.mean(): -22.59638786315918 
model_pd.lagr.mean(): -22.333364486694336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3793], device='cuda:0')), ('power', tensor([-22.9757], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.263024240732193
epoch£º778	 i:1 	 global-step:15561	 l-p:0.14117807149887085
epoch£º778	 i:2 	 global-step:15562	 l-p:-0.15164460241794586
epoch£º778	 i:3 	 global-step:15563	 l-p:0.14457468688488007
epoch£º778	 i:4 	 global-step:15564	 l-p:0.17564114928245544
epoch£º778	 i:5 	 global-step:15565	 l-p:0.14495831727981567
epoch£º778	 i:6 	 global-step:15566	 l-p:0.14479772746562958
epoch£º778	 i:7 	 global-step:15567	 l-p:0.13473963737487793
epoch£º778	 i:8 	 global-step:15568	 l-p:0.11746785789728165
epoch£º778	 i:9 	 global-step:15569	 l-p:0.14698852598667145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6109, 3.2728, 3.3781],
        [3.6109, 2.9216, 2.6465],
        [3.6109, 3.6107, 3.6109],
        [3.6109, 3.4549, 3.5542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.011988671496510506 
model_pd.l_d.mean(): -23.470849990844727 
model_pd.lagr.mean(): -23.458860397338867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2083], device='cuda:0')), ('power', tensor([-23.6791], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.011988671496510506
epoch£º779	 i:1 	 global-step:15581	 l-p:0.14385463297367096
epoch£º779	 i:2 	 global-step:15582	 l-p:0.133567675948143
epoch£º779	 i:3 	 global-step:15583	 l-p:0.12564660608768463
epoch£º779	 i:4 	 global-step:15584	 l-p:0.1539691537618637
epoch£º779	 i:5 	 global-step:15585	 l-p:0.5579254627227783
epoch£º779	 i:6 	 global-step:15586	 l-p:0.13617874681949615
epoch£º779	 i:7 	 global-step:15587	 l-p:0.12696190178394318
epoch£º779	 i:8 	 global-step:15588	 l-p:0.15964950621128082
epoch£º779	 i:9 	 global-step:15589	 l-p:0.1483597457408905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5195, 3.5095, 3.5190],
        [3.5195, 3.4839, 3.5151],
        [3.5195, 2.8896, 2.2835],
        [3.5195, 2.8855, 2.2790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.18211939930915833 
model_pd.l_d.mean(): -22.404661178588867 
model_pd.lagr.mean(): -22.22254180908203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3936], device='cuda:0')), ('power', tensor([-22.7982], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.18211939930915833
epoch£º780	 i:1 	 global-step:15601	 l-p:0.0020760679617524147
epoch£º780	 i:2 	 global-step:15602	 l-p:0.14936301112174988
epoch£º780	 i:3 	 global-step:15603	 l-p:0.10718989372253418
epoch£º780	 i:4 	 global-step:15604	 l-p:0.16445426642894745
epoch£º780	 i:5 	 global-step:15605	 l-p:0.11044498533010483
epoch£º780	 i:6 	 global-step:15606	 l-p:0.17320388555526733
epoch£º780	 i:7 	 global-step:15607	 l-p:0.0655968189239502
epoch£º780	 i:8 	 global-step:15608	 l-p:0.5123451948165894
epoch£º780	 i:9 	 global-step:15609	 l-p:0.13540592789649963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6095, 2.9990, 2.3823],
        [3.6095, 3.6095, 3.6095],
        [3.6095, 3.5981, 3.6088],
        [3.6095, 3.6044, 3.6093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.15968602895736694 
model_pd.l_d.mean(): -23.517383575439453 
model_pd.lagr.mean(): -23.357698440551758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2397], device='cuda:0')), ('power', tensor([-23.7571], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.15968602895736694
epoch£º781	 i:1 	 global-step:15621	 l-p:0.061188261955976486
epoch£º781	 i:2 	 global-step:15622	 l-p:0.06148197129368782
epoch£º781	 i:3 	 global-step:15623	 l-p:0.2092456966638565
epoch£º781	 i:4 	 global-step:15624	 l-p:-2.884943723678589
epoch£º781	 i:5 	 global-step:15625	 l-p:0.11734432727098465
epoch£º781	 i:6 	 global-step:15626	 l-p:0.10746479779481888
epoch£º781	 i:7 	 global-step:15627	 l-p:0.15262018144130707
epoch£º781	 i:8 	 global-step:15628	 l-p:0.12515266239643097
epoch£º781	 i:9 	 global-step:15629	 l-p:0.11604370176792145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7196, 3.6850, 3.7153],
        [3.7196, 3.0709, 2.4428],
        [3.7196, 3.5938, 3.6806],
        [3.7196, 3.5860, 3.6762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.1613701432943344 
model_pd.l_d.mean(): -23.216392517089844 
model_pd.lagr.mean(): -23.055023193359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1399], device='cuda:0')), ('power', tensor([-23.3563], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:0.1613701432943344
epoch£º782	 i:1 	 global-step:15641	 l-p:0.08741550147533417
epoch£º782	 i:2 	 global-step:15642	 l-p:0.11824169009923935
epoch£º782	 i:3 	 global-step:15643	 l-p:0.12329348176717758
epoch£º782	 i:4 	 global-step:15644	 l-p:0.1126365065574646
epoch£º782	 i:5 	 global-step:15645	 l-p:0.14628493785858154
epoch£º782	 i:6 	 global-step:15646	 l-p:0.1397646814584732
epoch£º782	 i:7 	 global-step:15647	 l-p:0.1246705949306488
epoch£º782	 i:8 	 global-step:15648	 l-p:0.23600336909294128
epoch£º782	 i:9 	 global-step:15649	 l-p:0.13583648204803467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6881, 2.9804, 2.3706],
        [3.6881, 3.1498, 3.1113],
        [3.6881, 2.9658, 2.3703],
        [3.6881, 3.6023, 3.6682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.13768289983272552 
model_pd.l_d.mean(): -23.563692092895508 
model_pd.lagr.mean(): -23.426010131835938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1413], device='cuda:0')), ('power', tensor([-23.7050], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.13768289983272552
epoch£º783	 i:1 	 global-step:15661	 l-p:0.17481368780136108
epoch£º783	 i:2 	 global-step:15662	 l-p:0.2345150262117386
epoch£º783	 i:3 	 global-step:15663	 l-p:0.2841826379299164
epoch£º783	 i:4 	 global-step:15664	 l-p:0.12036243826150894
epoch£º783	 i:5 	 global-step:15665	 l-p:0.34769946336746216
epoch£º783	 i:6 	 global-step:15666	 l-p:0.16242176294326782
epoch£º783	 i:7 	 global-step:15667	 l-p:0.13161270320415497
epoch£º783	 i:8 	 global-step:15668	 l-p:0.13661792874336243
epoch£º783	 i:9 	 global-step:15669	 l-p:0.14445920288562775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6732, 2.9611, 2.3528],
        [3.6732, 3.6732, 3.6733],
        [3.6732, 3.4680, 3.5801],
        [3.6732, 3.4507, 3.5654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.14523909986019135 
model_pd.l_d.mean(): -23.29815673828125 
model_pd.lagr.mean(): -23.152917861938477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2562], device='cuda:0')), ('power', tensor([-23.5544], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.14523909986019135
epoch£º784	 i:1 	 global-step:15681	 l-p:0.12934574484825134
epoch£º784	 i:2 	 global-step:15682	 l-p:0.14839649200439453
epoch£º784	 i:3 	 global-step:15683	 l-p:0.14889883995056152
epoch£º784	 i:4 	 global-step:15684	 l-p:0.15849344432353973
epoch£º784	 i:5 	 global-step:15685	 l-p:0.13553577661514282
epoch£º784	 i:6 	 global-step:15686	 l-p:-0.1329713761806488
epoch£º784	 i:7 	 global-step:15687	 l-p:-0.7767543792724609
epoch£º784	 i:8 	 global-step:15688	 l-p:0.330941379070282
epoch£º784	 i:9 	 global-step:15689	 l-p:0.11325519531965256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6720, 3.6720, 3.6720],
        [3.6720, 3.6636, 3.6715],
        [3.6720, 2.9388, 2.5195],
        [3.6720, 3.3092, 3.4051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.13601841032505035 
model_pd.l_d.mean(): -23.135400772094727 
model_pd.lagr.mean(): -22.99938201904297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1887], device='cuda:0')), ('power', tensor([-23.3241], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.13601841032505035
epoch£º785	 i:1 	 global-step:15701	 l-p:0.2579873502254486
epoch£º785	 i:2 	 global-step:15702	 l-p:0.1356801986694336
epoch£º785	 i:3 	 global-step:15703	 l-p:0.10182346403598785
epoch£º785	 i:4 	 global-step:15704	 l-p:0.17389126121997833
epoch£º785	 i:5 	 global-step:15705	 l-p:0.14009250700473785
epoch£º785	 i:6 	 global-step:15706	 l-p:0.14760704338550568
epoch£º785	 i:7 	 global-step:15707	 l-p:0.13304747641086578
epoch£º785	 i:8 	 global-step:15708	 l-p:0.15652470290660858
epoch£º785	 i:9 	 global-step:15709	 l-p:0.1339370310306549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7188, 3.7188, 3.7188],
        [3.7188, 3.3053, 2.7090],
        [3.7188, 3.0320, 2.4104],
        [3.7188, 3.5926, 3.6797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.1711229681968689 
model_pd.l_d.mean(): -23.572053909301758 
model_pd.lagr.mean(): -23.400930404663086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1295], device='cuda:0')), ('power', tensor([-23.7016], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.1711229681968689
epoch£º786	 i:1 	 global-step:15721	 l-p:0.13005585968494415
epoch£º786	 i:2 	 global-step:15722	 l-p:0.14762502908706665
epoch£º786	 i:3 	 global-step:15723	 l-p:0.12779468297958374
epoch£º786	 i:4 	 global-step:15724	 l-p:0.21877993643283844
epoch£º786	 i:5 	 global-step:15725	 l-p:0.16285817325115204
epoch£º786	 i:6 	 global-step:15726	 l-p:0.11797988414764404
epoch£º786	 i:7 	 global-step:15727	 l-p:0.12632958590984344
epoch£º786	 i:8 	 global-step:15728	 l-p:0.08750786632299423
epoch£º786	 i:9 	 global-step:15729	 l-p:0.12892009317874908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6825, 2.9669, 2.3591],
        [3.6825, 3.5964, 3.6625],
        [3.6825, 3.6478, 3.6782],
        [3.6825, 3.5247, 3.6245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.14745759963989258 
model_pd.l_d.mean(): -22.816923141479492 
model_pd.lagr.mean(): -22.669466018676758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3144], device='cuda:0')), ('power', tensor([-23.1313], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.14745759963989258
epoch£º787	 i:1 	 global-step:15741	 l-p:0.19129802286624908
epoch£º787	 i:2 	 global-step:15742	 l-p:0.12956452369689941
epoch£º787	 i:3 	 global-step:15743	 l-p:0.19796748459339142
epoch£º787	 i:4 	 global-step:15744	 l-p:0.1893490105867386
epoch£º787	 i:5 	 global-step:15745	 l-p:0.1355341523885727
epoch£º787	 i:6 	 global-step:15746	 l-p:0.1142600029706955
epoch£º787	 i:7 	 global-step:15747	 l-p:0.19718687236309052
epoch£º787	 i:8 	 global-step:15748	 l-p:0.14177215099334717
epoch£º787	 i:9 	 global-step:15749	 l-p:0.11968469619750977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6879, 3.1019, 3.0053],
        [3.6879, 3.6879, 3.6879],
        [3.6879, 3.0628, 2.9059],
        [3.6879, 3.0530, 2.4250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.12596285343170166 
model_pd.l_d.mean(): -23.566204071044922 
model_pd.lagr.mean(): -23.44024085998535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1325], device='cuda:0')), ('power', tensor([-23.6987], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.12596285343170166
epoch£º788	 i:1 	 global-step:15761	 l-p:0.10507813841104507
epoch£º788	 i:2 	 global-step:15762	 l-p:0.14809176325798035
epoch£º788	 i:3 	 global-step:15763	 l-p:0.4687362611293793
epoch£º788	 i:4 	 global-step:15764	 l-p:0.1381552517414093
epoch£º788	 i:5 	 global-step:15765	 l-p:0.2775830328464508
epoch£º788	 i:6 	 global-step:15766	 l-p:0.16031458973884583
epoch£º788	 i:7 	 global-step:15767	 l-p:0.12087653577327728
epoch£º788	 i:8 	 global-step:15768	 l-p:0.13567815721035004
epoch£º788	 i:9 	 global-step:15769	 l-p:0.12677104771137238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6846, 3.5959, 3.6636],
        [3.6846, 3.1337, 3.0836],
        [3.6846, 3.6847, 3.6846],
        [3.6846, 3.6846, 3.6847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.18556959927082062 
model_pd.l_d.mean(): -23.39470863342285 
model_pd.lagr.mean(): -23.209138870239258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1824], device='cuda:0')), ('power', tensor([-23.5771], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.18556959927082062
epoch£º789	 i:1 	 global-step:15781	 l-p:0.21755507588386536
epoch£º789	 i:2 	 global-step:15782	 l-p:0.16650748252868652
epoch£º789	 i:3 	 global-step:15783	 l-p:0.19445091485977173
epoch£º789	 i:4 	 global-step:15784	 l-p:0.13165654242038727
epoch£º789	 i:5 	 global-step:15785	 l-p:0.1554078757762909
epoch£º789	 i:6 	 global-step:15786	 l-p:0.11021777987480164
epoch£º789	 i:7 	 global-step:15787	 l-p:0.07673182338476181
epoch£º789	 i:8 	 global-step:15788	 l-p:0.12838856875896454
epoch£º789	 i:9 	 global-step:15789	 l-p:0.15280331671237946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7018, 3.5579, 3.6526],
        [3.7018, 3.7018, 3.7018],
        [3.7018, 2.9964, 2.3811],
        [3.7018, 3.1519, 3.1016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.15789446234703064 
model_pd.l_d.mean(): -23.719846725463867 
model_pd.lagr.mean(): -23.561952590942383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1231], device='cuda:0')), ('power', tensor([-23.8430], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.15789446234703064
epoch£º790	 i:1 	 global-step:15801	 l-p:0.15237727761268616
epoch£º790	 i:2 	 global-step:15802	 l-p:0.1280394196510315
epoch£º790	 i:3 	 global-step:15803	 l-p:-1.237693428993225
epoch£º790	 i:4 	 global-step:15804	 l-p:-0.2137378305196762
epoch£º790	 i:5 	 global-step:15805	 l-p:0.12765279412269592
epoch£º790	 i:6 	 global-step:15806	 l-p:0.14431257545948029
epoch£º790	 i:7 	 global-step:15807	 l-p:0.12644971907138824
epoch£º790	 i:8 	 global-step:15808	 l-p:0.12144395709037781
epoch£º790	 i:9 	 global-step:15809	 l-p:0.18280059099197388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5910, 3.0705, 3.0637],
        [3.5910, 3.1715, 2.5879],
        [3.5910, 3.3710, 3.4867],
        [3.5910, 3.1622, 3.2294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.15191340446472168 
model_pd.l_d.mean(): -23.55609130859375 
model_pd.lagr.mean(): -23.404178619384766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1615], device='cuda:0')), ('power', tensor([-23.7176], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.15191340446472168
epoch£º791	 i:1 	 global-step:15821	 l-p:0.14194370806217194
epoch£º791	 i:2 	 global-step:15822	 l-p:0.24491550028324127
epoch£º791	 i:3 	 global-step:15823	 l-p:0.12785907089710236
epoch£º791	 i:4 	 global-step:15824	 l-p:0.08905866742134094
epoch£º791	 i:5 	 global-step:15825	 l-p:0.1411781907081604
epoch£º791	 i:6 	 global-step:15826	 l-p:0.09486927837133408
epoch£º791	 i:7 	 global-step:15827	 l-p:0.15678322315216064
epoch£º791	 i:8 	 global-step:15828	 l-p:0.13439443707466125
epoch£º791	 i:9 	 global-step:15829	 l-p:0.15624022483825684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5712, 2.8617, 2.5637],
        [3.5712, 2.8100, 2.2433],
        [3.5712, 3.4347, 3.5270],
        [3.5712, 3.2802, 3.3976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.13928721845149994 
model_pd.l_d.mean(): -23.674877166748047 
model_pd.lagr.mean(): -23.53558921813965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1829], device='cuda:0')), ('power', tensor([-23.8578], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.13928721845149994
epoch£º792	 i:1 	 global-step:15841	 l-p:-0.24832899868488312
epoch£º792	 i:2 	 global-step:15842	 l-p:0.1467345803976059
epoch£º792	 i:3 	 global-step:15843	 l-p:0.1425180733203888
epoch£º792	 i:4 	 global-step:15844	 l-p:0.12047053128480911
epoch£º792	 i:5 	 global-step:15845	 l-p:0.12538877129554749
epoch£º792	 i:6 	 global-step:15846	 l-p:-0.34302371740341187
epoch£º792	 i:7 	 global-step:15847	 l-p:0.1558208018541336
epoch£º792	 i:8 	 global-step:15848	 l-p:0.13108082115650177
epoch£º792	 i:9 	 global-step:15849	 l-p:0.21052321791648865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5555, 3.5554, 3.5555],
        [3.5555, 3.5503, 3.5553],
        [3.5555, 3.5554, 3.5555],
        [3.5555, 3.4641, 3.5336]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.13420067727565765 
model_pd.l_d.mean(): -23.644336700439453 
model_pd.lagr.mean(): -23.510135650634766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1593], device='cuda:0')), ('power', tensor([-23.8036], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.13420067727565765
epoch£º793	 i:1 	 global-step:15861	 l-p:0.03008308820426464
epoch£º793	 i:2 	 global-step:15862	 l-p:0.41832831501960754
epoch£º793	 i:3 	 global-step:15863	 l-p:0.05172334611415863
epoch£º793	 i:4 	 global-step:15864	 l-p:0.08357064425945282
epoch£º793	 i:5 	 global-step:15865	 l-p:0.1532093584537506
epoch£º793	 i:6 	 global-step:15866	 l-p:0.13085946440696716
epoch£º793	 i:7 	 global-step:15867	 l-p:0.1277865767478943
epoch£º793	 i:8 	 global-step:15868	 l-p:0.04347463324666023
epoch£º793	 i:9 	 global-step:15869	 l-p:0.1485293209552765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6367, 2.9774, 2.7734],
        [3.6367, 3.5956, 3.6310],
        [3.6367, 3.0026, 2.8436],
        [3.6367, 3.6367, 3.6367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.041881054639816284 
model_pd.l_d.mean(): -23.31574058532715 
model_pd.lagr.mean(): -23.27385902404785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2449], device='cuda:0')), ('power', tensor([-23.5606], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.041881054639816284
epoch£º794	 i:1 	 global-step:15881	 l-p:0.09714614599943161
epoch£º794	 i:2 	 global-step:15882	 l-p:0.14731092751026154
epoch£º794	 i:3 	 global-step:15883	 l-p:0.12705141305923462
epoch£º794	 i:4 	 global-step:15884	 l-p:0.18560771644115448
epoch£º794	 i:5 	 global-step:15885	 l-p:0.06336288899183273
epoch£º794	 i:6 	 global-step:15886	 l-p:0.24433356523513794
epoch£º794	 i:7 	 global-step:15887	 l-p:0.15021732449531555
epoch£º794	 i:8 	 global-step:15888	 l-p:0.11373469978570938
epoch£º794	 i:9 	 global-step:15889	 l-p:0.04164028540253639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6366, 3.6366, 3.6366],
        [3.6366, 2.8840, 2.4182],
        [3.6366, 3.5551, 3.6185],
        [3.6366, 3.5589, 3.6200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.21113498508930206 
model_pd.l_d.mean(): -23.379444122314453 
model_pd.lagr.mean(): -23.16830825805664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1981], device='cuda:0')), ('power', tensor([-23.5776], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.21113498508930206
epoch£º795	 i:1 	 global-step:15901	 l-p:0.1299894154071808
epoch£º795	 i:2 	 global-step:15902	 l-p:0.14434154331684113
epoch£º795	 i:3 	 global-step:15903	 l-p:-0.11227396875619888
epoch£º795	 i:4 	 global-step:15904	 l-p:0.13717705011367798
epoch£º795	 i:5 	 global-step:15905	 l-p:0.15579284727573395
epoch£º795	 i:6 	 global-step:15906	 l-p:0.20044152438640594
epoch£º795	 i:7 	 global-step:15907	 l-p:0.10847116261720657
epoch£º795	 i:8 	 global-step:15908	 l-p:-0.01401874516159296
epoch£º795	 i:9 	 global-step:15909	 l-p:-0.23444925248622894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6621, 3.5637, 3.6371],
        [3.6621, 2.9449, 2.3307],
        [3.6621, 3.3931, 3.5107],
        [3.6621, 3.6621, 3.6621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.9141334891319275 
model_pd.l_d.mean(): -23.15445899963379 
model_pd.lagr.mean(): -22.240325927734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2371], device='cuda:0')), ('power', tensor([-23.3916], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.9141334891319275
epoch£º796	 i:1 	 global-step:15921	 l-p:0.12196040153503418
epoch£º796	 i:2 	 global-step:15922	 l-p:0.23495332896709442
epoch£º796	 i:3 	 global-step:15923	 l-p:0.17887260019779205
epoch£º796	 i:4 	 global-step:15924	 l-p:0.12497609853744507
epoch£º796	 i:5 	 global-step:15925	 l-p:0.1481626182794571
epoch£º796	 i:6 	 global-step:15926	 l-p:0.10719548165798187
epoch£º796	 i:7 	 global-step:15927	 l-p:0.1557452380657196
epoch£º796	 i:8 	 global-step:15928	 l-p:0.12986257672309875
epoch£º796	 i:9 	 global-step:15929	 l-p:0.1371823400259018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6898, 3.5132, 3.6192],
        [3.6898, 3.6590, 3.6864],
        [3.6898, 3.6887, 3.6898],
        [3.6898, 3.6898, 3.6898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.22547702491283417 
model_pd.l_d.mean(): -23.473587036132812 
model_pd.lagr.mean(): -23.248109817504883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1868], device='cuda:0')), ('power', tensor([-23.6603], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.22547702491283417
epoch£º797	 i:1 	 global-step:15941	 l-p:0.09634459018707275
epoch£º797	 i:2 	 global-step:15942	 l-p:0.13689707219600677
epoch£º797	 i:3 	 global-step:15943	 l-p:0.11532014608383179
epoch£º797	 i:4 	 global-step:15944	 l-p:-0.0019211482722312212
epoch£º797	 i:5 	 global-step:15945	 l-p:0.1186598539352417
epoch£º797	 i:6 	 global-step:15946	 l-p:-0.2336972951889038
epoch£º797	 i:7 	 global-step:15947	 l-p:0.08679952472448349
epoch£º797	 i:8 	 global-step:15948	 l-p:0.16254088282585144
epoch£º797	 i:9 	 global-step:15949	 l-p:-1.5795202255249023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5800, 2.8942, 2.2761],
        [3.5800, 3.5714, 3.5796],
        [3.5800, 3.3050, 3.4238],
        [3.5800, 3.0490, 2.4452]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.14286431670188904 
model_pd.l_d.mean(): -23.136558532714844 
model_pd.lagr.mean(): -22.993694305419922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2411], device='cuda:0')), ('power', tensor([-23.3776], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.14286431670188904
epoch£º798	 i:1 	 global-step:15961	 l-p:0.11357682198286057
epoch£º798	 i:2 	 global-step:15962	 l-p:0.1463407725095749
epoch£º798	 i:3 	 global-step:15963	 l-p:0.14202158153057098
epoch£º798	 i:4 	 global-step:15964	 l-p:0.12702001631259918
epoch£º798	 i:5 	 global-step:15965	 l-p:0.1277434378862381
epoch£º798	 i:6 	 global-step:15966	 l-p:0.09751921147108078
epoch£º798	 i:7 	 global-step:15967	 l-p:0.1959787756204605
epoch£º798	 i:8 	 global-step:15968	 l-p:0.10531166940927505
epoch£º798	 i:9 	 global-step:15969	 l-p:0.16498610377311707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5384, 3.4560, 3.5202],
        [3.5384, 3.5359, 3.5383],
        [3.5384, 3.5384, 3.5384],
        [3.5384, 3.5333, 3.5382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.016488060355186462 
model_pd.l_d.mean(): -23.793846130371094 
model_pd.lagr.mean(): -23.777359008789062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1532], device='cuda:0')), ('power', tensor([-23.9471], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.016488060355186462
epoch£º799	 i:1 	 global-step:15981	 l-p:0.1668836921453476
epoch£º799	 i:2 	 global-step:15982	 l-p:0.14115114510059357
epoch£º799	 i:3 	 global-step:15983	 l-p:0.59831702709198
epoch£º799	 i:4 	 global-step:15984	 l-p:0.08426017314195633
epoch£º799	 i:5 	 global-step:15985	 l-p:0.26686951518058777
epoch£º799	 i:6 	 global-step:15986	 l-p:-4.0738701820373535
epoch£º799	 i:7 	 global-step:15987	 l-p:0.08975084871053696
epoch£º799	 i:8 	 global-step:15988	 l-p:0.1601390391588211
epoch£º799	 i:9 	 global-step:15989	 l-p:0.16919396817684174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7474, 3.2590, 3.2731],
        [3.7474, 3.1882, 2.5590],
        [3.7474, 3.7330, 3.7464],
        [3.7474, 3.0941, 2.8827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.09612945467233658 
model_pd.l_d.mean(): -22.282896041870117 
model_pd.lagr.mean(): -22.186765670776367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3041], device='cuda:0')), ('power', tensor([-22.5870], device='cuda:0'))])
epoch£º800	 i:0 	 global-step:16000	 l-p:0.09612945467233658
epoch£º800	 i:1 	 global-step:16001	 l-p:0.13175027072429657
epoch£º800	 i:2 	 global-step:16002	 l-p:0.1379677951335907
epoch£º800	 i:3 	 global-step:16003	 l-p:0.15381509065628052
epoch£º800	 i:4 	 global-step:16004	 l-p:0.12596727907657623
epoch£º800	 i:5 	 global-step:16005	 l-p:0.11180201917886734
epoch£º800	 i:6 	 global-step:16006	 l-p:0.1302669197320938
epoch£º800	 i:7 	 global-step:16007	 l-p:0.11869513988494873
epoch£º800	 i:8 	 global-step:16008	 l-p:0.13970375061035156
epoch£º800	 i:9 	 global-step:16009	 l-p:0.10772465169429779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7815,  0.7198,  1.0000,  0.6630,
          1.0000,  0.9211, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7501,  0.6816,  1.0000,  0.6193,
          1.0000,  0.9086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228]], device='cuda:0')
 pt:tensor([[3.7452, 3.1852, 2.5561],
        [3.7452, 3.1538, 2.5209],
        [3.7452, 3.0070, 2.5451],
        [3.7452, 3.2464, 3.2510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.1340673863887787 
model_pd.l_d.mean(): -22.95940399169922 
model_pd.lagr.mean(): -22.825336456298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1888], device='cuda:0')), ('power', tensor([-23.1482], device='cuda:0'))])
epoch£º801	 i:0 	 global-step:16020	 l-p:0.1340673863887787
epoch£º801	 i:1 	 global-step:16021	 l-p:0.11126594990491867
epoch£º801	 i:2 	 global-step:16022	 l-p:0.18965168297290802
epoch£º801	 i:3 	 global-step:16023	 l-p:0.15911456942558289
epoch£º801	 i:4 	 global-step:16024	 l-p:0.13714800775051117
epoch£º801	 i:5 	 global-step:16025	 l-p:0.13396801054477692
epoch£º801	 i:6 	 global-step:16026	 l-p:0.08608784526586533
epoch£º801	 i:7 	 global-step:16027	 l-p:0.15246155858039856
epoch£º801	 i:8 	 global-step:16028	 l-p:0.13604307174682617
epoch£º801	 i:9 	 global-step:16029	 l-p:0.2686989903450012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6783, 3.6718, 3.6780],
        [3.6783, 3.4375, 3.5548],
        [3.6783, 3.6783, 3.6783],
        [3.6783, 3.0577, 2.9175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 0.28366318345069885 
model_pd.l_d.mean(): -22.630346298217773 
model_pd.lagr.mean(): -22.346683502197266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2578], device='cuda:0')), ('power', tensor([-22.8881], device='cuda:0'))])
epoch£º802	 i:0 	 global-step:16040	 l-p:0.28366318345069885
epoch£º802	 i:1 	 global-step:16041	 l-p:0.13636240363121033
epoch£º802	 i:2 	 global-step:16042	 l-p:0.27019286155700684
epoch£º802	 i:3 	 global-step:16043	 l-p:0.16634532809257507
epoch£º802	 i:4 	 global-step:16044	 l-p:0.5370978713035583
epoch£º802	 i:5 	 global-step:16045	 l-p:0.12760908901691437
epoch£º802	 i:6 	 global-step:16046	 l-p:0.12082351744174957
epoch£º802	 i:7 	 global-step:16047	 l-p:0.11332914233207703
epoch£º802	 i:8 	 global-step:16048	 l-p:0.12737727165222168
epoch£º802	 i:9 	 global-step:16049	 l-p:0.1269180327653885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4241,  0.3187,  1.0000,  0.2394,
          1.0000,  0.7513, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3293,  0.2274,  1.0000,  0.1570,
          1.0000,  0.6906, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2741,  0.1781,  1.0000,  0.1157,
          1.0000,  0.6496, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[3.6696, 2.9298, 2.5154],
        [3.6696, 3.0087, 2.7995],
        [3.6696, 3.0908, 3.0126],
        [3.6696, 3.0164, 2.8219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.7902359366416931 
model_pd.l_d.mean(): -23.619842529296875 
model_pd.lagr.mean(): -22.829607009887695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1779], device='cuda:0')), ('power', tensor([-23.7977], device='cuda:0'))])
epoch£º803	 i:0 	 global-step:16060	 l-p:0.7902359366416931
epoch£º803	 i:1 	 global-step:16061	 l-p:0.11946910619735718
epoch£º803	 i:2 	 global-step:16062	 l-p:0.3314169645309448
epoch£º803	 i:3 	 global-step:16063	 l-p:0.13438120484352112
epoch£º803	 i:4 	 global-step:16064	 l-p:0.16435027122497559
epoch£º803	 i:5 	 global-step:16065	 l-p:0.13545021414756775
epoch£º803	 i:6 	 global-step:16066	 l-p:0.15584388375282288
epoch£º803	 i:7 	 global-step:16067	 l-p:0.13258983194828033
epoch£º803	 i:8 	 global-step:16068	 l-p:0.13871334493160248
epoch£º803	 i:9 	 global-step:16069	 l-p:-0.30168259143829346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6534, 3.6484, 3.6532],
        [3.6534, 3.5866, 3.6406],
        [3.6534, 3.0537, 2.9490],
        [3.6534, 2.9026, 2.3257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.15045520663261414 
model_pd.l_d.mean(): -23.063825607299805 
model_pd.lagr.mean(): -22.91337013244629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2963], device='cuda:0')), ('power', tensor([-23.3602], device='cuda:0'))])
epoch£º804	 i:0 	 global-step:16080	 l-p:0.15045520663261414
epoch£º804	 i:1 	 global-step:16081	 l-p:0.13506555557250977
epoch£º804	 i:2 	 global-step:16082	 l-p:0.1824943572282791
epoch£º804	 i:3 	 global-step:16083	 l-p:0.1318284571170807
epoch£º804	 i:4 	 global-step:16084	 l-p:-0.060923345386981964
epoch£º804	 i:5 	 global-step:16085	 l-p:2.0469233989715576
epoch£º804	 i:6 	 global-step:16086	 l-p:0.13842317461967468
epoch£º804	 i:7 	 global-step:16087	 l-p:-0.5113486647605896
epoch£º804	 i:8 	 global-step:16088	 l-p:0.1464058756828308
epoch£º804	 i:9 	 global-step:16089	 l-p:0.1355813443660736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6655, 3.6655, 3.6655],
        [3.6655, 3.6655, 3.6655],
        [3.6655, 3.1705, 2.5615],
        [3.6655, 3.4317, 3.5487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.11512627452611923 
model_pd.l_d.mean(): -22.83060073852539 
model_pd.lagr.mean(): -22.71547508239746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2672], device='cuda:0')), ('power', tensor([-23.0978], device='cuda:0'))])
epoch£º805	 i:0 	 global-step:16100	 l-p:0.11512627452611923
epoch£º805	 i:1 	 global-step:16101	 l-p:0.12592530250549316
epoch£º805	 i:2 	 global-step:16102	 l-p:0.17729933559894562
epoch£º805	 i:3 	 global-step:16103	 l-p:1.3079743385314941
epoch£º805	 i:4 	 global-step:16104	 l-p:0.3149750232696533
epoch£º805	 i:5 	 global-step:16105	 l-p:0.2077246755361557
epoch£º805	 i:6 	 global-step:16106	 l-p:0.13813786208629608
epoch£º805	 i:7 	 global-step:16107	 l-p:0.13005216419696808
epoch£º805	 i:8 	 global-step:16108	 l-p:0.353309690952301
epoch£º805	 i:9 	 global-step:16109	 l-p:0.12173169106245041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6805, 3.6804, 3.6805],
        [3.6805, 3.6793, 3.6805],
        [3.6805, 3.6805, 3.6805],
        [3.6805, 3.6759, 3.6803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 0.2256569266319275 
model_pd.l_d.mean(): -23.38180923461914 
model_pd.lagr.mean(): -23.156152725219727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2036], device='cuda:0')), ('power', tensor([-23.5854], device='cuda:0'))])
epoch£º806	 i:0 	 global-step:16120	 l-p:0.2256569266319275
epoch£º806	 i:1 	 global-step:16121	 l-p:0.1408827006816864
epoch£º806	 i:2 	 global-step:16122	 l-p:0.1864064782857895
epoch£º806	 i:3 	 global-step:16123	 l-p:0.1389312446117401
epoch£º806	 i:4 	 global-step:16124	 l-p:0.09586267918348312
epoch£º806	 i:5 	 global-step:16125	 l-p:0.1292778104543686
epoch£º806	 i:6 	 global-step:16126	 l-p:0.35767942667007446
epoch£º806	 i:7 	 global-step:16127	 l-p:0.12048755586147308
epoch£º806	 i:8 	 global-step:16128	 l-p:0.13781069219112396
epoch£º806	 i:9 	 global-step:16129	 l-p:0.12060492485761642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6813, 3.6802, 3.6813],
        [3.6813, 3.6804, 3.6813],
        [3.6813, 3.6789, 3.6813],
        [3.6813, 3.6813, 3.6813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.25060516595840454 
model_pd.l_d.mean(): -23.710363388061523 
model_pd.lagr.mean(): -23.459758758544922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1203], device='cuda:0')), ('power', tensor([-23.8306], device='cuda:0'))])
epoch£º807	 i:0 	 global-step:16140	 l-p:0.25060516595840454
epoch£º807	 i:1 	 global-step:16141	 l-p:0.12994645535945892
epoch£º807	 i:2 	 global-step:16142	 l-p:0.5613638758659363
epoch£º807	 i:3 	 global-step:16143	 l-p:0.1322365403175354
epoch£º807	 i:4 	 global-step:16144	 l-p:0.10045382380485535
epoch£º807	 i:5 	 global-step:16145	 l-p:0.1395101398229599
epoch£º807	 i:6 	 global-step:16146	 l-p:0.1574603021144867
epoch£º807	 i:7 	 global-step:16147	 l-p:0.13336433470249176
epoch£º807	 i:8 	 global-step:16148	 l-p:0.11789340525865555
epoch£º807	 i:9 	 global-step:16149	 l-p:0.15362390875816345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6565, 3.6464, 3.6559],
        [3.6565, 3.6319, 3.6541],
        [3.6565, 3.6565, 3.6565],
        [3.6565, 3.6488, 3.6561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.13639730215072632 
model_pd.l_d.mean(): -23.18259048461914 
model_pd.lagr.mean(): -23.046194076538086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1864], device='cuda:0')), ('power', tensor([-23.3690], device='cuda:0'))])
epoch£º808	 i:0 	 global-step:16160	 l-p:0.13639730215072632
epoch£º808	 i:1 	 global-step:16161	 l-p:0.12104152143001556
epoch£º808	 i:2 	 global-step:16162	 l-p:0.1149957999587059
epoch£º808	 i:3 	 global-step:16163	 l-p:0.1437227725982666
epoch£º808	 i:4 	 global-step:16164	 l-p:0.19433364272117615
epoch£º808	 i:5 	 global-step:16165	 l-p:0.09970393776893616
epoch£º808	 i:6 	 global-step:16166	 l-p:0.14380121231079102
epoch£º808	 i:7 	 global-step:16167	 l-p:-0.13633503019809723
epoch£º808	 i:8 	 global-step:16168	 l-p:0.12969417870044708
epoch£º808	 i:9 	 global-step:16169	 l-p:0.08480466902256012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5913, 3.5913, 3.5913],
        [3.5913, 3.5904, 3.5913],
        [3.5913, 3.4740, 3.5576],
        [3.5913, 3.5911, 3.5913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): 0.6097283959388733 
model_pd.l_d.mean(): -22.768192291259766 
model_pd.lagr.mean(): -22.158464431762695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3333], device='cuda:0')), ('power', tensor([-23.1015], device='cuda:0'))])
epoch£º809	 i:0 	 global-step:16180	 l-p:0.6097283959388733
epoch£º809	 i:1 	 global-step:16181	 l-p:0.2255420684814453
epoch£º809	 i:2 	 global-step:16182	 l-p:0.12817679345607758
epoch£º809	 i:3 	 global-step:16183	 l-p:-0.023559145629405975
epoch£º809	 i:4 	 global-step:16184	 l-p:0.06461450457572937
epoch£º809	 i:5 	 global-step:16185	 l-p:0.12522393465042114
epoch£º809	 i:6 	 global-step:16186	 l-p:0.14448308944702148
epoch£º809	 i:7 	 global-step:16187	 l-p:0.15590718388557434
epoch£º809	 i:8 	 global-step:16188	 l-p:0.1286764144897461
epoch£º809	 i:9 	 global-step:16189	 l-p:0.1374477744102478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5917, 3.1800, 3.2598],
        [3.5917, 2.8746, 2.5637],
        [3.5917, 3.2080, 2.6309],
        [3.5917, 3.5879, 3.5916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.1566586196422577 
model_pd.l_d.mean(): -23.115232467651367 
model_pd.lagr.mean(): -22.958574295043945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2311], device='cuda:0')), ('power', tensor([-23.3464], device='cuda:0'))])
epoch£º810	 i:0 	 global-step:16200	 l-p:0.1566586196422577
epoch£º810	 i:1 	 global-step:16201	 l-p:0.15500973165035248
epoch£º810	 i:2 	 global-step:16202	 l-p:0.0004966139676980674
epoch£º810	 i:3 	 global-step:16203	 l-p:0.08873212337493896
epoch£º810	 i:4 	 global-step:16204	 l-p:0.37427034974098206
epoch£º810	 i:5 	 global-step:16205	 l-p:0.19692249596118927
epoch£º810	 i:6 	 global-step:16206	 l-p:0.08833868056535721
epoch£º810	 i:7 	 global-step:16207	 l-p:0.11495328694581985
epoch£º810	 i:8 	 global-step:16208	 l-p:0.12574271857738495
epoch£º810	 i:9 	 global-step:16209	 l-p:0.06943575292825699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6329, 2.9745, 2.7798],
        [3.6329, 2.8723, 2.3146],
        [3.6329, 3.4558, 3.5626],
        [3.6329, 3.3458, 3.4634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.07484342157840729 
model_pd.l_d.mean(): -22.93260955810547 
model_pd.lagr.mean(): -22.857765197753906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3237], device='cuda:0')), ('power', tensor([-23.2563], device='cuda:0'))])
epoch£º811	 i:0 	 global-step:16220	 l-p:0.07484342157840729
epoch£º811	 i:1 	 global-step:16221	 l-p:0.14986267685890198
epoch£º811	 i:2 	 global-step:16222	 l-p:0.08530540019273758
epoch£º811	 i:3 	 global-step:16223	 l-p:0.12459802627563477
epoch£º811	 i:4 	 global-step:16224	 l-p:0.12855172157287598
epoch£º811	 i:5 	 global-step:16225	 l-p:0.14146766066551208
epoch£º811	 i:6 	 global-step:16226	 l-p:0.125356525182724
epoch£º811	 i:7 	 global-step:16227	 l-p:0.7179754376411438
epoch£º811	 i:8 	 global-step:16228	 l-p:0.16841819882392883
epoch£º811	 i:9 	 global-step:16229	 l-p:0.12201505154371262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6649, 3.6534, 3.6642],
        [3.6649, 3.6237, 3.6593],
        [3.6649, 3.0861, 2.4631],
        [3.6649, 3.6164, 3.6575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.20089708268642426 
model_pd.l_d.mean(): -23.619598388671875 
model_pd.lagr.mean(): -23.418701171875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1918], device='cuda:0')), ('power', tensor([-23.8114], device='cuda:0'))])
epoch£º812	 i:0 	 global-step:16240	 l-p:0.20089708268642426
epoch£º812	 i:1 	 global-step:16241	 l-p:3.4443254470825195
epoch£º812	 i:2 	 global-step:16242	 l-p:0.24995887279510498
epoch£º812	 i:3 	 global-step:16243	 l-p:0.175927996635437
epoch£º812	 i:4 	 global-step:16244	 l-p:0.14321239292621613
epoch£º812	 i:5 	 global-step:16245	 l-p:0.1312056928873062
epoch£º812	 i:6 	 global-step:16246	 l-p:0.10815654695034027
epoch£º812	 i:7 	 global-step:16247	 l-p:0.1267516314983368
epoch£º812	 i:8 	 global-step:16248	 l-p:0.18937022984027863
epoch£º812	 i:9 	 global-step:16249	 l-p:0.11856307834386826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6924, 3.4800, 3.5942],
        [3.6924, 3.1503, 3.1163],
        [3.6924, 3.1876, 3.1925],
        [3.6924, 3.6924, 3.6924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.13381940126419067 
model_pd.l_d.mean(): -23.069791793823242 
model_pd.lagr.mean(): -22.935972213745117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2127], device='cuda:0')), ('power', tensor([-23.2825], device='cuda:0'))])
epoch£º813	 i:0 	 global-step:16260	 l-p:0.13381940126419067
epoch£º813	 i:1 	 global-step:16261	 l-p:0.1553715318441391
epoch£º813	 i:2 	 global-step:16262	 l-p:0.12178801000118256
epoch£º813	 i:3 	 global-step:16263	 l-p:0.2895791828632355
epoch£º813	 i:4 	 global-step:16264	 l-p:0.15134470164775848
epoch£º813	 i:5 	 global-step:16265	 l-p:0.2677786946296692
epoch£º813	 i:6 	 global-step:16266	 l-p:0.1635071188211441
epoch£º813	 i:7 	 global-step:16267	 l-p:0.13284777104854584
epoch£º813	 i:8 	 global-step:16268	 l-p:-0.31387290358543396
epoch£º813	 i:9 	 global-step:16269	 l-p:0.1823863983154297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6634, 3.6634, 3.6634],
        [3.6634, 3.6634, 3.6634],
        [3.6634, 3.5787, 3.6442],
        [3.6634, 3.6634, 3.6634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.13126498460769653 
model_pd.l_d.mean(): -23.578866958618164 
model_pd.lagr.mean(): -23.447601318359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1275], device='cuda:0')), ('power', tensor([-23.7064], device='cuda:0'))])
epoch£º814	 i:0 	 global-step:16280	 l-p:0.13126498460769653
epoch£º814	 i:1 	 global-step:16281	 l-p:0.4193875789642334
epoch£º814	 i:2 	 global-step:16282	 l-p:0.16674456000328064
epoch£º814	 i:3 	 global-step:16283	 l-p:8.341012001037598
epoch£º814	 i:4 	 global-step:16284	 l-p:0.16571490466594696
epoch£º814	 i:5 	 global-step:16285	 l-p:0.14582040905952454
epoch£º814	 i:6 	 global-step:16286	 l-p:0.13092967867851257
epoch£º814	 i:7 	 global-step:16287	 l-p:0.19400347769260406
epoch£º814	 i:8 	 global-step:16288	 l-p:0.11577771604061127
epoch£º814	 i:9 	 global-step:16289	 l-p:0.13945111632347107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6888, 3.6888, 3.6888],
        [3.6888, 3.1156, 2.4902],
        [3.6888, 3.5242, 3.6269],
        [3.6888, 3.4842, 3.5971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.18116407096385956 
model_pd.l_d.mean(): -22.60625457763672 
model_pd.lagr.mean(): -22.425090789794922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3162], device='cuda:0')), ('power', tensor([-22.9225], device='cuda:0'))])
epoch£º815	 i:0 	 global-step:16300	 l-p:0.18116407096385956
epoch£º815	 i:1 	 global-step:16301	 l-p:0.13391268253326416
epoch£º815	 i:2 	 global-step:16302	 l-p:0.12802185118198395
epoch£º815	 i:3 	 global-step:16303	 l-p:0.14098617434501648
epoch£º815	 i:4 	 global-step:16304	 l-p:0.2596328556537628
epoch£º815	 i:5 	 global-step:16305	 l-p:0.12286756187677383
epoch£º815	 i:6 	 global-step:16306	 l-p:0.4669336974620819
epoch£º815	 i:7 	 global-step:16307	 l-p:0.12029420584440231
epoch£º815	 i:8 	 global-step:16308	 l-p:0.13741080462932587
epoch£º815	 i:9 	 global-step:16309	 l-p:0.3385050892829895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6631, 3.5031, 3.6044],
        [3.6631, 3.3699, 3.4865],
        [3.6631, 3.6480, 3.6620],
        [3.6631, 3.5784, 3.6439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.13180574774742126 
model_pd.l_d.mean(): -23.661216735839844 
model_pd.lagr.mean(): -23.52941131591797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1056], device='cuda:0')), ('power', tensor([-23.7668], device='cuda:0'))])
epoch£º816	 i:0 	 global-step:16320	 l-p:0.13180574774742126
epoch£º816	 i:1 	 global-step:16321	 l-p:0.10690769553184509
epoch£º816	 i:2 	 global-step:16322	 l-p:0.12703512609004974
epoch£º816	 i:3 	 global-step:16323	 l-p:0.115371473133564
epoch£º816	 i:4 	 global-step:16324	 l-p:0.13973911106586456
epoch£º816	 i:5 	 global-step:16325	 l-p:0.17481479048728943
epoch£º816	 i:6 	 global-step:16326	 l-p:-0.011968602426350117
epoch£º816	 i:7 	 global-step:16327	 l-p:0.14071287214756012
epoch£º816	 i:8 	 global-step:16328	 l-p:0.11813198029994965
epoch£º816	 i:9 	 global-step:16329	 l-p:0.0238796416670084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6404, 3.0631, 2.4432],
        [3.6404, 3.6402, 3.6404],
        [3.6404, 3.6384, 3.6404],
        [3.6404, 2.9311, 2.6285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): 0.010406222194433212 
model_pd.l_d.mean(): -22.32406997680664 
model_pd.lagr.mean(): -22.313663482666016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3443], device='cuda:0')), ('power', tensor([-22.6684], device='cuda:0'))])
epoch£º817	 i:0 	 global-step:16340	 l-p:0.010406222194433212
epoch£º817	 i:1 	 global-step:16341	 l-p:0.2130664438009262
epoch£º817	 i:2 	 global-step:16342	 l-p:0.1565222591161728
epoch£º817	 i:3 	 global-step:16343	 l-p:0.13745228946208954
epoch£º817	 i:4 	 global-step:16344	 l-p:-0.21364611387252808
epoch£º817	 i:5 	 global-step:16345	 l-p:0.19621433317661285
epoch£º817	 i:6 	 global-step:16346	 l-p:0.12725359201431274
epoch£º817	 i:7 	 global-step:16347	 l-p:0.1481640636920929
epoch£º817	 i:8 	 global-step:16348	 l-p:0.1160796508193016
epoch£º817	 i:9 	 global-step:16349	 l-p:0.2668323516845703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6741, 3.1640, 2.5506],
        [3.6741, 3.0878, 2.4623],
        [3.6741, 3.6741, 3.6741],
        [3.6741, 3.5342, 3.6279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.18356087803840637 
model_pd.l_d.mean(): -22.729537963867188 
model_pd.lagr.mean(): -22.545976638793945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3351], device='cuda:0')), ('power', tensor([-23.0646], device='cuda:0'))])
epoch£º818	 i:0 	 global-step:16360	 l-p:0.18356087803840637
epoch£º818	 i:1 	 global-step:16361	 l-p:0.13732503354549408
epoch£º818	 i:2 	 global-step:16362	 l-p:0.1567467898130417
epoch£º818	 i:3 	 global-step:16363	 l-p:0.10809268802404404
epoch£º818	 i:4 	 global-step:16364	 l-p:0.1230873093008995
epoch£º818	 i:5 	 global-step:16365	 l-p:0.2501605451107025
epoch£º818	 i:6 	 global-step:16366	 l-p:1.4877697229385376
epoch£º818	 i:7 	 global-step:16367	 l-p:0.12318210303783417
epoch£º818	 i:8 	 global-step:16368	 l-p:0.12429068982601166
epoch£º818	 i:9 	 global-step:16369	 l-p:0.14160388708114624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6613, 3.4265, 3.5441],
        [3.6613, 2.9336, 2.5750],
        [3.6613, 3.6524, 3.6609],
        [3.6613, 3.0012, 2.3720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.1931496411561966 
model_pd.l_d.mean(): -23.720870971679688 
model_pd.lagr.mean(): -23.527721405029297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1717], device='cuda:0')), ('power', tensor([-23.8926], device='cuda:0'))])
epoch£º819	 i:0 	 global-step:16380	 l-p:0.1931496411561966
epoch£º819	 i:1 	 global-step:16381	 l-p:0.1586860567331314
epoch£º819	 i:2 	 global-step:16382	 l-p:0.1675570011138916
epoch£º819	 i:3 	 global-step:16383	 l-p:-0.11175021529197693
epoch£º819	 i:4 	 global-step:16384	 l-p:-0.021167507395148277
epoch£º819	 i:5 	 global-step:16385	 l-p:0.1150815337896347
epoch£º819	 i:6 	 global-step:16386	 l-p:0.09350369870662689
epoch£º819	 i:7 	 global-step:16387	 l-p:0.13636916875839233
epoch£º819	 i:8 	 global-step:16388	 l-p:0.1379893273115158
epoch£º819	 i:9 	 global-step:16389	 l-p:-0.08858875185251236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6418, 2.8925, 2.4678],
        [3.6418, 3.6394, 3.6418],
        [3.6418, 2.9469, 2.6791],
        [3.6418, 3.6418, 3.6418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.15411065518856049 
model_pd.l_d.mean(): -22.91204833984375 
model_pd.lagr.mean(): -22.757938385009766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2487], device='cuda:0')), ('power', tensor([-23.1608], device='cuda:0'))])
epoch£º820	 i:0 	 global-step:16400	 l-p:0.15411065518856049
epoch£º820	 i:1 	 global-step:16401	 l-p:-0.019929885864257812
epoch£º820	 i:2 	 global-step:16402	 l-p:0.17888052761554718
epoch£º820	 i:3 	 global-step:16403	 l-p:0.16955646872520447
epoch£º820	 i:4 	 global-step:16404	 l-p:0.12580789625644684
epoch£º820	 i:5 	 global-step:16405	 l-p:0.16137486696243286
epoch£º820	 i:6 	 global-step:16406	 l-p:0.14372338354587555
epoch£º820	 i:7 	 global-step:16407	 l-p:0.12737348675727844
epoch£º820	 i:8 	 global-step:16408	 l-p:0.14569798111915588
epoch£º820	 i:9 	 global-step:16409	 l-p:0.030052121728658676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6325, 3.3163, 3.4302],
        [3.6325, 3.4667, 3.5702],
        [3.6325, 3.2886, 3.3957],
        [3.6325, 2.9901, 2.8264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.1760220229625702 
model_pd.l_d.mean(): -23.77056121826172 
model_pd.lagr.mean(): -23.594539642333984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1344], device='cuda:0')), ('power', tensor([-23.9050], device='cuda:0'))])
epoch£º821	 i:0 	 global-step:16420	 l-p:0.1760220229625702
epoch£º821	 i:1 	 global-step:16421	 l-p:0.06307961791753769
epoch£º821	 i:2 	 global-step:16422	 l-p:-0.009127769619226456
epoch£º821	 i:3 	 global-step:16423	 l-p:0.13871656358242035
epoch£º821	 i:4 	 global-step:16424	 l-p:0.13809144496917725
epoch£º821	 i:5 	 global-step:16425	 l-p:0.015277688391506672
epoch£º821	 i:6 	 global-step:16426	 l-p:0.1549416482448578
epoch£º821	 i:7 	 global-step:16427	 l-p:0.19080333411693573
epoch£º821	 i:8 	 global-step:16428	 l-p:0.10963165760040283
epoch£º821	 i:9 	 global-step:16429	 l-p:0.20269645750522614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6458, 3.6318, 3.6449],
        [3.6458, 2.9148, 2.3016],
        [3.6458, 2.8944, 2.3039],
        [3.6458, 3.6452, 3.6458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.11580424010753632 
model_pd.l_d.mean(): -23.211374282836914 
model_pd.lagr.mean(): -23.095569610595703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2217], device='cuda:0')), ('power', tensor([-23.4331], device='cuda:0'))])
epoch£º822	 i:0 	 global-step:16440	 l-p:0.11580424010753632
epoch£º822	 i:1 	 global-step:16441	 l-p:0.1695290356874466
epoch£º822	 i:2 	 global-step:16442	 l-p:0.13682341575622559
epoch£º822	 i:3 	 global-step:16443	 l-p:0.16199728846549988
epoch£º822	 i:4 	 global-step:16444	 l-p:0.06567907333374023
epoch£º822	 i:5 	 global-step:16445	 l-p:0.1250735968351364
epoch£º822	 i:6 	 global-step:16446	 l-p:-0.017031230032444
epoch£º822	 i:7 	 global-step:16447	 l-p:-4.929618835449219
epoch£º822	 i:8 	 global-step:16448	 l-p:0.13465112447738647
epoch£º822	 i:9 	 global-step:16449	 l-p:0.1447554975748062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6617, 3.4933, 3.5975],
        [3.6617, 3.6617, 3.6617],
        [3.6617, 3.1539, 3.1599],
        [3.6617, 3.1689, 2.5601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.16318465769290924 
model_pd.l_d.mean(): -23.30047607421875 
model_pd.lagr.mean(): -23.137290954589844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3021], device='cuda:0')), ('power', tensor([-23.6026], device='cuda:0'))])
epoch£º823	 i:0 	 global-step:16460	 l-p:0.16318465769290924
epoch£º823	 i:1 	 global-step:16461	 l-p:0.1423339545726776
epoch£º823	 i:2 	 global-step:16462	 l-p:7.160717487335205
epoch£º823	 i:3 	 global-step:16463	 l-p:0.14531472325325012
epoch£º823	 i:4 	 global-step:16464	 l-p:0.20926536619663239
epoch£º823	 i:5 	 global-step:16465	 l-p:-0.07539251446723938
epoch£º823	 i:6 	 global-step:16466	 l-p:0.0035444211680442095
epoch£º823	 i:7 	 global-step:16467	 l-p:0.13532744348049164
epoch£º823	 i:8 	 global-step:16468	 l-p:0.13704271614551544
epoch£º823	 i:9 	 global-step:16469	 l-p:0.11674578487873077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6583, 2.9946, 2.7890],
        [3.6583, 3.6096, 3.6508],
        [3.6583, 3.4503, 3.5644],
        [3.6583, 3.6582, 3.6583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.16804984211921692 
model_pd.l_d.mean(): -23.36322021484375 
model_pd.lagr.mean(): -23.19516944885254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2292], device='cuda:0')), ('power', tensor([-23.5924], device='cuda:0'))])
epoch£º824	 i:0 	 global-step:16480	 l-p:0.16804984211921692
epoch£º824	 i:1 	 global-step:16481	 l-p:-0.16396841406822205
epoch£º824	 i:2 	 global-step:16482	 l-p:0.4179444909095764
epoch£º824	 i:3 	 global-step:16483	 l-p:0.11873277276754379
epoch£º824	 i:4 	 global-step:16484	 l-p:0.11294279992580414
epoch£º824	 i:5 	 global-step:16485	 l-p:0.3506668508052826
epoch£º824	 i:6 	 global-step:16486	 l-p:0.12780164182186127
epoch£º824	 i:7 	 global-step:16487	 l-p:0.13349966704845428
epoch£º824	 i:8 	 global-step:16488	 l-p:0.12930946052074432
epoch£º824	 i:9 	 global-step:16489	 l-p:0.16964766383171082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6799, 3.2448, 2.6455],
        [3.6799, 3.6791, 3.6799],
        [3.6799, 3.6799, 3.6799],
        [3.6799, 3.6023, 3.6634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.14290186762809753 
model_pd.l_d.mean(): -23.026809692382812 
model_pd.lagr.mean(): -22.883907318115234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2406], device='cuda:0')), ('power', tensor([-23.2674], device='cuda:0'))])
epoch£º825	 i:0 	 global-step:16500	 l-p:0.14290186762809753
epoch£º825	 i:1 	 global-step:16501	 l-p:0.1128324493765831
epoch£º825	 i:2 	 global-step:16502	 l-p:0.13031083345413208
epoch£º825	 i:3 	 global-step:16503	 l-p:0.7761587500572205
epoch£º825	 i:4 	 global-step:16504	 l-p:0.14861330389976501
epoch£º825	 i:5 	 global-step:16505	 l-p:0.1180286854505539
epoch£º825	 i:6 	 global-step:16506	 l-p:0.09035912901163101
epoch£º825	 i:7 	 global-step:16507	 l-p:0.42142805457115173
epoch£º825	 i:8 	 global-step:16508	 l-p:0.20104151964187622
epoch£º825	 i:9 	 global-step:16509	 l-p:0.16407103836536407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6710, 3.6643, 3.6707],
        [3.6710, 3.6525, 3.6695],
        [3.6710, 3.6660, 3.6708],
        [3.6710, 3.0749, 2.9776]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.12361249327659607 
model_pd.l_d.mean(): -23.57758903503418 
model_pd.lagr.mean(): -23.453975677490234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1525], device='cuda:0')), ('power', tensor([-23.7301], device='cuda:0'))])
epoch£º826	 i:0 	 global-step:16520	 l-p:0.12361249327659607
epoch£º826	 i:1 	 global-step:16521	 l-p:0.3048659563064575
epoch£º826	 i:2 	 global-step:16522	 l-p:0.12696442008018494
epoch£º826	 i:3 	 global-step:16523	 l-p:0.125120148062706
epoch£º826	 i:4 	 global-step:16524	 l-p:0.39698585867881775
epoch£º826	 i:5 	 global-step:16525	 l-p:0.12139944732189178
epoch£º826	 i:6 	 global-step:16526	 l-p:0.16029155254364014
epoch£º826	 i:7 	 global-step:16527	 l-p:0.13134247064590454
epoch£º826	 i:8 	 global-step:16528	 l-p:0.13591612875461578
epoch£º826	 i:9 	 global-step:16529	 l-p:0.2207145243883133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6932, 3.6918, 3.6932],
        [3.6932, 3.6153, 3.6766],
        [3.6932, 3.6932, 3.6932],
        [3.6932, 3.3827, 3.4965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.11992871016263962 
model_pd.l_d.mean(): -23.364591598510742 
model_pd.lagr.mean(): -23.24466323852539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1705], device='cuda:0')), ('power', tensor([-23.5351], device='cuda:0'))])
epoch£º827	 i:0 	 global-step:16540	 l-p:0.11992871016263962
epoch£º827	 i:1 	 global-step:16541	 l-p:0.22208882868289948
epoch£º827	 i:2 	 global-step:16542	 l-p:0.2278163731098175
epoch£º827	 i:3 	 global-step:16543	 l-p:0.14407619833946228
epoch£º827	 i:4 	 global-step:16544	 l-p:0.1411614567041397
epoch£º827	 i:5 	 global-step:16545	 l-p:0.2323024570941925
epoch£º827	 i:6 	 global-step:16546	 l-p:0.13973946869373322
epoch£º827	 i:7 	 global-step:16547	 l-p:0.12064628303050995
epoch£º827	 i:8 	 global-step:16548	 l-p:0.0981982946395874
epoch£º827	 i:9 	 global-step:16549	 l-p:0.11328809708356857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6980, 3.4721, 3.5885],
        [3.6980, 3.2813, 3.3548],
        [3.6980, 3.6116, 3.6781],
        [3.6980, 3.6236, 3.6826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.11398374289274216 
model_pd.l_d.mean(): -23.424861907958984 
model_pd.lagr.mean(): -23.31087875366211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1613], device='cuda:0')), ('power', tensor([-23.5862], device='cuda:0'))])
epoch£º828	 i:0 	 global-step:16560	 l-p:0.11398374289274216
epoch£º828	 i:1 	 global-step:16561	 l-p:0.1432151198387146
epoch£º828	 i:2 	 global-step:16562	 l-p:0.2086864858865738
epoch£º828	 i:3 	 global-step:16563	 l-p:0.16340979933738708
epoch£º828	 i:4 	 global-step:16564	 l-p:0.13884063065052032
epoch£º828	 i:5 	 global-step:16565	 l-p:0.1430683732032776
epoch£º828	 i:6 	 global-step:16566	 l-p:0.13023167848587036
epoch£º828	 i:7 	 global-step:16567	 l-p:0.13608704507350922
epoch£º828	 i:8 	 global-step:16568	 l-p:-0.5237778425216675
epoch£º828	 i:9 	 global-step:16569	 l-p:0.5916265249252319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6599, 3.5123, 3.6092],
        [3.6599, 2.9191, 2.3134],
        [3.6599, 3.6596, 3.6599],
        [3.6599, 3.6587, 3.6599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.1466834545135498 
model_pd.l_d.mean(): -22.92658805847168 
model_pd.lagr.mean(): -22.779905319213867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2408], device='cuda:0')), ('power', tensor([-23.1674], device='cuda:0'))])
epoch£º829	 i:0 	 global-step:16580	 l-p:0.1466834545135498
epoch£º829	 i:1 	 global-step:16581	 l-p:-0.12871551513671875
epoch£º829	 i:2 	 global-step:16582	 l-p:0.13473203778266907
epoch£º829	 i:3 	 global-step:16583	 l-p:0.590900719165802
epoch£º829	 i:4 	 global-step:16584	 l-p:0.12219194322824478
epoch£º829	 i:5 	 global-step:16585	 l-p:0.9250022768974304
epoch£º829	 i:6 	 global-step:16586	 l-p:0.15203148126602173
epoch£º829	 i:7 	 global-step:16587	 l-p:0.14322368800640106
epoch£º829	 i:8 	 global-step:16588	 l-p:0.12835201621055603
epoch£º829	 i:9 	 global-step:16589	 l-p:0.1546764373779297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6765, 3.6753, 3.6764],
        [3.6765, 3.1743, 3.1853],
        [3.6765, 3.0292, 2.8517],
        [3.6765, 3.6765, 3.6765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.13252002000808716 
model_pd.l_d.mean(): -23.549837112426758 
model_pd.lagr.mean(): -23.417316436767578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1724], device='cuda:0')), ('power', tensor([-23.7222], device='cuda:0'))])
epoch£º830	 i:0 	 global-step:16600	 l-p:0.13252002000808716
epoch£º830	 i:1 	 global-step:16601	 l-p:0.15874426066875458
epoch£º830	 i:2 	 global-step:16602	 l-p:0.13021023571491241
epoch£º830	 i:3 	 global-step:16603	 l-p:0.7487369179725647
epoch£º830	 i:4 	 global-step:16604	 l-p:0.269151508808136
epoch£º830	 i:5 	 global-step:16605	 l-p:0.13061749935150146
epoch£º830	 i:6 	 global-step:16606	 l-p:0.1326671689748764
epoch£º830	 i:7 	 global-step:16607	 l-p:0.13228753209114075
epoch£º830	 i:8 	 global-step:16608	 l-p:0.4450642168521881
epoch£º830	 i:9 	 global-step:16609	 l-p:0.094575896859169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6742, 3.6741, 3.6742],
        [3.6742, 3.6691, 3.6740],
        [3.6742, 3.6468, 3.6713],
        [3.6742, 3.6742, 3.6742]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.13158972561359406 
model_pd.l_d.mean(): -23.194721221923828 
model_pd.lagr.mean(): -23.06313133239746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2070], device='cuda:0')), ('power', tensor([-23.4017], device='cuda:0'))])
epoch£º831	 i:0 	 global-step:16620	 l-p:0.13158972561359406
epoch£º831	 i:1 	 global-step:16621	 l-p:0.15774595737457275
epoch£º831	 i:2 	 global-step:16622	 l-p:0.1323794275522232
epoch£º831	 i:3 	 global-step:16623	 l-p:0.16661189496517181
epoch£º831	 i:4 	 global-step:16624	 l-p:0.13366080820560455
epoch£º831	 i:5 	 global-step:16625	 l-p:0.2284061312675476
epoch£º831	 i:6 	 global-step:16626	 l-p:0.14246024191379547
epoch£º831	 i:7 	 global-step:16627	 l-p:0.31713348627090454
epoch£º831	 i:8 	 global-step:16628	 l-p:0.23839080333709717
epoch£º831	 i:9 	 global-step:16629	 l-p:0.12314534932374954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6976, 3.6899, 3.6973],
        [3.6976, 3.1914, 3.1972],
        [3.6976, 2.9435, 2.3808],
        [3.6976, 3.1911, 3.1966]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.14564408361911774 
model_pd.l_d.mean(): -23.300996780395508 
model_pd.lagr.mean(): -23.155353546142578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1906], device='cuda:0')), ('power', tensor([-23.4916], device='cuda:0'))])
epoch£º832	 i:0 	 global-step:16640	 l-p:0.14564408361911774
epoch£º832	 i:1 	 global-step:16641	 l-p:0.19536249339580536
epoch£º832	 i:2 	 global-step:16642	 l-p:0.20762218534946442
epoch£º832	 i:3 	 global-step:16643	 l-p:0.08460899442434311
epoch£º832	 i:4 	 global-step:16644	 l-p:0.1231585294008255
epoch£º832	 i:5 	 global-step:16645	 l-p:0.12515966594219208
epoch£º832	 i:6 	 global-step:16646	 l-p:0.14910922944545746
epoch£º832	 i:7 	 global-step:16647	 l-p:0.12174475193023682
epoch£º832	 i:8 	 global-step:16648	 l-p:0.11442067474126816
epoch£º832	 i:9 	 global-step:16649	 l-p:0.17341044545173645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7318, 3.7319, 3.7319],
        [3.7318, 3.7189, 3.7310],
        [3.7318, 3.5120, 3.6274],
        [3.7318, 3.7249, 3.7315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.1195274218916893 
model_pd.l_d.mean(): -23.040760040283203 
model_pd.lagr.mean(): -22.921232223510742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2577], device='cuda:0')), ('power', tensor([-23.2985], device='cuda:0'))])
epoch£º833	 i:0 	 global-step:16660	 l-p:0.1195274218916893
epoch£º833	 i:1 	 global-step:16661	 l-p:0.11843981593847275
epoch£º833	 i:2 	 global-step:16662	 l-p:0.08966759592294693
epoch£º833	 i:3 	 global-step:16663	 l-p:0.1723897010087967
epoch£º833	 i:4 	 global-step:16664	 l-p:0.15468010306358337
epoch£º833	 i:5 	 global-step:16665	 l-p:0.1447596251964569
epoch£º833	 i:6 	 global-step:16666	 l-p:0.14575184881687164
epoch£º833	 i:7 	 global-step:16667	 l-p:0.15497656166553497
epoch£º833	 i:8 	 global-step:16668	 l-p:0.15094108879566193
epoch£º833	 i:9 	 global-step:16669	 l-p:0.14204251766204834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7192, 3.7192, 3.7192],
        [3.7192, 3.3209, 2.7252],
        [3.7192, 3.7191, 3.7192],
        [3.7192, 3.4051, 3.5178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.10726813971996307 
model_pd.l_d.mean(): -22.990934371948242 
model_pd.lagr.mean(): -22.8836669921875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3085], device='cuda:0')), ('power', tensor([-23.2994], device='cuda:0'))])
epoch£º834	 i:0 	 global-step:16680	 l-p:0.10726813971996307
epoch£º834	 i:1 	 global-step:16681	 l-p:0.1153433546423912
epoch£º834	 i:2 	 global-step:16682	 l-p:0.10858621448278427
epoch£º834	 i:3 	 global-step:16683	 l-p:0.12397687882184982
epoch£º834	 i:4 	 global-step:16684	 l-p:0.15610694885253906
epoch£º834	 i:5 	 global-step:16685	 l-p:0.33101966977119446
epoch£º834	 i:6 	 global-step:16686	 l-p:0.15775200724601746
epoch£º834	 i:7 	 global-step:16687	 l-p:0.17572374641895294
epoch£º834	 i:8 	 global-step:16688	 l-p:2.5187032222747803
epoch£º834	 i:9 	 global-step:16689	 l-p:-0.024968890473246574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6544, 3.6542, 3.6545],
        [3.6544, 3.4117, 3.5304],
        [3.6544, 2.9510, 2.6658],
        [3.6544, 2.8901, 2.3717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.21035854518413544 
model_pd.l_d.mean(): -23.088205337524414 
model_pd.lagr.mean(): -22.87784767150879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2351], device='cuda:0')), ('power', tensor([-23.3233], device='cuda:0'))])
epoch£º835	 i:0 	 global-step:16700	 l-p:0.21035854518413544
epoch£º835	 i:1 	 global-step:16701	 l-p:0.12538404762744904
epoch£º835	 i:2 	 global-step:16702	 l-p:-0.9979808330535889
epoch£º835	 i:3 	 global-step:16703	 l-p:0.15750207006931305
epoch£º835	 i:4 	 global-step:16704	 l-p:0.13028119504451752
epoch£º835	 i:5 	 global-step:16705	 l-p:0.12293022125959396
epoch£º835	 i:6 	 global-step:16706	 l-p:0.1544019728899002
epoch£º835	 i:7 	 global-step:16707	 l-p:0.1409025639295578
epoch£º835	 i:8 	 global-step:16708	 l-p:0.1367313116788864
epoch£º835	 i:9 	 global-step:16709	 l-p:-0.04073770344257355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6374, 3.6132, 3.6351],
        [3.6374, 2.9462, 2.3194],
        [3.6374, 3.6355, 3.6373],
        [3.6374, 3.4284, 3.5431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.18979933857917786 
model_pd.l_d.mean(): -23.524282455444336 
model_pd.lagr.mean(): -23.334482192993164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2168], device='cuda:0')), ('power', tensor([-23.7410], device='cuda:0'))])
epoch£º836	 i:0 	 global-step:16720	 l-p:0.18979933857917786
epoch£º836	 i:1 	 global-step:16721	 l-p:0.1072685718536377
epoch£º836	 i:2 	 global-step:16722	 l-p:0.11474674195051193
epoch£º836	 i:3 	 global-step:16723	 l-p:-0.08964911848306656
epoch£º836	 i:4 	 global-step:16724	 l-p:0.04943171516060829
epoch£º836	 i:5 	 global-step:16725	 l-p:0.12836450338363647
epoch£º836	 i:6 	 global-step:16726	 l-p:0.12407242506742477
epoch£º836	 i:7 	 global-step:16727	 l-p:0.13725429773330688
epoch£º836	 i:8 	 global-step:16728	 l-p:0.18522638082504272
epoch£º836	 i:9 	 global-step:16729	 l-p:0.2477552890777588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6428, 3.5054, 3.5982],
        [3.6428, 3.6423, 3.6428],
        [3.6428, 2.8967, 2.2923],
        [3.6428, 3.1597, 3.1905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.15905839204788208 
model_pd.l_d.mean(): -23.756837844848633 
model_pd.lagr.mean(): -23.597780227661133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1825], device='cuda:0')), ('power', tensor([-23.9393], device='cuda:0'))])
epoch£º837	 i:0 	 global-step:16740	 l-p:0.15905839204788208
epoch£º837	 i:1 	 global-step:16741	 l-p:0.11448834091424942
epoch£º837	 i:2 	 global-step:16742	 l-p:0.08977583795785904
epoch£º837	 i:3 	 global-step:16743	 l-p:0.12518636882305145
epoch£º837	 i:4 	 global-step:16744	 l-p:0.11927749216556549
epoch£º837	 i:5 	 global-step:16745	 l-p:0.11910949647426605
epoch£º837	 i:6 	 global-step:16746	 l-p:0.07385849952697754
epoch£º837	 i:7 	 global-step:16747	 l-p:0.4635566473007202
epoch£º837	 i:8 	 global-step:16748	 l-p:0.12914054095745087
epoch£º837	 i:9 	 global-step:16749	 l-p:0.1262333244085312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6169, 3.6169, 3.6169],
        [3.6169, 3.1573, 2.5602],
        [3.6169, 3.3042, 3.4198],
        [3.6169, 3.3725, 3.4918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 11.197178840637207 
model_pd.l_d.mean(): -23.058549880981445 
model_pd.lagr.mean(): -11.861371040344238 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3788], device='cuda:0')), ('power', tensor([-23.4373], device='cuda:0'))])
epoch£º838	 i:0 	 global-step:16760	 l-p:11.197178840637207
epoch£º838	 i:1 	 global-step:16761	 l-p:0.09214988350868225
epoch£º838	 i:2 	 global-step:16762	 l-p:0.12342049926519394
epoch£º838	 i:3 	 global-step:16763	 l-p:0.12685279548168182
epoch£º838	 i:4 	 global-step:16764	 l-p:0.2112933099269867
epoch£º838	 i:5 	 global-step:16765	 l-p:0.10024919360876083
epoch£º838	 i:6 	 global-step:16766	 l-p:0.11492127925157547
epoch£º838	 i:7 	 global-step:16767	 l-p:0.1679106503725052
epoch£º838	 i:8 	 global-step:16768	 l-p:0.13188576698303223
epoch£º838	 i:9 	 global-step:16769	 l-p:0.11943689733743668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6048, 3.6044, 3.6049],
        [3.6048, 3.6045, 3.6048],
        [3.6048, 3.6048, 3.6048],
        [3.6048, 3.5998, 3.6047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.11260797083377838 
model_pd.l_d.mean(): -23.373699188232422 
model_pd.lagr.mean(): -23.261091232299805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2510], device='cuda:0')), ('power', tensor([-23.6247], device='cuda:0'))])
epoch£º839	 i:0 	 global-step:16780	 l-p:0.11260797083377838
epoch£º839	 i:1 	 global-step:16781	 l-p:0.1591007560491562
epoch£º839	 i:2 	 global-step:16782	 l-p:0.13818201422691345
epoch£º839	 i:3 	 global-step:16783	 l-p:0.16845175623893738
epoch£º839	 i:4 	 global-step:16784	 l-p:0.15440165996551514
epoch£º839	 i:5 	 global-step:16785	 l-p:0.054804958403110504
epoch£º839	 i:6 	 global-step:16786	 l-p:0.696784496307373
epoch£º839	 i:7 	 global-step:16787	 l-p:0.12107782065868378
epoch£º839	 i:8 	 global-step:16788	 l-p:0.11355239897966385
epoch£º839	 i:9 	 global-step:16789	 l-p:0.052068039774894714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5653, 3.5653, 3.5653],
        [3.5653, 2.7938, 2.2112],
        [3.5653, 3.3126, 3.4332],
        [3.5653, 3.5653, 3.5653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.04080011323094368 
model_pd.l_d.mean(): -22.782522201538086 
model_pd.lagr.mean(): -22.741722106933594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3510], device='cuda:0')), ('power', tensor([-23.1335], device='cuda:0'))])
epoch£º840	 i:0 	 global-step:16800	 l-p:0.04080011323094368
epoch£º840	 i:1 	 global-step:16801	 l-p:0.08546752482652664
epoch£º840	 i:2 	 global-step:16802	 l-p:0.4814053177833557
epoch£º840	 i:3 	 global-step:16803	 l-p:0.11903396248817444
epoch£º840	 i:4 	 global-step:16804	 l-p:0.17339354753494263
epoch£º840	 i:5 	 global-step:16805	 l-p:0.15263120830059052
epoch£º840	 i:6 	 global-step:16806	 l-p:0.13208205997943878
epoch£º840	 i:7 	 global-step:16807	 l-p:0.14691348373889923
epoch£º840	 i:8 	 global-step:16808	 l-p:0.04279369115829468
epoch£º840	 i:9 	 global-step:16809	 l-p:0.12109756469726562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5911, 2.8469, 2.4761],
        [3.5911, 3.5803, 3.5905],
        [3.5911, 3.5872, 3.5910],
        [3.5911, 2.8247, 2.2383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): -5.925023078918457 
model_pd.l_d.mean(): -23.526561737060547 
model_pd.lagr.mean(): -29.451583862304688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2553], device='cuda:0')), ('power', tensor([-23.7819], device='cuda:0'))])
epoch£º841	 i:0 	 global-step:16820	 l-p:-5.925023078918457
epoch£º841	 i:1 	 global-step:16821	 l-p:0.22564826905727386
epoch£º841	 i:2 	 global-step:16822	 l-p:0.12494603544473648
epoch£º841	 i:3 	 global-step:16823	 l-p:0.1699059009552002
epoch£º841	 i:4 	 global-step:16824	 l-p:0.10145427286624908
epoch£º841	 i:5 	 global-step:16825	 l-p:0.11593326181173325
epoch£º841	 i:6 	 global-step:16826	 l-p:0.14548417925834656
epoch£º841	 i:7 	 global-step:16827	 l-p:-0.2070700079202652
epoch£º841	 i:8 	 global-step:16828	 l-p:0.13218331336975098
epoch£º841	 i:9 	 global-step:16829	 l-p:0.13657823204994202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6099, 3.4453, 3.5488],
        [3.6099, 2.8391, 2.3456],
        [3.6099, 3.0107, 2.3910],
        [3.6099, 3.6099, 3.6099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): -0.1730259656906128 
model_pd.l_d.mean(): -23.740907669067383 
model_pd.lagr.mean(): -23.91393280029297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1592], device='cuda:0')), ('power', tensor([-23.9001], device='cuda:0'))])
epoch£º842	 i:0 	 global-step:16840	 l-p:-0.1730259656906128
epoch£º842	 i:1 	 global-step:16841	 l-p:0.15989825129508972
epoch£º842	 i:2 	 global-step:16842	 l-p:0.1505875140428543
epoch£º842	 i:3 	 global-step:16843	 l-p:0.07271003723144531
epoch£º842	 i:4 	 global-step:16844	 l-p:0.1324436515569687
epoch£º842	 i:5 	 global-step:16845	 l-p:0.05548165738582611
epoch£º842	 i:6 	 global-step:16846	 l-p:0.0719570517539978
epoch£º842	 i:7 	 global-step:16847	 l-p:0.1930599808692932
epoch£º842	 i:8 	 global-step:16848	 l-p:0.1708354651927948
epoch£º842	 i:9 	 global-step:16849	 l-p:0.11577530950307846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6466, 2.8856, 2.3046],
        [3.6466, 3.6466, 3.6466],
        [3.6466, 3.6181, 3.6436],
        [3.6466, 3.0520, 2.4278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.17332175374031067 
model_pd.l_d.mean(): -23.156082153320312 
model_pd.lagr.mean(): -22.982759475708008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2421], device='cuda:0')), ('power', tensor([-23.3981], device='cuda:0'))])
epoch£º843	 i:0 	 global-step:16860	 l-p:0.17332175374031067
epoch£º843	 i:1 	 global-step:16861	 l-p:0.1128590926527977
epoch£º843	 i:2 	 global-step:16862	 l-p:-0.01863384246826172
epoch£º843	 i:3 	 global-step:16863	 l-p:0.13090677559375763
epoch£º843	 i:4 	 global-step:16864	 l-p:0.14648382365703583
epoch£º843	 i:5 	 global-step:16865	 l-p:-0.49347975850105286
epoch£º843	 i:6 	 global-step:16866	 l-p:0.13357509672641754
epoch£º843	 i:7 	 global-step:16867	 l-p:0.19863224029541016
epoch£º843	 i:8 	 global-step:16868	 l-p:0.1329791098833084
epoch£º843	 i:9 	 global-step:16869	 l-p:0.15336063504219055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6502, 2.8851, 2.3215],
        [3.6502, 3.6502, 3.6502],
        [3.6502, 3.1084, 3.0839],
        [3.6502, 3.3738, 3.4931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): 0.016479024663567543 
model_pd.l_d.mean(): -23.567005157470703 
model_pd.lagr.mean(): -23.550525665283203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1728], device='cuda:0')), ('power', tensor([-23.7398], device='cuda:0'))])
epoch£º844	 i:0 	 global-step:16880	 l-p:0.016479024663567543
epoch£º844	 i:1 	 global-step:16881	 l-p:0.1289222538471222
epoch£º844	 i:2 	 global-step:16882	 l-p:0.1638500690460205
epoch£º844	 i:3 	 global-step:16883	 l-p:0.11199303716421127
epoch£º844	 i:4 	 global-step:16884	 l-p:0.13545909523963928
epoch£º844	 i:5 	 global-step:16885	 l-p:0.19030864536762238
epoch£º844	 i:6 	 global-step:16886	 l-p:0.04992581903934479
epoch£º844	 i:7 	 global-step:16887	 l-p:0.13791707158088684
epoch£º844	 i:8 	 global-step:16888	 l-p:0.11839347332715988
epoch£º844	 i:9 	 global-step:16889	 l-p:0.12479493021965027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6281, 3.6280, 3.6281],
        [3.6281, 3.5950, 3.6242],
        [3.6281, 3.2517, 3.3492],
        [3.6281, 3.0098, 2.8908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.12284461408853531 
model_pd.l_d.mean(): -23.449108123779297 
model_pd.lagr.mean(): -23.326263427734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2225], device='cuda:0')), ('power', tensor([-23.6717], device='cuda:0'))])
epoch£º845	 i:0 	 global-step:16900	 l-p:0.12284461408853531
epoch£º845	 i:1 	 global-step:16901	 l-p:0.15152476727962494
epoch£º845	 i:2 	 global-step:16902	 l-p:0.09382674843072891
epoch£º845	 i:3 	 global-step:16903	 l-p:0.1375097930431366
epoch£º845	 i:4 	 global-step:16904	 l-p:0.242548868060112
epoch£º845	 i:5 	 global-step:16905	 l-p:0.12223368138074875
epoch£º845	 i:6 	 global-step:16906	 l-p:-0.6657996773719788
epoch£º845	 i:7 	 global-step:16907	 l-p:0.12846845388412476
epoch£º845	 i:8 	 global-step:16908	 l-p:0.08134473860263824
epoch£º845	 i:9 	 global-step:16909	 l-p:0.1412331908941269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6126, 3.4013, 3.5169],
        [3.6126, 3.5372, 3.5970],
        [3.6126, 3.4676, 3.5639],
        [3.6126, 3.6126, 3.6126]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.08438704162836075 
model_pd.l_d.mean(): -22.90593910217285 
model_pd.lagr.mean(): -22.821552276611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2888], device='cuda:0')), ('power', tensor([-23.1947], device='cuda:0'))])
epoch£º846	 i:0 	 global-step:16920	 l-p:0.08438704162836075
epoch£º846	 i:1 	 global-step:16921	 l-p:0.10842126607894897
epoch£º846	 i:2 	 global-step:16922	 l-p:0.1560795158147812
epoch£º846	 i:3 	 global-step:16923	 l-p:0.22705304622650146
epoch£º846	 i:4 	 global-step:16924	 l-p:0.12675948441028595
epoch£º846	 i:5 	 global-step:16925	 l-p:-3.5479135513305664
epoch£º846	 i:6 	 global-step:16926	 l-p:0.11647753417491913
epoch£º846	 i:7 	 global-step:16927	 l-p:0.09183837473392487
epoch£º846	 i:8 	 global-step:16928	 l-p:0.14192380011081696
epoch£º846	 i:9 	 global-step:16929	 l-p:0.13201501965522766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6193, 3.5265, 3.5970],
        [3.6193, 3.6193, 3.6193],
        [3.6193, 3.5197, 3.5941],
        [3.6193, 3.4033, 3.5197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.12454378604888916 
model_pd.l_d.mean(): -23.626375198364258 
model_pd.lagr.mean(): -23.5018310546875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1635], device='cuda:0')), ('power', tensor([-23.7899], device='cuda:0'))])
epoch£º847	 i:0 	 global-step:16940	 l-p:0.12454378604888916
epoch£º847	 i:1 	 global-step:16941	 l-p:0.14641104638576508
epoch£º847	 i:2 	 global-step:16942	 l-p:0.1465139389038086
epoch£º847	 i:3 	 global-step:16943	 l-p:-0.06738688796758652
epoch£º847	 i:4 	 global-step:16944	 l-p:0.1406712681055069
epoch£º847	 i:5 	 global-step:16945	 l-p:0.13963642716407776
epoch£º847	 i:6 	 global-step:16946	 l-p:0.21666857600212097
epoch£º847	 i:7 	 global-step:16947	 l-p:0.13005927205085754
epoch£º847	 i:8 	 global-step:16948	 l-p:0.12420403957366943
epoch£º847	 i:9 	 global-step:16949	 l-p:-3.527233600616455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5974, 3.4553, 3.5504],
        [3.5974, 3.1527, 3.2161],
        [3.5974, 3.5966, 3.5974],
        [3.5974, 2.8268, 2.3587]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.15322789549827576 
model_pd.l_d.mean(): -23.67416000366211 
model_pd.lagr.mean(): -23.520931243896484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1674], device='cuda:0')), ('power', tensor([-23.8415], device='cuda:0'))])
epoch£º848	 i:0 	 global-step:16960	 l-p:0.15322789549827576
epoch£º848	 i:1 	 global-step:16961	 l-p:0.05282372236251831
epoch£º848	 i:2 	 global-step:16962	 l-p:0.1329510360956192
epoch£º848	 i:3 	 global-step:16963	 l-p:0.15133629739284515
epoch£º848	 i:4 	 global-step:16964	 l-p:0.14738130569458008
epoch£º848	 i:5 	 global-step:16965	 l-p:0.13566559553146362
epoch£º848	 i:6 	 global-step:16966	 l-p:0.13580350577831268
epoch£º848	 i:7 	 global-step:16967	 l-p:0.1515471190214157
epoch£º848	 i:8 	 global-step:16968	 l-p:-0.39007505774497986
epoch£º848	 i:9 	 global-step:16969	 l-p:0.06473326683044434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5871, 3.0391, 3.0150],
        [3.5871, 2.8548, 2.2374],
        [3.5871, 3.0814, 3.0989],
        [3.5871, 3.4945, 3.5650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.12331116199493408 
model_pd.l_d.mean(): -22.556474685668945 
model_pd.lagr.mean(): -22.433162689208984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3420], device='cuda:0')), ('power', tensor([-22.8985], device='cuda:0'))])
epoch£º849	 i:0 	 global-step:16980	 l-p:0.12331116199493408
epoch£º849	 i:1 	 global-step:16981	 l-p:0.14299514889717102
epoch£º849	 i:2 	 global-step:16982	 l-p:0.13469384610652924
epoch£º849	 i:3 	 global-step:16983	 l-p:0.10736436396837234
epoch£º849	 i:4 	 global-step:16984	 l-p:0.03769500553607941
epoch£º849	 i:5 	 global-step:16985	 l-p:0.14004048705101013
epoch£º849	 i:6 	 global-step:16986	 l-p:0.14418290555477142
epoch£º849	 i:7 	 global-step:16987	 l-p:0.20400916039943695
epoch£º849	 i:8 	 global-step:16988	 l-p:4.778376579284668
epoch£º849	 i:9 	 global-step:16989	 l-p:0.15554668009281158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8903,  0.8564,  1.0000,  0.8239,
          1.0000,  0.9620, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228]], device='cuda:0')
 pt:tensor([[3.6016, 3.1077, 2.5053],
        [3.6016, 2.8956, 2.6259],
        [3.6016, 3.2816, 3.3970],
        [3.6016, 3.1195, 2.5193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.08810463547706604 
model_pd.l_d.mean(): -23.40349006652832 
model_pd.lagr.mean(): -23.315385818481445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2601], device='cuda:0')), ('power', tensor([-23.6636], device='cuda:0'))])
epoch£º850	 i:0 	 global-step:17000	 l-p:0.08810463547706604
epoch£º850	 i:1 	 global-step:17001	 l-p:0.13750334084033966
epoch£º850	 i:2 	 global-step:17002	 l-p:0.11665626615285873
epoch£º850	 i:3 	 global-step:17003	 l-p:0.1404622346162796
epoch£º850	 i:4 	 global-step:17004	 l-p:0.36044052243232727
epoch£º850	 i:5 	 global-step:17005	 l-p:0.13698498904705048
epoch£º850	 i:6 	 global-step:17006	 l-p:0.11426882445812225
epoch£º850	 i:7 	 global-step:17007	 l-p:0.16749103367328644
epoch£º850	 i:8 	 global-step:17008	 l-p:-0.10629075020551682
epoch£º850	 i:9 	 global-step:17009	 l-p:0.2040160745382309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6156, 3.6156, 3.6156],
        [3.6156, 3.6152, 3.6156],
        [3.6156, 3.5195, 3.5920],
        [3.6156, 2.8401, 2.3151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.13451330363750458 
model_pd.l_d.mean(): -22.996047973632812 
model_pd.lagr.mean(): -22.861534118652344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3419], device='cuda:0')), ('power', tensor([-23.3379], device='cuda:0'))])
epoch£º851	 i:0 	 global-step:17020	 l-p:0.13451330363750458
epoch£º851	 i:1 	 global-step:17021	 l-p:0.0669109970331192
epoch£º851	 i:2 	 global-step:17022	 l-p:0.1602810025215149
epoch£º851	 i:3 	 global-step:17023	 l-p:0.72502201795578
epoch£º851	 i:4 	 global-step:17024	 l-p:0.10942034423351288
epoch£º851	 i:5 	 global-step:17025	 l-p:0.14807292819023132
epoch£º851	 i:6 	 global-step:17026	 l-p:0.1709471046924591
epoch£º851	 i:7 	 global-step:17027	 l-p:0.09823465347290039
epoch£º851	 i:8 	 global-step:17028	 l-p:0.12909527122974396
epoch£º851	 i:9 	 global-step:17029	 l-p:0.11901049315929413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6356, 3.2507, 2.6665],
        [3.6356, 2.9787, 2.7992],
        [3.6356, 3.0296, 2.9298],
        [3.6356, 3.6355, 3.6356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.11070279031991959 
model_pd.l_d.mean(): -23.016813278198242 
model_pd.lagr.mean(): -22.906110763549805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2767], device='cuda:0')), ('power', tensor([-23.2935], device='cuda:0'))])
epoch£º852	 i:0 	 global-step:17040	 l-p:0.11070279031991959
epoch£º852	 i:1 	 global-step:17041	 l-p:0.010785181075334549
epoch£º852	 i:2 	 global-step:17042	 l-p:0.12106605619192123
epoch£º852	 i:3 	 global-step:17043	 l-p:0.12363342940807343
epoch£º852	 i:4 	 global-step:17044	 l-p:0.2731182277202606
epoch£º852	 i:5 	 global-step:17045	 l-p:0.11913558840751648
epoch£º852	 i:6 	 global-step:17046	 l-p:0.14853885769844055
epoch£º852	 i:7 	 global-step:17047	 l-p:0.13688106834888458
epoch£º852	 i:8 	 global-step:17048	 l-p:0.14507177472114563
epoch£º852	 i:9 	 global-step:17049	 l-p:0.13724833726882935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6303, 2.9381, 2.3097],
        [3.6303, 3.6091, 3.6284],
        [3.6303, 3.2950, 3.4066],
        [3.6303, 3.3497, 3.4696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.1370745748281479 
model_pd.l_d.mean(): -22.899900436401367 
model_pd.lagr.mean(): -22.76282501220703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2757], device='cuda:0')), ('power', tensor([-23.1756], device='cuda:0'))])
epoch£º853	 i:0 	 global-step:17060	 l-p:0.1370745748281479
epoch£º853	 i:1 	 global-step:17061	 l-p:0.12475325167179108
epoch£º853	 i:2 	 global-step:17062	 l-p:0.08009165525436401
epoch£º853	 i:3 	 global-step:17063	 l-p:0.09516695141792297
epoch£º853	 i:4 	 global-step:17064	 l-p:0.14027874171733856
epoch£º853	 i:5 	 global-step:17065	 l-p:0.1362307369709015
epoch£º853	 i:6 	 global-step:17066	 l-p:0.12726345658302307
epoch£º853	 i:7 	 global-step:17067	 l-p:0.1399756371974945
epoch£º853	 i:8 	 global-step:17068	 l-p:0.14588482677936554
epoch£º853	 i:9 	 global-step:17069	 l-p:-0.4303343892097473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5900, 2.8117, 2.3125],
        [3.5900, 3.5899, 3.5900],
        [3.5900, 3.0894, 2.4872],
        [3.5900, 3.5900, 3.5900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 0.10776893049478531 
model_pd.l_d.mean(): -23.26962661743164 
model_pd.lagr.mean(): -23.16185760498047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3275], device='cuda:0')), ('power', tensor([-23.5971], device='cuda:0'))])
epoch£º854	 i:0 	 global-step:17080	 l-p:0.10776893049478531
epoch£º854	 i:1 	 global-step:17081	 l-p:0.18449382483959198
epoch£º854	 i:2 	 global-step:17082	 l-p:0.09259381890296936
epoch£º854	 i:3 	 global-step:17083	 l-p:0.14964352548122406
epoch£º854	 i:4 	 global-step:17084	 l-p:0.12761066854000092
epoch£º854	 i:5 	 global-step:17085	 l-p:-0.03076777793467045
epoch£º854	 i:6 	 global-step:17086	 l-p:0.21077582240104675
epoch£º854	 i:7 	 global-step:17087	 l-p:0.12647123634815216
epoch£º854	 i:8 	 global-step:17088	 l-p:0.29349714517593384
epoch£º854	 i:9 	 global-step:17089	 l-p:0.09568227827548981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6202, 2.8639, 2.2588],
        [3.6202, 2.9875, 2.3618],
        [3.6202, 3.6097, 3.6197],
        [3.6202, 3.0738, 3.0498]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.14230525493621826 
model_pd.l_d.mean(): -23.585779190063477 
model_pd.lagr.mean(): -23.44347381591797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2101], device='cuda:0')), ('power', tensor([-23.7959], device='cuda:0'))])
epoch£º855	 i:0 	 global-step:17100	 l-p:0.14230525493621826
epoch£º855	 i:1 	 global-step:17101	 l-p:0.128226175904274
epoch£º855	 i:2 	 global-step:17102	 l-p:0.11939501762390137
epoch£º855	 i:3 	 global-step:17103	 l-p:0.06655165553092957
epoch£º855	 i:4 	 global-step:17104	 l-p:0.1283806562423706
epoch£º855	 i:5 	 global-step:17105	 l-p:0.13513043522834778
epoch£º855	 i:6 	 global-step:17106	 l-p:0.1337999552488327
epoch£º855	 i:7 	 global-step:17107	 l-p:0.19724522531032562
epoch£º855	 i:8 	 global-step:17108	 l-p:0.5097035765647888
epoch£º855	 i:9 	 global-step:17109	 l-p:0.07454615086317062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6377, 3.2041, 3.2737],
        [3.6377, 3.6259, 3.6370],
        [3.6377, 2.8646, 2.3541],
        [3.6377, 3.6377, 3.6377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.13317903876304626 
model_pd.l_d.mean(): -23.06865119934082 
model_pd.lagr.mean(): -22.93547248840332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2758], device='cuda:0')), ('power', tensor([-23.3444], device='cuda:0'))])
epoch£º856	 i:0 	 global-step:17120	 l-p:0.13317903876304626
epoch£º856	 i:1 	 global-step:17121	 l-p:0.13087397813796997
epoch£º856	 i:2 	 global-step:17122	 l-p:0.07616643607616425
epoch£º856	 i:3 	 global-step:17123	 l-p:0.03133344650268555
epoch£º856	 i:4 	 global-step:17124	 l-p:0.14606818556785583
epoch£º856	 i:5 	 global-step:17125	 l-p:0.142905592918396
epoch£º856	 i:6 	 global-step:17126	 l-p:-0.046206872910261154
epoch£º856	 i:7 	 global-step:17127	 l-p:0.20622722804546356
epoch£º856	 i:8 	 global-step:17128	 l-p:0.10815747082233429
epoch£º856	 i:9 	 global-step:17129	 l-p:0.18591372668743134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6589, 2.9088, 2.3005],
        [3.6589, 3.2138, 3.2751],
        [3.6589, 3.6232, 3.6545],
        [3.6589, 2.9255, 2.3045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.10796213150024414 
model_pd.l_d.mean(): -22.753498077392578 
model_pd.lagr.mean(): -22.645536422729492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3665], device='cuda:0')), ('power', tensor([-23.1200], device='cuda:0'))])
epoch£º857	 i:0 	 global-step:17140	 l-p:0.10796213150024414
epoch£º857	 i:1 	 global-step:17141	 l-p:0.10411763191223145
epoch£º857	 i:2 	 global-step:17142	 l-p:0.1464848816394806
epoch£º857	 i:3 	 global-step:17143	 l-p:0.16216939687728882
epoch£º857	 i:4 	 global-step:17144	 l-p:0.1480906456708908
epoch£º857	 i:5 	 global-step:17145	 l-p:-0.27306118607521057
epoch£º857	 i:6 	 global-step:17146	 l-p:1.52431058883667
epoch£º857	 i:7 	 global-step:17147	 l-p:0.1401735246181488
epoch£º857	 i:8 	 global-step:17148	 l-p:0.12653867900371552
epoch£º857	 i:9 	 global-step:17149	 l-p:0.17263644933700562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6724, 3.6715, 3.6723],
        [3.6724, 3.3260, 3.4338],
        [3.6724, 3.6723, 3.6724],
        [3.6724, 3.6583, 3.6714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.14462973177433014 
model_pd.l_d.mean(): -23.55472755432129 
model_pd.lagr.mean(): -23.410097122192383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1524], device='cuda:0')), ('power', tensor([-23.7072], device='cuda:0'))])
epoch£º858	 i:0 	 global-step:17160	 l-p:0.14462973177433014
epoch£º858	 i:1 	 global-step:17161	 l-p:0.12353416532278061
epoch£º858	 i:2 	 global-step:17162	 l-p:0.12557339668273926
epoch£º858	 i:3 	 global-step:17163	 l-p:0.1378299444913864
epoch£º858	 i:4 	 global-step:17164	 l-p:-0.8768652081489563
epoch£º858	 i:5 	 global-step:17165	 l-p:0.1943451315164566
epoch£º858	 i:6 	 global-step:17166	 l-p:0.10843086987733841
epoch£º858	 i:7 	 global-step:17167	 l-p:0.01280710194259882
epoch£º858	 i:8 	 global-step:17168	 l-p:0.14095067977905273
epoch£º858	 i:9 	 global-step:17169	 l-p:0.2054864764213562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6507, 2.9455, 2.6705],
        [3.6507, 3.6505, 3.6507],
        [3.6507, 3.2200, 2.6234],
        [3.6507, 3.5533, 3.6265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.1174568235874176 
model_pd.l_d.mean(): -23.50647735595703 
model_pd.lagr.mean(): -23.389020919799805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1776], device='cuda:0')), ('power', tensor([-23.6841], device='cuda:0'))])
epoch£º859	 i:0 	 global-step:17180	 l-p:0.1174568235874176
epoch£º859	 i:1 	 global-step:17181	 l-p:0.13582898676395416
epoch£º859	 i:2 	 global-step:17182	 l-p:-0.025969019159674644
epoch£º859	 i:3 	 global-step:17183	 l-p:0.11028427630662918
epoch£º859	 i:4 	 global-step:17184	 l-p:-0.014878234826028347
epoch£º859	 i:5 	 global-step:17185	 l-p:0.1392214149236679
epoch£º859	 i:6 	 global-step:17186	 l-p:0.18444865942001343
epoch£º859	 i:7 	 global-step:17187	 l-p:0.14656589925289154
epoch£º859	 i:8 	 global-step:17188	 l-p:0.13590814173221588
epoch£º859	 i:9 	 global-step:17189	 l-p:0.1721300482749939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6516, 3.5631, 3.6311],
        [3.6516, 3.6364, 3.6506],
        [3.6516, 3.3323, 3.4473],
        [3.6516, 3.6240, 3.6488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.01907147280871868 
model_pd.l_d.mean(): -23.462881088256836 
model_pd.lagr.mean(): -23.443809509277344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1943], device='cuda:0')), ('power', tensor([-23.6572], device='cuda:0'))])
epoch£º860	 i:0 	 global-step:17200	 l-p:0.01907147280871868
epoch£º860	 i:1 	 global-step:17201	 l-p:0.11528952419757843
epoch£º860	 i:2 	 global-step:17202	 l-p:0.127750962972641
epoch£º860	 i:3 	 global-step:17203	 l-p:0.14588256180286407
epoch£º860	 i:4 	 global-step:17204	 l-p:0.19681613147258759
epoch£º860	 i:5 	 global-step:17205	 l-p:-0.11414553225040436
epoch£º860	 i:6 	 global-step:17206	 l-p:0.128747820854187
epoch£º860	 i:7 	 global-step:17207	 l-p:0.27585312724113464
epoch£º860	 i:8 	 global-step:17208	 l-p:0.14362913370132446
epoch£º860	 i:9 	 global-step:17209	 l-p:0.13972041010856628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6429, 3.0219, 2.9023],
        [3.6429, 3.0107, 2.3817],
        [3.6429, 3.4689, 3.5755],
        [3.6429, 3.6200, 3.6408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.3135315179824829 
model_pd.l_d.mean(): -23.542795181274414 
model_pd.lagr.mean(): -23.229263305664062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1855], device='cuda:0')), ('power', tensor([-23.7283], device='cuda:0'))])
epoch£º861	 i:0 	 global-step:17220	 l-p:0.3135315179824829
epoch£º861	 i:1 	 global-step:17221	 l-p:0.0019982908852398396
epoch£º861	 i:2 	 global-step:17222	 l-p:0.12078554183244705
epoch£º861	 i:3 	 global-step:17223	 l-p:0.13944782316684723
epoch£º861	 i:4 	 global-step:17224	 l-p:0.1331913024187088
epoch£º861	 i:5 	 global-step:17225	 l-p:0.06474597752094269
epoch£º861	 i:6 	 global-step:17226	 l-p:0.12091559171676636
epoch£º861	 i:7 	 global-step:17227	 l-p:0.1183919683098793
epoch£º861	 i:8 	 global-step:17228	 l-p:0.1243835836648941
epoch£º861	 i:9 	 global-step:17229	 l-p:0.12632369995117188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6325, 2.8758, 2.4660],
        [3.6325, 3.2096, 3.2865],
        [3.6325, 3.6177, 3.6315],
        [3.6325, 3.6325, 3.6325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.13118509948253632 
model_pd.l_d.mean(): -23.432287216186523 
model_pd.lagr.mean(): -23.301101684570312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2050], device='cuda:0')), ('power', tensor([-23.6373], device='cuda:0'))])
epoch£º862	 i:0 	 global-step:17240	 l-p:0.13118509948253632
epoch£º862	 i:1 	 global-step:17241	 l-p:0.12960204482078552
epoch£º862	 i:2 	 global-step:17242	 l-p:0.07320909202098846
epoch£º862	 i:3 	 global-step:17243	 l-p:-0.17703275382518768
epoch£º862	 i:4 	 global-step:17244	 l-p:0.1563362181186676
epoch£º862	 i:5 	 global-step:17245	 l-p:0.357584685087204
epoch£º862	 i:6 	 global-step:17246	 l-p:0.06736782193183899
epoch£º862	 i:7 	 global-step:17247	 l-p:0.2118404656648636
epoch£º862	 i:8 	 global-step:17248	 l-p:0.1261569708585739
epoch£º862	 i:9 	 global-step:17249	 l-p:0.1266045719385147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6105, 3.5346, 3.5948],
        [3.6105, 3.5716, 3.6054],
        [3.6105, 3.4500, 3.5524],
        [3.6105, 2.8454, 2.4155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.14273878931999207 
model_pd.l_d.mean(): -22.92535400390625 
model_pd.lagr.mean(): -22.782615661621094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2624], device='cuda:0')), ('power', tensor([-23.1878], device='cuda:0'))])
epoch£º863	 i:0 	 global-step:17260	 l-p:0.14273878931999207
epoch£º863	 i:1 	 global-step:17261	 l-p:0.15452979505062103
epoch£º863	 i:2 	 global-step:17262	 l-p:0.13245227932929993
epoch£º863	 i:3 	 global-step:17263	 l-p:0.09976066648960114
epoch£º863	 i:4 	 global-step:17264	 l-p:0.24843280017375946
epoch£º863	 i:5 	 global-step:17265	 l-p:0.12498073279857635
epoch£º863	 i:6 	 global-step:17266	 l-p:0.09579767286777496
epoch£º863	 i:7 	 global-step:17267	 l-p:-0.04829670861363411
epoch£º863	 i:8 	 global-step:17268	 l-p:0.1475798636674881
epoch£º863	 i:9 	 global-step:17269	 l-p:0.034157805144786835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5957, 2.8324, 2.2276],
        [3.5957, 3.5956, 3.5957],
        [3.5957, 3.5957, 3.5957],
        [3.5957, 3.1425, 2.5479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.156050905585289 
model_pd.l_d.mean(): -22.8878116607666 
model_pd.lagr.mean(): -22.731760025024414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2558], device='cuda:0')), ('power', tensor([-23.1436], device='cuda:0'))])
epoch£º864	 i:0 	 global-step:17280	 l-p:0.156050905585289
epoch£º864	 i:1 	 global-step:17281	 l-p:0.12513746321201324
epoch£º864	 i:2 	 global-step:17282	 l-p:-0.010128965601325035
epoch£º864	 i:3 	 global-step:17283	 l-p:0.11253820359706879
epoch£º864	 i:4 	 global-step:17284	 l-p:0.13087762892246246
epoch£º864	 i:5 	 global-step:17285	 l-p:0.11618243902921677
epoch£º864	 i:6 	 global-step:17286	 l-p:0.11986703425645828
epoch£º864	 i:7 	 global-step:17287	 l-p:0.07581455260515213
epoch£º864	 i:8 	 global-step:17288	 l-p:0.1403394490480423
epoch£º864	 i:9 	 global-step:17289	 l-p:0.2069610357284546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6437, 3.6437, 3.6437],
        [3.6437, 3.6419, 3.6437],
        [3.6437, 3.6438, 3.6437],
        [3.6437, 3.6437, 3.6438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.12350543588399887 
model_pd.l_d.mean(): -23.380767822265625 
model_pd.lagr.mean(): -23.25726318359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1737], device='cuda:0')), ('power', tensor([-23.5545], device='cuda:0'))])
epoch£º865	 i:0 	 global-step:17300	 l-p:0.12350543588399887
epoch£º865	 i:1 	 global-step:17301	 l-p:0.15435446798801422
epoch£º865	 i:2 	 global-step:17302	 l-p:0.13510780036449432
epoch£º865	 i:3 	 global-step:17303	 l-p:0.15372073650360107
epoch£º865	 i:4 	 global-step:17304	 l-p:0.14231854677200317
epoch£º865	 i:5 	 global-step:17305	 l-p:0.1928350031375885
epoch£º865	 i:6 	 global-step:17306	 l-p:-0.04037018120288849
epoch£º865	 i:7 	 global-step:17307	 l-p:-0.038983553647994995
epoch£º865	 i:8 	 global-step:17308	 l-p:0.398173451423645
epoch£º865	 i:9 	 global-step:17309	 l-p:0.13707362115383148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6916, 3.0954, 2.4635],
        [3.6916, 3.3201, 2.7318],
        [3.6916, 3.5733, 3.6575],
        [3.6916, 3.6866, 3.6915]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.13892637193202972 
model_pd.l_d.mean(): -22.947240829467773 
model_pd.lagr.mean(): -22.80831527709961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2359], device='cuda:0')), ('power', tensor([-23.1832], device='cuda:0'))])
epoch£º866	 i:0 	 global-step:17320	 l-p:0.13892637193202972
epoch£º866	 i:1 	 global-step:17321	 l-p:0.1367805004119873
epoch£º866	 i:2 	 global-step:17322	 l-p:0.19218949973583221
epoch£º866	 i:3 	 global-step:17323	 l-p:0.13386984169483185
epoch£º866	 i:4 	 global-step:17324	 l-p:0.14933829009532928
epoch£º866	 i:5 	 global-step:17325	 l-p:0.14129750430583954
epoch£º866	 i:6 	 global-step:17326	 l-p:0.13377515971660614
epoch£º866	 i:7 	 global-step:17327	 l-p:0.1837853640317917
epoch£º866	 i:8 	 global-step:17328	 l-p:0.14872866868972778
epoch£º866	 i:9 	 global-step:17329	 l-p:0.10918301343917847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7385, 3.7386, 3.7385],
        [3.7385, 3.3775, 3.4794],
        [3.7385, 3.1166, 2.4763],
        [3.7385, 3.7322, 3.7383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.11615588515996933 
model_pd.l_d.mean(): -22.844745635986328 
model_pd.lagr.mean(): -22.72859001159668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2597], device='cuda:0')), ('power', tensor([-23.1044], device='cuda:0'))])
epoch£º867	 i:0 	 global-step:17340	 l-p:0.11615588515996933
epoch£º867	 i:1 	 global-step:17341	 l-p:0.14692950248718262
epoch£º867	 i:2 	 global-step:17342	 l-p:0.19426564872264862
epoch£º867	 i:3 	 global-step:17343	 l-p:0.11874080449342728
epoch£º867	 i:4 	 global-step:17344	 l-p:0.0666348859667778
epoch£º867	 i:5 	 global-step:17345	 l-p:0.20625968277454376
epoch£º867	 i:6 	 global-step:17346	 l-p:0.11767960339784622
epoch£º867	 i:7 	 global-step:17347	 l-p:0.1338602900505066
epoch£º867	 i:8 	 global-step:17348	 l-p:0.13473068177700043
epoch£º867	 i:9 	 global-step:17349	 l-p:0.13581936061382294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7108,  0.6343,  1.0000,  0.5661,
          1.0000,  0.8924, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2047,  0.1207,  1.0000,  0.0711,
          1.0000,  0.5894, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228]], device='cuda:0')
 pt:tensor([[3.7313, 3.0597, 2.8424],
        [3.7313, 3.0803, 2.4392],
        [3.7313, 3.2880, 3.3483],
        [3.7313, 3.0193, 2.7125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.1687011867761612 
model_pd.l_d.mean(): -23.527835845947266 
model_pd.lagr.mean(): -23.359134674072266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1834], device='cuda:0')), ('power', tensor([-23.7113], device='cuda:0'))])
epoch£º868	 i:0 	 global-step:17360	 l-p:0.1687011867761612
epoch£º868	 i:1 	 global-step:17361	 l-p:0.11098697781562805
epoch£º868	 i:2 	 global-step:17362	 l-p:0.12799784541130066
epoch£º868	 i:3 	 global-step:17363	 l-p:0.1421889215707779
epoch£º868	 i:4 	 global-step:17364	 l-p:0.17904707789421082
epoch£º868	 i:5 	 global-step:17365	 l-p:0.22645384073257446
epoch£º868	 i:6 	 global-step:17366	 l-p:0.2115233987569809
epoch£º868	 i:7 	 global-step:17367	 l-p:0.13072548806667328
epoch£º868	 i:8 	 global-step:17368	 l-p:0.09826651960611343
epoch£º868	 i:9 	 global-step:17369	 l-p:0.13218970596790314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7034, 3.2852, 2.6844],
        [3.7034, 3.6966, 3.7031],
        [3.7034, 2.9352, 2.3923],
        [3.7034, 3.3887, 3.5041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.11906365305185318 
model_pd.l_d.mean(): -22.919837951660156 
model_pd.lagr.mean(): -22.80077362060547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3233], device='cuda:0')), ('power', tensor([-23.2432], device='cuda:0'))])
epoch£º869	 i:0 	 global-step:17380	 l-p:0.11906365305185318
epoch£º869	 i:1 	 global-step:17381	 l-p:0.213457390666008
epoch£º869	 i:2 	 global-step:17382	 l-p:0.25267693400382996
epoch£º869	 i:3 	 global-step:17383	 l-p:0.13856098055839539
epoch£º869	 i:4 	 global-step:17384	 l-p:0.1196322813630104
epoch£º869	 i:5 	 global-step:17385	 l-p:0.14755864441394806
epoch£º869	 i:6 	 global-step:17386	 l-p:0.1294633448123932
epoch£º869	 i:7 	 global-step:17387	 l-p:0.24977964162826538
epoch£º869	 i:8 	 global-step:17388	 l-p:0.11900488287210464
epoch£º869	 i:9 	 global-step:17389	 l-p:0.18569450080394745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2005,  0.1173,  1.0000,  0.0687,
          1.0000,  0.5853, 31.6228]], device='cuda:0')
 pt:tensor([[3.6827, 3.2353, 3.2958],
        [3.6827, 3.0221, 2.8356],
        [3.6827, 3.0917, 3.0121],
        [3.6827, 3.2466, 3.3144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.12321247160434723 
model_pd.l_d.mean(): -23.763025283813477 
model_pd.lagr.mean(): -23.639812469482422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1160], device='cuda:0')), ('power', tensor([-23.8790], device='cuda:0'))])
epoch£º870	 i:0 	 global-step:17400	 l-p:0.12321247160434723
epoch£º870	 i:1 	 global-step:17401	 l-p:0.4327966272830963
epoch£º870	 i:2 	 global-step:17402	 l-p:-0.745551347732544
epoch£º870	 i:3 	 global-step:17403	 l-p:0.10971658676862717
epoch£º870	 i:4 	 global-step:17404	 l-p:0.15883682668209076
epoch£º870	 i:5 	 global-step:17405	 l-p:0.12904639542102814
epoch£º870	 i:6 	 global-step:17406	 l-p:0.12442948669195175
epoch£º870	 i:7 	 global-step:17407	 l-p:0.1346641331911087
epoch£º870	 i:8 	 global-step:17408	 l-p:0.11807157099246979
epoch£º870	 i:9 	 global-step:17409	 l-p:0.069999098777771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6386, 3.0177, 2.9025],
        [3.6386, 3.6058, 3.6348],
        [3.6386, 2.8687, 2.2768],
        [3.6386, 3.1211, 3.1283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.047165654599666595 
model_pd.l_d.mean(): -23.36130142211914 
model_pd.lagr.mean(): -23.314136505126953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2795], device='cuda:0')), ('power', tensor([-23.6408], device='cuda:0'))])
epoch£º871	 i:0 	 global-step:17420	 l-p:0.047165654599666595
epoch£º871	 i:1 	 global-step:17421	 l-p:0.14182862639427185
epoch£º871	 i:2 	 global-step:17422	 l-p:0.13447770476341248
epoch£º871	 i:3 	 global-step:17423	 l-p:0.12560011446475983
epoch£º871	 i:4 	 global-step:17424	 l-p:0.09931537508964539
epoch£º871	 i:5 	 global-step:17425	 l-p:0.11708495765924454
epoch£º871	 i:6 	 global-step:17426	 l-p:0.1253870129585266
epoch£º871	 i:7 	 global-step:17427	 l-p:0.13783687353134155
epoch£º871	 i:8 	 global-step:17428	 l-p:0.28919467329978943
epoch£º871	 i:9 	 global-step:17429	 l-p:-0.11655105650424957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5872, 3.5073, 3.5701],
        [3.5872, 2.8348, 2.2192],
        [3.5872, 3.5872, 3.5872],
        [3.5872, 3.5260, 3.5764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): -0.02560347504913807 
model_pd.l_d.mean(): -23.479379653930664 
model_pd.lagr.mean(): -23.50498390197754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2636], device='cuda:0')), ('power', tensor([-23.7430], device='cuda:0'))])
epoch£º872	 i:0 	 global-step:17440	 l-p:-0.02560347504913807
epoch£º872	 i:1 	 global-step:17441	 l-p:0.13506092131137848
epoch£º872	 i:2 	 global-step:17442	 l-p:0.28357169032096863
epoch£º872	 i:3 	 global-step:17443	 l-p:0.09198203682899475
epoch£º872	 i:4 	 global-step:17444	 l-p:0.1396549642086029
epoch£º872	 i:5 	 global-step:17445	 l-p:0.13566181063652039
epoch£º872	 i:6 	 global-step:17446	 l-p:0.15887556970119476
epoch£º872	 i:7 	 global-step:17447	 l-p:0.02911951392889023
epoch£º872	 i:8 	 global-step:17448	 l-p:0.16221004724502563
epoch£º872	 i:9 	 global-step:17449	 l-p:0.1070757582783699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6023, 3.3434, 3.4650],
        [3.6023, 3.5881, 3.6014],
        [3.6023, 2.9885, 2.8886],
        [3.6023, 3.0813, 3.0884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.10864545404911041 
model_pd.l_d.mean(): -22.832962036132812 
model_pd.lagr.mean(): -22.724315643310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3080], device='cuda:0')), ('power', tensor([-23.1410], device='cuda:0'))])
epoch£º873	 i:0 	 global-step:17460	 l-p:0.10864545404911041
epoch£º873	 i:1 	 global-step:17461	 l-p:0.1275182068347931
epoch£º873	 i:2 	 global-step:17462	 l-p:0.15610499680042267
epoch£º873	 i:3 	 global-step:17463	 l-p:0.21893684566020966
epoch£º873	 i:4 	 global-step:17464	 l-p:0.13588301837444305
epoch£º873	 i:5 	 global-step:17465	 l-p:0.14696821570396423
epoch£º873	 i:6 	 global-step:17466	 l-p:0.14625799655914307
epoch£º873	 i:7 	 global-step:17467	 l-p:-0.28195905685424805
epoch£º873	 i:8 	 global-step:17468	 l-p:0.09904486685991287
epoch£º873	 i:9 	 global-step:17469	 l-p:0.11816659569740295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5874, 3.5874, 3.5874],
        [3.5874, 3.5868, 3.5874],
        [3.5874, 3.5874, 3.5874],
        [3.5874, 2.9779, 2.3583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): 0.09718313068151474 
model_pd.l_d.mean(): -23.576467514038086 
model_pd.lagr.mean(): -23.479284286499023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1715], device='cuda:0')), ('power', tensor([-23.7480], device='cuda:0'))])
epoch£º874	 i:0 	 global-step:17480	 l-p:0.09718313068151474
epoch£º874	 i:1 	 global-step:17481	 l-p:0.13709641993045807
epoch£º874	 i:2 	 global-step:17482	 l-p:0.12297546118497849
epoch£º874	 i:3 	 global-step:17483	 l-p:0.13630397617816925
epoch£º874	 i:4 	 global-step:17484	 l-p:-0.13746118545532227
epoch£º874	 i:5 	 global-step:17485	 l-p:0.12517695128917694
epoch£º874	 i:6 	 global-step:17486	 l-p:0.17727242410182953
epoch£º874	 i:7 	 global-step:17487	 l-p:0.11720668524503708
epoch£º874	 i:8 	 global-step:17488	 l-p:0.09609507024288177
epoch£º874	 i:9 	 global-step:17489	 l-p:0.043601807206869125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6056, 3.5219, 3.5871],
        [3.6056, 3.2709, 3.3849],
        [3.6056, 3.6057, 3.6057],
        [3.6056, 2.8204, 2.2818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): -0.0005110549973323941 
model_pd.l_d.mean(): -23.57919692993164 
model_pd.lagr.mean(): -23.579708099365234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1568], device='cuda:0')), ('power', tensor([-23.7360], device='cuda:0'))])
epoch£º875	 i:0 	 global-step:17500	 l-p:-0.0005110549973323941
epoch£º875	 i:1 	 global-step:17501	 l-p:0.1462768167257309
epoch£º875	 i:2 	 global-step:17502	 l-p:0.32560959458351135
epoch£º875	 i:3 	 global-step:17503	 l-p:0.15846215188503265
epoch£º875	 i:4 	 global-step:17504	 l-p:0.07554449886083603
epoch£º875	 i:5 	 global-step:17505	 l-p:0.02182580903172493
epoch£º875	 i:6 	 global-step:17506	 l-p:0.12989787757396698
epoch£º875	 i:7 	 global-step:17507	 l-p:0.09750707447528839
epoch£º875	 i:8 	 global-step:17508	 l-p:0.1534028947353363
epoch£º875	 i:9 	 global-step:17509	 l-p:0.1679268628358841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6533, 2.8802, 2.2986],
        [3.6533, 3.2618, 2.6726],
        [3.6533, 3.2274, 2.6306],
        [3.6533, 3.6532, 3.6533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.1477772742509842 
model_pd.l_d.mean(): -22.85841941833496 
model_pd.lagr.mean(): -22.710641860961914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2848], device='cuda:0')), ('power', tensor([-23.1432], device='cuda:0'))])
epoch£º876	 i:0 	 global-step:17520	 l-p:0.1477772742509842
epoch£º876	 i:1 	 global-step:17521	 l-p:0.08493059873580933
epoch£º876	 i:2 	 global-step:17522	 l-p:0.002006359165534377
epoch£º876	 i:3 	 global-step:17523	 l-p:0.13097722828388214
epoch£º876	 i:4 	 global-step:17524	 l-p:-0.08882362395524979
epoch£º876	 i:5 	 global-step:17525	 l-p:0.16570869088172913
epoch£º876	 i:6 	 global-step:17526	 l-p:0.12767913937568665
epoch£º876	 i:7 	 global-step:17527	 l-p:0.13353203237056732
epoch£º876	 i:8 	 global-step:17528	 l-p:0.28674837946891785
epoch£º876	 i:9 	 global-step:17529	 l-p:0.1401893049478531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6880, 3.6852, 3.6879],
        [3.6880, 3.6880, 3.6880],
        [3.6880, 2.9355, 2.5308],
        [3.6880, 3.3071, 2.7163]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.34019747376441956 
model_pd.l_d.mean(): -22.674667358398438 
model_pd.lagr.mean(): -22.334470748901367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2417], device='cuda:0')), ('power', tensor([-22.9163], device='cuda:0'))])
epoch£º877	 i:0 	 global-step:17540	 l-p:0.34019747376441956
epoch£º877	 i:1 	 global-step:17541	 l-p:0.13185547292232513
epoch£º877	 i:2 	 global-step:17542	 l-p:0.16127721965312958
epoch£º877	 i:3 	 global-step:17543	 l-p:0.3320322036743164
epoch£º877	 i:4 	 global-step:17544	 l-p:0.15787816047668457
epoch£º877	 i:5 	 global-step:17545	 l-p:0.13687142729759216
epoch£º877	 i:6 	 global-step:17546	 l-p:0.12814469635486603
epoch£º877	 i:7 	 global-step:17547	 l-p:0.1264810711145401
epoch£º877	 i:8 	 global-step:17548	 l-p:0.12241397798061371
epoch£º877	 i:9 	 global-step:17549	 l-p:0.21655799448490143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7004, 3.7002, 3.7004],
        [3.7004, 3.2239, 2.6105],
        [3.7004, 3.2295, 2.6172],
        [3.7004, 3.7002, 3.7004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.1344863325357437 
model_pd.l_d.mean(): -23.536300659179688 
model_pd.lagr.mean(): -23.401813507080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1407], device='cuda:0')), ('power', tensor([-23.6770], device='cuda:0'))])
epoch£º878	 i:0 	 global-step:17560	 l-p:0.1344863325357437
epoch£º878	 i:1 	 global-step:17561	 l-p:0.17950411140918732
epoch£º878	 i:2 	 global-step:17562	 l-p:0.14794623851776123
epoch£º878	 i:3 	 global-step:17563	 l-p:0.20750735700130463
epoch£º878	 i:4 	 global-step:17564	 l-p:0.11104635149240494
epoch£º878	 i:5 	 global-step:17565	 l-p:0.12574423849582672
epoch£º878	 i:6 	 global-step:17566	 l-p:0.16577233374118805
epoch£º878	 i:7 	 global-step:17567	 l-p:0.24450761079788208
epoch£º878	 i:8 	 global-step:17568	 l-p:0.13110092282295227
epoch£º878	 i:9 	 global-step:17569	 l-p:0.22280104458332062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6980, 3.6977, 3.6980],
        [3.6980, 3.5325, 3.6365],
        [3.6980, 2.9272, 2.4239],
        [3.6980, 3.6364, 3.6870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.29694709181785583 
model_pd.l_d.mean(): -23.520885467529297 
model_pd.lagr.mean(): -23.22393798828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1674], device='cuda:0')), ('power', tensor([-23.6883], device='cuda:0'))])
epoch£º879	 i:0 	 global-step:17580	 l-p:0.29694709181785583
epoch£º879	 i:1 	 global-step:17581	 l-p:0.2262190729379654
epoch£º879	 i:2 	 global-step:17582	 l-p:0.22826886177062988
epoch£º879	 i:3 	 global-step:17583	 l-p:0.1442970633506775
epoch£º879	 i:4 	 global-step:17584	 l-p:0.1625284105539322
epoch£º879	 i:5 	 global-step:17585	 l-p:0.13550396263599396
epoch£º879	 i:6 	 global-step:17586	 l-p:0.1029272973537445
epoch£º879	 i:7 	 global-step:17587	 l-p:0.09370964020490646
epoch£º879	 i:8 	 global-step:17588	 l-p:0.13245442509651184
epoch£º879	 i:9 	 global-step:17589	 l-p:0.12190532684326172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7183, 3.0113, 2.7271],
        [3.7183, 3.5400, 3.6479],
        [3.7183, 3.1517, 3.1011],
        [3.7183, 3.7180, 3.7183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.20685327053070068 
model_pd.l_d.mean(): -23.303035736083984 
model_pd.lagr.mean(): -23.096181869506836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2748], device='cuda:0')), ('power', tensor([-23.5778], device='cuda:0'))])
epoch£º880	 i:0 	 global-step:17600	 l-p:0.20685327053070068
epoch£º880	 i:1 	 global-step:17601	 l-p:0.12054267525672913
epoch£º880	 i:2 	 global-step:17602	 l-p:0.12997636198997498
epoch£º880	 i:3 	 global-step:17603	 l-p:0.1511659175157547
epoch£º880	 i:4 	 global-step:17604	 l-p:0.13351412117481232
epoch£º880	 i:5 	 global-step:17605	 l-p:0.09297362715005875
epoch£º880	 i:6 	 global-step:17606	 l-p:0.2550695836544037
epoch£º880	 i:7 	 global-step:17607	 l-p:0.1255834698677063
epoch£º880	 i:8 	 global-step:17608	 l-p:0.26493480801582336
epoch£º880	 i:9 	 global-step:17609	 l-p:0.17551398277282715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6904, 3.6904, 3.6904],
        [3.6904, 3.4792, 3.5950],
        [3.6904, 3.1215, 3.0713],
        [3.6904, 3.1967, 2.5810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.12387875467538834 
model_pd.l_d.mean(): -22.74968719482422 
model_pd.lagr.mean(): -22.625808715820312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2242], device='cuda:0')), ('power', tensor([-22.9739], device='cuda:0'))])
epoch£º881	 i:0 	 global-step:17620	 l-p:0.12387875467538834
epoch£º881	 i:1 	 global-step:17621	 l-p:0.16715236008167267
epoch£º881	 i:2 	 global-step:17622	 l-p:0.12389924377202988
epoch£º881	 i:3 	 global-step:17623	 l-p:-1.9440860748291016
epoch£º881	 i:4 	 global-step:17624	 l-p:1.44588041305542
epoch£º881	 i:5 	 global-step:17625	 l-p:0.13718867301940918
epoch£º881	 i:6 	 global-step:17626	 l-p:0.11733077466487885
epoch£º881	 i:7 	 global-step:17627	 l-p:0.20695066452026367
epoch£º881	 i:8 	 global-step:17628	 l-p:0.12768353521823883
epoch£º881	 i:9 	 global-step:17629	 l-p:0.14313828945159912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6680, 3.1194, 3.0947],
        [3.6680, 2.9274, 2.5718],
        [3.6680, 3.6680, 3.6680],
        [3.6680, 3.6607, 3.6677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): -0.02810189686715603 
model_pd.l_d.mean(): -23.4337215423584 
model_pd.lagr.mean(): -23.461822509765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2269], device='cuda:0')), ('power', tensor([-23.6606], device='cuda:0'))])
epoch£º882	 i:0 	 global-step:17640	 l-p:-0.02810189686715603
epoch£º882	 i:1 	 global-step:17641	 l-p:0.13882218301296234
epoch£º882	 i:2 	 global-step:17642	 l-p:0.14809325337409973
epoch£º882	 i:3 	 global-step:17643	 l-p:-0.025420980527997017
epoch£º882	 i:4 	 global-step:17644	 l-p:0.15469688177108765
epoch£º882	 i:5 	 global-step:17645	 l-p:0.13038985431194305
epoch£º882	 i:6 	 global-step:17646	 l-p:-0.203071728348732
epoch£º882	 i:7 	 global-step:17647	 l-p:0.13056175410747528
epoch£º882	 i:8 	 global-step:17648	 l-p:0.11568178981542587
epoch£º882	 i:9 	 global-step:17649	 l-p:0.16112512350082397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6530, 3.6479, 3.6528],
        [3.6530, 3.6521, 3.6530],
        [3.6530, 3.6293, 3.6508],
        [3.6530, 3.3191, 3.4326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.12794479727745056 
model_pd.l_d.mean(): -23.589168548583984 
model_pd.lagr.mean(): -23.461223602294922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1902], device='cuda:0')), ('power', tensor([-23.7794], device='cuda:0'))])
epoch£º883	 i:0 	 global-step:17660	 l-p:0.12794479727745056
epoch£º883	 i:1 	 global-step:17661	 l-p:0.08932619541883469
epoch£º883	 i:2 	 global-step:17662	 l-p:0.12536640465259552
epoch£º883	 i:3 	 global-step:17663	 l-p:0.22258636355400085
epoch£º883	 i:4 	 global-step:17664	 l-p:0.1383499950170517
epoch£º883	 i:5 	 global-step:17665	 l-p:0.13622432947158813
epoch£º883	 i:6 	 global-step:17666	 l-p:0.0982210710644722
epoch£º883	 i:7 	 global-step:17667	 l-p:0.37612074613571167
epoch£º883	 i:8 	 global-step:17668	 l-p:0.10677515715360641
epoch£º883	 i:9 	 global-step:17669	 l-p:0.11165288835763931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6338, 3.6338, 3.6338],
        [3.6338, 2.9073, 2.6015],
        [3.6338, 3.6338, 3.6338],
        [3.6338, 2.8503, 2.3242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.04136626049876213 
model_pd.l_d.mean(): -23.16078758239746 
model_pd.lagr.mean(): -23.119421005249023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2384], device='cuda:0')), ('power', tensor([-23.3992], device='cuda:0'))])
epoch£º884	 i:0 	 global-step:17680	 l-p:0.04136626049876213
epoch£º884	 i:1 	 global-step:17681	 l-p:0.1675654500722885
epoch£º884	 i:2 	 global-step:17682	 l-p:0.13537049293518066
epoch£º884	 i:3 	 global-step:17683	 l-p:0.12798763811588287
epoch£º884	 i:4 	 global-step:17684	 l-p:0.07808759808540344
epoch£º884	 i:5 	 global-step:17685	 l-p:0.440755158662796
epoch£º884	 i:6 	 global-step:17686	 l-p:0.1383199542760849
epoch£º884	 i:7 	 global-step:17687	 l-p:0.1470607966184616
epoch£º884	 i:8 	 global-step:17688	 l-p:0.09331158548593521
epoch£º884	 i:9 	 global-step:17689	 l-p:0.13100053369998932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6400, 2.8592, 2.2885],
        [3.6400, 3.6400, 3.6400],
        [3.6400, 3.0446, 2.9683],
        [3.6400, 2.9413, 2.3083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.1345691829919815 
model_pd.l_d.mean(): -23.428279876708984 
model_pd.lagr.mean(): -23.293710708618164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2465], device='cuda:0')), ('power', tensor([-23.6748], device='cuda:0'))])
epoch£º885	 i:0 	 global-step:17700	 l-p:0.1345691829919815
epoch£º885	 i:1 	 global-step:17701	 l-p:0.12117014825344086
epoch£º885	 i:2 	 global-step:17702	 l-p:-0.024854516610503197
epoch£º885	 i:3 	 global-step:17703	 l-p:0.13776201009750366
epoch£º885	 i:4 	 global-step:17704	 l-p:0.1281387060880661
epoch£º885	 i:5 	 global-step:17705	 l-p:0.7325728535652161
epoch£º885	 i:6 	 global-step:17706	 l-p:0.11693128198385239
epoch£º885	 i:7 	 global-step:17707	 l-p:0.2905213534832001
epoch£º885	 i:8 	 global-step:17708	 l-p:0.11548595130443573
epoch£º885	 i:9 	 global-step:17709	 l-p:0.1380789577960968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6234, 3.6203, 3.6234],
        [3.6234, 3.3042, 3.4217],
        [3.6234, 3.6092, 3.6225],
        [3.6234, 3.4398, 3.5500]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.1410897672176361 
model_pd.l_d.mean(): -22.832962036132812 
model_pd.lagr.mean(): -22.691871643066406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3607], device='cuda:0')), ('power', tensor([-23.1937], device='cuda:0'))])
epoch£º886	 i:0 	 global-step:17720	 l-p:0.1410897672176361
epoch£º886	 i:1 	 global-step:17721	 l-p:-0.10276512801647186
epoch£º886	 i:2 	 global-step:17722	 l-p:0.14176040887832642
epoch£º886	 i:3 	 global-step:17723	 l-p:0.13461779057979584
epoch£º886	 i:4 	 global-step:17724	 l-p:0.11372410506010056
epoch£º886	 i:5 	 global-step:17725	 l-p:0.19568797945976257
epoch£º886	 i:6 	 global-step:17726	 l-p:0.11635512113571167
epoch£º886	 i:7 	 global-step:17727	 l-p:0.05629453435540199
epoch£º886	 i:8 	 global-step:17728	 l-p:0.14820066094398499
epoch£º886	 i:9 	 global-step:17729	 l-p:-1.3278188705444336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6036, 3.5612, 3.5978],
        [3.6036, 2.9468, 2.7863],
        [3.6036, 3.3708, 3.4910],
        [3.6036, 3.4202, 3.5305]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.10151836276054382 
model_pd.l_d.mean(): -23.776039123535156 
model_pd.lagr.mean(): -23.67452049255371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1352], device='cuda:0')), ('power', tensor([-23.9112], device='cuda:0'))])
epoch£º887	 i:0 	 global-step:17740	 l-p:0.10151836276054382
epoch£º887	 i:1 	 global-step:17741	 l-p:0.11687709391117096
epoch£º887	 i:2 	 global-step:17742	 l-p:0.10851730406284332
epoch£º887	 i:3 	 global-step:17743	 l-p:0.21681207418441772
epoch£º887	 i:4 	 global-step:17744	 l-p:0.1540614515542984
epoch£º887	 i:5 	 global-step:17745	 l-p:0.11518742889165878
epoch£º887	 i:6 	 global-step:17746	 l-p:0.1423211693763733
epoch£º887	 i:7 	 global-step:17747	 l-p:-4.819687843322754
epoch£º887	 i:8 	 global-step:17748	 l-p:0.16504904627799988
epoch£º887	 i:9 	 global-step:17749	 l-p:0.14573368430137634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[3.6362, 3.1257, 2.5136],
        [3.6362, 3.0761, 3.0433],
        [3.6362, 3.1093, 2.4944],
        [3.6362, 3.0834, 3.0586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.167589470744133 
model_pd.l_d.mean(): -22.83425521850586 
model_pd.lagr.mean(): -22.66666603088379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3486], device='cuda:0')), ('power', tensor([-23.1829], device='cuda:0'))])
epoch£º888	 i:0 	 global-step:17760	 l-p:0.167589470744133
epoch£º888	 i:1 	 global-step:17761	 l-p:0.16238082945346832
epoch£º888	 i:2 	 global-step:17762	 l-p:0.1264524906873703
epoch£º888	 i:3 	 global-step:17763	 l-p:0.3171451985836029
epoch£º888	 i:4 	 global-step:17764	 l-p:0.14627818763256073
epoch£º888	 i:5 	 global-step:17765	 l-p:0.06665229052305222
epoch£º888	 i:6 	 global-step:17766	 l-p:0.012720436789095402
epoch£º888	 i:7 	 global-step:17767	 l-p:0.08263883739709854
epoch£º888	 i:8 	 global-step:17768	 l-p:0.12633301317691803
epoch£º888	 i:9 	 global-step:17769	 l-p:0.15607808530330658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6593, 3.6593, 3.6593],
        [3.6593, 3.6593, 3.6593],
        [3.6593, 3.3096, 3.4193],
        [3.6593, 3.5396, 3.6248]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.15161149203777313 
model_pd.l_d.mean(): -23.741409301757812 
model_pd.lagr.mean(): -23.589797973632812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1699], device='cuda:0')), ('power', tensor([-23.9113], device='cuda:0'))])
epoch£º889	 i:0 	 global-step:17780	 l-p:0.15161149203777313
epoch£º889	 i:1 	 global-step:17781	 l-p:0.11205971240997314
epoch£º889	 i:2 	 global-step:17782	 l-p:0.12859030067920685
epoch£º889	 i:3 	 global-step:17783	 l-p:0.1373465657234192
epoch£º889	 i:4 	 global-step:17784	 l-p:0.16855975985527039
epoch£º889	 i:5 	 global-step:17785	 l-p:0.1312357485294342
epoch£º889	 i:6 	 global-step:17786	 l-p:-0.019856397062540054
epoch£º889	 i:7 	 global-step:17787	 l-p:0.133503258228302
epoch£º889	 i:8 	 global-step:17788	 l-p:0.1693609654903412
epoch£º889	 i:9 	 global-step:17789	 l-p:-0.35787081718444824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6698, 3.6698, 3.6698],
        [3.6698, 3.6698, 3.6698],
        [3.6698, 3.2187, 3.2800],
        [3.6698, 3.1959, 3.2410]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): 0.09866947680711746 
model_pd.l_d.mean(): -23.44849967956543 
model_pd.lagr.mean(): -23.349830627441406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2108], device='cuda:0')), ('power', tensor([-23.6593], device='cuda:0'))])
epoch£º890	 i:0 	 global-step:17800	 l-p:0.09866947680711746
epoch£º890	 i:1 	 global-step:17801	 l-p:-0.193570077419281
epoch£º890	 i:2 	 global-step:17802	 l-p:0.15701909363269806
epoch£º890	 i:3 	 global-step:17803	 l-p:-0.41653791069984436
epoch£º890	 i:4 	 global-step:17804	 l-p:0.19395974278450012
epoch£º890	 i:5 	 global-step:17805	 l-p:0.2406025379896164
epoch£º890	 i:6 	 global-step:17806	 l-p:0.13235725462436676
epoch£º890	 i:7 	 global-step:17807	 l-p:0.13076061010360718
epoch£º890	 i:8 	 global-step:17808	 l-p:0.10284014791250229
epoch£º890	 i:9 	 global-step:17809	 l-p:0.15941964089870453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7190, 3.6800, 3.7139],
        [3.7190, 3.0851, 2.4440],
        [3.7190, 3.6065, 3.6879],
        [3.7190, 3.7146, 3.7188]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.15788553655147552 
model_pd.l_d.mean(): -22.652406692504883 
model_pd.lagr.mean(): -22.494522094726562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3007], device='cuda:0')), ('power', tensor([-22.9532], device='cuda:0'))])
epoch£º891	 i:0 	 global-step:17820	 l-p:0.15788553655147552
epoch£º891	 i:1 	 global-step:17821	 l-p:0.1721726804971695
epoch£º891	 i:2 	 global-step:17822	 l-p:0.1212974339723587
epoch£º891	 i:3 	 global-step:17823	 l-p:0.13866442441940308
epoch£º891	 i:4 	 global-step:17824	 l-p:0.15594777464866638
epoch£º891	 i:5 	 global-step:17825	 l-p:0.15446853637695312
epoch£º891	 i:6 	 global-step:17826	 l-p:0.1534561961889267
epoch£º891	 i:7 	 global-step:17827	 l-p:0.08624451607465744
epoch£º891	 i:8 	 global-step:17828	 l-p:0.1309778243303299
epoch£º891	 i:9 	 global-step:17829	 l-p:0.15892431139945984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7346, 3.2038, 3.1937],
        [3.7346, 3.7346, 3.7346],
        [3.7346, 2.9772, 2.3740],
        [3.7346, 3.7264, 3.7342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.11642621457576752 
model_pd.l_d.mean(): -23.183486938476562 
model_pd.lagr.mean(): -23.067060470581055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2754], device='cuda:0')), ('power', tensor([-23.4589], device='cuda:0'))])
epoch£º892	 i:0 	 global-step:17840	 l-p:0.11642621457576752
epoch£º892	 i:1 	 global-step:17841	 l-p:0.11183810234069824
epoch£º892	 i:2 	 global-step:17842	 l-p:0.13238726556301117
epoch£º892	 i:3 	 global-step:17843	 l-p:0.13110722601413727
epoch£º892	 i:4 	 global-step:17844	 l-p:0.14314012229442596
epoch£º892	 i:5 	 global-step:17845	 l-p:0.23372535407543182
epoch£º892	 i:6 	 global-step:17846	 l-p:0.14709605276584625
epoch£º892	 i:7 	 global-step:17847	 l-p:0.20271939039230347
epoch£º892	 i:8 	 global-step:17848	 l-p:0.1252918541431427
epoch£º892	 i:9 	 global-step:17849	 l-p:0.1293051838874817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7155, 3.0594, 2.4174],
        [3.7155, 3.2902, 2.6852],
        [3.7155, 3.6908, 3.7131],
        [3.7155, 3.2803, 3.3495]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): 0.18701866269111633 
model_pd.l_d.mean(): -22.810176849365234 
model_pd.lagr.mean(): -22.623157501220703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3430], device='cuda:0')), ('power', tensor([-23.1532], device='cuda:0'))])
epoch£º893	 i:0 	 global-step:17860	 l-p:0.18701866269111633
epoch£º893	 i:1 	 global-step:17861	 l-p:0.23054102063179016
epoch£º893	 i:2 	 global-step:17862	 l-p:0.12511573731899261
epoch£º893	 i:3 	 global-step:17863	 l-p:0.1195555254817009
epoch£º893	 i:4 	 global-step:17864	 l-p:0.15022236108779907
epoch£º893	 i:5 	 global-step:17865	 l-p:0.11547078937292099
epoch£º893	 i:6 	 global-step:17866	 l-p:0.11145240813493729
epoch£º893	 i:7 	 global-step:17867	 l-p:0.13852199912071228
epoch£º893	 i:8 	 global-step:17868	 l-p:0.6247733235359192
epoch£º893	 i:9 	 global-step:17869	 l-p:0.14775756001472473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6762, 3.6753, 3.6762],
        [3.6762, 3.6653, 3.6756],
        [3.6762, 2.8983, 2.3929],
        [3.6762, 3.0098, 2.8222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.12222500890493393 
model_pd.l_d.mean(): -23.21364402770996 
model_pd.lagr.mean(): -23.091419219970703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2238], device='cuda:0')), ('power', tensor([-23.4374], device='cuda:0'))])
epoch£º894	 i:0 	 global-step:17880	 l-p:0.12222500890493393
epoch£º894	 i:1 	 global-step:17881	 l-p:-0.038143377751111984
epoch£º894	 i:2 	 global-step:17882	 l-p:0.12778867781162262
epoch£º894	 i:3 	 global-step:17883	 l-p:0.11933836340904236
epoch£º894	 i:4 	 global-step:17884	 l-p:0.1596788465976715
epoch£º894	 i:5 	 global-step:17885	 l-p:0.21674315631389618
epoch£º894	 i:6 	 global-step:17886	 l-p:-0.030992954969406128
epoch£º894	 i:7 	 global-step:17887	 l-p:0.1686183363199234
epoch£º894	 i:8 	 global-step:17888	 l-p:0.04417029395699501
epoch£º894	 i:9 	 global-step:17889	 l-p:0.16305960714817047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6555, 3.6555, 3.6555],
        [3.6555, 3.6317, 3.6533],
        [3.6555, 3.1492, 2.5352],
        [3.6555, 2.9869, 2.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): 0.168686181306839 
model_pd.l_d.mean(): -22.533891677856445 
model_pd.lagr.mean(): -22.365205764770508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3543], device='cuda:0')), ('power', tensor([-22.8882], device='cuda:0'))])
epoch£º895	 i:0 	 global-step:17900	 l-p:0.168686181306839
epoch£º895	 i:1 	 global-step:17901	 l-p:0.02950568124651909
epoch£º895	 i:2 	 global-step:17902	 l-p:0.22635018825531006
epoch£º895	 i:3 	 global-step:17903	 l-p:0.13079595565795898
epoch£º895	 i:4 	 global-step:17904	 l-p:0.13477206230163574
epoch£º895	 i:5 	 global-step:17905	 l-p:0.11445852369070053
epoch£º895	 i:6 	 global-step:17906	 l-p:0.13140545785427094
epoch£º895	 i:7 	 global-step:17907	 l-p:-0.10955561697483063
epoch£º895	 i:8 	 global-step:17908	 l-p:0.14969244599342346
epoch£º895	 i:9 	 global-step:17909	 l-p:0.05059608444571495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6560, 3.2164, 2.6154],
        [3.6560, 3.5619, 3.6334],
        [3.6560, 3.6560, 3.6560],
        [3.6560, 3.0015, 2.8383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.12927871942520142 
model_pd.l_d.mean(): -23.3496150970459 
model_pd.lagr.mean(): -23.2203369140625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2380], device='cuda:0')), ('power', tensor([-23.5876], device='cuda:0'))])
epoch£º896	 i:0 	 global-step:17920	 l-p:0.12927871942520142
epoch£º896	 i:1 	 global-step:17921	 l-p:0.05074206367135048
epoch£º896	 i:2 	 global-step:17922	 l-p:-0.013213081285357475
epoch£º896	 i:3 	 global-step:17923	 l-p:0.17473949491977692
epoch£º896	 i:4 	 global-step:17924	 l-p:0.14301542937755585
epoch£º896	 i:5 	 global-step:17925	 l-p:0.1783880591392517
epoch£º896	 i:6 	 global-step:17926	 l-p:0.20206096768379211
epoch£º896	 i:7 	 global-step:17927	 l-p:0.13936038315296173
epoch£º896	 i:8 	 global-step:17928	 l-p:0.09859512746334076
epoch£º896	 i:9 	 global-step:17929	 l-p:-0.012544393539428711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6710, 3.5872, 3.6524],
        [3.6710, 3.6628, 3.6706],
        [3.6710, 3.5842, 3.6513],
        [3.6710, 3.1121, 3.0784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.13643969595432281 
model_pd.l_d.mean(): -23.515161514282227 
model_pd.lagr.mean(): -23.378721237182617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1658], device='cuda:0')), ('power', tensor([-23.6810], device='cuda:0'))])
epoch£º897	 i:0 	 global-step:17940	 l-p:0.13643969595432281
epoch£º897	 i:1 	 global-step:17941	 l-p:-0.029505155980587006
epoch£º897	 i:2 	 global-step:17942	 l-p:0.11659932881593704
epoch£º897	 i:3 	 global-step:17943	 l-p:-0.18952032923698425
epoch£º897	 i:4 	 global-step:17944	 l-p:0.1322479546070099
epoch£º897	 i:5 	 global-step:17945	 l-p:0.130312979221344
epoch£º897	 i:6 	 global-step:17946	 l-p:0.14604145288467407
epoch£º897	 i:7 	 global-step:17947	 l-p:0.1991153359413147
epoch£º897	 i:8 	 global-step:17948	 l-p:0.15882974863052368
epoch£º897	 i:9 	 global-step:17949	 l-p:-0.10566873103380203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6602, 3.1081, 3.0830],
        [3.6602, 3.6590, 3.6601],
        [3.6602, 3.5763, 3.6416],
        [3.6602, 3.2705, 2.6799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.13542544841766357 
model_pd.l_d.mean(): -22.963848114013672 
model_pd.lagr.mean(): -22.82842254638672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2645], device='cuda:0')), ('power', tensor([-23.2283], device='cuda:0'))])
epoch£º898	 i:0 	 global-step:17960	 l-p:0.13542544841766357
epoch£º898	 i:1 	 global-step:17961	 l-p:0.14016973972320557
epoch£º898	 i:2 	 global-step:17962	 l-p:0.019726816564798355
epoch£º898	 i:3 	 global-step:17963	 l-p:0.14304393529891968
epoch£º898	 i:4 	 global-step:17964	 l-p:0.2705124318599701
epoch£º898	 i:5 	 global-step:17965	 l-p:0.1738618165254593
epoch£º898	 i:6 	 global-step:17966	 l-p:0.105813167989254
epoch£º898	 i:7 	 global-step:17967	 l-p:0.14507292211055756
epoch£º898	 i:8 	 global-step:17968	 l-p:0.09436295926570892
epoch£º898	 i:9 	 global-step:17969	 l-p:0.10796425491571426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6357, 2.8699, 2.2579],
        [3.6357, 2.8846, 2.5181],
        [3.6357, 3.0822, 3.0579],
        [3.6357, 3.6338, 3.6357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.14664313197135925 
model_pd.l_d.mean(): -23.2437686920166 
model_pd.lagr.mean(): -23.097126007080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2110], device='cuda:0')), ('power', tensor([-23.4548], device='cuda:0'))])
epoch£º899	 i:0 	 global-step:17980	 l-p:0.14664313197135925
epoch£º899	 i:1 	 global-step:17981	 l-p:0.1644073873758316
epoch£º899	 i:2 	 global-step:17982	 l-p:-1.2546501159667969
epoch£º899	 i:3 	 global-step:17983	 l-p:0.12328485399484634
epoch£º899	 i:4 	 global-step:17984	 l-p:0.12917959690093994
epoch£º899	 i:5 	 global-step:17985	 l-p:0.17098428308963776
epoch£º899	 i:6 	 global-step:17986	 l-p:0.14897511899471283
epoch£º899	 i:7 	 global-step:17987	 l-p:0.1538764089345932
epoch£º899	 i:8 	 global-step:17988	 l-p:0.1287432461977005
epoch£º899	 i:9 	 global-step:17989	 l-p:0.1282501220703125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6104, 2.9490, 2.7823],
        [3.6104, 3.3953, 3.5131],
        [3.6104, 3.6103, 3.6104],
        [3.6104, 3.4099, 3.5246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.14900986850261688 
model_pd.l_d.mean(): -23.61910629272461 
model_pd.lagr.mean(): -23.470096588134766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2038], device='cuda:0')), ('power', tensor([-23.8229], device='cuda:0'))])
epoch£º900	 i:0 	 global-step:18000	 l-p:0.14900986850261688
epoch£º900	 i:1 	 global-step:18001	 l-p:0.15816561877727509
epoch£º900	 i:2 	 global-step:18002	 l-p:0.15147298574447632
epoch£º900	 i:3 	 global-step:18003	 l-p:0.1566581279039383
epoch£º900	 i:4 	 global-step:18004	 l-p:0.10729323327541351
epoch£º900	 i:5 	 global-step:18005	 l-p:0.12456653267145157
epoch£º900	 i:6 	 global-step:18006	 l-p:0.09345323592424393
epoch£º900	 i:7 	 global-step:18007	 l-p:0.12403052300214767
epoch£º900	 i:8 	 global-step:18008	 l-p:0.0006952643161639571
epoch£º900	 i:9 	 global-step:18009	 l-p:0.11890929192304611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5900, 3.5096, 3.5729],
        [3.5900, 3.5820, 3.5896],
        [3.5900, 2.9367, 2.3110],
        [3.5900, 3.0756, 3.0937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.17648562788963318 
model_pd.l_d.mean(): -23.49120330810547 
model_pd.lagr.mean(): -23.31471824645996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2668], device='cuda:0')), ('power', tensor([-23.7580], device='cuda:0'))])
epoch£º901	 i:0 	 global-step:18020	 l-p:0.17648562788963318
epoch£º901	 i:1 	 global-step:18021	 l-p:0.1389954686164856
epoch£º901	 i:2 	 global-step:18022	 l-p:0.08261609077453613
epoch£º901	 i:3 	 global-step:18023	 l-p:0.06512140482664108
epoch£º901	 i:4 	 global-step:18024	 l-p:0.12950287759304047
epoch£º901	 i:5 	 global-step:18025	 l-p:0.10691837221384048
epoch£º901	 i:6 	 global-step:18026	 l-p:0.15256251394748688
epoch£º901	 i:7 	 global-step:18027	 l-p:-0.4499082863330841
epoch£º901	 i:8 	 global-step:18028	 l-p:0.15058457851409912
epoch£º901	 i:9 	 global-step:18029	 l-p:0.24497687816619873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6063, 3.6007, 3.6061],
        [3.6063, 3.0820, 3.0900],
        [3.6063, 2.8252, 2.2280],
        [3.6063, 3.4656, 3.5608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.1308290660381317 
model_pd.l_d.mean(): -22.939537048339844 
model_pd.lagr.mean(): -22.80870819091797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2541], device='cuda:0')), ('power', tensor([-23.1936], device='cuda:0'))])
epoch£º902	 i:0 	 global-step:18040	 l-p:0.1308290660381317
epoch£º902	 i:1 	 global-step:18041	 l-p:0.14057472348213196
epoch£º902	 i:2 	 global-step:18042	 l-p:0.04911039397120476
epoch£º902	 i:3 	 global-step:18043	 l-p:0.14827114343643188
epoch£º902	 i:4 	 global-step:18044	 l-p:0.4890783131122589
epoch£º902	 i:5 	 global-step:18045	 l-p:0.14406976103782654
epoch£º902	 i:6 	 global-step:18046	 l-p:0.04685453698039055
epoch£º902	 i:7 	 global-step:18047	 l-p:0.18557073175907135
epoch£º902	 i:8 	 global-step:18048	 l-p:0.08393850922584534
epoch£º902	 i:9 	 global-step:18049	 l-p:0.10711050033569336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6562, 3.5666, 3.6355],
        [3.6562, 3.1486, 2.5339],
        [3.6562, 3.2736, 3.3727],
        [3.6562, 3.3616, 3.4824]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.03133474290370941 
model_pd.l_d.mean(): -23.36104965209961 
model_pd.lagr.mean(): -23.329715728759766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2819], device='cuda:0')), ('power', tensor([-23.6429], device='cuda:0'))])
epoch£º903	 i:0 	 global-step:18060	 l-p:0.03133474290370941
epoch£º903	 i:1 	 global-step:18061	 l-p:0.12003665417432785
epoch£º903	 i:2 	 global-step:18062	 l-p:0.17457173764705658
epoch£º903	 i:3 	 global-step:18063	 l-p:0.16941973567008972
epoch£º903	 i:4 	 global-step:18064	 l-p:0.30326393246650696
epoch£º903	 i:5 	 global-step:18065	 l-p:0.1421186774969101
epoch£º903	 i:6 	 global-step:18066	 l-p:0.1297159194946289
epoch£º903	 i:7 	 global-step:18067	 l-p:0.27450984716415405
epoch£º903	 i:8 	 global-step:18068	 l-p:0.1233176589012146
epoch£º903	 i:9 	 global-step:18069	 l-p:0.15537786483764648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7178, 3.6708, 3.7109],
        [3.7178, 3.3449, 3.4458],
        [3.7178, 3.7178, 3.7178],
        [3.7178, 3.7100, 3.7175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.1117791160941124 
model_pd.l_d.mean(): -22.76741600036621 
model_pd.lagr.mean(): -22.655637741088867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2635], device='cuda:0')), ('power', tensor([-23.0309], device='cuda:0'))])
epoch£º904	 i:0 	 global-step:18080	 l-p:0.1117791160941124
epoch£º904	 i:1 	 global-step:18081	 l-p:0.13339845836162567
epoch£º904	 i:2 	 global-step:18082	 l-p:0.13431809842586517
epoch£º904	 i:3 	 global-step:18083	 l-p:0.14026668667793274
epoch£º904	 i:4 	 global-step:18084	 l-p:0.12379927188158035
epoch£º904	 i:5 	 global-step:18085	 l-p:0.22528982162475586
epoch£º904	 i:6 	 global-step:18086	 l-p:0.19385354220867157
epoch£º904	 i:7 	 global-step:18087	 l-p:0.11164787411689758
epoch£º904	 i:8 	 global-step:18088	 l-p:0.2167717069387436
epoch£º904	 i:9 	 global-step:18089	 l-p:0.12112294882535934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7322, 3.7307, 3.7321],
        [3.7322, 3.5193, 3.6356],
        [3.7322, 3.0126, 2.7055],
        [3.7322, 3.1621, 3.1106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.12944285571575165 
model_pd.l_d.mean(): -23.480539321899414 
model_pd.lagr.mean(): -23.351097106933594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1188], device='cuda:0')), ('power', tensor([-23.5993], device='cuda:0'))])
epoch£º905	 i:0 	 global-step:18100	 l-p:0.12944285571575165
epoch£º905	 i:1 	 global-step:18101	 l-p:0.1181253045797348
epoch£º905	 i:2 	 global-step:18102	 l-p:0.08288578689098358
epoch£º905	 i:3 	 global-step:18103	 l-p:0.24396708607673645
epoch£º905	 i:4 	 global-step:18104	 l-p:0.15666772425174713
epoch£º905	 i:5 	 global-step:18105	 l-p:0.13236390054225922
epoch£º905	 i:6 	 global-step:18106	 l-p:0.17442597448825836
epoch£º905	 i:7 	 global-step:18107	 l-p:0.1210351437330246
epoch£º905	 i:8 	 global-step:18108	 l-p:0.1191902682185173
epoch£º905	 i:9 	 global-step:18109	 l-p:0.1331155151128769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7445, 2.9781, 2.5032],
        [3.7445, 2.9746, 2.4724],
        [3.7445, 3.1136, 2.4690],
        [3.7445, 3.7439, 3.7445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.161397784948349 
model_pd.l_d.mean(): -23.27310562133789 
model_pd.lagr.mean(): -23.11170768737793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1770], device='cuda:0')), ('power', tensor([-23.4501], device='cuda:0'))])
epoch£º906	 i:0 	 global-step:18120	 l-p:0.161397784948349
epoch£º906	 i:1 	 global-step:18121	 l-p:0.15219418704509735
epoch£º906	 i:2 	 global-step:18122	 l-p:0.12467548996210098
epoch£º906	 i:3 	 global-step:18123	 l-p:0.20631133019924164
epoch£º906	 i:4 	 global-step:18124	 l-p:0.0813840851187706
epoch£º906	 i:5 	 global-step:18125	 l-p:0.13435691595077515
epoch£º906	 i:6 	 global-step:18126	 l-p:0.12733735144138336
epoch£º906	 i:7 	 global-step:18127	 l-p:0.16798236966133118
epoch£º906	 i:8 	 global-step:18128	 l-p:0.1075797900557518
epoch£º906	 i:9 	 global-step:18129	 l-p:0.13962410390377045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7230, 3.6723, 3.7152],
        [3.7230, 3.4763, 3.5968],
        [3.7230, 3.5727, 3.6714],
        [3.7230, 2.9560, 2.4960]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.21320581436157227 
model_pd.l_d.mean(): -22.92622184753418 
model_pd.lagr.mean(): -22.713016510009766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1943], device='cuda:0')), ('power', tensor([-23.1206], device='cuda:0'))])
epoch£º907	 i:0 	 global-step:18140	 l-p:0.21320581436157227
epoch£º907	 i:1 	 global-step:18141	 l-p:0.12340760231018066
epoch£º907	 i:2 	 global-step:18142	 l-p:0.25230759382247925
epoch£º907	 i:3 	 global-step:18143	 l-p:0.1164182797074318
epoch£º907	 i:4 	 global-step:18144	 l-p:0.15568368136882782
epoch£º907	 i:5 	 global-step:18145	 l-p:0.19110709428787231
epoch£º907	 i:6 	 global-step:18146	 l-p:0.12876369059085846
epoch£º907	 i:7 	 global-step:18147	 l-p:0.08958019316196442
epoch£º907	 i:8 	 global-step:18148	 l-p:0.13698086142539978
epoch£º907	 i:9 	 global-step:18149	 l-p:0.12174513190984726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1810,  0.1024,  1.0000,  0.0579,
          1.0000,  0.5657, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4788,  0.3746,  1.0000,  0.2931,
          1.0000,  0.7823, 31.6228]], device='cuda:0')
 pt:tensor([[3.7083, 3.1974, 2.5750],
        [3.7083, 3.3171, 3.4112],
        [3.7083, 2.9305, 2.3972],
        [3.7083, 2.9312, 2.4136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.20214305818080902 
model_pd.l_d.mean(): -22.760173797607422 
model_pd.lagr.mean(): -22.55803108215332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3332], device='cuda:0')), ('power', tensor([-23.0934], device='cuda:0'))])
epoch£º908	 i:0 	 global-step:18160	 l-p:0.20214305818080902
epoch£º908	 i:1 	 global-step:18161	 l-p:0.16667945683002472
epoch£º908	 i:2 	 global-step:18162	 l-p:0.24094931781291962
epoch£º908	 i:3 	 global-step:18163	 l-p:0.1249309629201889
epoch£º908	 i:4 	 global-step:18164	 l-p:0.14336588978767395
epoch£º908	 i:5 	 global-step:18165	 l-p:0.16546450555324554
epoch£º908	 i:6 	 global-step:18166	 l-p:0.14739936590194702
epoch£º908	 i:7 	 global-step:18167	 l-p:0.12346282601356506
epoch£º908	 i:8 	 global-step:18168	 l-p:0.13418053090572357
epoch£º908	 i:9 	 global-step:18169	 l-p:0.1539328247308731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7026, 3.6768, 3.7000],
        [3.7026, 3.7025, 3.7026],
        [3.7026, 2.9454, 2.3279],
        [3.7026, 3.7026, 3.7026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.12833650410175323 
model_pd.l_d.mean(): -23.412769317626953 
model_pd.lagr.mean(): -23.284433364868164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1805], device='cuda:0')), ('power', tensor([-23.5933], device='cuda:0'))])
epoch£º909	 i:0 	 global-step:18180	 l-p:0.12833650410175323
epoch£º909	 i:1 	 global-step:18181	 l-p:0.16638870537281036
epoch£º909	 i:2 	 global-step:18182	 l-p:0.11589210480451584
epoch£º909	 i:3 	 global-step:18183	 l-p:0.1578265279531479
epoch£º909	 i:4 	 global-step:18184	 l-p:0.12446895241737366
epoch£º909	 i:5 	 global-step:18185	 l-p:0.3381292521953583
epoch£º909	 i:6 	 global-step:18186	 l-p:0.15582969784736633
epoch£º909	 i:7 	 global-step:18187	 l-p:-0.2853256165981293
epoch£º909	 i:8 	 global-step:18188	 l-p:7.247988700866699
epoch£º909	 i:9 	 global-step:18189	 l-p:0.11654375493526459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6857, 3.6857, 3.6857],
        [3.6857, 3.6850, 3.6857],
        [3.6857, 3.6857, 3.6857],
        [3.6857, 2.9093, 2.4296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.13973303139209747 
model_pd.l_d.mean(): -22.788789749145508 
model_pd.lagr.mean(): -22.649057388305664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2951], device='cuda:0')), ('power', tensor([-23.0838], device='cuda:0'))])
epoch£º910	 i:0 	 global-step:18200	 l-p:0.13973303139209747
epoch£º910	 i:1 	 global-step:18201	 l-p:1.9038138389587402
epoch£º910	 i:2 	 global-step:18202	 l-p:0.1679524928331375
epoch£º910	 i:3 	 global-step:18203	 l-p:0.13203273713588715
epoch£º910	 i:4 	 global-step:18204	 l-p:0.13937275111675262
epoch£º910	 i:5 	 global-step:18205	 l-p:0.3558718264102936
epoch£º910	 i:6 	 global-step:18206	 l-p:0.10574770718812943
epoch£º910	 i:7 	 global-step:18207	 l-p:0.11930613964796066
epoch£º910	 i:8 	 global-step:18208	 l-p:0.17027081549167633
epoch£º910	 i:9 	 global-step:18209	 l-p:-0.15636897087097168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6806, 3.2601, 3.3412],
        [3.6806, 3.6805, 3.6806],
        [3.6806, 2.9091, 2.3066],
        [3.6806, 3.1874, 3.2189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.19362804293632507 
model_pd.l_d.mean(): -23.61666488647461 
model_pd.lagr.mean(): -23.423036575317383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1693], device='cuda:0')), ('power', tensor([-23.7860], device='cuda:0'))])
epoch£º911	 i:0 	 global-step:18220	 l-p:0.19362804293632507
epoch£º911	 i:1 	 global-step:18221	 l-p:0.1580527275800705
epoch£º911	 i:2 	 global-step:18222	 l-p:0.12163285911083221
epoch£º911	 i:3 	 global-step:18223	 l-p:0.1304401308298111
epoch£º911	 i:4 	 global-step:18224	 l-p:0.12547360360622406
epoch£º911	 i:5 	 global-step:18225	 l-p:0.1405237913131714
epoch£º911	 i:6 	 global-step:18226	 l-p:-0.36312925815582275
epoch£º911	 i:7 	 global-step:18227	 l-p:0.1264553815126419
epoch£º911	 i:8 	 global-step:18228	 l-p:0.17812621593475342
epoch£º911	 i:9 	 global-step:18229	 l-p:-0.0002199315931648016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6558, 3.0163, 2.3816],
        [3.6558, 3.5115, 3.6081],
        [3.6558, 2.9001, 2.5220],
        [3.6558, 3.6463, 3.6553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.24853046238422394 
model_pd.l_d.mean(): -22.848936080932617 
model_pd.lagr.mean(): -22.600404739379883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3597], device='cuda:0')), ('power', tensor([-23.2086], device='cuda:0'))])
epoch£º912	 i:0 	 global-step:18240	 l-p:0.24853046238422394
epoch£º912	 i:1 	 global-step:18241	 l-p:0.08741731196641922
epoch£º912	 i:2 	 global-step:18242	 l-p:0.044845521450042725
epoch£º912	 i:3 	 global-step:18243	 l-p:0.1508883237838745
epoch£º912	 i:4 	 global-step:18244	 l-p:0.1447625756263733
epoch£º912	 i:5 	 global-step:18245	 l-p:0.0971594974398613
epoch£º912	 i:6 	 global-step:18246	 l-p:0.13474726676940918
epoch£º912	 i:7 	 global-step:18247	 l-p:0.6194578409194946
epoch£º912	 i:8 	 global-step:18248	 l-p:0.13782796263694763
epoch£º912	 i:9 	 global-step:18249	 l-p:0.1258983016014099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6845, 3.6844, 3.6845],
        [3.6845, 2.9321, 2.5539],
        [3.6845, 3.6837, 3.6845],
        [3.6845, 2.9022, 2.3838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.10219182819128036 
model_pd.l_d.mean(): -23.24667739868164 
model_pd.lagr.mean(): -23.144485473632812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2501], device='cuda:0')), ('power', tensor([-23.4968], device='cuda:0'))])
epoch£º913	 i:0 	 global-step:18260	 l-p:0.10219182819128036
epoch£º913	 i:1 	 global-step:18261	 l-p:0.13622793555259705
epoch£º913	 i:2 	 global-step:18262	 l-p:0.1228964775800705
epoch£º913	 i:3 	 global-step:18263	 l-p:0.13006962835788727
epoch£º913	 i:4 	 global-step:18264	 l-p:-0.11154523491859436
epoch£º913	 i:5 	 global-step:18265	 l-p:0.13841073215007782
epoch£º913	 i:6 	 global-step:18266	 l-p:0.14494489133358002
epoch£º913	 i:7 	 global-step:18267	 l-p:0.2253108024597168
epoch£º913	 i:8 	 global-step:18268	 l-p:0.13631446659564972
epoch£º913	 i:9 	 global-step:18269	 l-p:-0.07947442680597305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6548, 3.0375, 2.9362],
        [3.6548, 3.6546, 3.6548],
        [3.6548, 3.5602, 3.6321],
        [3.6548, 2.9653, 2.7460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.12640196084976196 
model_pd.l_d.mean(): -23.175588607788086 
model_pd.lagr.mean(): -23.04918670654297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2562], device='cuda:0')), ('power', tensor([-23.4317], device='cuda:0'))])
epoch£º914	 i:0 	 global-step:18280	 l-p:0.12640196084976196
epoch£º914	 i:1 	 global-step:18281	 l-p:0.1621100902557373
epoch£º914	 i:2 	 global-step:18282	 l-p:0.16064360737800598
epoch£º914	 i:3 	 global-step:18283	 l-p:0.1317719668149948
epoch£º914	 i:4 	 global-step:18284	 l-p:0.07007613778114319
epoch£º914	 i:5 	 global-step:18285	 l-p:0.14712519943714142
epoch£º914	 i:6 	 global-step:18286	 l-p:0.15064065158367157
epoch£º914	 i:7 	 global-step:18287	 l-p:0.12425185739994049
epoch£º914	 i:8 	 global-step:18288	 l-p:0.15459132194519043
epoch£º914	 i:9 	 global-step:18289	 l-p:0.1477140337228775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6482, 3.5047, 3.6010],
        [3.6482, 3.6442, 3.6480],
        [3.6482, 3.6481, 3.6482],
        [3.6482, 3.6399, 3.6478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.30943602323532104 
model_pd.l_d.mean(): -23.609960556030273 
model_pd.lagr.mean(): -23.30052375793457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2171], device='cuda:0')), ('power', tensor([-23.8271], device='cuda:0'))])
epoch£º915	 i:0 	 global-step:18300	 l-p:0.30943602323532104
epoch£º915	 i:1 	 global-step:18301	 l-p:0.12412197142839432
epoch£º915	 i:2 	 global-step:18302	 l-p:0.18271876871585846
epoch£º915	 i:3 	 global-step:18303	 l-p:0.13316959142684937
epoch£º915	 i:4 	 global-step:18304	 l-p:0.10187260061502457
epoch£º915	 i:5 	 global-step:18305	 l-p:0.12111013382673264
epoch£º915	 i:6 	 global-step:18306	 l-p:0.15058651566505432
epoch£º915	 i:7 	 global-step:18307	 l-p:0.00831028912216425
epoch£º915	 i:8 	 global-step:18308	 l-p:0.10705222934484482
epoch£º915	 i:9 	 global-step:18309	 l-p:0.12199964374303818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6546, 3.2635, 2.6724],
        [3.6546, 3.6546, 3.6546],
        [3.6546, 3.6071, 3.6476],
        [3.6546, 2.8739, 2.4045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.1319623589515686 
model_pd.l_d.mean(): -23.151338577270508 
model_pd.lagr.mean(): -23.019376754760742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1981], device='cuda:0')), ('power', tensor([-23.3495], device='cuda:0'))])
epoch£º916	 i:0 	 global-step:18320	 l-p:0.1319623589515686
epoch£º916	 i:1 	 global-step:18321	 l-p:0.03996225818991661
epoch£º916	 i:2 	 global-step:18322	 l-p:0.09312249720096588
epoch£º916	 i:3 	 global-step:18323	 l-p:0.1932397186756134
epoch£º916	 i:4 	 global-step:18324	 l-p:0.12155667692422867
epoch£º916	 i:5 	 global-step:18325	 l-p:0.24992850422859192
epoch£º916	 i:6 	 global-step:18326	 l-p:0.1789514124393463
epoch£º916	 i:7 	 global-step:18327	 l-p:0.13314615190029144
epoch£º916	 i:8 	 global-step:18328	 l-p:0.12290788441896439
epoch£º916	 i:9 	 global-step:18329	 l-p:-0.07135113328695297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6653, 3.5850, 3.6482],
        [3.6653, 3.6653, 3.6653],
        [3.6653, 3.1371, 2.5168],
        [3.6653, 3.5329, 3.6244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.12721872329711914 
model_pd.l_d.mean(): -23.713085174560547 
model_pd.lagr.mean(): -23.585866928100586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1160], device='cuda:0')), ('power', tensor([-23.8291], device='cuda:0'))])
epoch£º917	 i:0 	 global-step:18340	 l-p:0.12721872329711914
epoch£º917	 i:1 	 global-step:18341	 l-p:-0.09229415655136108
epoch£º917	 i:2 	 global-step:18342	 l-p:0.12037161737680435
epoch£º917	 i:3 	 global-step:18343	 l-p:0.04495057091116905
epoch£º917	 i:4 	 global-step:18344	 l-p:0.13266780972480774
epoch£º917	 i:5 	 global-step:18345	 l-p:0.1195182204246521
epoch£º917	 i:6 	 global-step:18346	 l-p:0.2366948276758194
epoch£º917	 i:7 	 global-step:18347	 l-p:0.14581437408924103
epoch£º917	 i:8 	 global-step:18348	 l-p:0.11516761779785156
epoch£º917	 i:9 	 global-step:18349	 l-p:0.07013475149869919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6562, 3.6562, 3.6562],
        [3.6562, 3.6308, 3.6537],
        [3.6562, 3.6562, 3.6562],
        [3.6562, 2.8766, 2.2784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): 0.0005578040727414191 
model_pd.l_d.mean(): -22.204824447631836 
model_pd.lagr.mean(): -22.204267501831055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3801], device='cuda:0')), ('power', tensor([-22.5849], device='cuda:0'))])
epoch£º918	 i:0 	 global-step:18360	 l-p:0.0005578040727414191
epoch£º918	 i:1 	 global-step:18361	 l-p:0.05815596133470535
epoch£º918	 i:2 	 global-step:18362	 l-p:0.09427104890346527
epoch£º918	 i:3 	 global-step:18363	 l-p:0.18844066560268402
epoch£º918	 i:4 	 global-step:18364	 l-p:0.15016911923885345
epoch£º918	 i:5 	 global-step:18365	 l-p:0.1317998319864273
epoch£º918	 i:6 	 global-step:18366	 l-p:0.13083025813102722
epoch£º918	 i:7 	 global-step:18367	 l-p:0.14687387645244598
epoch£º918	 i:8 	 global-step:18368	 l-p:0.1617252677679062
epoch£º918	 i:9 	 global-step:18369	 l-p:0.11790474504232407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228]], device='cuda:0')
 pt:tensor([[3.6664, 3.2202, 2.6156],
        [3.6664, 2.9286, 2.2956],
        [3.6664, 2.9928, 2.3540],
        [3.6664, 3.2376, 3.3155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.13485877215862274 
model_pd.l_d.mean(): -22.86004066467285 
model_pd.lagr.mean(): -22.725181579589844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2639], device='cuda:0')), ('power', tensor([-23.1239], device='cuda:0'))])
epoch£º919	 i:0 	 global-step:18380	 l-p:0.13485877215862274
epoch£º919	 i:1 	 global-step:18381	 l-p:0.15933039784431458
epoch£º919	 i:2 	 global-step:18382	 l-p:0.1428893655538559
epoch£º919	 i:3 	 global-step:18383	 l-p:0.1123182475566864
epoch£º919	 i:4 	 global-step:18384	 l-p:0.14384005963802338
epoch£º919	 i:5 	 global-step:18385	 l-p:0.11814574897289276
epoch£º919	 i:6 	 global-step:18386	 l-p:0.08588355779647827
epoch£º919	 i:7 	 global-step:18387	 l-p:0.09732115268707275
epoch£º919	 i:8 	 global-step:18388	 l-p:-0.18020707368850708
epoch£º919	 i:9 	 global-step:18389	 l-p:0.24012526869773865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6282, 2.8360, 2.3171],
        [3.6282, 3.6281, 3.6282],
        [3.6282, 3.1853, 3.2568],
        [3.6282, 3.0701, 3.0455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.12474964559078217 
model_pd.l_d.mean(): -23.116670608520508 
model_pd.lagr.mean(): -22.991920471191406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2753], device='cuda:0')), ('power', tensor([-23.3920], device='cuda:0'))])
epoch£º920	 i:0 	 global-step:18400	 l-p:0.12474964559078217
epoch£º920	 i:1 	 global-step:18401	 l-p:0.04388931021094322
epoch£º920	 i:2 	 global-step:18402	 l-p:0.14206689596176147
epoch£º920	 i:3 	 global-step:18403	 l-p:0.2604931890964508
epoch£º920	 i:4 	 global-step:18404	 l-p:0.11867973953485489
epoch£º920	 i:5 	 global-step:18405	 l-p:0.20461082458496094
epoch£º920	 i:6 	 global-step:18406	 l-p:0.13324101269245148
epoch£º920	 i:7 	 global-step:18407	 l-p:0.10090306401252747
epoch£º920	 i:8 	 global-step:18408	 l-p:0.1599404513835907
epoch£º920	 i:9 	 global-step:18409	 l-p:-0.12621179223060608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6251, 3.6251, 3.6252],
        [3.6251, 2.8493, 2.2382],
        [3.6251, 3.4795, 3.5769],
        [3.6251, 3.6252, 3.6251]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.1260344684123993 
model_pd.l_d.mean(): -22.749197006225586 
model_pd.lagr.mean(): -22.6231632232666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3444], device='cuda:0')), ('power', tensor([-23.0936], device='cuda:0'))])
epoch£º921	 i:0 	 global-step:18420	 l-p:0.1260344684123993
epoch£º921	 i:1 	 global-step:18421	 l-p:0.08959376066923141
epoch£º921	 i:2 	 global-step:18422	 l-p:0.15028640627861023
epoch£º921	 i:3 	 global-step:18423	 l-p:-0.9090710878372192
epoch£º921	 i:4 	 global-step:18424	 l-p:0.13817061483860016
epoch£º921	 i:5 	 global-step:18425	 l-p:0.20773237943649292
epoch£º921	 i:6 	 global-step:18426	 l-p:0.12538760900497437
epoch£º921	 i:7 	 global-step:18427	 l-p:0.12218412011861801
epoch£º921	 i:8 	 global-step:18428	 l-p:0.025830669328570366
epoch£º921	 i:9 	 global-step:18429	 l-p:0.13918064534664154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6584, 3.6584, 3.6584],
        [3.6584, 3.2635, 3.3589],
        [3.6584, 3.1465, 2.5300],
        [3.6584, 3.0126, 2.3765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.14102868735790253 
model_pd.l_d.mean(): -23.488815307617188 
model_pd.lagr.mean(): -23.34778594970703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2092], device='cuda:0')), ('power', tensor([-23.6980], device='cuda:0'))])
epoch£º922	 i:0 	 global-step:18440	 l-p:0.14102868735790253
epoch£º922	 i:1 	 global-step:18441	 l-p:-0.0035136316437274218
epoch£º922	 i:2 	 global-step:18442	 l-p:0.12982751429080963
epoch£º922	 i:3 	 global-step:18443	 l-p:0.10640229284763336
epoch£º922	 i:4 	 global-step:18444	 l-p:0.04017171636223793
epoch£º922	 i:5 	 global-step:18445	 l-p:0.17709815502166748
epoch£º922	 i:6 	 global-step:18446	 l-p:0.16394591331481934
epoch£º922	 i:7 	 global-step:18447	 l-p:0.1461665779352188
epoch£º922	 i:8 	 global-step:18448	 l-p:0.02842879667878151
epoch£º922	 i:9 	 global-step:18449	 l-p:0.18442779779434204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6810, 2.8989, 2.4075],
        [3.6810, 3.1685, 3.1849],
        [3.6810, 3.3805, 3.5011],
        [3.6810, 3.6765, 3.6808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): -0.45903241634368896 
model_pd.l_d.mean(): -23.453758239746094 
model_pd.lagr.mean(): -23.912790298461914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1786], device='cuda:0')), ('power', tensor([-23.6324], device='cuda:0'))])
epoch£º923	 i:0 	 global-step:18460	 l-p:-0.45903241634368896
epoch£º923	 i:1 	 global-step:18461	 l-p:0.14804421365261078
epoch£º923	 i:2 	 global-step:18462	 l-p:0.1618858128786087
epoch£º923	 i:3 	 global-step:18463	 l-p:0.15032650530338287
epoch£º923	 i:4 	 global-step:18464	 l-p:0.21196509897708893
epoch£º923	 i:5 	 global-step:18465	 l-p:0.12424306571483612
epoch£º923	 i:6 	 global-step:18466	 l-p:0.09940261393785477
epoch£º923	 i:7 	 global-step:18467	 l-p:0.13511022925376892
epoch£º923	 i:8 	 global-step:18468	 l-p:0.19166557490825653
epoch£º923	 i:9 	 global-step:18469	 l-p:0.14252500236034393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7301, 3.3019, 3.3775],
        [3.7301, 2.9530, 2.4445],
        [3.7301, 3.6173, 3.6991],
        [3.7301, 3.4291, 3.5487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.2557750344276428 
model_pd.l_d.mean(): -23.30619239807129 
model_pd.lagr.mean(): -23.050416946411133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2222], device='cuda:0')), ('power', tensor([-23.5284], device='cuda:0'))])
epoch£º924	 i:0 	 global-step:18480	 l-p:0.2557750344276428
epoch£º924	 i:1 	 global-step:18481	 l-p:0.11106645315885544
epoch£º924	 i:2 	 global-step:18482	 l-p:0.129288911819458
epoch£º924	 i:3 	 global-step:18483	 l-p:0.12368481606245041
epoch£º924	 i:4 	 global-step:18484	 l-p:0.12020673602819443
epoch£º924	 i:5 	 global-step:18485	 l-p:0.12203454971313477
epoch£º924	 i:6 	 global-step:18486	 l-p:0.14650963246822357
epoch£º924	 i:7 	 global-step:18487	 l-p:0.14923790097236633
epoch£º924	 i:8 	 global-step:18488	 l-p:0.12957309186458588
epoch£º924	 i:9 	 global-step:18489	 l-p:0.18285514414310455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7166, 3.7165, 3.7166],
        [3.7166, 3.0004, 2.3584],
        [3.7166, 2.9412, 2.3518],
        [3.7166, 2.9391, 2.4476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.14250993728637695 
model_pd.l_d.mean(): -23.450300216674805 
model_pd.lagr.mean(): -23.307790756225586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1866], device='cuda:0')), ('power', tensor([-23.6369], device='cuda:0'))])
epoch£º925	 i:0 	 global-step:18500	 l-p:0.14250993728637695
epoch£º925	 i:1 	 global-step:18501	 l-p:0.21086090803146362
epoch£º925	 i:2 	 global-step:18502	 l-p:0.12098140269517899
epoch£º925	 i:3 	 global-step:18503	 l-p:0.14017684757709503
epoch£º925	 i:4 	 global-step:18504	 l-p:0.10757579654455185
epoch£º925	 i:5 	 global-step:18505	 l-p:0.9825940728187561
epoch£º925	 i:6 	 global-step:18506	 l-p:0.12495177239179611
epoch£º925	 i:7 	 global-step:18507	 l-p:0.11753339320421219
epoch£º925	 i:8 	 global-step:18508	 l-p:0.12737521529197693
epoch£º925	 i:9 	 global-step:18509	 l-p:0.21672557294368744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6650, 3.6649, 3.6650],
        [3.6650, 3.0053, 2.8409],
        [3.6650, 2.9901, 2.7996],
        [3.6650, 2.9064, 2.2811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.12788641452789307 
model_pd.l_d.mean(): -23.13292694091797 
model_pd.lagr.mean(): -23.005041122436523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1914], device='cuda:0')), ('power', tensor([-23.3244], device='cuda:0'))])
epoch£º926	 i:0 	 global-step:18520	 l-p:0.12788641452789307
epoch£º926	 i:1 	 global-step:18521	 l-p:0.1267237663269043
epoch£º926	 i:2 	 global-step:18522	 l-p:0.019513454288244247
epoch£º926	 i:3 	 global-step:18523	 l-p:0.2915985882282257
epoch£º926	 i:4 	 global-step:18524	 l-p:0.17078444361686707
epoch£º926	 i:5 	 global-step:18525	 l-p:0.04892141371965408
epoch£º926	 i:6 	 global-step:18526	 l-p:0.07059604674577713
epoch£º926	 i:7 	 global-step:18527	 l-p:0.10966417193412781
epoch£º926	 i:8 	 global-step:18528	 l-p:0.16023963689804077
epoch£º926	 i:9 	 global-step:18529	 l-p:0.17440184950828552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6727, 3.6715, 3.6727],
        [3.6727, 3.5291, 3.6255],
        [3.6727, 3.6652, 3.6724],
        [3.6727, 3.5046, 3.6102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.12498931586742401 
model_pd.l_d.mean(): -22.345325469970703 
model_pd.lagr.mean(): -22.2203369140625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2871], device='cuda:0')), ('power', tensor([-22.6324], device='cuda:0'))])
epoch£º927	 i:0 	 global-step:18540	 l-p:0.12498931586742401
epoch£º927	 i:1 	 global-step:18541	 l-p:-0.41978004574775696
epoch£º927	 i:2 	 global-step:18542	 l-p:0.126743882894516
epoch£º927	 i:3 	 global-step:18543	 l-p:0.1273115575313568
epoch£º927	 i:4 	 global-step:18544	 l-p:-0.013558096252381802
epoch£º927	 i:5 	 global-step:18545	 l-p:0.1939927637577057
epoch£º927	 i:6 	 global-step:18546	 l-p:0.1577530950307846
epoch£º927	 i:7 	 global-step:18547	 l-p:0.15284034609794617
epoch£º927	 i:8 	 global-step:18548	 l-p:-0.8891436457633972
epoch£º927	 i:9 	 global-step:18549	 l-p:0.11846385896205902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6890, 3.6890, 3.6890],
        [3.6890, 3.4771, 3.5942],
        [3.6890, 3.6889, 3.6890],
        [3.6890, 3.6858, 3.6889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.3469580411911011 
model_pd.l_d.mean(): -23.406570434570312 
model_pd.lagr.mean(): -23.059612274169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1676], device='cuda:0')), ('power', tensor([-23.5742], device='cuda:0'))])
epoch£º928	 i:0 	 global-step:18560	 l-p:0.3469580411911011
epoch£º928	 i:1 	 global-step:18561	 l-p:0.15364429354667664
epoch£º928	 i:2 	 global-step:18562	 l-p:0.14210520684719086
epoch£º928	 i:3 	 global-step:18563	 l-p:0.11675550043582916
epoch£º928	 i:4 	 global-step:18564	 l-p:0.1523236483335495
epoch£º928	 i:5 	 global-step:18565	 l-p:0.10638649016618729
epoch£º928	 i:6 	 global-step:18566	 l-p:0.40470439195632935
epoch£º928	 i:7 	 global-step:18567	 l-p:0.26770147681236267
epoch£º928	 i:8 	 global-step:18568	 l-p:0.1312180459499359
epoch£º928	 i:9 	 global-step:18569	 l-p:0.1521238088607788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7086, 3.7086, 3.7086],
        [3.7086, 3.6170, 3.6870],
        [3.7086, 3.0100, 2.7668],
        [3.7086, 3.7085, 3.7086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.1318686604499817 
model_pd.l_d.mean(): -23.530181884765625 
model_pd.lagr.mean(): -23.398313522338867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1531], device='cuda:0')), ('power', tensor([-23.6833], device='cuda:0'))])
epoch£º929	 i:0 	 global-step:18580	 l-p:0.1318686604499817
epoch£º929	 i:1 	 global-step:18581	 l-p:0.318773478269577
epoch£º929	 i:2 	 global-step:18582	 l-p:0.14832083880901337
epoch£º929	 i:3 	 global-step:18583	 l-p:0.15196287631988525
epoch£º929	 i:4 	 global-step:18584	 l-p:0.3512207269668579
epoch£º929	 i:5 	 global-step:18585	 l-p:0.13183251023292542
epoch£º929	 i:6 	 global-step:18586	 l-p:0.1379881352186203
epoch£º929	 i:7 	 global-step:18587	 l-p:0.10304935276508331
epoch£º929	 i:8 	 global-step:18588	 l-p:0.3520967662334442
epoch£º929	 i:9 	 global-step:18589	 l-p:0.12618938088417053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6871, 3.6721, 3.6861],
        [3.6871, 2.9114, 2.3063],
        [3.6871, 3.6502, 3.6825],
        [3.6871, 3.6850, 3.6871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.16514228284358978 
model_pd.l_d.mean(): -23.487342834472656 
model_pd.lagr.mean(): -23.322200775146484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1851], device='cuda:0')), ('power', tensor([-23.6724], device='cuda:0'))])
epoch£º930	 i:0 	 global-step:18600	 l-p:0.16514228284358978
epoch£º930	 i:1 	 global-step:18601	 l-p:0.1093188226222992
epoch£º930	 i:2 	 global-step:18602	 l-p:0.1290167272090912
epoch£º930	 i:3 	 global-step:18603	 l-p:0.13675729930400848
epoch£º930	 i:4 	 global-step:18604	 l-p:0.14322923123836517
epoch£º930	 i:5 	 global-step:18605	 l-p:0.13021427392959595
epoch£º930	 i:6 	 global-step:18606	 l-p:0.1318829506635666
epoch£º930	 i:7 	 global-step:18607	 l-p:0.1536615937948227
epoch£º930	 i:8 	 global-step:18608	 l-p:-0.0042741200886666775
epoch£º930	 i:9 	 global-step:18609	 l-p:0.0200938880443573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6542, 3.6502, 3.6541],
        [3.6542, 2.9821, 2.8002],
        [3.6542, 3.6542, 3.6542],
        [3.6542, 3.4380, 3.5563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.20136234164237976 
model_pd.l_d.mean(): -23.328893661499023 
model_pd.lagr.mean(): -23.127531051635742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2067], device='cuda:0')), ('power', tensor([-23.5356], device='cuda:0'))])
epoch£º931	 i:0 	 global-step:18620	 l-p:0.20136234164237976
epoch£º931	 i:1 	 global-step:18621	 l-p:0.32925039529800415
epoch£º931	 i:2 	 global-step:18622	 l-p:0.05805151164531708
epoch£º931	 i:3 	 global-step:18623	 l-p:0.0808737725019455
epoch£º931	 i:4 	 global-step:18624	 l-p:0.11780467629432678
epoch£º931	 i:5 	 global-step:18625	 l-p:0.12537908554077148
epoch£º931	 i:6 	 global-step:18626	 l-p:-0.09731822460889816
epoch£º931	 i:7 	 global-step:18627	 l-p:0.13124896585941315
epoch£º931	 i:8 	 global-step:18628	 l-p:0.1419549435377121
epoch£º931	 i:9 	 global-step:18629	 l-p:0.10631045699119568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6638, 2.8797, 2.2841],
        [3.6638, 3.5008, 3.6048],
        [3.6638, 3.1793, 3.2217],
        [3.6638, 3.6383, 3.6613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.13356776535511017 
model_pd.l_d.mean(): -22.62405014038086 
model_pd.lagr.mean(): -22.490482330322266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3370], device='cuda:0')), ('power', tensor([-22.9610], device='cuda:0'))])
epoch£º932	 i:0 	 global-step:18640	 l-p:0.13356776535511017
epoch£º932	 i:1 	 global-step:18641	 l-p:0.06391382217407227
epoch£º932	 i:2 	 global-step:18642	 l-p:0.025551866739988327
epoch£º932	 i:3 	 global-step:18643	 l-p:0.15769201517105103
epoch£º932	 i:4 	 global-step:18644	 l-p:0.09387791156768799
epoch£º932	 i:5 	 global-step:18645	 l-p:0.2885919511318207
epoch£º932	 i:6 	 global-step:18646	 l-p:0.12351946532726288
epoch£º932	 i:7 	 global-step:18647	 l-p:0.1539248824119568
epoch£º932	 i:8 	 global-step:18648	 l-p:0.13120339810848236
epoch£º932	 i:9 	 global-step:18649	 l-p:0.15772569179534912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6577, 2.8760, 2.2718],
        [3.6577, 3.0981, 3.0719],
        [3.6577, 2.9405, 2.6716],
        [3.6577, 3.6577, 3.6577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): 0.049688585102558136 
model_pd.l_d.mean(): -23.553977966308594 
model_pd.lagr.mean(): -23.504289627075195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2073], device='cuda:0')), ('power', tensor([-23.7613], device='cuda:0'))])
epoch£º933	 i:0 	 global-step:18660	 l-p:0.049688585102558136
epoch£º933	 i:1 	 global-step:18661	 l-p:0.22257143259048462
epoch£º933	 i:2 	 global-step:18662	 l-p:0.1280602216720581
epoch£º933	 i:3 	 global-step:18663	 l-p:0.12347457557916641
epoch£º933	 i:4 	 global-step:18664	 l-p:0.23121272027492523
epoch£º933	 i:5 	 global-step:18665	 l-p:-0.18742592632770538
epoch£º933	 i:6 	 global-step:18666	 l-p:0.12030446529388428
epoch£º933	 i:7 	 global-step:18667	 l-p:0.13037213683128357
epoch£º933	 i:8 	 global-step:18668	 l-p:-0.0415085107088089
epoch£º933	 i:9 	 global-step:18669	 l-p:0.13439205288887024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6782, 3.6782, 3.6782],
        [3.6782, 3.6782, 3.6782],
        [3.6782, 3.3659, 3.4857],
        [3.6782, 3.4498, 3.5699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.13861940801143646 
model_pd.l_d.mean(): -23.19303321838379 
model_pd.lagr.mean(): -23.054414749145508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2327], device='cuda:0')), ('power', tensor([-23.4257], device='cuda:0'))])
epoch£º934	 i:0 	 global-step:18680	 l-p:0.13861940801143646
epoch£º934	 i:1 	 global-step:18681	 l-p:0.1283121109008789
epoch£º934	 i:2 	 global-step:18682	 l-p:0.14048615097999573
epoch£º934	 i:3 	 global-step:18683	 l-p:0.11700136214494705
epoch£º934	 i:4 	 global-step:18684	 l-p:0.1796165406703949
epoch£º934	 i:5 	 global-step:18685	 l-p:0.053762633353471756
epoch£º934	 i:6 	 global-step:18686	 l-p:-0.03241604566574097
epoch£º934	 i:7 	 global-step:18687	 l-p:0.11299795657396317
epoch£º934	 i:8 	 global-step:18688	 l-p:0.21712933480739594
epoch£º934	 i:9 	 global-step:18689	 l-p:-0.015388750471174717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6761, 2.8875, 2.3774],
        [3.6761, 3.6493, 3.6734],
        [3.6761, 3.6617, 3.6751],
        [3.6761, 3.3722, 3.4932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): -0.016501817852258682 
model_pd.l_d.mean(): -23.469240188598633 
model_pd.lagr.mean(): -23.485742568969727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2191], device='cuda:0')), ('power', tensor([-23.6883], device='cuda:0'))])
epoch£º935	 i:0 	 global-step:18700	 l-p:-0.016501817852258682
epoch£º935	 i:1 	 global-step:18701	 l-p:0.11964961886405945
epoch£º935	 i:2 	 global-step:18702	 l-p:0.11996078491210938
epoch£º935	 i:3 	 global-step:18703	 l-p:0.12643621861934662
epoch£º935	 i:4 	 global-step:18704	 l-p:0.17994052171707153
epoch£º935	 i:5 	 global-step:18705	 l-p:0.12844859063625336
epoch£º935	 i:6 	 global-step:18706	 l-p:0.5772128105163574
epoch£º935	 i:7 	 global-step:18707	 l-p:0.17561081051826477
epoch£º935	 i:8 	 global-step:18708	 l-p:2.4306979179382324
epoch£º935	 i:9 	 global-step:18709	 l-p:0.13321815431118011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7004, 3.4849, 3.6028],
        [3.7004, 3.6388, 3.6896],
        [3.7004, 3.0723, 2.9549],
        [3.7004, 3.0871, 2.4479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.16417960822582245 
model_pd.l_d.mean(): -23.018779754638672 
model_pd.lagr.mean(): -22.85460090637207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2146], device='cuda:0')), ('power', tensor([-23.2334], device='cuda:0'))])
epoch£º936	 i:0 	 global-step:18720	 l-p:0.16417960822582245
epoch£º936	 i:1 	 global-step:18721	 l-p:0.16073603928089142
epoch£º936	 i:2 	 global-step:18722	 l-p:0.13575519621372223
epoch£º936	 i:3 	 global-step:18723	 l-p:0.1261110007762909
epoch£º936	 i:4 	 global-step:18724	 l-p:0.12275651842355728
epoch£º936	 i:5 	 global-step:18725	 l-p:0.2385585755109787
epoch£º936	 i:6 	 global-step:18726	 l-p:0.15030275285243988
epoch£º936	 i:7 	 global-step:18727	 l-p:0.117058664560318
epoch£º936	 i:8 	 global-step:18728	 l-p:0.11659776419401169
epoch£º936	 i:9 	 global-step:18729	 l-p:0.15355221927165985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7357, 3.1664, 3.1221],
        [3.7357, 3.0489, 2.4007],
        [3.7357, 3.2138, 3.2197],
        [3.7357, 3.7351, 3.7357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.17017769813537598 
model_pd.l_d.mean(): -23.599605560302734 
model_pd.lagr.mean(): -23.429428100585938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1427], device='cuda:0')), ('power', tensor([-23.7423], device='cuda:0'))])
epoch£º937	 i:0 	 global-step:18740	 l-p:0.17017769813537598
epoch£º937	 i:1 	 global-step:18741	 l-p:0.15985146164894104
epoch£º937	 i:2 	 global-step:18742	 l-p:0.1260480433702469
epoch£º937	 i:3 	 global-step:18743	 l-p:0.11059165000915527
epoch£º937	 i:4 	 global-step:18744	 l-p:0.1915365308523178
epoch£º937	 i:5 	 global-step:18745	 l-p:0.12081047147512436
epoch£º937	 i:6 	 global-step:18746	 l-p:0.17803636193275452
epoch£º937	 i:7 	 global-step:18747	 l-p:0.1508086919784546
epoch£º937	 i:8 	 global-step:18748	 l-p:0.11455171555280685
epoch£º937	 i:9 	 global-step:18749	 l-p:0.0795450359582901
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7469, 3.7468, 3.7469],
        [3.7469, 3.6908, 3.7376],
        [3.7469, 3.0640, 2.8475],
        [3.7469, 3.4910, 3.6127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.11938552558422089 
model_pd.l_d.mean(): -22.813186645507812 
model_pd.lagr.mean(): -22.693801879882812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1910], device='cuda:0')), ('power', tensor([-23.0042], device='cuda:0'))])
epoch£º938	 i:0 	 global-step:18760	 l-p:0.11938552558422089
epoch£º938	 i:1 	 global-step:18761	 l-p:0.12531988322734833
epoch£º938	 i:2 	 global-step:18762	 l-p:0.11931376904249191
epoch£º938	 i:3 	 global-step:18763	 l-p:0.20518243312835693
epoch£º938	 i:4 	 global-step:18764	 l-p:0.1310460865497589
epoch£º938	 i:5 	 global-step:18765	 l-p:0.11517215520143509
epoch£º938	 i:6 	 global-step:18766	 l-p:0.1814103126525879
epoch£º938	 i:7 	 global-step:18767	 l-p:0.1466735452413559
epoch£º938	 i:8 	 global-step:18768	 l-p:0.19358287751674652
epoch£º938	 i:9 	 global-step:18769	 l-p:0.1406422257423401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6033,  0.5098,  1.0000,  0.4308,
          1.0000,  0.8450, 31.6228]], device='cuda:0')
 pt:tensor([[3.7333, 3.2060, 2.5758],
        [3.7333, 2.9509, 2.4104],
        [3.7333, 3.0751, 2.9061],
        [3.7333, 2.9839, 2.3515]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.13411256670951843 
model_pd.l_d.mean(): -23.23392677307129 
model_pd.lagr.mean(): -23.09981346130371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2201], device='cuda:0')), ('power', tensor([-23.4540], device='cuda:0'))])
epoch£º939	 i:0 	 global-step:18780	 l-p:0.13411256670951843
epoch£º939	 i:1 	 global-step:18781	 l-p:0.14994600415229797
epoch£º939	 i:2 	 global-step:18782	 l-p:0.11716628074645996
epoch£º939	 i:3 	 global-step:18783	 l-p:0.13516710698604584
epoch£º939	 i:4 	 global-step:18784	 l-p:0.22269511222839355
epoch£º939	 i:5 	 global-step:18785	 l-p:0.1418965458869934
epoch£º939	 i:6 	 global-step:18786	 l-p:0.2154180109500885
epoch£º939	 i:7 	 global-step:18787	 l-p:0.13850723206996918
epoch£º939	 i:8 	 global-step:18788	 l-p:0.12979505956172943
epoch£º939	 i:9 	 global-step:18789	 l-p:0.20428606867790222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7031, 3.7011, 3.7030],
        [3.7031, 2.9158, 2.3752],
        [3.7031, 3.6763, 3.7004],
        [3.7031, 3.5583, 3.6552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.12205635011196136 
model_pd.l_d.mean(): -22.94818878173828 
model_pd.lagr.mean(): -22.82613182067871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2815], device='cuda:0')), ('power', tensor([-23.2296], device='cuda:0'))])
epoch£º940	 i:0 	 global-step:18800	 l-p:0.12205635011196136
epoch£º940	 i:1 	 global-step:18801	 l-p:0.14228267967700958
epoch£º940	 i:2 	 global-step:18802	 l-p:0.1302548497915268
epoch£º940	 i:3 	 global-step:18803	 l-p:-0.5325164794921875
epoch£º940	 i:4 	 global-step:18804	 l-p:0.13607722520828247
epoch£º940	 i:5 	 global-step:18805	 l-p:0.09500975161790848
epoch£º940	 i:6 	 global-step:18806	 l-p:0.14016175270080566
epoch£º940	 i:7 	 global-step:18807	 l-p:0.07948985695838928
epoch£º940	 i:8 	 global-step:18808	 l-p:0.2357204258441925
epoch£º940	 i:9 	 global-step:18809	 l-p:0.24665629863739014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6473, 2.8619, 2.2577],
        [3.6473, 3.6470, 3.6473],
        [3.6473, 3.0427, 2.9655],
        [3.6473, 2.8668, 2.4282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): 0.07060185074806213 
model_pd.l_d.mean(): -22.946697235107422 
model_pd.lagr.mean(): -22.876094818115234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2745], device='cuda:0')), ('power', tensor([-23.2212], device='cuda:0'))])
epoch£º941	 i:0 	 global-step:18820	 l-p:0.07060185074806213
epoch£º941	 i:1 	 global-step:18821	 l-p:0.13055486977100372
epoch£º941	 i:2 	 global-step:18822	 l-p:0.13588370382785797
epoch£º941	 i:3 	 global-step:18823	 l-p:0.2064506709575653
epoch£º941	 i:4 	 global-step:18824	 l-p:0.14098012447357178
epoch£º941	 i:5 	 global-step:18825	 l-p:0.15685170888900757
epoch£º941	 i:6 	 global-step:18826	 l-p:0.07969758659601212
epoch£º941	 i:7 	 global-step:18827	 l-p:0.11799701303243637
epoch£º941	 i:8 	 global-step:18828	 l-p:-0.23696675896644592
epoch£º941	 i:9 	 global-step:18829	 l-p:0.20760871469974518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6313, 3.2217, 3.3133],
        [3.6313, 3.1830, 2.5814],
        [3.6313, 2.8346, 2.3236],
        [3.6313, 3.6216, 3.6308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.12212114036083221 
model_pd.l_d.mean(): -23.359954833984375 
model_pd.lagr.mean(): -23.23783302307129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1896], device='cuda:0')), ('power', tensor([-23.5495], device='cuda:0'))])
epoch£º942	 i:0 	 global-step:18840	 l-p:0.12212114036083221
epoch£º942	 i:1 	 global-step:18841	 l-p:0.14837829768657684
epoch£º942	 i:2 	 global-step:18842	 l-p:0.3331141173839569
epoch£º942	 i:3 	 global-step:18843	 l-p:0.15202829241752625
epoch£º942	 i:4 	 global-step:18844	 l-p:0.13456740975379944
epoch£º942	 i:5 	 global-step:18845	 l-p:0.08080142736434937
epoch£º942	 i:6 	 global-step:18846	 l-p:0.03031625598669052
epoch£º942	 i:7 	 global-step:18847	 l-p:0.12500321865081787
epoch£º942	 i:8 	 global-step:18848	 l-p:0.1395425945520401
epoch£º942	 i:9 	 global-step:18849	 l-p:0.16905933618545532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6189, 3.6189, 3.6189],
        [3.6189, 3.0114, 2.9343],
        [3.6189, 3.6189, 3.6189],
        [3.6189, 3.6189, 3.6189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.12417300790548325 
model_pd.l_d.mean(): -23.40032196044922 
model_pd.lagr.mean(): -23.27614974975586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1679], device='cuda:0')), ('power', tensor([-23.5682], device='cuda:0'))])
epoch£º943	 i:0 	 global-step:18860	 l-p:0.12417300790548325
epoch£º943	 i:1 	 global-step:18861	 l-p:0.1381215900182724
epoch£º943	 i:2 	 global-step:18862	 l-p:0.13772279024124146
epoch£º943	 i:3 	 global-step:18863	 l-p:-5.668975830078125
epoch£º943	 i:4 	 global-step:18864	 l-p:0.10607738047838211
epoch£º943	 i:5 	 global-step:18865	 l-p:0.23604294657707214
epoch£º943	 i:6 	 global-step:18866	 l-p:0.13465850055217743
epoch£º943	 i:7 	 global-step:18867	 l-p:0.13278278708457947
epoch£º943	 i:8 	 global-step:18868	 l-p:-0.0032677745912224054
epoch£º943	 i:9 	 global-step:18869	 l-p:0.12902070581912994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6128,  0.5205,  1.0000,  0.4421,
          1.0000,  0.8494, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1755,  0.0983,  1.0000,  0.0550,
          1.0000,  0.5599, 31.6228]], device='cuda:0')
 pt:tensor([[3.6325, 2.8568, 2.4452],
        [3.6325, 2.8682, 2.2417],
        [3.6325, 3.0865, 3.0784],
        [3.6325, 3.2482, 3.3507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.09171133488416672 
model_pd.l_d.mean(): -23.795623779296875 
model_pd.lagr.mean(): -23.70391273498535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1355], device='cuda:0')), ('power', tensor([-23.9312], device='cuda:0'))])
epoch£º944	 i:0 	 global-step:18880	 l-p:0.09171133488416672
epoch£º944	 i:1 	 global-step:18881	 l-p:0.07165882736444473
epoch£º944	 i:2 	 global-step:18882	 l-p:0.4416542053222656
epoch£º944	 i:3 	 global-step:18883	 l-p:0.11437402665615082
epoch£º944	 i:4 	 global-step:18884	 l-p:0.11483555287122726
epoch£º944	 i:5 	 global-step:18885	 l-p:0.16340570151805878
epoch£º944	 i:6 	 global-step:18886	 l-p:0.13391132652759552
epoch£º944	 i:7 	 global-step:18887	 l-p:0.1341848224401474
epoch£º944	 i:8 	 global-step:18888	 l-p:0.0958043709397316
epoch£º944	 i:9 	 global-step:18889	 l-p:0.12537217140197754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7028, 2.9283, 2.3167],
        [3.7028, 3.6952, 3.7025],
        [3.7028, 3.5176, 3.6286],
        [3.7028, 3.7027, 3.7028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.16007936000823975 
model_pd.l_d.mean(): -22.9400634765625 
model_pd.lagr.mean(): -22.779983520507812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1777], device='cuda:0')), ('power', tensor([-23.1177], device='cuda:0'))])
epoch£º945	 i:0 	 global-step:18900	 l-p:0.16007936000823975
epoch£º945	 i:1 	 global-step:18901	 l-p:0.12003947794437408
epoch£º945	 i:2 	 global-step:18902	 l-p:0.11736827343702316
epoch£º945	 i:3 	 global-step:18903	 l-p:0.10909722745418549
epoch£º945	 i:4 	 global-step:18904	 l-p:0.20935720205307007
epoch£º945	 i:5 	 global-step:18905	 l-p:0.1430543065071106
epoch£º945	 i:6 	 global-step:18906	 l-p:0.14486680924892426
epoch£º945	 i:7 	 global-step:18907	 l-p:0.34432509541511536
epoch£º945	 i:8 	 global-step:18908	 l-p:0.12662817537784576
epoch£º945	 i:9 	 global-step:18909	 l-p:0.1591179370880127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7187, 2.9468, 2.3342],
        [3.7187, 3.0091, 2.3632],
        [3.7187, 3.7186, 3.7187],
        [3.7187, 3.3312, 2.7323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.13749024271965027 
model_pd.l_d.mean(): -22.971174240112305 
model_pd.lagr.mean(): -22.83368492126465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2642], device='cuda:0')), ('power', tensor([-23.2354], device='cuda:0'))])
epoch£º946	 i:0 	 global-step:18920	 l-p:0.13749024271965027
epoch£º946	 i:1 	 global-step:18921	 l-p:0.11408410966396332
epoch£º946	 i:2 	 global-step:18922	 l-p:0.15694423019886017
epoch£º946	 i:3 	 global-step:18923	 l-p:0.3399786949157715
epoch£º946	 i:4 	 global-step:18924	 l-p:0.2702440917491913
epoch£º946	 i:5 	 global-step:18925	 l-p:0.12854143977165222
epoch£º946	 i:6 	 global-step:18926	 l-p:0.1250539869070053
epoch£º946	 i:7 	 global-step:18927	 l-p:-2.0291192531585693
epoch£º946	 i:8 	 global-step:18928	 l-p:0.12819267809391022
epoch£º946	 i:9 	 global-step:18929	 l-p:0.11233571916818619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6914, 3.6666, 3.6891],
        [3.6914, 3.6876, 3.6913],
        [3.6914, 3.6915, 3.6914],
        [3.6914, 3.6915, 3.6914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.12857995927333832 
model_pd.l_d.mean(): -23.690561294555664 
model_pd.lagr.mean(): -23.561981201171875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1561], device='cuda:0')), ('power', tensor([-23.8466], device='cuda:0'))])
epoch£º947	 i:0 	 global-step:18940	 l-p:0.12857995927333832
epoch£º947	 i:1 	 global-step:18941	 l-p:-0.6493930816650391
epoch£º947	 i:2 	 global-step:18942	 l-p:0.12480030953884125
epoch£º947	 i:3 	 global-step:18943	 l-p:0.11278034001588821
epoch£º947	 i:4 	 global-step:18944	 l-p:0.24354009330272675
epoch£º947	 i:5 	 global-step:18945	 l-p:0.13395264744758606
epoch£º947	 i:6 	 global-step:18946	 l-p:0.043305061757564545
epoch£º947	 i:7 	 global-step:18947	 l-p:0.15175893902778625
epoch£º947	 i:8 	 global-step:18948	 l-p:-0.021137980744242668
epoch£º947	 i:9 	 global-step:18949	 l-p:0.13534314930438995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6672, 2.9721, 2.3306],
        [3.6672, 3.5253, 3.6212],
        [3.6672, 3.6670, 3.6672],
        [3.6672, 3.4007, 3.5242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.11004526168107986 
model_pd.l_d.mean(): -22.759286880493164 
model_pd.lagr.mean(): -22.649242401123047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3328], device='cuda:0')), ('power', tensor([-23.0921], device='cuda:0'))])
epoch£º948	 i:0 	 global-step:18960	 l-p:0.11004526168107986
epoch£º948	 i:1 	 global-step:18961	 l-p:0.16772142052650452
epoch£º948	 i:2 	 global-step:18962	 l-p:0.11516055464744568
epoch£º948	 i:3 	 global-step:18963	 l-p:0.049462318420410156
epoch£º948	 i:4 	 global-step:18964	 l-p:0.13887228071689606
epoch£º948	 i:5 	 global-step:18965	 l-p:0.14773216843605042
epoch£º948	 i:6 	 global-step:18966	 l-p:0.14741338789463043
epoch£º948	 i:7 	 global-step:18967	 l-p:0.16250140964984894
epoch£º948	 i:8 	 global-step:18968	 l-p:0.13739171624183655
epoch£º948	 i:9 	 global-step:18969	 l-p:0.20797599852085114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6434, 3.6414, 3.6434],
        [3.6434, 3.4116, 3.5328],
        [3.6434, 3.4969, 3.5949],
        [3.6434, 3.5409, 3.6175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.06878150999546051 
model_pd.l_d.mean(): -23.62247085571289 
model_pd.lagr.mean(): -23.55368995666504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1377], device='cuda:0')), ('power', tensor([-23.7602], device='cuda:0'))])
epoch£º949	 i:0 	 global-step:18980	 l-p:0.06878150999546051
epoch£º949	 i:1 	 global-step:18981	 l-p:0.20449675619602203
epoch£º949	 i:2 	 global-step:18982	 l-p:0.6026620864868164
epoch£º949	 i:3 	 global-step:18983	 l-p:0.13591094315052032
epoch£º949	 i:4 	 global-step:18984	 l-p:0.12581585347652435
epoch£º949	 i:5 	 global-step:18985	 l-p:0.1489534080028534
epoch£º949	 i:6 	 global-step:18986	 l-p:0.06225339695811272
epoch£º949	 i:7 	 global-step:18987	 l-p:0.15405651926994324
epoch£º949	 i:8 	 global-step:18988	 l-p:0.1429443657398224
epoch£º949	 i:9 	 global-step:18989	 l-p:0.025934968143701553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6715, 3.6571, 3.6706],
        [3.6715, 2.9790, 2.7624],
        [3.6715, 2.9126, 2.5417],
        [3.6715, 3.6115, 3.6612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): 0.020451311022043228 
model_pd.l_d.mean(): -23.029294967651367 
model_pd.lagr.mean(): -23.00884437561035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2540], device='cuda:0')), ('power', tensor([-23.2833], device='cuda:0'))])
epoch£º950	 i:0 	 global-step:19000	 l-p:0.020451311022043228
epoch£º950	 i:1 	 global-step:19001	 l-p:0.1287047564983368
epoch£º950	 i:2 	 global-step:19002	 l-p:0.1410883665084839
epoch£º950	 i:3 	 global-step:19003	 l-p:0.17946666479110718
epoch£º950	 i:4 	 global-step:19004	 l-p:0.1096934899687767
epoch£º950	 i:5 	 global-step:19005	 l-p:0.13285714387893677
epoch£º950	 i:6 	 global-step:19006	 l-p:0.14167505502700806
epoch£º950	 i:7 	 global-step:19007	 l-p:1.4848763942718506
epoch£º950	 i:8 	 global-step:19008	 l-p:0.23610281944274902
epoch£º950	 i:9 	 global-step:19009	 l-p:-0.31230151653289795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6875, 3.3319, 3.4428],
        [3.6875, 3.6875, 3.6875],
        [3.6875, 3.6875, 3.6875],
        [3.6875, 2.9965, 2.7797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.14384834468364716 
model_pd.l_d.mean(): -22.607995986938477 
model_pd.lagr.mean(): -22.464147567749023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2729], device='cuda:0')), ('power', tensor([-22.8809], device='cuda:0'))])
epoch£º951	 i:0 	 global-step:19020	 l-p:0.14384834468364716
epoch£º951	 i:1 	 global-step:19021	 l-p:0.11933505535125732
epoch£º951	 i:2 	 global-step:19022	 l-p:0.17117390036582947
epoch£º951	 i:3 	 global-step:19023	 l-p:0.208572655916214
epoch£º951	 i:4 	 global-step:19024	 l-p:0.12065395712852478
epoch£º951	 i:5 	 global-step:19025	 l-p:0.1229914054274559
epoch£º951	 i:6 	 global-step:19026	 l-p:0.13006341457366943
epoch£º951	 i:7 	 global-step:19027	 l-p:0.13430418074131012
epoch£º951	 i:8 	 global-step:19028	 l-p:-0.19456501305103302
epoch£º951	 i:9 	 global-step:19029	 l-p:-1.202236533164978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6798, 3.6687, 3.6792],
        [3.6798, 2.8976, 2.2906],
        [3.6798, 3.6024, 3.6638],
        [3.6798, 3.6370, 3.6739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): -0.028648193925619125 
model_pd.l_d.mean(): -23.383434295654297 
model_pd.lagr.mean(): -23.41208267211914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2025], device='cuda:0')), ('power', tensor([-23.5859], device='cuda:0'))])
epoch£º952	 i:0 	 global-step:19040	 l-p:-0.028648193925619125
epoch£º952	 i:1 	 global-step:19041	 l-p:-5.805459976196289
epoch£º952	 i:2 	 global-step:19042	 l-p:0.16092613339424133
epoch£º952	 i:3 	 global-step:19043	 l-p:0.13256293535232544
epoch£º952	 i:4 	 global-step:19044	 l-p:0.17450203001499176
epoch£º952	 i:5 	 global-step:19045	 l-p:0.10973569750785828
epoch£º952	 i:6 	 global-step:19046	 l-p:0.109078548848629
epoch£º952	 i:7 	 global-step:19047	 l-p:0.12493310868740082
epoch£º952	 i:8 	 global-step:19048	 l-p:0.1170048788189888
epoch£º952	 i:9 	 global-step:19049	 l-p:0.1471448838710785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6898, 2.9692, 2.6924],
        [3.6898, 3.6895, 3.6898],
        [3.6898, 3.6897, 3.6898],
        [3.6898, 3.6829, 3.6895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): -2.147458076477051 
model_pd.l_d.mean(): -23.45301628112793 
model_pd.lagr.mean(): -25.600475311279297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2214], device='cuda:0')), ('power', tensor([-23.6744], device='cuda:0'))])
epoch£º953	 i:0 	 global-step:19060	 l-p:-2.147458076477051
epoch£º953	 i:1 	 global-step:19061	 l-p:0.12825274467468262
epoch£º953	 i:2 	 global-step:19062	 l-p:-0.040183600038290024
epoch£º953	 i:3 	 global-step:19063	 l-p:0.13561472296714783
epoch£º953	 i:4 	 global-step:19064	 l-p:0.13969206809997559
epoch£º953	 i:5 	 global-step:19065	 l-p:0.12832508981227875
epoch£º953	 i:6 	 global-step:19066	 l-p:-0.02324102260172367
epoch£º953	 i:7 	 global-step:19067	 l-p:0.18359221518039703
epoch£º953	 i:8 	 global-step:19068	 l-p:0.1725766509771347
epoch£º953	 i:9 	 global-step:19069	 l-p:0.14160726964473724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6558, 3.5280, 3.6177],
        [3.6558, 3.2059, 2.6005],
        [3.6558, 3.6504, 3.6556],
        [3.6558, 3.2716, 3.3740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 0.2900528013706207 
model_pd.l_d.mean(): -23.09327507019043 
model_pd.lagr.mean(): -22.80322265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3264], device='cuda:0')), ('power', tensor([-23.4197], device='cuda:0'))])
epoch£º954	 i:0 	 global-step:19080	 l-p:0.2900528013706207
epoch£º954	 i:1 	 global-step:19081	 l-p:0.05665932968258858
epoch£º954	 i:2 	 global-step:19082	 l-p:0.1347421258687973
epoch£º954	 i:3 	 global-step:19083	 l-p:0.151896670460701
epoch£º954	 i:4 	 global-step:19084	 l-p:0.08967772871255875
epoch£º954	 i:5 	 global-step:19085	 l-p:0.11838481575250626
epoch£º954	 i:6 	 global-step:19086	 l-p:0.15363538265228271
epoch£º954	 i:7 	 global-step:19087	 l-p:0.06481577455997467
epoch£º954	 i:8 	 global-step:19088	 l-p:0.14852508902549744
epoch£º954	 i:9 	 global-step:19089	 l-p:0.12750720977783203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6530, 3.6531, 3.6530],
        [3.6530, 3.2019, 2.5966],
        [3.6530, 3.5037, 3.6029],
        [3.6530, 3.5758, 3.6371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.1567947268486023 
model_pd.l_d.mean(): -23.30768585205078 
model_pd.lagr.mean(): -23.150890350341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2745], device='cuda:0')), ('power', tensor([-23.5822], device='cuda:0'))])
epoch£º955	 i:0 	 global-step:19100	 l-p:0.1567947268486023
epoch£º955	 i:1 	 global-step:19101	 l-p:-0.018491454422473907
epoch£º955	 i:2 	 global-step:19102	 l-p:0.1755833625793457
epoch£º955	 i:3 	 global-step:19103	 l-p:0.1554391086101532
epoch£º955	 i:4 	 global-step:19104	 l-p:0.13097602128982544
epoch£º955	 i:5 	 global-step:19105	 l-p:0.3520788550376892
epoch£º955	 i:6 	 global-step:19106	 l-p:0.1370435208082199
epoch£º955	 i:7 	 global-step:19107	 l-p:0.07018844038248062
epoch£º955	 i:8 	 global-step:19108	 l-p:0.1325766146183014
epoch£º955	 i:9 	 global-step:19109	 l-p:0.1649082750082016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6485, 2.9216, 2.2830],
        [3.6485, 3.6473, 3.6485],
        [3.6485, 3.6465, 3.6484],
        [3.6485, 2.9524, 2.7357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.14277102053165436 
model_pd.l_d.mean(): -23.788803100585938 
model_pd.lagr.mean(): -23.646032333374023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1543], device='cuda:0')), ('power', tensor([-23.9431], device='cuda:0'))])
epoch£º956	 i:0 	 global-step:19120	 l-p:0.14277102053165436
epoch£º956	 i:1 	 global-step:19121	 l-p:0.1421092450618744
epoch£º956	 i:2 	 global-step:19122	 l-p:0.0891377329826355
epoch£º956	 i:3 	 global-step:19123	 l-p:0.1368669718503952
epoch£º956	 i:4 	 global-step:19124	 l-p:0.08074283599853516
epoch£º956	 i:5 	 global-step:19125	 l-p:0.06298600882291794
epoch£º956	 i:6 	 global-step:19126	 l-p:0.1375051587820053
epoch£º956	 i:7 	 global-step:19127	 l-p:0.508371889591217
epoch£º956	 i:8 	 global-step:19128	 l-p:0.13392938673496246
epoch£º956	 i:9 	 global-step:19129	 l-p:0.1109926775097847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6286, 3.6279, 3.6286],
        [3.6286, 3.2700, 3.3822],
        [3.6286, 3.5887, 3.6234],
        [3.6286, 3.5855, 3.6227]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.11428836733102798 
model_pd.l_d.mean(): -23.623188018798828 
model_pd.lagr.mean(): -23.508899688720703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2033], device='cuda:0')), ('power', tensor([-23.8265], device='cuda:0'))])
epoch£º957	 i:0 	 global-step:19140	 l-p:0.11428836733102798
epoch£º957	 i:1 	 global-step:19141	 l-p:0.1309240162372589
epoch£º957	 i:2 	 global-step:19142	 l-p:0.1420043408870697
epoch£º957	 i:3 	 global-step:19143	 l-p:0.1016925796866417
epoch£º957	 i:4 	 global-step:19144	 l-p:-0.3501998484134674
epoch£º957	 i:5 	 global-step:19145	 l-p:0.11678462475538254
epoch£º957	 i:6 	 global-step:19146	 l-p:0.14547120034694672
epoch£º957	 i:7 	 global-step:19147	 l-p:0.1028917208313942
epoch£º957	 i:8 	 global-step:19148	 l-p:0.2129448503255844
epoch£º957	 i:9 	 global-step:19149	 l-p:0.3037259876728058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6361, 3.5931, 3.6302],
        [3.6361, 3.5018, 3.5946],
        [3.6361, 3.4864, 3.5858],
        [3.6361, 2.8650, 2.2392]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): -0.2575899064540863 
model_pd.l_d.mean(): -23.678285598754883 
model_pd.lagr.mean(): -23.935874938964844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1550], device='cuda:0')), ('power', tensor([-23.8333], device='cuda:0'))])
epoch£º958	 i:0 	 global-step:19160	 l-p:-0.2575899064540863
epoch£º958	 i:1 	 global-step:19161	 l-p:0.14350317418575287
epoch£º958	 i:2 	 global-step:19162	 l-p:0.11846723407506943
epoch£º958	 i:3 	 global-step:19163	 l-p:0.18936628103256226
epoch£º958	 i:4 	 global-step:19164	 l-p:0.13742338120937347
epoch£º958	 i:5 	 global-step:19165	 l-p:0.017696397379040718
epoch£º958	 i:6 	 global-step:19166	 l-p:0.18039560317993164
epoch£º958	 i:7 	 global-step:19167	 l-p:0.14253868162631989
epoch£º958	 i:8 	 global-step:19168	 l-p:0.11873339116573334
epoch£º958	 i:9 	 global-step:19169	 l-p:0.12885461747646332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6496, 2.8517, 2.2722],
        [3.6496, 3.6496, 3.6496],
        [3.6496, 2.8556, 2.3700],
        [3.6496, 3.6444, 3.6494]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.2493205964565277 
model_pd.l_d.mean(): -23.53766441345215 
model_pd.lagr.mean(): -23.28834342956543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1985], device='cuda:0')), ('power', tensor([-23.7361], device='cuda:0'))])
epoch£º959	 i:0 	 global-step:19180	 l-p:0.2493205964565277
epoch£º959	 i:1 	 global-step:19181	 l-p:0.125928595662117
epoch£º959	 i:2 	 global-step:19182	 l-p:0.1482274830341339
epoch£º959	 i:3 	 global-step:19183	 l-p:0.13435538113117218
epoch£º959	 i:4 	 global-step:19184	 l-p:0.07032511383295059
epoch£º959	 i:5 	 global-step:19185	 l-p:0.12743061780929565
epoch£º959	 i:6 	 global-step:19186	 l-p:0.13985101878643036
epoch£º959	 i:7 	 global-step:19187	 l-p:0.13210080564022064
epoch£º959	 i:8 	 global-step:19188	 l-p:0.04084188863635063
epoch£º959	 i:9 	 global-step:19189	 l-p:-0.38051170110702515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6378, 3.6377, 3.6379],
        [3.6378, 3.6313, 3.6376],
        [3.6378, 3.1095, 2.4913],
        [3.6378, 3.1014, 2.4819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.0806417465209961 
model_pd.l_d.mean(): -22.92384910583496 
model_pd.lagr.mean(): -22.84320831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2732], device='cuda:0')), ('power', tensor([-23.1971], device='cuda:0'))])
epoch£º960	 i:0 	 global-step:19200	 l-p:0.0806417465209961
epoch£º960	 i:1 	 global-step:19201	 l-p:0.1576865315437317
epoch£º960	 i:2 	 global-step:19202	 l-p:0.12943719327449799
epoch£º960	 i:3 	 global-step:19203	 l-p:0.2532065510749817
epoch£º960	 i:4 	 global-step:19204	 l-p:0.1372627168893814
epoch£º960	 i:5 	 global-step:19205	 l-p:0.1915489137172699
epoch£º960	 i:6 	 global-step:19206	 l-p:0.07052921503782272
epoch£º960	 i:7 	 global-step:19207	 l-p:0.06374692171812057
epoch£º960	 i:8 	 global-step:19208	 l-p:0.3350363075733185
epoch£º960	 i:9 	 global-step:19209	 l-p:0.10394084453582764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6670, 2.8797, 2.2723],
        [3.6670, 3.5266, 3.6221],
        [3.6670, 2.8737, 2.2815],
        [3.6670, 3.6581, 3.6666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.12318748235702515 
model_pd.l_d.mean(): -22.95699691772461 
model_pd.lagr.mean(): -22.83380889892578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3673], device='cuda:0')), ('power', tensor([-23.3243], device='cuda:0'))])
epoch£º961	 i:0 	 global-step:19220	 l-p:0.12318748235702515
epoch£º961	 i:1 	 global-step:19221	 l-p:0.1209154799580574
epoch£º961	 i:2 	 global-step:19222	 l-p:0.12845328450202942
epoch£º961	 i:3 	 global-step:19223	 l-p:0.13310888409614563
epoch£º961	 i:4 	 global-step:19224	 l-p:0.14073190093040466
epoch£º961	 i:5 	 global-step:19225	 l-p:-0.1891222596168518
epoch£º961	 i:6 	 global-step:19226	 l-p:0.16194161772727966
epoch£º961	 i:7 	 global-step:19227	 l-p:0.13303275406360626
epoch£º961	 i:8 	 global-step:19228	 l-p:0.18567869067192078
epoch£º961	 i:9 	 global-step:19229	 l-p:0.2272757887840271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6740, 3.5830, 3.6529],
        [3.6740, 3.6740, 3.6740],
        [3.6740, 3.6715, 3.6740],
        [3.6740, 3.4798, 3.5937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.17534811794757843 
model_pd.l_d.mean(): -23.67096519470215 
model_pd.lagr.mean(): -23.495616912841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1322], device='cuda:0')), ('power', tensor([-23.8031], device='cuda:0'))])
epoch£º962	 i:0 	 global-step:19240	 l-p:0.17534811794757843
epoch£º962	 i:1 	 global-step:19241	 l-p:0.18141913414001465
epoch£º962	 i:2 	 global-step:19242	 l-p:0.12494780868291855
epoch£º962	 i:3 	 global-step:19243	 l-p:0.13330431282520294
epoch£º962	 i:4 	 global-step:19244	 l-p:-2.485797882080078
epoch£º962	 i:5 	 global-step:19245	 l-p:0.16684940457344055
epoch£º962	 i:6 	 global-step:19246	 l-p:-1.5724468231201172
epoch£º962	 i:7 	 global-step:19247	 l-p:0.29734644293785095
epoch£º962	 i:8 	 global-step:19248	 l-p:0.12965232133865356
epoch£º962	 i:9 	 global-step:19249	 l-p:0.15147346258163452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7074, 3.7029, 3.7073],
        [3.7074, 3.7046, 3.7073],
        [3.7074, 3.6044, 3.6812],
        [3.7074, 3.6263, 3.6901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.15813343226909637 
model_pd.l_d.mean(): -23.224512100219727 
model_pd.lagr.mean(): -23.06637954711914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2313], device='cuda:0')), ('power', tensor([-23.4558], device='cuda:0'))])
epoch£º963	 i:0 	 global-step:19260	 l-p:0.15813343226909637
epoch£º963	 i:1 	 global-step:19261	 l-p:0.1274702548980713
epoch£º963	 i:2 	 global-step:19262	 l-p:0.11961624026298523
epoch£º963	 i:3 	 global-step:19263	 l-p:0.5461108684539795
epoch£º963	 i:4 	 global-step:19264	 l-p:0.3133572041988373
epoch£º963	 i:5 	 global-step:19265	 l-p:0.13604779541492462
epoch£º963	 i:6 	 global-step:19266	 l-p:0.10302302241325378
epoch£º963	 i:7 	 global-step:19267	 l-p:0.13683660328388214
epoch£º963	 i:8 	 global-step:19268	 l-p:0.17824774980545044
epoch£º963	 i:9 	 global-step:19269	 l-p:0.10786587744951248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7153, 3.7124, 3.7152],
        [3.7153, 3.7147, 3.7153],
        [3.7153, 3.3267, 2.7269],
        [3.7153, 2.9237, 2.3681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.1317160576581955 
model_pd.l_d.mean(): -22.32962989807129 
model_pd.lagr.mean(): -22.197914123535156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3243], device='cuda:0')), ('power', tensor([-22.6539], device='cuda:0'))])
epoch£º964	 i:0 	 global-step:19280	 l-p:0.1317160576581955
epoch£º964	 i:1 	 global-step:19281	 l-p:0.1343219131231308
epoch£º964	 i:2 	 global-step:19282	 l-p:0.13833613693714142
epoch£º964	 i:3 	 global-step:19283	 l-p:0.13266053795814514
epoch£º964	 i:4 	 global-step:19284	 l-p:-0.911919355392456
epoch£º964	 i:5 	 global-step:19285	 l-p:0.13706472516059875
epoch£º964	 i:6 	 global-step:19286	 l-p:0.12415552139282227
epoch£º964	 i:7 	 global-step:19287	 l-p:0.10121124982833862
epoch£º964	 i:8 	 global-step:19288	 l-p:0.11136344075202942
epoch£º964	 i:9 	 global-step:19289	 l-p:0.19575940072536469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[3.6629, 2.8650, 2.3550],
        [3.6629, 3.0364, 2.9340],
        [3.6629, 2.8645, 2.3504],
        [3.6629, 3.0992, 3.0740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.14074741303920746 
model_pd.l_d.mean(): -23.48833465576172 
model_pd.lagr.mean(): -23.34758758544922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1869], device='cuda:0')), ('power', tensor([-23.6752], device='cuda:0'))])
epoch£º965	 i:0 	 global-step:19300	 l-p:0.14074741303920746
epoch£º965	 i:1 	 global-step:19301	 l-p:0.1995583325624466
epoch£º965	 i:2 	 global-step:19302	 l-p:0.11717748641967773
epoch£º965	 i:3 	 global-step:19303	 l-p:0.39192283153533936
epoch£º965	 i:4 	 global-step:19304	 l-p:0.12776754796504974
epoch£º965	 i:5 	 global-step:19305	 l-p:0.1710311472415924
epoch£º965	 i:6 	 global-step:19306	 l-p:0.08536666631698608
epoch£º965	 i:7 	 global-step:19307	 l-p:0.13503627479076385
epoch£º965	 i:8 	 global-step:19308	 l-p:0.12885603308677673
epoch£º965	 i:9 	 global-step:19309	 l-p:0.1362069696187973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6351, 2.9807, 2.3447],
        [3.6351, 3.6341, 3.6351],
        [3.6351, 2.9611, 2.3235],
        [3.6351, 3.0853, 3.0773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.14997494220733643 
model_pd.l_d.mean(): -22.89458656311035 
model_pd.lagr.mean(): -22.744611740112305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3166], device='cuda:0')), ('power', tensor([-23.2112], device='cuda:0'))])
epoch£º966	 i:0 	 global-step:19320	 l-p:0.14997494220733643
epoch£º966	 i:1 	 global-step:19321	 l-p:0.10742653906345367
epoch£º966	 i:2 	 global-step:19322	 l-p:-0.020170606672763824
epoch£º966	 i:3 	 global-step:19323	 l-p:0.15616068243980408
epoch£º966	 i:4 	 global-step:19324	 l-p:0.13233767449855804
epoch£º966	 i:5 	 global-step:19325	 l-p:0.13661174476146698
epoch£º966	 i:6 	 global-step:19326	 l-p:0.13531626760959625
epoch£º966	 i:7 	 global-step:19327	 l-p:0.08736487478017807
epoch£º966	 i:8 	 global-step:19328	 l-p:0.13952504098415375
epoch£º966	 i:9 	 global-step:19329	 l-p:0.11938083171844482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6353, 3.6350, 3.6353],
        [3.6353, 3.6353, 3.6353],
        [3.6353, 3.6352, 3.6353],
        [3.6353, 2.8666, 2.4936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.1608216017484665 
model_pd.l_d.mean(): -23.746238708496094 
model_pd.lagr.mean(): -23.585416793823242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2188], device='cuda:0')), ('power', tensor([-23.9650], device='cuda:0'))])
epoch£º967	 i:0 	 global-step:19340	 l-p:0.1608216017484665
epoch£º967	 i:1 	 global-step:19341	 l-p:0.1409895271062851
epoch£º967	 i:2 	 global-step:19342	 l-p:0.12716419994831085
epoch£º967	 i:3 	 global-step:19343	 l-p:0.12400799244642258
epoch£º967	 i:4 	 global-step:19344	 l-p:0.08961497247219086
epoch£º967	 i:5 	 global-step:19345	 l-p:0.0712885931134224
epoch£º967	 i:6 	 global-step:19346	 l-p:0.21167172491550446
epoch£º967	 i:7 	 global-step:19347	 l-p:0.0006991767440922558
epoch£º967	 i:8 	 global-step:19348	 l-p:0.14139191806316376
epoch£º967	 i:9 	 global-step:19349	 l-p:0.29368507862091064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6394, 2.8817, 2.2472],
        [3.6394, 2.8441, 2.2378],
        [3.6394, 3.4742, 3.5796],
        [3.6394, 2.8671, 2.4833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): -0.3681177794933319 
model_pd.l_d.mean(): -23.389114379882812 
model_pd.lagr.mean(): -23.757232666015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2908], device='cuda:0')), ('power', tensor([-23.6799], device='cuda:0'))])
epoch£º968	 i:0 	 global-step:19360	 l-p:-0.3681177794933319
epoch£º968	 i:1 	 global-step:19361	 l-p:0.16055218875408173
epoch£º968	 i:2 	 global-step:19362	 l-p:0.21551711857318878
epoch£º968	 i:3 	 global-step:19363	 l-p:0.17789889872074127
epoch£º968	 i:4 	 global-step:19364	 l-p:0.041223447769880295
epoch£º968	 i:5 	 global-step:19365	 l-p:0.14506636559963226
epoch£º968	 i:6 	 global-step:19366	 l-p:0.09392871707677841
epoch£º968	 i:7 	 global-step:19367	 l-p:0.14318512380123138
epoch£º968	 i:8 	 global-step:19368	 l-p:0.14497160911560059
epoch£º968	 i:9 	 global-step:19369	 l-p:0.13224369287490845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6847, 2.9012, 2.4594],
        [3.6847, 3.0138, 2.3691],
        [3.6847, 2.8961, 2.2879],
        [3.6847, 3.4439, 3.5664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.14440429210662842 
model_pd.l_d.mean(): -23.738161087036133 
model_pd.lagr.mean(): -23.59375762939453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1181], device='cuda:0')), ('power', tensor([-23.8563], device='cuda:0'))])
epoch£º969	 i:0 	 global-step:19380	 l-p:0.14440429210662842
epoch£º969	 i:1 	 global-step:19381	 l-p:0.12357284128665924
epoch£º969	 i:2 	 global-step:19382	 l-p:-0.08096617460250854
epoch£º969	 i:3 	 global-step:19383	 l-p:0.0986739918589592
epoch£º969	 i:4 	 global-step:19384	 l-p:0.2271076738834381
epoch£º969	 i:5 	 global-step:19385	 l-p:0.15427154302597046
epoch£º969	 i:6 	 global-step:19386	 l-p:0.11995013803243637
epoch£º969	 i:7 	 global-step:19387	 l-p:0.17532256245613098
epoch£º969	 i:8 	 global-step:19388	 l-p:0.14258664846420288
epoch£º969	 i:9 	 global-step:19389	 l-p:-0.0953693836927414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6752, 3.6751, 3.6753],
        [3.6752, 3.1837, 3.2254],
        [3.6752, 3.3700, 3.4927],
        [3.6752, 3.0343, 2.9114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.16113702952861786 
model_pd.l_d.mean(): -23.575315475463867 
model_pd.lagr.mean(): -23.4141788482666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1823], device='cuda:0')), ('power', tensor([-23.7576], device='cuda:0'))])
epoch£º970	 i:0 	 global-step:19400	 l-p:0.16113702952861786
epoch£º970	 i:1 	 global-step:19401	 l-p:-0.13225674629211426
epoch£º970	 i:2 	 global-step:19402	 l-p:0.15417200326919556
epoch£º970	 i:3 	 global-step:19403	 l-p:0.14591678977012634
epoch£º970	 i:4 	 global-step:19404	 l-p:0.13009360432624817
epoch£º970	 i:5 	 global-step:19405	 l-p:0.17581340670585632
epoch£º970	 i:6 	 global-step:19406	 l-p:0.11833376437425613
epoch£º970	 i:7 	 global-step:19407	 l-p:-0.1598423570394516
epoch£º970	 i:8 	 global-step:19408	 l-p:0.20246821641921997
epoch£º970	 i:9 	 global-step:19409	 l-p:0.10931853950023651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6809, 3.5015, 3.6115],
        [3.6809, 3.5106, 3.6177],
        [3.6809, 3.6794, 3.6809],
        [3.6809, 2.9383, 2.6233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.1701338291168213 
model_pd.l_d.mean(): -23.638532638549805 
model_pd.lagr.mean(): -23.468399047851562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1565], device='cuda:0')), ('power', tensor([-23.7950], device='cuda:0'))])
epoch£º971	 i:0 	 global-step:19420	 l-p:0.1701338291168213
epoch£º971	 i:1 	 global-step:19421	 l-p:0.13129602372646332
epoch£º971	 i:2 	 global-step:19422	 l-p:-0.3862462639808655
epoch£º971	 i:3 	 global-step:19423	 l-p:0.1368827372789383
epoch£º971	 i:4 	 global-step:19424	 l-p:-0.015032157301902771
epoch£º971	 i:5 	 global-step:19425	 l-p:0.12675558030605316
epoch£º971	 i:6 	 global-step:19426	 l-p:0.14097605645656586
epoch£º971	 i:7 	 global-step:19427	 l-p:0.18133608996868134
epoch£º971	 i:8 	 global-step:19428	 l-p:0.12488149106502533
epoch£º971	 i:9 	 global-step:19429	 l-p:-0.02864738367497921
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6805, 3.5521, 3.6421],
        [3.6805, 3.6775, 3.6804],
        [3.6805, 3.1165, 3.0911],
        [3.6805, 3.6752, 3.6803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.12942273914813995 
model_pd.l_d.mean(): -23.14673614501953 
model_pd.lagr.mean(): -23.01731300354004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2785], device='cuda:0')), ('power', tensor([-23.4252], device='cuda:0'))])
epoch£º972	 i:0 	 global-step:19440	 l-p:0.12942273914813995
epoch£º972	 i:1 	 global-step:19441	 l-p:0.14616096019744873
epoch£º972	 i:2 	 global-step:19442	 l-p:0.13293147087097168
epoch£º972	 i:3 	 global-step:19443	 l-p:0.1402377337217331
epoch£º972	 i:4 	 global-step:19444	 l-p:0.1440112292766571
epoch£º972	 i:5 	 global-step:19445	 l-p:0.2620322108268738
epoch£º972	 i:6 	 global-step:19446	 l-p:0.09105250239372253
epoch£º972	 i:7 	 global-step:19447	 l-p:0.06217086687684059
epoch£º972	 i:8 	 global-step:19448	 l-p:0.10644975304603577
epoch£º972	 i:9 	 global-step:19449	 l-p:0.03840244188904762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6672, 3.6421, 3.6648],
        [3.6672, 3.2164, 3.2871],
        [3.6672, 3.4497, 3.5691],
        [3.6672, 2.9269, 2.2857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.14759038388729095 
model_pd.l_d.mean(): -23.181339263916016 
model_pd.lagr.mean(): -23.033748626708984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1919], device='cuda:0')), ('power', tensor([-23.3733], device='cuda:0'))])
epoch£º973	 i:0 	 global-step:19460	 l-p:0.14759038388729095
epoch£º973	 i:1 	 global-step:19461	 l-p:0.17810766398906708
epoch£º973	 i:2 	 global-step:19462	 l-p:0.05715456232428551
epoch£º973	 i:3 	 global-step:19463	 l-p:0.12871591746807098
epoch£º973	 i:4 	 global-step:19464	 l-p:0.14890311658382416
epoch£º973	 i:5 	 global-step:19465	 l-p:0.11919750273227692
epoch£º973	 i:6 	 global-step:19466	 l-p:0.12787505984306335
epoch£º973	 i:7 	 global-step:19467	 l-p:0.1492573469877243
epoch£º973	 i:8 	 global-step:19468	 l-p:0.11309292912483215
epoch£º973	 i:9 	 global-step:19469	 l-p:0.1562456637620926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6441, 3.0303, 2.9497],
        [3.6441, 3.5433, 3.6191],
        [3.6441, 3.1802, 3.2439],
        [3.6441, 2.9452, 2.3045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.1357535868883133 
model_pd.l_d.mean(): -22.73431968688965 
model_pd.lagr.mean(): -22.59856605529785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2661], device='cuda:0')), ('power', tensor([-23.0004], device='cuda:0'))])
epoch£º974	 i:0 	 global-step:19480	 l-p:0.1357535868883133
epoch£º974	 i:1 	 global-step:19481	 l-p:0.12024736404418945
epoch£º974	 i:2 	 global-step:19482	 l-p:-0.0125875324010849
epoch£º974	 i:3 	 global-step:19483	 l-p:0.7749889492988586
epoch£º974	 i:4 	 global-step:19484	 l-p:0.1349102258682251
epoch£º974	 i:5 	 global-step:19485	 l-p:0.20730549097061157
epoch£º974	 i:6 	 global-step:19486	 l-p:0.1335023045539856
epoch£º974	 i:7 	 global-step:19487	 l-p:0.09076741337776184
epoch£º974	 i:8 	 global-step:19488	 l-p:0.13843265175819397
epoch£º974	 i:9 	 global-step:19489	 l-p:0.1426873803138733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6248, 3.6248, 3.6248],
        [3.6248, 3.6248, 3.6248],
        [3.6248, 2.8890, 2.6058],
        [3.6248, 3.3628, 3.4878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.08593113720417023 
model_pd.l_d.mean(): -22.894704818725586 
model_pd.lagr.mean(): -22.808773040771484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2327], device='cuda:0')), ('power', tensor([-23.1274], device='cuda:0'))])
epoch£º975	 i:0 	 global-step:19500	 l-p:0.08593113720417023
epoch£º975	 i:1 	 global-step:19501	 l-p:0.16691963374614716
epoch£º975	 i:2 	 global-step:19502	 l-p:0.23673205077648163
epoch£º975	 i:3 	 global-step:19503	 l-p:0.13533011078834534
epoch£º975	 i:4 	 global-step:19504	 l-p:0.04031088575720787
epoch£º975	 i:5 	 global-step:19505	 l-p:0.14548979699611664
epoch£º975	 i:6 	 global-step:19506	 l-p:0.10406316816806793
epoch£º975	 i:7 	 global-step:19507	 l-p:0.044092703610658646
epoch£º975	 i:8 	 global-step:19508	 l-p:0.5650221705436707
epoch£º975	 i:9 	 global-step:19509	 l-p:0.14150340855121613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6283, 3.0299, 2.9709],
        [3.6283, 3.5799, 3.6212],
        [3.6283, 2.9298, 2.2911],
        [3.6283, 3.6283, 3.6283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.1630379855632782 
model_pd.l_d.mean(): -23.53032684326172 
model_pd.lagr.mean(): -23.36728858947754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2378], device='cuda:0')), ('power', tensor([-23.7682], device='cuda:0'))])
epoch£º976	 i:0 	 global-step:19520	 l-p:0.1630379855632782
epoch£º976	 i:1 	 global-step:19521	 l-p:0.24303008615970612
epoch£º976	 i:2 	 global-step:19522	 l-p:0.13262172043323517
epoch£º976	 i:3 	 global-step:19523	 l-p:1.0205472707748413
epoch£º976	 i:4 	 global-step:19524	 l-p:0.12579117715358734
epoch£º976	 i:5 	 global-step:19525	 l-p:0.127735435962677
epoch£º976	 i:6 	 global-step:19526	 l-p:-0.024691171944141388
epoch£º976	 i:7 	 global-step:19527	 l-p:0.09205415844917297
epoch£º976	 i:8 	 global-step:19528	 l-p:0.09288259595632553
epoch£º976	 i:9 	 global-step:19529	 l-p:0.1365291327238083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6414, 3.6414, 3.6414],
        [3.6414, 3.6414, 3.6414],
        [3.6414, 3.4566, 3.5685],
        [3.6414, 3.0743, 3.0500]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.15819720923900604 
model_pd.l_d.mean(): -23.610584259033203 
model_pd.lagr.mean(): -23.4523868560791 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2237], device='cuda:0')), ('power', tensor([-23.8343], device='cuda:0'))])
epoch£º977	 i:0 	 global-step:19540	 l-p:0.15819720923900604
epoch£º977	 i:1 	 global-step:19541	 l-p:0.145121231675148
epoch£º977	 i:2 	 global-step:19542	 l-p:0.18587572872638702
epoch£º977	 i:3 	 global-step:19543	 l-p:0.12269105762243271
epoch£º977	 i:4 	 global-step:19544	 l-p:0.14461509883403778
epoch£º977	 i:5 	 global-step:19545	 l-p:0.24092742800712585
epoch£º977	 i:6 	 global-step:19546	 l-p:0.03752358257770538
epoch£º977	 i:7 	 global-step:19547	 l-p:0.14313837885856628
epoch£º977	 i:8 	 global-step:19548	 l-p:-0.08025967329740524
epoch£º977	 i:9 	 global-step:19549	 l-p:0.13085012137889862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6367, 3.6355, 3.6367],
        [3.6367, 3.4812, 3.5832],
        [3.6367, 3.6367, 3.6367],
        [3.6367, 2.9203, 2.6765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.12415872514247894 
model_pd.l_d.mean(): -23.373619079589844 
model_pd.lagr.mean(): -23.249460220336914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2427], device='cuda:0')), ('power', tensor([-23.6163], device='cuda:0'))])
epoch£º978	 i:0 	 global-step:19560	 l-p:0.12415872514247894
epoch£º978	 i:1 	 global-step:19561	 l-p:0.15168702602386475
epoch£º978	 i:2 	 global-step:19562	 l-p:0.16632080078125
epoch£º978	 i:3 	 global-step:19563	 l-p:0.06651564687490463
epoch£º978	 i:4 	 global-step:19564	 l-p:0.13148215413093567
epoch£º978	 i:5 	 global-step:19565	 l-p:0.13333362340927124
epoch£º978	 i:6 	 global-step:19566	 l-p:0.13497406244277954
epoch£º978	 i:7 	 global-step:19567	 l-p:-0.2025485783815384
epoch£º978	 i:8 	 global-step:19568	 l-p:0.2141694575548172
epoch£º978	 i:9 	 global-step:19569	 l-p:0.0487714558839798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228]], device='cuda:0')
 pt:tensor([[3.6251, 3.1823, 2.5808],
        [3.6251, 2.8628, 2.5182],
        [3.6251, 2.8249, 2.3555],
        [3.6251, 2.9814, 2.8636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.14754140377044678 
model_pd.l_d.mean(): -23.11370849609375 
model_pd.lagr.mean(): -22.966167449951172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3289], device='cuda:0')), ('power', tensor([-23.4426], device='cuda:0'))])
epoch£º979	 i:0 	 global-step:19580	 l-p:0.14754140377044678
epoch£º979	 i:1 	 global-step:19581	 l-p:0.18372733891010284
epoch£º979	 i:2 	 global-step:19582	 l-p:0.15324759483337402
epoch£º979	 i:3 	 global-step:19583	 l-p:0.12605969607830048
epoch£º979	 i:4 	 global-step:19584	 l-p:0.08656293898820877
epoch£º979	 i:5 	 global-step:19585	 l-p:0.24773041903972626
epoch£º979	 i:6 	 global-step:19586	 l-p:0.8022284507751465
epoch£º979	 i:7 	 global-step:19587	 l-p:0.13816441595554352
epoch£º979	 i:8 	 global-step:19588	 l-p:0.03008677437901497
epoch£º979	 i:9 	 global-step:19589	 l-p:0.11056932061910629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6749, 3.6749, 3.6749],
        [3.6749, 3.6695, 3.6747],
        [3.6749, 3.5890, 3.6559],
        [3.6749, 3.6742, 3.6749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.15561798214912415 
model_pd.l_d.mean(): -23.5805721282959 
model_pd.lagr.mean(): -23.42495346069336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1743], device='cuda:0')), ('power', tensor([-23.7549], device='cuda:0'))])
epoch£º980	 i:0 	 global-step:19600	 l-p:0.15561798214912415
epoch£º980	 i:1 	 global-step:19601	 l-p:0.1340237408876419
epoch£º980	 i:2 	 global-step:19602	 l-p:0.12997473776340485
epoch£º980	 i:3 	 global-step:19603	 l-p:-0.024951903149485588
epoch£º980	 i:4 	 global-step:19604	 l-p:0.1751829832792282
epoch£º980	 i:5 	 global-step:19605	 l-p:0.14257988333702087
epoch£º980	 i:6 	 global-step:19606	 l-p:0.11419970542192459
epoch£º980	 i:7 	 global-step:19607	 l-p:0.0022051380947232246
epoch£º980	 i:8 	 global-step:19608	 l-p:0.12456080317497253
epoch£º980	 i:9 	 global-step:19609	 l-p:0.24976134300231934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6689, 3.4269, 3.5501],
        [3.6689, 3.6689, 3.6689],
        [3.6689, 3.6650, 3.6688],
        [3.6689, 2.8748, 2.4093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.13205714523792267 
model_pd.l_d.mean(): -23.0478572845459 
model_pd.lagr.mean(): -22.915800094604492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2434], device='cuda:0')), ('power', tensor([-23.2913], device='cuda:0'))])
epoch£º981	 i:0 	 global-step:19620	 l-p:0.13205714523792267
epoch£º981	 i:1 	 global-step:19621	 l-p:0.13199084997177124
epoch£º981	 i:2 	 global-step:19622	 l-p:0.13334494829177856
epoch£º981	 i:3 	 global-step:19623	 l-p:0.12828238308429718
epoch£º981	 i:4 	 global-step:19624	 l-p:0.08269670605659485
epoch£º981	 i:5 	 global-step:19625	 l-p:0.0984020009636879
epoch£º981	 i:6 	 global-step:19626	 l-p:0.14027507603168488
epoch£º981	 i:7 	 global-step:19627	 l-p:0.14544108510017395
epoch£º981	 i:8 	 global-step:19628	 l-p:0.18289104104042053
epoch£º981	 i:9 	 global-step:19629	 l-p:0.17846719920635223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6327, 3.0796, 3.0717],
        [3.6327, 3.0065, 2.3733],
        [3.6327, 3.6327, 3.6327],
        [3.6327, 3.4659, 3.5722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.09955835342407227 
model_pd.l_d.mean(): -23.503190994262695 
model_pd.lagr.mean(): -23.40363311767578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2310], device='cuda:0')), ('power', tensor([-23.7342], device='cuda:0'))])
epoch£º982	 i:0 	 global-step:19640	 l-p:0.09955835342407227
epoch£º982	 i:1 	 global-step:19641	 l-p:0.15115413069725037
epoch£º982	 i:2 	 global-step:19642	 l-p:0.1406145691871643
epoch£º982	 i:3 	 global-step:19643	 l-p:0.14704784750938416
epoch£º982	 i:4 	 global-step:19644	 l-p:0.09648367017507553
epoch£º982	 i:5 	 global-step:19645	 l-p:0.9490580558776855
epoch£º982	 i:6 	 global-step:19646	 l-p:0.1309458464384079
epoch£º982	 i:7 	 global-step:19647	 l-p:0.123508982360363
epoch£º982	 i:8 	 global-step:19648	 l-p:0.2498692125082016
epoch£º982	 i:9 	 global-step:19649	 l-p:0.15153111517429352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6169, 3.2713, 3.3892],
        [3.6169, 3.3539, 3.4794],
        [3.6169, 3.4498, 3.5563],
        [3.6169, 3.6114, 3.6167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): -0.17763495445251465 
model_pd.l_d.mean(): -23.624727249145508 
model_pd.lagr.mean(): -23.8023624420166 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2272], device='cuda:0')), ('power', tensor([-23.8519], device='cuda:0'))])
epoch£º983	 i:0 	 global-step:19660	 l-p:-0.17763495445251465
epoch£º983	 i:1 	 global-step:19661	 l-p:0.11922432482242584
epoch£º983	 i:2 	 global-step:19662	 l-p:0.07984675467014313
epoch£º983	 i:3 	 global-step:19663	 l-p:0.10546350479125977
epoch£º983	 i:4 	 global-step:19664	 l-p:0.13343724608421326
epoch£º983	 i:5 	 global-step:19665	 l-p:0.11494580656290054
epoch£º983	 i:6 	 global-step:19666	 l-p:0.11913667619228363
epoch£º983	 i:7 	 global-step:19667	 l-p:0.14435744285583496
epoch£º983	 i:8 	 global-step:19668	 l-p:0.1765945702791214
epoch£º983	 i:9 	 global-step:19669	 l-p:0.1493285745382309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6088, 2.7957, 2.2147],
        [3.6088, 3.5975, 3.6082],
        [3.6088, 3.6086, 3.6088],
        [3.6088, 3.5994, 3.6083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.330864816904068 
model_pd.l_d.mean(): -23.303613662719727 
model_pd.lagr.mean(): -22.972749710083008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2719], device='cuda:0')), ('power', tensor([-23.5755], device='cuda:0'))])
epoch£º984	 i:0 	 global-step:19680	 l-p:0.330864816904068
epoch£º984	 i:1 	 global-step:19681	 l-p:0.16480639576911926
epoch£º984	 i:2 	 global-step:19682	 l-p:0.10262630134820938
epoch£º984	 i:3 	 global-step:19683	 l-p:0.13887253403663635
epoch£º984	 i:4 	 global-step:19684	 l-p:0.09985796362161636
epoch£º984	 i:5 	 global-step:19685	 l-p:0.046103525906801224
epoch£º984	 i:6 	 global-step:19686	 l-p:0.7551699876785278
epoch£º984	 i:7 	 global-step:19687	 l-p:0.11948706954717636
epoch£º984	 i:8 	 global-step:19688	 l-p:0.13543212413787842
epoch£º984	 i:9 	 global-step:19689	 l-p:0.1372293084859848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6589, 3.2067, 2.5988],
        [3.6589, 3.3147, 3.4319],
        [3.6589, 2.9621, 2.3185],
        [3.6589, 3.6589, 3.6589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.0627945065498352 
model_pd.l_d.mean(): -23.45738983154297 
model_pd.lagr.mean(): -23.394596099853516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2210], device='cuda:0')), ('power', tensor([-23.6784], device='cuda:0'))])
epoch£º985	 i:0 	 global-step:19700	 l-p:0.0627945065498352
epoch£º985	 i:1 	 global-step:19701	 l-p:0.06586501747369766
epoch£º985	 i:2 	 global-step:19702	 l-p:0.023371979594230652
epoch£º985	 i:3 	 global-step:19703	 l-p:0.17988136410713196
epoch£º985	 i:4 	 global-step:19704	 l-p:0.12635798752307892
epoch£º985	 i:5 	 global-step:19705	 l-p:0.17229652404785156
epoch£º985	 i:6 	 global-step:19706	 l-p:0.12519767880439758
epoch£º985	 i:7 	 global-step:19707	 l-p:0.09698711335659027
epoch£º985	 i:8 	 global-step:19708	 l-p:0.13918428122997284
epoch£º985	 i:9 	 global-step:19709	 l-p:0.13177859783172607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7379, 3.7379, 3.7379],
        [3.7379, 3.5621, 3.6709],
        [3.7379, 3.7299, 3.7376],
        [3.7379, 3.4847, 3.6082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.1357959657907486 
model_pd.l_d.mean(): -23.468759536743164 
model_pd.lagr.mean(): -23.332963943481445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1829], device='cuda:0')), ('power', tensor([-23.6516], device='cuda:0'))])
epoch£º986	 i:0 	 global-step:19720	 l-p:0.1357959657907486
epoch£º986	 i:1 	 global-step:19721	 l-p:0.128420889377594
epoch£º986	 i:2 	 global-step:19722	 l-p:0.14868979156017303
epoch£º986	 i:3 	 global-step:19723	 l-p:0.21019217371940613
epoch£º986	 i:4 	 global-step:19724	 l-p:0.13531503081321716
epoch£º986	 i:5 	 global-step:19725	 l-p:0.13474096357822418
epoch£º986	 i:6 	 global-step:19726	 l-p:0.14649835228919983
epoch£º986	 i:7 	 global-step:19727	 l-p:0.1278390884399414
epoch£º986	 i:8 	 global-step:19728	 l-p:0.8784619569778442
epoch£º986	 i:9 	 global-step:19729	 l-p:0.8718591332435608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7002, 3.7002, 3.7002],
        [3.7002, 3.7002, 3.7002],
        [3.7002, 3.6086, 3.6790],
        [3.7002, 3.5342, 3.6399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.11334390193223953 
model_pd.l_d.mean(): -22.75953483581543 
model_pd.lagr.mean(): -22.646190643310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2035], device='cuda:0')), ('power', tensor([-22.9630], device='cuda:0'))])
epoch£º987	 i:0 	 global-step:19740	 l-p:0.11334390193223953
epoch£º987	 i:1 	 global-step:19741	 l-p:-0.5832245945930481
epoch£º987	 i:2 	 global-step:19742	 l-p:1.0037078857421875
epoch£º987	 i:3 	 global-step:19743	 l-p:0.15451394021511078
epoch£º987	 i:4 	 global-step:19744	 l-p:0.13947489857673645
epoch£º987	 i:5 	 global-step:19745	 l-p:-0.19780635833740234
epoch£º987	 i:6 	 global-step:19746	 l-p:0.13002483546733856
epoch£º987	 i:7 	 global-step:19747	 l-p:0.1364491581916809
epoch£º987	 i:8 	 global-step:19748	 l-p:0.1389899104833603
epoch£º987	 i:9 	 global-step:19749	 l-p:0.13767637312412262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6877, 2.9377, 2.6120],
        [3.6877, 3.0141, 2.8435],
        [3.6877, 3.6870, 3.6877],
        [3.6877, 3.6877, 3.6877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.14699812233448029 
model_pd.l_d.mean(): -23.613794326782227 
model_pd.lagr.mean(): -23.466796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1439], device='cuda:0')), ('power', tensor([-23.7577], device='cuda:0'))])
epoch£º988	 i:0 	 global-step:19760	 l-p:0.14699812233448029
epoch£º988	 i:1 	 global-step:19761	 l-p:0.20634004473686218
epoch£º988	 i:2 	 global-step:19762	 l-p:0.13028067350387573
epoch£º988	 i:3 	 global-step:19763	 l-p:0.059464938938617706
epoch£º988	 i:4 	 global-step:19764	 l-p:0.1466410756111145
epoch£º988	 i:5 	 global-step:19765	 l-p:0.10619039088487625
epoch£º988	 i:6 	 global-step:19766	 l-p:0.143819659948349
epoch£º988	 i:7 	 global-step:19767	 l-p:0.13337068259716034
epoch£º988	 i:8 	 global-step:19768	 l-p:0.09085189551115036
epoch£º988	 i:9 	 global-step:19769	 l-p:0.1850309669971466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6425, 3.2007, 3.2800],
        [3.6425, 3.6367, 3.6423],
        [3.6425, 3.3867, 3.5117],
        [3.6425, 3.6425, 3.6425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.12831789255142212 
model_pd.l_d.mean(): -23.63152503967285 
model_pd.lagr.mean(): -23.503206253051758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1510], device='cuda:0')), ('power', tensor([-23.7825], device='cuda:0'))])
epoch£º989	 i:0 	 global-step:19780	 l-p:0.12831789255142212
epoch£º989	 i:1 	 global-step:19781	 l-p:0.11615623533725739
epoch£º989	 i:2 	 global-step:19782	 l-p:0.14691106975078583
epoch£º989	 i:3 	 global-step:19783	 l-p:-0.020237330347299576
epoch£º989	 i:4 	 global-step:19784	 l-p:0.10704054683446884
epoch£º989	 i:5 	 global-step:19785	 l-p:0.14788818359375
epoch£º989	 i:6 	 global-step:19786	 l-p:0.1324833780527115
epoch£º989	 i:7 	 global-step:19787	 l-p:0.14204145967960358
epoch£º989	 i:8 	 global-step:19788	 l-p:0.27452269196510315
epoch£º989	 i:9 	 global-step:19789	 l-p:0.12809529900550842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228]], device='cuda:0')
 pt:tensor([[3.6378, 3.1008, 2.4798],
        [3.6378, 3.0037, 2.9010],
        [3.6378, 2.8323, 2.2315],
        [3.6378, 3.1192, 2.5013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): -0.1071619838476181 
model_pd.l_d.mean(): -22.697893142700195 
model_pd.lagr.mean(): -22.805055618286133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3796], device='cuda:0')), ('power', tensor([-23.0775], device='cuda:0'))])
epoch£º990	 i:0 	 global-step:19800	 l-p:-0.1071619838476181
epoch£º990	 i:1 	 global-step:19801	 l-p:0.08613760769367218
epoch£º990	 i:2 	 global-step:19802	 l-p:0.12274712324142456
epoch£º990	 i:3 	 global-step:19803	 l-p:0.27790239453315735
epoch£º990	 i:4 	 global-step:19804	 l-p:0.11112949997186661
epoch£º990	 i:5 	 global-step:19805	 l-p:0.14684616029262543
epoch£º990	 i:6 	 global-step:19806	 l-p:0.1466660350561142
epoch£º990	 i:7 	 global-step:19807	 l-p:0.13801303505897522
epoch£º990	 i:8 	 global-step:19808	 l-p:0.10710006207227707
epoch£º990	 i:9 	 global-step:19809	 l-p:0.20277610421180725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6350, 3.5400, 3.6125],
        [3.6350, 2.9432, 2.7515],
        [3.6350, 3.4806, 3.5824],
        [3.6350, 3.6274, 3.6346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.11039146035909653 
model_pd.l_d.mean(): -23.739242553710938 
model_pd.lagr.mean(): -23.62885093688965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1434], device='cuda:0')), ('power', tensor([-23.8827], device='cuda:0'))])
epoch£º991	 i:0 	 global-step:19820	 l-p:0.11039146035909653
epoch£º991	 i:1 	 global-step:19821	 l-p:0.15446580946445465
epoch£º991	 i:2 	 global-step:19822	 l-p:0.13184209167957306
epoch£º991	 i:3 	 global-step:19823	 l-p:0.15573355555534363
epoch£º991	 i:4 	 global-step:19824	 l-p:0.1259530782699585
epoch£º991	 i:5 	 global-step:19825	 l-p:-0.06618000566959381
epoch£º991	 i:6 	 global-step:19826	 l-p:0.13509881496429443
epoch£º991	 i:7 	 global-step:19827	 l-p:0.15373548865318298
epoch£º991	 i:8 	 global-step:19828	 l-p:0.48495566844940186
epoch£º991	 i:9 	 global-step:19829	 l-p:0.10539504885673523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6261, 3.6259, 3.6261],
        [3.6261, 3.5987, 3.6234],
        [3.6261, 2.9218, 2.2821],
        [3.6261, 2.8214, 2.2135]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.09693463891744614 
model_pd.l_d.mean(): -23.608659744262695 
model_pd.lagr.mean(): -23.5117244720459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1895], device='cuda:0')), ('power', tensor([-23.7981], device='cuda:0'))])
epoch£º992	 i:0 	 global-step:19840	 l-p:0.09693463891744614
epoch£º992	 i:1 	 global-step:19841	 l-p:0.06022867187857628
epoch£º992	 i:2 	 global-step:19842	 l-p:0.10074157267808914
epoch£º992	 i:3 	 global-step:19843	 l-p:0.10751523822546005
epoch£º992	 i:4 	 global-step:19844	 l-p:0.15306007862091064
epoch£º992	 i:5 	 global-step:19845	 l-p:0.8563845157623291
epoch£º992	 i:6 	 global-step:19846	 l-p:0.16060778498649597
epoch£º992	 i:7 	 global-step:19847	 l-p:0.13250792026519775
epoch£º992	 i:8 	 global-step:19848	 l-p:0.23635289072990417
epoch£º992	 i:9 	 global-step:19849	 l-p:0.11664684116840363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6251, 2.9597, 2.3227],
        [3.6251, 2.8865, 2.6064],
        [3.6251, 3.6251, 3.6251],
        [3.6251, 3.6050, 3.6235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.19830672442913055 
model_pd.l_d.mean(): -23.53716468811035 
model_pd.lagr.mean(): -23.338857650756836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2582], device='cuda:0')), ('power', tensor([-23.7954], device='cuda:0'))])
epoch£º993	 i:0 	 global-step:19860	 l-p:0.19830672442913055
epoch£º993	 i:1 	 global-step:19861	 l-p:0.14600753784179688
epoch£º993	 i:2 	 global-step:19862	 l-p:0.12080895900726318
epoch£º993	 i:3 	 global-step:19863	 l-p:0.15043187141418457
epoch£º993	 i:4 	 global-step:19864	 l-p:-4.733333587646484
epoch£º993	 i:5 	 global-step:19865	 l-p:0.23361188173294067
epoch£º993	 i:6 	 global-step:19866	 l-p:0.12036176770925522
epoch£º993	 i:7 	 global-step:19867	 l-p:0.12368306517601013
epoch£º993	 i:8 	 global-step:19868	 l-p:0.1352858990430832
epoch£º993	 i:9 	 global-step:19869	 l-p:0.056588247418403625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6152, 3.6062, 3.6148],
        [3.6152, 2.9021, 2.2636],
        [3.6152, 2.8013, 2.2874],
        [3.6152, 3.6149, 3.6152]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.13256455957889557 
model_pd.l_d.mean(): -23.427095413208008 
model_pd.lagr.mean(): -23.294530868530273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2450], device='cuda:0')), ('power', tensor([-23.6721], device='cuda:0'))])
epoch£º994	 i:0 	 global-step:19880	 l-p:0.13256455957889557
epoch£º994	 i:1 	 global-step:19881	 l-p:0.0935574546456337
epoch£º994	 i:2 	 global-step:19882	 l-p:0.11026032269001007
epoch£º994	 i:3 	 global-step:19883	 l-p:0.07152025401592255
epoch£º994	 i:4 	 global-step:19884	 l-p:0.5510021448135376
epoch£º994	 i:5 	 global-step:19885	 l-p:0.132407546043396
epoch£º994	 i:6 	 global-step:19886	 l-p:0.04318218678236008
epoch£º994	 i:7 	 global-step:19887	 l-p:0.15188150107860565
epoch£º994	 i:8 	 global-step:19888	 l-p:0.15823353826999664
epoch£º994	 i:9 	 global-step:19889	 l-p:0.1429838091135025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6006, 3.3660, 3.4896],
        [3.6006, 2.7824, 2.2027],
        [3.6006, 3.1935, 3.2926],
        [3.6006, 3.5967, 3.6005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.14836421608924866 
model_pd.l_d.mean(): -23.651248931884766 
model_pd.lagr.mean(): -23.502883911132812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2199], device='cuda:0')), ('power', tensor([-23.8711], device='cuda:0'))])
epoch£º995	 i:0 	 global-step:19900	 l-p:0.14836421608924866
epoch£º995	 i:1 	 global-step:19901	 l-p:0.08686124533414841
epoch£º995	 i:2 	 global-step:19902	 l-p:0.12644712626934052
epoch£º995	 i:3 	 global-step:19903	 l-p:0.2042216658592224
epoch£º995	 i:4 	 global-step:19904	 l-p:0.15528447926044464
epoch£º995	 i:5 	 global-step:19905	 l-p:0.16278384625911713
epoch£º995	 i:6 	 global-step:19906	 l-p:0.12179650366306305
epoch£º995	 i:7 	 global-step:19907	 l-p:0.04773418977856636
epoch£º995	 i:8 	 global-step:19908	 l-p:0.06603261083364487
epoch£º995	 i:9 	 global-step:19909	 l-p:0.11610371619462967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5780, 3.1082, 3.1745],
        [3.5780, 3.0510, 2.4401],
        [3.5780, 2.8368, 2.5635],
        [3.5780, 3.5768, 3.5780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): 0.009671077132225037 
model_pd.l_d.mean(): -22.402490615844727 
model_pd.lagr.mean(): -22.392820358276367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3993], device='cuda:0')), ('power', tensor([-22.8018], device='cuda:0'))])
epoch£º996	 i:0 	 global-step:19920	 l-p:0.009671077132225037
epoch£º996	 i:1 	 global-step:19921	 l-p:0.1740480214357376
epoch£º996	 i:2 	 global-step:19922	 l-p:0.03029368259012699
epoch£º996	 i:3 	 global-step:19923	 l-p:0.13812164962291718
epoch£º996	 i:4 	 global-step:19924	 l-p:0.15748468041419983
epoch£º996	 i:5 	 global-step:19925	 l-p:0.1351337730884552
epoch£º996	 i:6 	 global-step:19926	 l-p:0.1308375895023346
epoch£º996	 i:7 	 global-step:19927	 l-p:0.14218896627426147
epoch£º996	 i:8 	 global-step:19928	 l-p:0.13157238066196442
epoch£º996	 i:9 	 global-step:19929	 l-p:0.11533090472221375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6091, 3.4597, 3.5596],
        [3.6091, 3.6091, 3.6091],
        [3.6091, 2.9259, 2.7555],
        [3.6091, 2.9191, 2.2825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.14285171031951904 
model_pd.l_d.mean(): -23.179866790771484 
model_pd.lagr.mean(): -23.037015914916992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3373], device='cuda:0')), ('power', tensor([-23.5171], device='cuda:0'))])
epoch£º997	 i:0 	 global-step:19940	 l-p:0.14285171031951904
epoch£º997	 i:1 	 global-step:19941	 l-p:0.12505462765693665
epoch£º997	 i:2 	 global-step:19942	 l-p:0.06535413861274719
epoch£º997	 i:3 	 global-step:19943	 l-p:0.11135843396186829
epoch£º997	 i:4 	 global-step:19944	 l-p:0.14632736146450043
epoch£º997	 i:5 	 global-step:19945	 l-p:0.08375249803066254
epoch£º997	 i:6 	 global-step:19946	 l-p:-0.321703165769577
epoch£º997	 i:7 	 global-step:19947	 l-p:0.1342622935771942
epoch£º997	 i:8 	 global-step:19948	 l-p:0.10985942929983139
epoch£º997	 i:9 	 global-step:19949	 l-p:0.1649167239665985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5998, 2.7836, 2.1901],
        [3.5998, 3.5968, 3.5998],
        [3.5998, 3.5998, 3.5998],
        [3.5998, 3.5288, 3.5863]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.13993120193481445 
model_pd.l_d.mean(): -23.81732177734375 
model_pd.lagr.mean(): -23.677391052246094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1353], device='cuda:0')), ('power', tensor([-23.9527], device='cuda:0'))])
epoch£º998	 i:0 	 global-step:19960	 l-p:0.13993120193481445
epoch£º998	 i:1 	 global-step:19961	 l-p:0.04201457276940346
epoch£º998	 i:2 	 global-step:19962	 l-p:0.15803121030330658
epoch£º998	 i:3 	 global-step:19963	 l-p:0.18450500071048737
epoch£º998	 i:4 	 global-step:19964	 l-p:-2.4780728816986084
epoch£º998	 i:5 	 global-step:19965	 l-p:0.14111140370368958
epoch£º998	 i:6 	 global-step:19966	 l-p:0.15179748833179474
epoch£º998	 i:7 	 global-step:19967	 l-p:0.061321672052145004
epoch£º998	 i:8 	 global-step:19968	 l-p:-0.06124438717961311
epoch£º998	 i:9 	 global-step:19969	 l-p:0.14050336182117462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6239, 3.6235, 3.6239],
        [3.6239, 3.2100, 3.3056],
        [3.6239, 3.6186, 3.6237],
        [3.6239, 3.6157, 3.6235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.1376263052225113 
model_pd.l_d.mean(): -22.002899169921875 
model_pd.lagr.mean(): -21.865272521972656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3806], device='cuda:0')), ('power', tensor([-22.3835], device='cuda:0'))])
epoch£º999	 i:0 	 global-step:19980	 l-p:0.1376263052225113
epoch£º999	 i:1 	 global-step:19981	 l-p:0.07489875704050064
epoch£º999	 i:2 	 global-step:19982	 l-p:0.11791571974754333
epoch£º999	 i:3 	 global-step:19983	 l-p:0.11789239943027496
epoch£º999	 i:4 	 global-step:19984	 l-p:0.19469328224658966
epoch£º999	 i:5 	 global-step:19985	 l-p:0.13511250913143158
epoch£º999	 i:6 	 global-step:19986	 l-p:0.22021563351154327
epoch£º999	 i:7 	 global-step:19987	 l-p:0.33527639508247375
epoch£º999	 i:8 	 global-step:19988	 l-p:0.077492356300354
epoch£º999	 i:9 	 global-step:19989	 l-p:0.11159320920705795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6871, 3.5202, 3.6265],
        [3.6871, 2.8803, 2.3500],
        [3.6871, 3.5097, 3.6195],
        [3.6871, 2.9893, 2.3407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.1200527772307396 
model_pd.l_d.mean(): -23.475126266479492 
model_pd.lagr.mean(): -23.355073928833008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1755], device='cuda:0')), ('power', tensor([-23.6506], device='cuda:0'))])
epoch£º1000	 i:0 	 global-step:20000	 l-p:0.1200527772307396
epoch£º1000	 i:1 	 global-step:20001	 l-p:0.12295736372470856
epoch£º1000	 i:2 	 global-step:20002	 l-p:-0.04676022753119469
epoch£º1000	 i:3 	 global-step:20003	 l-p:-0.017683744430541992
epoch£º1000	 i:4 	 global-step:20004	 l-p:0.1271740198135376
epoch£º1000	 i:5 	 global-step:20005	 l-p:-1.3404079675674438
epoch£º1000	 i:6 	 global-step:20006	 l-p:0.13025061786174774
epoch£º1000	 i:7 	 global-step:20007	 l-p:0.22447562217712402
epoch£º1000	 i:8 	 global-step:20008	 l-p:0.1541847139596939
epoch£º1000	 i:9 	 global-step:20009	 l-p:0.14214858412742615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6972, 3.6951, 3.6972],
        [3.6972, 3.1467, 3.1394],
        [3.6972, 3.3704, 3.4912],
        [3.6972, 3.6971, 3.6972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): -0.3637944757938385 
model_pd.l_d.mean(): -22.820493698120117 
model_pd.lagr.mean(): -23.184288024902344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1977], device='cuda:0')), ('power', tensor([-23.0182], device='cuda:0'))])
epoch£º1001	 i:0 	 global-step:20020	 l-p:-0.3637944757938385
epoch£º1001	 i:1 	 global-step:20021	 l-p:0.13715137541294098
epoch£º1001	 i:2 	 global-step:20022	 l-p:0.11015739291906357
epoch£º1001	 i:3 	 global-step:20023	 l-p:0.15675820410251617
epoch£º1001	 i:4 	 global-step:20024	 l-p:0.14102581143379211
epoch£º1001	 i:5 	 global-step:20025	 l-p:0.6457183957099915
epoch£º1001	 i:6 	 global-step:20026	 l-p:-0.20334972441196442
epoch£º1001	 i:7 	 global-step:20027	 l-p:0.14398084580898285
epoch£º1001	 i:8 	 global-step:20028	 l-p:0.15394046902656555
epoch£º1001	 i:9 	 global-step:20029	 l-p:0.15076057612895966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7011, 3.6614, 3.6960],
        [3.7011, 3.2360, 3.2994],
        [3.7011, 3.2469, 2.6324],
        [3.7011, 3.4389, 3.5638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.1530817449092865 
model_pd.l_d.mean(): -23.466779708862305 
model_pd.lagr.mean(): -23.313697814941406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1857], device='cuda:0')), ('power', tensor([-23.6525], device='cuda:0'))])
epoch£º1002	 i:0 	 global-step:20040	 l-p:0.1530817449092865
epoch£º1002	 i:1 	 global-step:20041	 l-p:0.1516120433807373
epoch£º1002	 i:2 	 global-step:20042	 l-p:-0.3836780786514282
epoch£º1002	 i:3 	 global-step:20043	 l-p:0.10717608034610748
epoch£º1002	 i:4 	 global-step:20044	 l-p:0.1722954958677292
epoch£º1002	 i:5 	 global-step:20045	 l-p:41.59320831298828
epoch£º1002	 i:6 	 global-step:20046	 l-p:-0.06033327057957649
epoch£º1002	 i:7 	 global-step:20047	 l-p:0.16430479288101196
epoch£º1002	 i:8 	 global-step:20048	 l-p:0.12846125662326813
epoch£º1002	 i:9 	 global-step:20049	 l-p:0.12789377570152283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6975, 2.8986, 2.2919],
        [3.6975, 3.6975, 3.6975],
        [3.6975, 3.6975, 3.6975],
        [3.6975, 3.6189, 3.6812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.12745603919029236 
model_pd.l_d.mean(): -23.012958526611328 
model_pd.lagr.mean(): -22.885501861572266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2993], device='cuda:0')), ('power', tensor([-23.3122], device='cuda:0'))])
epoch£º1003	 i:0 	 global-step:20060	 l-p:0.12745603919029236
epoch£º1003	 i:1 	 global-step:20061	 l-p:0.1340705305337906
epoch£º1003	 i:2 	 global-step:20062	 l-p:0.13991127908229828
epoch£º1003	 i:3 	 global-step:20063	 l-p:0.13142119348049164
epoch£º1003	 i:4 	 global-step:20064	 l-p:0.1217128187417984
epoch£º1003	 i:5 	 global-step:20065	 l-p:0.20677450299263
epoch£º1003	 i:6 	 global-step:20066	 l-p:-0.8015625476837158
epoch£º1003	 i:7 	 global-step:20067	 l-p:-0.2391623854637146
epoch£º1003	 i:8 	 global-step:20068	 l-p:0.12934154272079468
epoch£º1003	 i:9 	 global-step:20069	 l-p:0.17514993250370026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6917, 2.9504, 2.6521],
        [3.6917, 2.9710, 2.3218],
        [3.6917, 3.1613, 3.1741],
        [3.6917, 3.6912, 3.6917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.2163141965866089 
model_pd.l_d.mean(): -23.38315200805664 
model_pd.lagr.mean(): -23.166837692260742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2625], device='cuda:0')), ('power', tensor([-23.6457], device='cuda:0'))])
epoch£º1004	 i:0 	 global-step:20080	 l-p:0.2163141965866089
epoch£º1004	 i:1 	 global-step:20081	 l-p:0.1342238336801529
epoch£º1004	 i:2 	 global-step:20082	 l-p:0.12267264723777771
epoch£º1004	 i:3 	 global-step:20083	 l-p:0.12496018409729004
epoch£º1004	 i:4 	 global-step:20084	 l-p:0.12989436089992523
epoch£º1004	 i:5 	 global-step:20085	 l-p:0.11883863806724548
epoch£º1004	 i:6 	 global-step:20086	 l-p:0.16364768147468567
epoch£º1004	 i:7 	 global-step:20087	 l-p:-0.08745905011892319
epoch£º1004	 i:8 	 global-step:20088	 l-p:0.11818138509988785
epoch£º1004	 i:9 	 global-step:20089	 l-p:-0.09315071254968643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6829, 3.4636, 3.5840],
        [3.6829, 3.1464, 3.1542],
        [3.6829, 3.6803, 3.6828],
        [3.6829, 3.3377, 3.4552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.12588541209697723 
model_pd.l_d.mean(): -23.465106964111328 
model_pd.lagr.mean(): -23.339221954345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1576], device='cuda:0')), ('power', tensor([-23.6227], device='cuda:0'))])
epoch£º1005	 i:0 	 global-step:20100	 l-p:0.12588541209697723
epoch£º1005	 i:1 	 global-step:20101	 l-p:0.11090946942567825
epoch£º1005	 i:2 	 global-step:20102	 l-p:-0.11560367047786713
epoch£º1005	 i:3 	 global-step:20103	 l-p:0.1389295905828476
epoch£º1005	 i:4 	 global-step:20104	 l-p:0.12716588377952576
epoch£º1005	 i:5 	 global-step:20105	 l-p:0.1338309645652771
epoch£º1005	 i:6 	 global-step:20106	 l-p:0.1479787528514862
epoch£º1005	 i:7 	 global-step:20107	 l-p:0.18211093544960022
epoch£º1005	 i:8 	 global-step:20108	 l-p:0.17959976196289062
epoch£º1005	 i:9 	 global-step:20109	 l-p:0.1075357049703598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]], device='cuda:0')
 pt:tensor([[3.6750, 3.1577, 2.5344],
        [3.6750, 2.8720, 2.2666],
        [3.6750, 3.0513, 2.4110],
        [3.6750, 3.0121, 2.3673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.00814970675855875 
model_pd.l_d.mean(): -22.68911361694336 
model_pd.lagr.mean(): -22.68096351623535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3847], device='cuda:0')), ('power', tensor([-23.0738], device='cuda:0'))])
epoch£º1006	 i:0 	 global-step:20120	 l-p:0.00814970675855875
epoch£º1006	 i:1 	 global-step:20121	 l-p:0.29592227935791016
epoch£º1006	 i:2 	 global-step:20122	 l-p:0.05357085540890694
epoch£º1006	 i:3 	 global-step:20123	 l-p:0.09080329537391663
epoch£º1006	 i:4 	 global-step:20124	 l-p:-0.0003863525344058871
epoch£º1006	 i:5 	 global-step:20125	 l-p:0.14262443780899048
epoch£º1006	 i:6 	 global-step:20126	 l-p:0.1559094935655594
epoch£º1006	 i:7 	 global-step:20127	 l-p:0.14033864438533783
epoch£º1006	 i:8 	 global-step:20128	 l-p:0.1342879682779312
epoch£º1006	 i:9 	 global-step:20129	 l-p:0.11625675112009048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6944, 3.5905, 3.6681],
        [3.6944, 3.6932, 3.6944],
        [3.6944, 3.6944, 3.6944],
        [3.6944, 3.6541, 3.6892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): -0.13482409715652466 
model_pd.l_d.mean(): -23.17568588256836 
model_pd.lagr.mean(): -23.310510635375977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2777], device='cuda:0')), ('power', tensor([-23.4534], device='cuda:0'))])
epoch£º1007	 i:0 	 global-step:20140	 l-p:-0.13482409715652466
epoch£º1007	 i:1 	 global-step:20141	 l-p:0.14906024932861328
epoch£º1007	 i:2 	 global-step:20142	 l-p:0.1275682896375656
epoch£º1007	 i:3 	 global-step:20143	 l-p:0.11440666019916534
epoch£º1007	 i:4 	 global-step:20144	 l-p:0.13804012537002563
epoch£º1007	 i:5 	 global-step:20145	 l-p:0.11178280413150787
epoch£º1007	 i:6 	 global-step:20146	 l-p:0.21444503962993622
epoch£º1007	 i:7 	 global-step:20147	 l-p:0.14954525232315063
epoch£º1007	 i:8 	 global-step:20148	 l-p:0.1319132298231125
epoch£º1007	 i:9 	 global-step:20149	 l-p:-0.14883187413215637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6812, 3.2767, 3.3746],
        [3.6812, 3.6181, 3.6700],
        [3.6812, 2.8739, 2.2867],
        [3.6812, 2.8784, 2.3904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.21397751569747925 
model_pd.l_d.mean(): -23.614660263061523 
model_pd.lagr.mean(): -23.40068244934082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2178], device='cuda:0')), ('power', tensor([-23.8325], device='cuda:0'))])
epoch£º1008	 i:0 	 global-step:20160	 l-p:0.21397751569747925
epoch£º1008	 i:1 	 global-step:20161	 l-p:0.2454366385936737
epoch£º1008	 i:2 	 global-step:20162	 l-p:0.09248855710029602
epoch£º1008	 i:3 	 global-step:20163	 l-p:0.12742549180984497
epoch£º1008	 i:4 	 global-step:20164	 l-p:-0.06506486237049103
epoch£º1008	 i:5 	 global-step:20165	 l-p:0.14531610906124115
epoch£º1008	 i:6 	 global-step:20166	 l-p:-0.09865943342447281
epoch£º1008	 i:7 	 global-step:20167	 l-p:0.13242025673389435
epoch£º1008	 i:8 	 global-step:20168	 l-p:0.13980163633823395
epoch£º1008	 i:9 	 global-step:20169	 l-p:0.12610319256782532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6783, 3.6730, 3.6781],
        [3.6783, 3.6781, 3.6783],
        [3.6783, 3.2916, 3.3969],
        [3.6783, 3.4430, 3.5660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.15138594806194305 
model_pd.l_d.mean(): -23.65399169921875 
model_pd.lagr.mean(): -23.502605438232422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1844], device='cuda:0')), ('power', tensor([-23.8384], device='cuda:0'))])
epoch£º1009	 i:0 	 global-step:20180	 l-p:0.15138594806194305
epoch£º1009	 i:1 	 global-step:20181	 l-p:0.17334440350532532
epoch£º1009	 i:2 	 global-step:20182	 l-p:0.13915091753005981
epoch£º1009	 i:3 	 global-step:20183	 l-p:0.023518651723861694
epoch£º1009	 i:4 	 global-step:20184	 l-p:0.14112576842308044
epoch£º1009	 i:5 	 global-step:20185	 l-p:0.11634668707847595
epoch£º1009	 i:6 	 global-step:20186	 l-p:0.13285820186138153
epoch£º1009	 i:7 	 global-step:20187	 l-p:0.12338119000196457
epoch£º1009	 i:8 	 global-step:20188	 l-p:0.27541130781173706
epoch£º1009	 i:9 	 global-step:20189	 l-p:0.1467852145433426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6610, 2.8587, 2.2460],
        [3.6610, 3.6608, 3.6610],
        [3.6610, 3.6610, 3.6610],
        [3.6610, 3.6205, 3.6557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.12223109602928162 
model_pd.l_d.mean(): -23.261981964111328 
model_pd.lagr.mean(): -23.139751434326172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2461], device='cuda:0')), ('power', tensor([-23.5081], device='cuda:0'))])
epoch£º1010	 i:0 	 global-step:20200	 l-p:0.12223109602928162
epoch£º1010	 i:1 	 global-step:20201	 l-p:0.14403995871543884
epoch£º1010	 i:2 	 global-step:20202	 l-p:0.11788144707679749
epoch£º1010	 i:3 	 global-step:20203	 l-p:0.1533200740814209
epoch£º1010	 i:4 	 global-step:20204	 l-p:0.1655396968126297
epoch£º1010	 i:5 	 global-step:20205	 l-p:0.1262248456478119
epoch£º1010	 i:6 	 global-step:20206	 l-p:0.10434059798717499
epoch£º1010	 i:7 	 global-step:20207	 l-p:-0.04487347602844238
epoch£º1010	 i:8 	 global-step:20208	 l-p:0.04639996588230133
epoch£º1010	 i:9 	 global-step:20209	 l-p:0.5147504806518555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6341, 2.9478, 2.3071],
        [3.6341, 2.9674, 2.3283],
        [3.6341, 2.9608, 2.3211],
        [3.6341, 3.6332, 3.6341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.1680913120508194 
model_pd.l_d.mean(): -22.27531623840332 
model_pd.lagr.mean(): -22.10722541809082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4176], device='cuda:0')), ('power', tensor([-22.6930], device='cuda:0'))])
epoch£º1011	 i:0 	 global-step:20220	 l-p:0.1680913120508194
epoch£º1011	 i:1 	 global-step:20221	 l-p:0.20579780638217926
epoch£º1011	 i:2 	 global-step:20222	 l-p:0.011354073882102966
epoch£º1011	 i:3 	 global-step:20223	 l-p:0.08485235273838043
epoch£º1011	 i:4 	 global-step:20224	 l-p:0.3442274332046509
epoch£º1011	 i:5 	 global-step:20225	 l-p:0.09704533219337463
epoch£º1011	 i:6 	 global-step:20226	 l-p:0.12452149391174316
epoch£º1011	 i:7 	 global-step:20227	 l-p:0.09422850608825684
epoch£º1011	 i:8 	 global-step:20228	 l-p:0.12323898822069168
epoch£º1011	 i:9 	 global-step:20229	 l-p:0.14050687849521637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6540, 3.3447, 3.4692],
        [3.6540, 3.6456, 3.6536],
        [3.6540, 2.8957, 2.5684],
        [3.6540, 3.4277, 3.5497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.12878204882144928 
model_pd.l_d.mean(): -23.656654357910156 
model_pd.lagr.mean(): -23.52787208557129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1779], device='cuda:0')), ('power', tensor([-23.8345], device='cuda:0'))])
epoch£º1012	 i:0 	 global-step:20240	 l-p:0.12878204882144928
epoch£º1012	 i:1 	 global-step:20241	 l-p:0.1294250339269638
epoch£º1012	 i:2 	 global-step:20242	 l-p:0.14551852643489838
epoch£º1012	 i:3 	 global-step:20243	 l-p:0.1464705914258957
epoch£º1012	 i:4 	 global-step:20244	 l-p:0.09080202132463455
epoch£º1012	 i:5 	 global-step:20245	 l-p:0.18806710839271545
epoch£º1012	 i:6 	 global-step:20246	 l-p:0.1248326301574707
epoch£º1012	 i:7 	 global-step:20247	 l-p:0.1435072273015976
epoch£º1012	 i:8 	 global-step:20248	 l-p:0.08211351931095123
epoch£º1012	 i:9 	 global-step:20249	 l-p:-0.06350157409906387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6457, 2.8581, 2.2279],
        [3.6457, 3.6457, 3.6457],
        [3.6457, 2.9364, 2.7166],
        [3.6457, 2.9006, 2.6071]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): 0.09392188489437103 
model_pd.l_d.mean(): -23.575366973876953 
model_pd.lagr.mean(): -23.4814453125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2235], device='cuda:0')), ('power', tensor([-23.7989], device='cuda:0'))])
epoch£º1013	 i:0 	 global-step:20260	 l-p:0.09392188489437103
epoch£º1013	 i:1 	 global-step:20261	 l-p:0.202920600771904
epoch£º1013	 i:2 	 global-step:20262	 l-p:0.12538979947566986
epoch£º1013	 i:3 	 global-step:20263	 l-p:0.12106353789567947
epoch£º1013	 i:4 	 global-step:20264	 l-p:0.23870892822742462
epoch£º1013	 i:5 	 global-step:20265	 l-p:0.10636457055807114
epoch£º1013	 i:6 	 global-step:20266	 l-p:2.2008769512176514
epoch£º1013	 i:7 	 global-step:20267	 l-p:0.13715291023254395
epoch£º1013	 i:8 	 global-step:20268	 l-p:0.14187243580818176
epoch£º1013	 i:9 	 global-step:20269	 l-p:0.07474258542060852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6615, 3.2431, 2.6409],
        [3.6615, 3.6608, 3.6615],
        [3.6615, 3.6615, 3.6615],
        [3.6615, 2.9236, 2.6427]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.08309590816497803 
model_pd.l_d.mean(): -23.124643325805664 
model_pd.lagr.mean(): -23.041547775268555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2367], device='cuda:0')), ('power', tensor([-23.3614], device='cuda:0'))])
epoch£º1014	 i:0 	 global-step:20280	 l-p:0.08309590816497803
epoch£º1014	 i:1 	 global-step:20281	 l-p:0.2356456071138382
epoch£º1014	 i:2 	 global-step:20282	 l-p:0.10579998791217804
epoch£º1014	 i:3 	 global-step:20283	 l-p:-0.03566454350948334
epoch£º1014	 i:4 	 global-step:20284	 l-p:0.12322399020195007
epoch£º1014	 i:5 	 global-step:20285	 l-p:0.14766208827495575
epoch£º1014	 i:6 	 global-step:20286	 l-p:0.12007761001586914
epoch£º1014	 i:7 	 global-step:20287	 l-p:0.2760288417339325
epoch£º1014	 i:8 	 global-step:20288	 l-p:0.1603732705116272
epoch£º1014	 i:9 	 global-step:20289	 l-p:0.13765433430671692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6739, 2.8750, 2.2578],
        [3.6739, 3.6629, 3.6733],
        [3.6739, 3.6704, 3.6738],
        [3.6739, 3.2135, 2.6009]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.14116553962230682 
model_pd.l_d.mean(): -23.32472801208496 
model_pd.lagr.mean(): -23.183563232421875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2783], device='cuda:0')), ('power', tensor([-23.6030], device='cuda:0'))])
epoch£º1015	 i:0 	 global-step:20300	 l-p:0.14116553962230682
epoch£º1015	 i:1 	 global-step:20301	 l-p:0.047937825322151184
epoch£º1015	 i:2 	 global-step:20302	 l-p:0.11415782570838928
epoch£º1015	 i:3 	 global-step:20303	 l-p:0.066501684486866
epoch£º1015	 i:4 	 global-step:20304	 l-p:0.22516335546970367
epoch£º1015	 i:5 	 global-step:20305	 l-p:0.1576654016971588
epoch£º1015	 i:6 	 global-step:20306	 l-p:0.14657261967658997
epoch£º1015	 i:7 	 global-step:20307	 l-p:0.12582899630069733
epoch£º1015	 i:8 	 global-step:20308	 l-p:0.11546051502227783
epoch£º1015	 i:9 	 global-step:20309	 l-p:0.1174892783164978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6775, 3.6769, 3.6775],
        [3.6775, 3.6776, 3.6776],
        [3.6775, 3.5728, 3.6509],
        [3.6775, 3.6747, 3.6775]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.1341278851032257 
model_pd.l_d.mean(): -22.629772186279297 
model_pd.lagr.mean(): -22.495643615722656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2252], device='cuda:0')), ('power', tensor([-22.8549], device='cuda:0'))])
epoch£º1016	 i:0 	 global-step:20320	 l-p:0.1341278851032257
epoch£º1016	 i:1 	 global-step:20321	 l-p:0.15573570132255554
epoch£º1016	 i:2 	 global-step:20322	 l-p:0.23902283608913422
epoch£º1016	 i:3 	 global-step:20323	 l-p:0.058859933167696
epoch£º1016	 i:4 	 global-step:20324	 l-p:0.15175069868564606
epoch£º1016	 i:5 	 global-step:20325	 l-p:0.09396007657051086
epoch£º1016	 i:6 	 global-step:20326	 l-p:0.14770330488681793
epoch£º1016	 i:7 	 global-step:20327	 l-p:0.13684223592281342
epoch£º1016	 i:8 	 global-step:20328	 l-p:0.1476336121559143
epoch£º1016	 i:9 	 global-step:20329	 l-p:-0.03538966178894043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6766, 3.6765, 3.6766],
        [3.6766, 3.5017, 3.6109],
        [3.6766, 3.1808, 3.2246],
        [3.6766, 3.6766, 3.6766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.11084560304880142 
model_pd.l_d.mean(): -23.245677947998047 
model_pd.lagr.mean(): -23.13483238220215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2401], device='cuda:0')), ('power', tensor([-23.4858], device='cuda:0'))])
epoch£º1017	 i:0 	 global-step:20340	 l-p:0.11084560304880142
epoch£º1017	 i:1 	 global-step:20341	 l-p:0.1260858178138733
epoch£º1017	 i:2 	 global-step:20342	 l-p:-0.009388904087245464
epoch£º1017	 i:3 	 global-step:20343	 l-p:0.15162311494350433
epoch£º1017	 i:4 	 global-step:20344	 l-p:0.1414938122034073
epoch£º1017	 i:5 	 global-step:20345	 l-p:0.06246437877416611
epoch£º1017	 i:6 	 global-step:20346	 l-p:0.30452761054039
epoch£º1017	 i:7 	 global-step:20347	 l-p:0.20248618721961975
epoch£º1017	 i:8 	 global-step:20348	 l-p:0.14073432981967926
epoch£º1017	 i:9 	 global-step:20349	 l-p:0.10589651018381119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6701, 3.6617, 3.6697],
        [3.6701, 3.5143, 3.6167],
        [3.6701, 2.9339, 2.2874],
        [3.6701, 3.0367, 2.9357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.11814364045858383 
model_pd.l_d.mean(): -22.95414924621582 
model_pd.lagr.mean(): -22.83600616455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2439], device='cuda:0')), ('power', tensor([-23.1980], device='cuda:0'))])
epoch£º1018	 i:0 	 global-step:20360	 l-p:0.11814364045858383
epoch£º1018	 i:1 	 global-step:20361	 l-p:0.16440926492214203
epoch£º1018	 i:2 	 global-step:20362	 l-p:0.03842803090810776
epoch£º1018	 i:3 	 global-step:20363	 l-p:0.13717293739318848
epoch£º1018	 i:4 	 global-step:20364	 l-p:0.1285427212715149
epoch£º1018	 i:5 	 global-step:20365	 l-p:0.1078977957367897
epoch£º1018	 i:6 	 global-step:20366	 l-p:0.2110072672367096
epoch£º1018	 i:7 	 global-step:20367	 l-p:0.056237153708934784
epoch£º1018	 i:8 	 global-step:20368	 l-p:0.11979915201663971
epoch£º1018	 i:9 	 global-step:20369	 l-p:0.3292407989501953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6680, 3.6485, 3.6665],
        [3.6680, 3.0738, 3.0232],
        [3.6680, 2.9626, 2.3158],
        [3.6680, 3.2303, 2.6231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.1368037909269333 
model_pd.l_d.mean(): -23.500823974609375 
model_pd.lagr.mean(): -23.3640193939209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1702], device='cuda:0')), ('power', tensor([-23.6710], device='cuda:0'))])
epoch£º1019	 i:0 	 global-step:20380	 l-p:0.1368037909269333
epoch£º1019	 i:1 	 global-step:20381	 l-p:0.14973826706409454
epoch£º1019	 i:2 	 global-step:20382	 l-p:0.14639291167259216
epoch£º1019	 i:3 	 global-step:20383	 l-p:0.3368690311908722
epoch£º1019	 i:4 	 global-step:20384	 l-p:0.12271719425916672
epoch£º1019	 i:5 	 global-step:20385	 l-p:0.13227447867393494
epoch£º1019	 i:6 	 global-step:20386	 l-p:0.05316932499408722
epoch£º1019	 i:7 	 global-step:20387	 l-p:0.08615739643573761
epoch£º1019	 i:8 	 global-step:20388	 l-p:0.13184881210327148
epoch£º1019	 i:9 	 global-step:20389	 l-p:0.1809634119272232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6658, 2.8687, 2.4221],
        [3.6658, 3.4933, 3.6018],
        [3.6658, 2.9277, 2.6465],
        [3.6658, 2.8723, 2.2466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.35537585616111755 
model_pd.l_d.mean(): -23.479259490966797 
model_pd.lagr.mean(): -23.123884201049805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2175], device='cuda:0')), ('power', tensor([-23.6967], device='cuda:0'))])
epoch£º1020	 i:0 	 global-step:20400	 l-p:0.35537585616111755
epoch£º1020	 i:1 	 global-step:20401	 l-p:0.13368725776672363
epoch£º1020	 i:2 	 global-step:20402	 l-p:0.15179802477359772
epoch£º1020	 i:3 	 global-step:20403	 l-p:0.15800824761390686
epoch£º1020	 i:4 	 global-step:20404	 l-p:-0.049738552421331406
epoch£º1020	 i:5 	 global-step:20405	 l-p:0.08021590858697891
epoch£º1020	 i:6 	 global-step:20406	 l-p:0.17642676830291748
epoch£º1020	 i:7 	 global-step:20407	 l-p:0.15014812350273132
epoch£º1020	 i:8 	 global-step:20408	 l-p:0.11648520827293396
epoch£º1020	 i:9 	 global-step:20409	 l-p:0.12649224698543549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6744, 2.9813, 2.7868],
        [3.6744, 3.6744, 3.6744],
        [3.6744, 2.8915, 2.4914],
        [3.6744, 3.1030, 3.0777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.126743882894516 
model_pd.l_d.mean(): -23.64546775817871 
model_pd.lagr.mean(): -23.51872444152832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1883], device='cuda:0')), ('power', tensor([-23.8338], device='cuda:0'))])
epoch£º1021	 i:0 	 global-step:20420	 l-p:0.126743882894516
epoch£º1021	 i:1 	 global-step:20421	 l-p:0.12166167795658112
epoch£º1021	 i:2 	 global-step:20422	 l-p:0.04481019824743271
epoch£º1021	 i:3 	 global-step:20423	 l-p:0.2039654552936554
epoch£º1021	 i:4 	 global-step:20424	 l-p:0.12437570095062256
epoch£º1021	 i:5 	 global-step:20425	 l-p:0.10467875003814697
epoch£º1021	 i:6 	 global-step:20426	 l-p:0.15462015569210052
epoch£º1021	 i:7 	 global-step:20427	 l-p:0.07446043938398361
epoch£º1021	 i:8 	 global-step:20428	 l-p:0.31053125858306885
epoch£º1021	 i:9 	 global-step:20429	 l-p:0.13141924142837524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6682, 3.6680, 3.6682],
        [3.6682, 2.8712, 2.4245],
        [3.6682, 3.2530, 3.3471],
        [3.6682, 3.6682, 3.6682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.13172626495361328 
model_pd.l_d.mean(): -23.58795166015625 
model_pd.lagr.mean(): -23.456226348876953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1713], device='cuda:0')), ('power', tensor([-23.7593], device='cuda:0'))])
epoch£º1022	 i:0 	 global-step:20440	 l-p:0.13172626495361328
epoch£º1022	 i:1 	 global-step:20441	 l-p:0.18225733935832977
epoch£º1022	 i:2 	 global-step:20442	 l-p:0.1309286206960678
epoch£º1022	 i:3 	 global-step:20443	 l-p:0.12426237016916275
epoch£º1022	 i:4 	 global-step:20444	 l-p:0.09755326062440872
epoch£º1022	 i:5 	 global-step:20445	 l-p:0.2002364844083786
epoch£º1022	 i:6 	 global-step:20446	 l-p:0.14210662245750427
epoch£º1022	 i:7 	 global-step:20447	 l-p:0.5737270712852478
epoch£º1022	 i:8 	 global-step:20448	 l-p:0.16013942658901215
epoch£º1022	 i:9 	 global-step:20449	 l-p:0.0177746769040823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6568, 3.6556, 3.6568],
        [3.6568, 2.8791, 2.5017],
        [3.6568, 3.6568, 3.6568],
        [3.6568, 3.3222, 3.4431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.13751137256622314 
model_pd.l_d.mean(): -23.80657958984375 
model_pd.lagr.mean(): -23.6690673828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0970], device='cuda:0')), ('power', tensor([-23.9036], device='cuda:0'))])
epoch£º1023	 i:0 	 global-step:20460	 l-p:0.13751137256622314
epoch£º1023	 i:1 	 global-step:20461	 l-p:3.65549373626709
epoch£º1023	 i:2 	 global-step:20462	 l-p:0.11063192039728165
epoch£º1023	 i:3 	 global-step:20463	 l-p:0.15571542084217072
epoch£º1023	 i:4 	 global-step:20464	 l-p:0.145461305975914
epoch£º1023	 i:5 	 global-step:20465	 l-p:0.24020151793956757
epoch£º1023	 i:6 	 global-step:20466	 l-p:0.12088727951049805
epoch£º1023	 i:7 	 global-step:20467	 l-p:0.12836787104606628
epoch£º1023	 i:8 	 global-step:20468	 l-p:0.08291913568973541
epoch£º1023	 i:9 	 global-step:20469	 l-p:0.0673023983836174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6577, 3.6577, 3.6577],
        [3.6577, 2.9182, 2.2731],
        [3.6577, 2.9038, 2.2604],
        [3.6577, 3.6577, 3.6577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.1483009159564972 
model_pd.l_d.mean(): -23.020328521728516 
model_pd.lagr.mean(): -22.872028350830078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3047], device='cuda:0')), ('power', tensor([-23.3250], device='cuda:0'))])
epoch£º1024	 i:0 	 global-step:20480	 l-p:0.1483009159564972
epoch£º1024	 i:1 	 global-step:20481	 l-p:0.7748582363128662
epoch£º1024	 i:2 	 global-step:20482	 l-p:0.07490847259759903
epoch£º1024	 i:3 	 global-step:20483	 l-p:0.14913350343704224
epoch£º1024	 i:4 	 global-step:20484	 l-p:0.12563247978687286
epoch£º1024	 i:5 	 global-step:20485	 l-p:0.17087465524673462
epoch£º1024	 i:6 	 global-step:20486	 l-p:0.14464400708675385
epoch£º1024	 i:7 	 global-step:20487	 l-p:0.08843078464269638
epoch£º1024	 i:8 	 global-step:20488	 l-p:0.062068235129117966
epoch£º1024	 i:9 	 global-step:20489	 l-p:0.1393505036830902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6705, 3.6680, 3.6705],
        [3.6705, 3.6609, 3.6700],
        [3.6705, 3.6705, 3.6705],
        [3.6705, 3.6690, 3.6705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.030035685747861862 
model_pd.l_d.mean(): -23.587873458862305 
model_pd.lagr.mean(): -23.557838439941406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1351], device='cuda:0')), ('power', tensor([-23.7229], device='cuda:0'))])
epoch£º1025	 i:0 	 global-step:20500	 l-p:0.030035685747861862
epoch£º1025	 i:1 	 global-step:20501	 l-p:0.14103029668331146
epoch£º1025	 i:2 	 global-step:20502	 l-p:0.13071158528327942
epoch£º1025	 i:3 	 global-step:20503	 l-p:0.11849790066480637
epoch£º1025	 i:4 	 global-step:20504	 l-p:0.13069432973861694
epoch£º1025	 i:5 	 global-step:20505	 l-p:0.12191701680421829
epoch£º1025	 i:6 	 global-step:20506	 l-p:0.06930667906999588
epoch£º1025	 i:7 	 global-step:20507	 l-p:0.11739940196275711
epoch£º1025	 i:8 	 global-step:20508	 l-p:0.20682792365550995
epoch£º1025	 i:9 	 global-step:20509	 l-p:0.31958234310150146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7151,  0.6395,  1.0000,  0.5719,
          1.0000,  0.8943, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4406,  0.3353,  1.0000,  0.2551,
          1.0000,  0.7609, 31.6228]], device='cuda:0')
 pt:tensor([[3.6692, 3.2309, 2.6233],
        [3.6692, 2.9148, 2.5963],
        [3.6692, 2.9669, 2.3197],
        [3.6692, 2.8665, 2.3972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.09772834926843643 
model_pd.l_d.mean(): -23.15803337097168 
model_pd.lagr.mean(): -23.060304641723633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3031], device='cuda:0')), ('power', tensor([-23.4611], device='cuda:0'))])
epoch£º1026	 i:0 	 global-step:20520	 l-p:0.09772834926843643
epoch£º1026	 i:1 	 global-step:20521	 l-p:0.12537391483783722
epoch£º1026	 i:2 	 global-step:20522	 l-p:0.1421470195055008
epoch£º1026	 i:3 	 global-step:20523	 l-p:0.13588783144950867
epoch£º1026	 i:4 	 global-step:20524	 l-p:0.14265741407871246
epoch£º1026	 i:5 	 global-step:20525	 l-p:0.06654234975576401
epoch£º1026	 i:6 	 global-step:20526	 l-p:0.18552687764167786
epoch£º1026	 i:7 	 global-step:20527	 l-p:0.31735923886299133
epoch£º1026	 i:8 	 global-step:20528	 l-p:0.1292881965637207
epoch£º1026	 i:9 	 global-step:20529	 l-p:0.04859910532832146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6704, 2.9484, 2.7013],
        [3.6704, 2.8675, 2.2533],
        [3.6704, 3.4995, 3.6075],
        [3.6704, 3.0972, 3.0708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.11631282418966293 
model_pd.l_d.mean(): -23.1893310546875 
model_pd.lagr.mean(): -23.07301902770996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3149], device='cuda:0')), ('power', tensor([-23.5042], device='cuda:0'))])
epoch£º1027	 i:0 	 global-step:20540	 l-p:0.11631282418966293
epoch£º1027	 i:1 	 global-step:20541	 l-p:0.13124777376651764
epoch£º1027	 i:2 	 global-step:20542	 l-p:0.07211490720510483
epoch£º1027	 i:3 	 global-step:20543	 l-p:0.1287374347448349
epoch£º1027	 i:4 	 global-step:20544	 l-p:0.17986075580120087
epoch£º1027	 i:5 	 global-step:20545	 l-p:0.14653432369232178
epoch£º1027	 i:6 	 global-step:20546	 l-p:0.3233369290828705
epoch£º1027	 i:7 	 global-step:20547	 l-p:0.16949217021465302
epoch£º1027	 i:8 	 global-step:20548	 l-p:0.031834088265895844
epoch£º1027	 i:9 	 global-step:20549	 l-p:0.0799814760684967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6750, 3.6738, 3.6750],
        [3.6750, 3.6750, 3.6750],
        [3.6750, 3.6668, 3.6746],
        [3.6750, 3.6350, 3.6698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.14867617189884186 
model_pd.l_d.mean(): -23.734586715698242 
model_pd.lagr.mean(): -23.58591079711914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0929], device='cuda:0')), ('power', tensor([-23.8275], device='cuda:0'))])
epoch£º1028	 i:0 	 global-step:20560	 l-p:0.14867617189884186
epoch£º1028	 i:1 	 global-step:20561	 l-p:0.1492319107055664
epoch£º1028	 i:2 	 global-step:20562	 l-p:0.04635998606681824
epoch£º1028	 i:3 	 global-step:20563	 l-p:0.23452168703079224
epoch£º1028	 i:4 	 global-step:20564	 l-p:0.13880859315395355
epoch£º1028	 i:5 	 global-step:20565	 l-p:0.13294826447963715
epoch£º1028	 i:6 	 global-step:20566	 l-p:0.1414162963628769
epoch£º1028	 i:7 	 global-step:20567	 l-p:0.02558816410601139
epoch£º1028	 i:8 	 global-step:20568	 l-p:-0.08441894501447678
epoch£º1028	 i:9 	 global-step:20569	 l-p:0.17174024879932404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6894, 2.8923, 2.4368],
        [3.6894, 3.6894, 3.6894],
        [3.6894, 3.6894, 3.6894],
        [3.6894, 3.2555, 3.3396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.1288791000843048 
model_pd.l_d.mean(): -23.671478271484375 
model_pd.lagr.mean(): -23.542598724365234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1306], device='cuda:0')), ('power', tensor([-23.8020], device='cuda:0'))])
epoch£º1029	 i:0 	 global-step:20580	 l-p:0.1288791000843048
epoch£º1029	 i:1 	 global-step:20581	 l-p:0.12386027723550797
epoch£º1029	 i:2 	 global-step:20582	 l-p:-0.5887305736541748
epoch£º1029	 i:3 	 global-step:20583	 l-p:0.14002665877342224
epoch£º1029	 i:4 	 global-step:20584	 l-p:0.17530062794685364
epoch£º1029	 i:5 	 global-step:20585	 l-p:0.1324576437473297
epoch£º1029	 i:6 	 global-step:20586	 l-p:0.18957825005054474
epoch£º1029	 i:7 	 global-step:20587	 l-p:0.1269453465938568
epoch£º1029	 i:8 	 global-step:20588	 l-p:-1.1134507656097412
epoch£º1029	 i:9 	 global-step:20589	 l-p:0.17098037898540497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7080, 3.6847, 3.7059],
        [3.7080, 3.6389, 3.6950],
        [3.7080, 3.0478, 2.3980],
        [3.7080, 3.2283, 3.2828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.1251586675643921 
model_pd.l_d.mean(): -23.30646514892578 
model_pd.lagr.mean(): -23.181306838989258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2190], device='cuda:0')), ('power', tensor([-23.5255], device='cuda:0'))])
epoch£º1030	 i:0 	 global-step:20600	 l-p:0.1251586675643921
epoch£º1030	 i:1 	 global-step:20601	 l-p:-3.0815882682800293
epoch£º1030	 i:2 	 global-step:20602	 l-p:0.12870709598064423
epoch£º1030	 i:3 	 global-step:20603	 l-p:0.31663092970848083
epoch£º1030	 i:4 	 global-step:20604	 l-p:0.09100443869829178
epoch£º1030	 i:5 	 global-step:20605	 l-p:0.13172510266304016
epoch£º1030	 i:6 	 global-step:20606	 l-p:0.12110129743814468
epoch£º1030	 i:7 	 global-step:20607	 l-p:0.14838163554668427
epoch£º1030	 i:8 	 global-step:20608	 l-p:0.1719510555267334
epoch£º1030	 i:9 	 global-step:20609	 l-p:0.13624443113803864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7124, 3.1804, 2.5486],
        [3.7124, 2.9242, 2.2954],
        [3.7124, 3.0982, 3.0192],
        [3.7124, 3.6720, 3.7071]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.1183047965168953 
model_pd.l_d.mean(): -23.125383377075195 
model_pd.lagr.mean(): -23.007078170776367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2300], device='cuda:0')), ('power', tensor([-23.3554], device='cuda:0'))])
epoch£º1031	 i:0 	 global-step:20620	 l-p:0.1183047965168953
epoch£º1031	 i:1 	 global-step:20621	 l-p:0.12471245229244232
epoch£º1031	 i:2 	 global-step:20622	 l-p:0.13694150745868683
epoch£º1031	 i:3 	 global-step:20623	 l-p:0.1502150446176529
epoch£º1031	 i:4 	 global-step:20624	 l-p:0.12143911421298981
epoch£º1031	 i:5 	 global-step:20625	 l-p:0.024411601945757866
epoch£º1031	 i:6 	 global-step:20626	 l-p:-0.09389279782772064
epoch£º1031	 i:7 	 global-step:20627	 l-p:0.13128545880317688
epoch£º1031	 i:8 	 global-step:20628	 l-p:0.1568213403224945
epoch£º1031	 i:9 	 global-step:20629	 l-p:-0.2817680537700653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6920, 3.1829, 3.2159],
        [3.6920, 3.5475, 3.6452],
        [3.6920, 3.6911, 3.6919],
        [3.6920, 3.4128, 3.5386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.14075599610805511 
model_pd.l_d.mean(): -23.29254150390625 
model_pd.lagr.mean(): -23.151784896850586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2019], device='cuda:0')), ('power', tensor([-23.4944], device='cuda:0'))])
epoch£º1032	 i:0 	 global-step:20640	 l-p:0.14075599610805511
epoch£º1032	 i:1 	 global-step:20641	 l-p:0.11057048290967941
epoch£º1032	 i:2 	 global-step:20642	 l-p:0.1776135265827179
epoch£º1032	 i:3 	 global-step:20643	 l-p:-0.3460199236869812
epoch£º1032	 i:4 	 global-step:20644	 l-p:0.12319297343492508
epoch£º1032	 i:5 	 global-step:20645	 l-p:0.13720117509365082
epoch£º1032	 i:6 	 global-step:20646	 l-p:-0.18464116752147675
epoch£º1032	 i:7 	 global-step:20647	 l-p:0.13667164742946625
epoch£º1032	 i:8 	 global-step:20648	 l-p:0.2321164608001709
epoch£º1032	 i:9 	 global-step:20649	 l-p:0.13167689740657806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6904, 3.6903, 3.6904],
        [3.6904, 2.9838, 2.7635],
        [3.6904, 3.2371, 3.3101],
        [3.6904, 3.6882, 3.6903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): -0.23844890296459198 
model_pd.l_d.mean(): -23.52568817138672 
model_pd.lagr.mean(): -23.764137268066406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2118], device='cuda:0')), ('power', tensor([-23.7375], device='cuda:0'))])
epoch£º1033	 i:0 	 global-step:20660	 l-p:-0.23844890296459198
epoch£º1033	 i:1 	 global-step:20661	 l-p:0.13185936212539673
epoch£º1033	 i:2 	 global-step:20662	 l-p:0.22906051576137543
epoch£º1033	 i:3 	 global-step:20663	 l-p:0.1352647840976715
epoch£º1033	 i:4 	 global-step:20664	 l-p:0.1310177445411682
epoch£º1033	 i:5 	 global-step:20665	 l-p:0.14024677872657776
epoch£º1033	 i:6 	 global-step:20666	 l-p:0.12143540382385254
epoch£º1033	 i:7 	 global-step:20667	 l-p:0.12356062978506088
epoch£º1033	 i:8 	 global-step:20668	 l-p:0.03438110277056694
epoch£º1033	 i:9 	 global-step:20669	 l-p:0.0815027579665184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6771, 2.8684, 2.3649],
        [3.6771, 3.6569, 3.6755],
        [3.6771, 3.5727, 3.6507],
        [3.6771, 3.6719, 3.6769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.16221244633197784 
model_pd.l_d.mean(): -23.701213836669922 
model_pd.lagr.mean(): -23.53900146484375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1114], device='cuda:0')), ('power', tensor([-23.8126], device='cuda:0'))])
epoch£º1034	 i:0 	 global-step:20680	 l-p:0.16221244633197784
epoch£º1034	 i:1 	 global-step:20681	 l-p:0.13664844632148743
epoch£º1034	 i:2 	 global-step:20682	 l-p:0.19086158275604248
epoch£º1034	 i:3 	 global-step:20683	 l-p:0.13772490620613098
epoch£º1034	 i:4 	 global-step:20684	 l-p:0.15648643672466278
epoch£º1034	 i:5 	 global-step:20685	 l-p:0.095004603266716
epoch£º1034	 i:6 	 global-step:20686	 l-p:0.35952189564704895
epoch£º1034	 i:7 	 global-step:20687	 l-p:0.03073091432452202
epoch£º1034	 i:8 	 global-step:20688	 l-p:0.03137638419866562
epoch£º1034	 i:9 	 global-step:20689	 l-p:0.10351594537496567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6733, 3.6494, 3.6711],
        [3.6733, 3.6572, 3.6721],
        [3.6733, 2.8755, 2.4282],
        [3.6733, 3.6391, 3.6693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.15767894685268402 
model_pd.l_d.mean(): -22.99520492553711 
model_pd.lagr.mean(): -22.837526321411133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2352], device='cuda:0')), ('power', tensor([-23.2304], device='cuda:0'))])
epoch£º1035	 i:0 	 global-step:20700	 l-p:0.15767894685268402
epoch£º1035	 i:1 	 global-step:20701	 l-p:0.19188328087329865
epoch£º1035	 i:2 	 global-step:20702	 l-p:0.14746417105197906
epoch£º1035	 i:3 	 global-step:20703	 l-p:0.10703278332948685
epoch£º1035	 i:4 	 global-step:20704	 l-p:0.015887370333075523
epoch£º1035	 i:5 	 global-step:20705	 l-p:-0.01298540085554123
epoch£º1035	 i:6 	 global-step:20706	 l-p:0.1361314356327057
epoch£º1035	 i:7 	 global-step:20707	 l-p:0.04066907614469528
epoch£º1035	 i:8 	 global-step:20708	 l-p:0.24851462244987488
epoch£º1035	 i:9 	 global-step:20709	 l-p:0.11453060060739517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6887, 3.6450, 3.6827],
        [3.6887, 3.2766, 2.6720],
        [3.6887, 3.2978, 3.4019],
        [3.6887, 3.3379, 3.4548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): -0.009712385945022106 
model_pd.l_d.mean(): -22.83675765991211 
model_pd.lagr.mean(): -22.84646987915039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3059], device='cuda:0')), ('power', tensor([-23.1427], device='cuda:0'))])
epoch£º1036	 i:0 	 global-step:20720	 l-p:-0.009712385945022106
epoch£º1036	 i:1 	 global-step:20721	 l-p:0.1370391994714737
epoch£º1036	 i:2 	 global-step:20722	 l-p:0.1451006680727005
epoch£º1036	 i:3 	 global-step:20723	 l-p:0.08174041658639908
epoch£º1036	 i:4 	 global-step:20724	 l-p:0.0018505763728171587
epoch£º1036	 i:5 	 global-step:20725	 l-p:0.15059269964694977
epoch£º1036	 i:6 	 global-step:20726	 l-p:0.20589730143547058
epoch£º1036	 i:7 	 global-step:20727	 l-p:-0.5765357613563538
epoch£º1036	 i:8 	 global-step:20728	 l-p:0.17608417570590973
epoch£º1036	 i:9 	 global-step:20729	 l-p:0.13402830064296722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7007, 3.6629, 3.6960],
        [3.7007, 3.3390, 3.4526],
        [3.7007, 2.9972, 2.7813],
        [3.7007, 3.6932, 3.7004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): -0.3342556953430176 
model_pd.l_d.mean(): -22.551176071166992 
model_pd.lagr.mean(): -22.88543128967285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2918], device='cuda:0')), ('power', tensor([-22.8430], device='cuda:0'))])
epoch£º1037	 i:0 	 global-step:20740	 l-p:-0.3342556953430176
epoch£º1037	 i:1 	 global-step:20741	 l-p:0.13200272619724274
epoch£º1037	 i:2 	 global-step:20742	 l-p:0.14022277295589447
epoch£º1037	 i:3 	 global-step:20743	 l-p:-0.299118310213089
epoch£º1037	 i:4 	 global-step:20744	 l-p:0.1674429327249527
epoch£º1037	 i:5 	 global-step:20745	 l-p:0.09599363803863525
epoch£º1037	 i:6 	 global-step:20746	 l-p:0.1318090558052063
epoch£º1037	 i:7 	 global-step:20747	 l-p:0.3489077687263489
epoch£º1037	 i:8 	 global-step:20748	 l-p:0.12379367649555206
epoch£º1037	 i:9 	 global-step:20749	 l-p:0.13094425201416016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7094, 3.1632, 3.1619],
        [3.7094, 2.9333, 2.2936],
        [3.7094, 3.1779, 3.1904],
        [3.7094, 3.7094, 3.7094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.1355392187833786 
model_pd.l_d.mean(): -23.057737350463867 
model_pd.lagr.mean(): -22.922197341918945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1669], device='cuda:0')), ('power', tensor([-23.2246], device='cuda:0'))])
epoch£º1038	 i:0 	 global-step:20760	 l-p:0.1355392187833786
epoch£º1038	 i:1 	 global-step:20761	 l-p:0.14623022079467773
epoch£º1038	 i:2 	 global-step:20762	 l-p:0.11781416833400726
epoch£º1038	 i:3 	 global-step:20763	 l-p:0.14372576773166656
epoch£º1038	 i:4 	 global-step:20764	 l-p:-1.0222023725509644
epoch£º1038	 i:5 	 global-step:20765	 l-p:0.6679571270942688
epoch£º1038	 i:6 	 global-step:20766	 l-p:0.12697601318359375
epoch£º1038	 i:7 	 global-step:20767	 l-p:-0.14275535941123962
epoch£º1038	 i:8 	 global-step:20768	 l-p:0.1469724029302597
epoch£º1038	 i:9 	 global-step:20769	 l-p:0.10164153575897217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6991, 3.5267, 3.6351],
        [3.6991, 3.3684, 3.4894],
        [3.6991, 3.5974, 3.6738],
        [3.6991, 2.8914, 2.3001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): -0.228616401553154 
model_pd.l_d.mean(): -23.57213020324707 
model_pd.lagr.mean(): -23.80074691772461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2255], device='cuda:0')), ('power', tensor([-23.7977], device='cuda:0'))])
epoch£º1039	 i:0 	 global-step:20780	 l-p:-0.228616401553154
epoch£º1039	 i:1 	 global-step:20781	 l-p:0.11864639073610306
epoch£º1039	 i:2 	 global-step:20782	 l-p:0.11814548075199127
epoch£º1039	 i:3 	 global-step:20783	 l-p:0.11319001764059067
epoch£º1039	 i:4 	 global-step:20784	 l-p:0.2001345157623291
epoch£º1039	 i:5 	 global-step:20785	 l-p:0.12490508705377579
epoch£º1039	 i:6 	 global-step:20786	 l-p:0.13831862807273865
epoch£º1039	 i:7 	 global-step:20787	 l-p:0.1207805797457695
epoch£º1039	 i:8 	 global-step:20788	 l-p:-0.023670367896556854
epoch£º1039	 i:9 	 global-step:20789	 l-p:0.14110814034938812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6787, 3.6787, 3.6787],
        [3.6787, 3.6145, 3.6673],
        [3.6787, 2.8945, 2.4935],
        [3.6787, 3.6733, 3.6785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.021429290995001793 
model_pd.l_d.mean(): -23.265670776367188 
model_pd.lagr.mean(): -23.24424171447754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2942], device='cuda:0')), ('power', tensor([-23.5599], device='cuda:0'))])
epoch£º1040	 i:0 	 global-step:20800	 l-p:0.021429290995001793
epoch£º1040	 i:1 	 global-step:20801	 l-p:0.13171760737895966
epoch£º1040	 i:2 	 global-step:20802	 l-p:0.13106784224510193
epoch£º1040	 i:3 	 global-step:20803	 l-p:0.14071433246135712
epoch£º1040	 i:4 	 global-step:20804	 l-p:0.05719625949859619
epoch£º1040	 i:5 	 global-step:20805	 l-p:0.05147513374686241
epoch£º1040	 i:6 	 global-step:20806	 l-p:0.1269134134054184
epoch£º1040	 i:7 	 global-step:20807	 l-p:0.15070229768753052
epoch£º1040	 i:8 	 global-step:20808	 l-p:0.41420912742614746
epoch£º1040	 i:9 	 global-step:20809	 l-p:0.12273064255714417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6666, 3.6665, 3.6666],
        [3.6666, 3.1096, 3.1014],
        [3.6666, 3.6589, 3.6663],
        [3.6666, 3.6228, 3.6606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.11724075675010681 
model_pd.l_d.mean(): -22.500993728637695 
model_pd.lagr.mean(): -22.383752822875977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3039], device='cuda:0')), ('power', tensor([-22.8049], device='cuda:0'))])
epoch£º1041	 i:0 	 global-step:20820	 l-p:0.11724075675010681
epoch£º1041	 i:1 	 global-step:20821	 l-p:0.36697113513946533
epoch£º1041	 i:2 	 global-step:20822	 l-p:0.06111066788434982
epoch£º1041	 i:3 	 global-step:20823	 l-p:0.20462536811828613
epoch£º1041	 i:4 	 global-step:20824	 l-p:0.14063698053359985
epoch£º1041	 i:5 	 global-step:20825	 l-p:0.17087441682815552
epoch£º1041	 i:6 	 global-step:20826	 l-p:0.1304376870393753
epoch£º1041	 i:7 	 global-step:20827	 l-p:0.15275590121746063
epoch£º1041	 i:8 	 global-step:20828	 l-p:0.07803023606538773
epoch£º1041	 i:9 	 global-step:20829	 l-p:0.13481812179088593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6596, 3.6596, 3.6596],
        [3.6596, 3.3119, 3.4304],
        [3.6596, 3.6593, 3.6596],
        [3.6596, 2.8446, 2.2589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.13104009628295898 
model_pd.l_d.mean(): -23.440311431884766 
model_pd.lagr.mean(): -23.30927085876465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2108], device='cuda:0')), ('power', tensor([-23.6511], device='cuda:0'))])
epoch£º1042	 i:0 	 global-step:20840	 l-p:0.13104009628295898
epoch£º1042	 i:1 	 global-step:20841	 l-p:0.15616947412490845
epoch£º1042	 i:2 	 global-step:20842	 l-p:0.09300941228866577
epoch£º1042	 i:3 	 global-step:20843	 l-p:0.11021057516336441
epoch£º1042	 i:4 	 global-step:20844	 l-p:0.13632169365882874
epoch£º1042	 i:5 	 global-step:20845	 l-p:0.12693282961845398
epoch£º1042	 i:6 	 global-step:20846	 l-p:0.5141512155532837
epoch£º1042	 i:7 	 global-step:20847	 l-p:0.1355346441268921
epoch£º1042	 i:8 	 global-step:20848	 l-p:0.14799682796001434
epoch£º1042	 i:9 	 global-step:20849	 l-p:0.13448466360569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6653, 3.6652, 3.6653],
        [3.6653, 3.4993, 3.6056],
        [3.6653, 3.1693, 2.5506],
        [3.6653, 3.1721, 3.2198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.14679403603076935 
model_pd.l_d.mean(): -23.56375503540039 
model_pd.lagr.mean(): -23.416961669921875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2125], device='cuda:0')), ('power', tensor([-23.7763], device='cuda:0'))])
epoch£º1043	 i:0 	 global-step:20860	 l-p:0.14679403603076935
epoch£º1043	 i:1 	 global-step:20861	 l-p:0.11233892291784286
epoch£º1043	 i:2 	 global-step:20862	 l-p:0.14730916917324066
epoch£º1043	 i:3 	 global-step:20863	 l-p:1.2460840940475464
epoch£º1043	 i:4 	 global-step:20864	 l-p:0.13579772412776947
epoch£º1043	 i:5 	 global-step:20865	 l-p:0.05941568315029144
epoch£º1043	 i:6 	 global-step:20866	 l-p:0.2883908450603485
epoch£º1043	 i:7 	 global-step:20867	 l-p:0.14743348956108093
epoch£º1043	 i:8 	 global-step:20868	 l-p:0.11371869593858719
epoch£º1043	 i:9 	 global-step:20869	 l-p:0.06147217005491257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6493, 3.6473, 3.6493],
        [3.6493, 3.6490, 3.6493],
        [3.6493, 3.5439, 3.6225],
        [3.6493, 2.8695, 2.2323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.16221891343593597 
model_pd.l_d.mean(): -23.369606018066406 
model_pd.lagr.mean(): -23.207387924194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2182], device='cuda:0')), ('power', tensor([-23.5878], device='cuda:0'))])
epoch£º1044	 i:0 	 global-step:20880	 l-p:0.16221891343593597
epoch£º1044	 i:1 	 global-step:20881	 l-p:0.14039908349514008
epoch£º1044	 i:2 	 global-step:20882	 l-p:0.3269384205341339
epoch£º1044	 i:3 	 global-step:20883	 l-p:0.13304193317890167
epoch£º1044	 i:4 	 global-step:20884	 l-p:0.12613672018051147
epoch£º1044	 i:5 	 global-step:20885	 l-p:0.1323685646057129
epoch£º1044	 i:6 	 global-step:20886	 l-p:0.032639920711517334
epoch£º1044	 i:7 	 global-step:20887	 l-p:0.09498964995145798
epoch£º1044	 i:8 	 global-step:20888	 l-p:0.05817769467830658
epoch£º1044	 i:9 	 global-step:20889	 l-p:0.12726275622844696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6359, 3.1047, 2.4838],
        [3.6359, 3.4608, 3.5705],
        [3.6359, 3.6249, 3.6353],
        [3.6359, 2.8644, 2.5161]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.14050810039043427 
model_pd.l_d.mean(): -22.61576271057129 
model_pd.lagr.mean(): -22.47525405883789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3082], device='cuda:0')), ('power', tensor([-22.9240], device='cuda:0'))])
epoch£º1045	 i:0 	 global-step:20900	 l-p:0.14050810039043427
epoch£º1045	 i:1 	 global-step:20901	 l-p:0.035859182476997375
epoch£º1045	 i:2 	 global-step:20902	 l-p:0.0967889353632927
epoch£º1045	 i:3 	 global-step:20903	 l-p:0.16998305916786194
epoch£º1045	 i:4 	 global-step:20904	 l-p:0.11269921064376831
epoch£º1045	 i:5 	 global-step:20905	 l-p:0.09953723847866058
epoch£º1045	 i:6 	 global-step:20906	 l-p:0.1895803064107895
epoch£º1045	 i:7 	 global-step:20907	 l-p:0.1388605237007141
epoch£º1045	 i:8 	 global-step:20908	 l-p:0.2537696063518524
epoch£º1045	 i:9 	 global-step:20909	 l-p:0.08590354770421982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6544, 3.0011, 2.8771],
        [3.6544, 3.6531, 3.6544],
        [3.6544, 2.8429, 2.2375],
        [3.6544, 3.6544, 3.6544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): -0.9290873408317566 
model_pd.l_d.mean(): -23.389572143554688 
model_pd.lagr.mean(): -24.31865882873535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2273], device='cuda:0')), ('power', tensor([-23.6168], device='cuda:0'))])
epoch£º1046	 i:0 	 global-step:20920	 l-p:-0.9290873408317566
epoch£º1046	 i:1 	 global-step:20921	 l-p:0.17263513803482056
epoch£º1046	 i:2 	 global-step:20922	 l-p:0.17466571927070618
epoch£º1046	 i:3 	 global-step:20923	 l-p:0.14675013720989227
epoch£º1046	 i:4 	 global-step:20924	 l-p:0.07304686307907104
epoch£º1046	 i:5 	 global-step:20925	 l-p:0.10460151731967926
epoch£º1046	 i:6 	 global-step:20926	 l-p:0.05701645091176033
epoch£º1046	 i:7 	 global-step:20927	 l-p:0.1344866156578064
epoch£º1046	 i:8 	 global-step:20928	 l-p:0.14500756561756134
epoch£º1046	 i:9 	 global-step:20929	 l-p:0.12704776227474213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6824, 3.3910, 3.5169],
        [3.6824, 3.6335, 3.6752],
        [3.6824, 2.9761, 2.7600],
        [3.6824, 3.5521, 3.6435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.0961199477314949 
model_pd.l_d.mean(): -23.154747009277344 
model_pd.lagr.mean(): -23.058626174926758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2053], device='cuda:0')), ('power', tensor([-23.3600], device='cuda:0'))])
epoch£º1047	 i:0 	 global-step:20940	 l-p:0.0961199477314949
epoch£º1047	 i:1 	 global-step:20941	 l-p:0.1654546558856964
epoch£º1047	 i:2 	 global-step:20942	 l-p:0.1292310357093811
epoch£º1047	 i:3 	 global-step:20943	 l-p:0.1254560947418213
epoch£º1047	 i:4 	 global-step:20944	 l-p:0.18183082342147827
epoch£º1047	 i:5 	 global-step:20945	 l-p:0.15903714299201965
epoch£º1047	 i:6 	 global-step:20946	 l-p:0.031812600791454315
epoch£º1047	 i:7 	 global-step:20947	 l-p:0.13808253407478333
epoch£º1047	 i:8 	 global-step:20948	 l-p:0.05022624880075455
epoch£º1047	 i:9 	 global-step:20949	 l-p:0.3185881972312927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6689, 3.0409, 2.4001],
        [3.6689, 3.3043, 3.4183],
        [3.6689, 3.6689, 3.6689],
        [3.6689, 2.8747, 2.4474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.1016366258263588 
model_pd.l_d.mean(): -23.11786651611328 
model_pd.lagr.mean(): -23.0162296295166 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2842], device='cuda:0')), ('power', tensor([-23.4021], device='cuda:0'))])
epoch£º1048	 i:0 	 global-step:20960	 l-p:0.1016366258263588
epoch£º1048	 i:1 	 global-step:20961	 l-p:0.20912064611911774
epoch£º1048	 i:2 	 global-step:20962	 l-p:0.14094528555870056
epoch£º1048	 i:3 	 global-step:20963	 l-p:0.1271214336156845
epoch£º1048	 i:4 	 global-step:20964	 l-p:0.11878791451454163
epoch£º1048	 i:5 	 global-step:20965	 l-p:0.17436614632606506
epoch£º1048	 i:6 	 global-step:20966	 l-p:0.11051023006439209
epoch£º1048	 i:7 	 global-step:20967	 l-p:0.15088288486003876
epoch£º1048	 i:8 	 global-step:20968	 l-p:0.13811549544334412
epoch£º1048	 i:9 	 global-step:20969	 l-p:0.13897840678691864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6663, 3.5232, 3.6205],
        [3.6663, 2.8558, 2.2513],
        [3.6663, 3.2061, 2.5940],
        [3.6663, 3.2738, 3.3786]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.12477841973304749 
model_pd.l_d.mean(): -22.862262725830078 
model_pd.lagr.mean(): -22.737483978271484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2716], device='cuda:0')), ('power', tensor([-23.1339], device='cuda:0'))])
epoch£º1049	 i:0 	 global-step:20980	 l-p:0.12477841973304749
epoch£º1049	 i:1 	 global-step:20981	 l-p:0.4456823766231537
epoch£º1049	 i:2 	 global-step:20982	 l-p:0.07887804508209229
epoch£º1049	 i:3 	 global-step:20983	 l-p:0.14107489585876465
epoch£º1049	 i:4 	 global-step:20984	 l-p:0.10736893117427826
epoch£º1049	 i:5 	 global-step:20985	 l-p:0.1986836940050125
epoch£º1049	 i:6 	 global-step:20986	 l-p:0.1321387141942978
epoch£º1049	 i:7 	 global-step:20987	 l-p:0.12643550336360931
epoch£º1049	 i:8 	 global-step:20988	 l-p:0.1315428912639618
epoch£º1049	 i:9 	 global-step:20989	 l-p:0.137411430478096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6580, 3.0378, 2.9590],
        [3.6580, 3.5731, 3.6395],
        [3.6580, 3.6554, 3.6579],
        [3.6580, 3.5884, 3.6449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 2.920771360397339 
model_pd.l_d.mean(): -23.735986709594727 
model_pd.lagr.mean(): -20.815216064453125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1719], device='cuda:0')), ('power', tensor([-23.9079], device='cuda:0'))])
epoch£º1050	 i:0 	 global-step:21000	 l-p:2.920771360397339
epoch£º1050	 i:1 	 global-step:21001	 l-p:0.10806803405284882
epoch£º1050	 i:2 	 global-step:21002	 l-p:0.11089282482862473
epoch£º1050	 i:3 	 global-step:21003	 l-p:0.14335450530052185
epoch£º1050	 i:4 	 global-step:21004	 l-p:0.12935194373130798
epoch£º1050	 i:5 	 global-step:21005	 l-p:0.20161834359169006
epoch£º1050	 i:6 	 global-step:21006	 l-p:0.33661338686943054
epoch£º1050	 i:7 	 global-step:21007	 l-p:0.1019682064652443
epoch£º1050	 i:8 	 global-step:21008	 l-p:0.1196148470044136
epoch£º1050	 i:9 	 global-step:21009	 l-p:0.11971213668584824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6397, 2.9129, 2.6664],
        [3.6397, 3.6377, 3.6396],
        [3.6397, 2.8990, 2.6239],
        [3.6397, 2.9564, 2.7884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.13324038684368134 
model_pd.l_d.mean(): -22.61248016357422 
model_pd.lagr.mean(): -22.47924041748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3192], device='cuda:0')), ('power', tensor([-22.9317], device='cuda:0'))])
epoch£º1051	 i:0 	 global-step:21020	 l-p:0.13324038684368134
epoch£º1051	 i:1 	 global-step:21021	 l-p:-0.01402577105909586
epoch£º1051	 i:2 	 global-step:21022	 l-p:0.14786379039287567
epoch£º1051	 i:3 	 global-step:21023	 l-p:0.21075427532196045
epoch£º1051	 i:4 	 global-step:21024	 l-p:0.1455691009759903
epoch£º1051	 i:5 	 global-step:21025	 l-p:0.41175028681755066
epoch£º1051	 i:6 	 global-step:21026	 l-p:0.1262759119272232
epoch£º1051	 i:7 	 global-step:21027	 l-p:0.0568217933177948
epoch£º1051	 i:8 	 global-step:21028	 l-p:0.13124039769172668
epoch£º1051	 i:9 	 global-step:21029	 l-p:0.14812462031841278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6383, 3.6384, 3.6384],
        [3.6383, 3.6384, 3.6383],
        [3.6383, 3.3647, 3.4917],
        [3.6383, 3.1012, 3.1151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.20484353601932526 
model_pd.l_d.mean(): -23.58599090576172 
model_pd.lagr.mean(): -23.381147384643555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1807], device='cuda:0')), ('power', tensor([-23.7667], device='cuda:0'))])
epoch£º1052	 i:0 	 global-step:21040	 l-p:0.20484353601932526
epoch£º1052	 i:1 	 global-step:21041	 l-p:0.12992548942565918
epoch£º1052	 i:2 	 global-step:21042	 l-p:0.13826614618301392
epoch£º1052	 i:3 	 global-step:21043	 l-p:0.1314586102962494
epoch£º1052	 i:4 	 global-step:21044	 l-p:0.1744469553232193
epoch£º1052	 i:5 	 global-step:21045	 l-p:0.06980746984481812
epoch£º1052	 i:6 	 global-step:21046	 l-p:-0.6682466864585876
epoch£º1052	 i:7 	 global-step:21047	 l-p:0.1789839118719101
epoch£º1052	 i:8 	 global-step:21048	 l-p:0.1329454630613327
epoch£º1052	 i:9 	 global-step:21049	 l-p:0.03315304219722748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228]], device='cuda:0')
 pt:tensor([[3.6269, 2.8040, 2.2554],
        [3.6269, 3.0800, 2.4576],
        [3.6269, 3.0487, 3.0231],
        [3.6269, 3.2083, 3.3036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.14238978922367096 
model_pd.l_d.mean(): -23.521520614624023 
model_pd.lagr.mean(): -23.379131317138672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2323], device='cuda:0')), ('power', tensor([-23.7538], device='cuda:0'))])
epoch£º1053	 i:0 	 global-step:21060	 l-p:0.14238978922367096
epoch£º1053	 i:1 	 global-step:21061	 l-p:0.09567270427942276
epoch£º1053	 i:2 	 global-step:21062	 l-p:0.14504608511924744
epoch£º1053	 i:3 	 global-step:21063	 l-p:0.2866952121257782
epoch£º1053	 i:4 	 global-step:21064	 l-p:0.0896109938621521
epoch£º1053	 i:5 	 global-step:21065	 l-p:0.05030336230993271
epoch£º1053	 i:6 	 global-step:21066	 l-p:0.09649128466844559
epoch£º1053	 i:7 	 global-step:21067	 l-p:6.1532721519470215
epoch£º1053	 i:8 	 global-step:21068	 l-p:0.15002436935901642
epoch£º1053	 i:9 	 global-step:21069	 l-p:0.12081415206193924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6383, 3.6328, 3.6381],
        [3.6383, 3.6383, 3.6383],
        [3.6383, 3.6377, 3.6383],
        [3.6383, 3.6306, 3.6380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.12061912566423416 
model_pd.l_d.mean(): -21.81326675415039 
model_pd.lagr.mean(): -21.69264793395996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4018], device='cuda:0')), ('power', tensor([-22.2151], device='cuda:0'))])
epoch£º1054	 i:0 	 global-step:21080	 l-p:0.12061912566423416
epoch£º1054	 i:1 	 global-step:21081	 l-p:0.12105405330657959
epoch£º1054	 i:2 	 global-step:21082	 l-p:0.12132831662893295
epoch£º1054	 i:3 	 global-step:21083	 l-p:0.07410161942243576
epoch£º1054	 i:4 	 global-step:21084	 l-p:0.1661335676908493
epoch£º1054	 i:5 	 global-step:21085	 l-p:0.235260009765625
epoch£º1054	 i:6 	 global-step:21086	 l-p:0.1352073699235916
epoch£º1054	 i:7 	 global-step:21087	 l-p:0.1347932368516922
epoch£º1054	 i:8 	 global-step:21088	 l-p:0.14681023359298706
epoch£º1054	 i:9 	 global-step:21089	 l-p:0.17561942338943481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6384, 3.6277, 3.6378],
        [3.6384, 3.5503, 3.6188],
        [3.6384, 2.8659, 2.5171],
        [3.6384, 3.3018, 3.4235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.13996626436710358 
model_pd.l_d.mean(): -22.866979598999023 
model_pd.lagr.mean(): -22.727012634277344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3679], device='cuda:0')), ('power', tensor([-23.2348], device='cuda:0'))])
epoch£º1055	 i:0 	 global-step:21100	 l-p:0.13996626436710358
epoch£º1055	 i:1 	 global-step:21101	 l-p:0.16086836159229279
epoch£º1055	 i:2 	 global-step:21102	 l-p:0.13964496552944183
epoch£º1055	 i:3 	 global-step:21103	 l-p:0.15115077793598175
epoch£º1055	 i:4 	 global-step:21104	 l-p:0.5143061876296997
epoch£º1055	 i:5 	 global-step:21105	 l-p:0.13178661465644836
epoch£º1055	 i:6 	 global-step:21106	 l-p:0.08059826493263245
epoch£º1055	 i:7 	 global-step:21107	 l-p:-0.04774383455514908
epoch£º1055	 i:8 	 global-step:21108	 l-p:0.14796510338783264
epoch£º1055	 i:9 	 global-step:21109	 l-p:0.13617831468582153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6418, 3.6419, 3.6418],
        [3.6418, 3.6409, 3.6418],
        [3.6418, 3.6280, 3.6410],
        [3.6418, 3.6417, 3.6418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.1582706719636917 
model_pd.l_d.mean(): -23.795482635498047 
model_pd.lagr.mean(): -23.6372127532959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1177], device='cuda:0')), ('power', tensor([-23.9132], device='cuda:0'))])
epoch£º1056	 i:0 	 global-step:21120	 l-p:0.1582706719636917
epoch£º1056	 i:1 	 global-step:21121	 l-p:0.12582805752754211
epoch£º1056	 i:2 	 global-step:21122	 l-p:0.15677353739738464
epoch£º1056	 i:3 	 global-step:21123	 l-p:0.07712411880493164
epoch£º1056	 i:4 	 global-step:21124	 l-p:0.12047166377305984
epoch£º1056	 i:5 	 global-step:21125	 l-p:0.158358633518219
epoch£º1056	 i:6 	 global-step:21126	 l-p:0.017981629818677902
epoch£º1056	 i:7 	 global-step:21127	 l-p:0.1419813334941864
epoch£º1056	 i:8 	 global-step:21128	 l-p:0.8537430167198181
epoch£º1056	 i:9 	 global-step:21129	 l-p:0.15735606849193573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6355, 3.5182, 3.6033],
        [3.6355, 2.9443, 2.3019],
        [3.6355, 3.6355, 3.6355],
        [3.6355, 3.4617, 3.5710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.14601479470729828 
model_pd.l_d.mean(): -23.490873336791992 
model_pd.lagr.mean(): -23.344858169555664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2094], device='cuda:0')), ('power', tensor([-23.7002], device='cuda:0'))])
epoch£º1057	 i:0 	 global-step:21140	 l-p:0.14601479470729828
epoch£º1057	 i:1 	 global-step:21141	 l-p:0.13391834497451782
epoch£º1057	 i:2 	 global-step:21142	 l-p:0.09419935196638107
epoch£º1057	 i:3 	 global-step:21143	 l-p:0.08096437156200409
epoch£º1057	 i:4 	 global-step:21144	 l-p:0.7463937401771545
epoch£º1057	 i:5 	 global-step:21145	 l-p:0.11569112539291382
epoch£º1057	 i:6 	 global-step:21146	 l-p:0.09427092969417572
epoch£º1057	 i:7 	 global-step:21147	 l-p:0.15852855145931244
epoch£º1057	 i:8 	 global-step:21148	 l-p:0.14386491477489471
epoch£º1057	 i:9 	 global-step:21149	 l-p:0.13073255121707916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6480, 3.6137, 3.6441],
        [3.6480, 3.1526, 3.2008],
        [3.6480, 3.0594, 3.0208],
        [3.6480, 3.5424, 3.6212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.13715578615665436 
model_pd.l_d.mean(): -23.659225463867188 
model_pd.lagr.mean(): -23.522069931030273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1672], device='cuda:0')), ('power', tensor([-23.8265], device='cuda:0'))])
epoch£º1058	 i:0 	 global-step:21160	 l-p:0.13715578615665436
epoch£º1058	 i:1 	 global-step:21161	 l-p:0.10830764472484589
epoch£º1058	 i:2 	 global-step:21162	 l-p:-0.07031945884227753
epoch£º1058	 i:3 	 global-step:21163	 l-p:0.12916730344295502
epoch£º1058	 i:4 	 global-step:21164	 l-p:0.1523551493883133
epoch£º1058	 i:5 	 global-step:21165	 l-p:0.16328582167625427
epoch£º1058	 i:6 	 global-step:21166	 l-p:0.0869884192943573
epoch£º1058	 i:7 	 global-step:21167	 l-p:0.5578178763389587
epoch£º1058	 i:8 	 global-step:21168	 l-p:0.12809985876083374
epoch£º1058	 i:9 	 global-step:21169	 l-p:0.16352395713329315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6359, 3.6293, 3.6357],
        [3.6359, 3.2450, 3.3518],
        [3.6359, 3.6016, 3.6320],
        [3.6359, 3.5952, 3.6307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.12411414831876755 
model_pd.l_d.mean(): -23.53980255126953 
model_pd.lagr.mean(): -23.415687561035156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2009], device='cuda:0')), ('power', tensor([-23.7407], device='cuda:0'))])
epoch£º1059	 i:0 	 global-step:21180	 l-p:0.12411414831876755
epoch£º1059	 i:1 	 global-step:21181	 l-p:1.066274881362915
epoch£º1059	 i:2 	 global-step:21182	 l-p:0.056722357869148254
epoch£º1059	 i:3 	 global-step:21183	 l-p:0.16285011172294617
epoch£º1059	 i:4 	 global-step:21184	 l-p:0.18240351974964142
epoch£º1059	 i:5 	 global-step:21185	 l-p:0.13961726427078247
epoch£º1059	 i:6 	 global-step:21186	 l-p:0.14091253280639648
epoch£º1059	 i:7 	 global-step:21187	 l-p:0.09168247133493423
epoch£º1059	 i:8 	 global-step:21188	 l-p:0.2137163281440735
epoch£º1059	 i:9 	 global-step:21189	 l-p:0.013709964230656624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6368, 3.6313, 3.6366],
        [3.6368, 2.8448, 2.4427],
        [3.6368, 2.8159, 2.2286],
        [3.6368, 3.6359, 3.6368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.13272543251514435 
model_pd.l_d.mean(): -23.12804412841797 
model_pd.lagr.mean(): -22.995319366455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1966], device='cuda:0')), ('power', tensor([-23.3246], device='cuda:0'))])
epoch£º1060	 i:0 	 global-step:21200	 l-p:0.13272543251514435
epoch£º1060	 i:1 	 global-step:21201	 l-p:0.15271158516407013
epoch£º1060	 i:2 	 global-step:21202	 l-p:0.11016826331615448
epoch£º1060	 i:3 	 global-step:21203	 l-p:0.15114474296569824
epoch£º1060	 i:4 	 global-step:21204	 l-p:0.08583388477563858
epoch£º1060	 i:5 	 global-step:21205	 l-p:0.580603837966919
epoch£º1060	 i:6 	 global-step:21206	 l-p:0.21388037502765656
epoch£º1060	 i:7 	 global-step:21207	 l-p:0.1126890629529953
epoch£º1060	 i:8 	 global-step:21208	 l-p:-0.04971538484096527
epoch£º1060	 i:9 	 global-step:21209	 l-p:0.1380237191915512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6508, 3.6508, 3.6508],
        [3.6508, 3.2569, 3.3622],
        [3.6508, 3.1781, 2.5654],
        [3.6508, 3.0291, 2.9507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.1261218786239624 
model_pd.l_d.mean(): -23.030805587768555 
model_pd.lagr.mean(): -22.90468406677246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2952], device='cuda:0')), ('power', tensor([-23.3260], device='cuda:0'))])
epoch£º1061	 i:0 	 global-step:21220	 l-p:0.1261218786239624
epoch£º1061	 i:1 	 global-step:21221	 l-p:0.10213594138622284
epoch£º1061	 i:2 	 global-step:21222	 l-p:0.13947558403015137
epoch£º1061	 i:3 	 global-step:21223	 l-p:1.3641307353973389
epoch£º1061	 i:4 	 global-step:21224	 l-p:0.10032883286476135
epoch£º1061	 i:5 	 global-step:21225	 l-p:0.2015816867351532
epoch£º1061	 i:6 	 global-step:21226	 l-p:0.11515025049448013
epoch£º1061	 i:7 	 global-step:21227	 l-p:0.12391261756420135
epoch£º1061	 i:8 	 global-step:21228	 l-p:0.15284784138202667
epoch£º1061	 i:9 	 global-step:21229	 l-p:0.1352386623620987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6820, 3.6812, 3.6820],
        [3.6820, 3.5971, 3.6636],
        [3.6820, 3.3297, 3.4472],
        [3.6820, 3.4014, 3.5280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.21797937154769897 
model_pd.l_d.mean(): -23.31561279296875 
model_pd.lagr.mean(): -23.097633361816406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2367], device='cuda:0')), ('power', tensor([-23.5524], device='cuda:0'))])
epoch£º1062	 i:0 	 global-step:21240	 l-p:0.21797937154769897
epoch£º1062	 i:1 	 global-step:21241	 l-p:0.05319778621196747
epoch£º1062	 i:2 	 global-step:21242	 l-p:0.022587822750210762
epoch£º1062	 i:3 	 global-step:21243	 l-p:0.1499629020690918
epoch£º1062	 i:4 	 global-step:21244	 l-p:0.10036874562501907
epoch£º1062	 i:5 	 global-step:21245	 l-p:-0.09867028892040253
epoch£º1062	 i:6 	 global-step:21246	 l-p:0.13000045716762543
epoch£º1062	 i:7 	 global-step:21247	 l-p:0.11969926208257675
epoch£º1062	 i:8 	 global-step:21248	 l-p:0.17561738193035126
epoch£º1062	 i:9 	 global-step:21249	 l-p:0.1637144386768341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6904, 3.6899, 3.6904],
        [3.6904, 3.1333, 3.1248],
        [3.6904, 3.6904, 3.6904],
        [3.6904, 3.0549, 2.9533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.13029339909553528 
model_pd.l_d.mean(): -23.538793563842773 
model_pd.lagr.mean(): -23.40850067138672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1517], device='cuda:0')), ('power', tensor([-23.6905], device='cuda:0'))])
epoch£º1063	 i:0 	 global-step:21260	 l-p:0.13029339909553528
epoch£º1063	 i:1 	 global-step:21261	 l-p:-0.2282431423664093
epoch£º1063	 i:2 	 global-step:21262	 l-p:0.1366702765226364
epoch£º1063	 i:3 	 global-step:21263	 l-p:0.13461682200431824
epoch£º1063	 i:4 	 global-step:21264	 l-p:0.1129622757434845
epoch£º1063	 i:5 	 global-step:21265	 l-p:0.16041502356529236
epoch£º1063	 i:6 	 global-step:21266	 l-p:0.1839955598115921
epoch£º1063	 i:7 	 global-step:21267	 l-p:0.17455506324768066
epoch£º1063	 i:8 	 global-step:21268	 l-p:0.11355149745941162
epoch£º1063	 i:9 	 global-step:21269	 l-p:0.13653922080993652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6731, 3.6727, 3.6731],
        [3.6731, 2.9593, 2.7335],
        [3.6731, 3.2146, 3.2868],
        [3.6731, 3.5278, 3.6261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.1583123505115509 
model_pd.l_d.mean(): -23.525480270385742 
model_pd.lagr.mean(): -23.367168426513672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1612], device='cuda:0')), ('power', tensor([-23.6866], device='cuda:0'))])
epoch£º1064	 i:0 	 global-step:21280	 l-p:0.1583123505115509
epoch£º1064	 i:1 	 global-step:21281	 l-p:0.07757538557052612
epoch£º1064	 i:2 	 global-step:21282	 l-p:0.06390801817178726
epoch£º1064	 i:3 	 global-step:21283	 l-p:0.12852677702903748
epoch£º1064	 i:4 	 global-step:21284	 l-p:0.1253129243850708
epoch£º1064	 i:5 	 global-step:21285	 l-p:0.11962850391864777
epoch£º1064	 i:6 	 global-step:21286	 l-p:1.06670081615448
epoch£º1064	 i:7 	 global-step:21287	 l-p:0.1740720272064209
epoch£º1064	 i:8 	 global-step:21288	 l-p:0.23911014199256897
epoch£º1064	 i:9 	 global-step:21289	 l-p:0.1316353976726532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6581, 3.6581, 3.6581],
        [3.6581, 3.6581, 3.6581],
        [3.6581, 2.8662, 2.2328],
        [3.6581, 3.6569, 3.6581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.13155505061149597 
model_pd.l_d.mean(): -23.108781814575195 
model_pd.lagr.mean(): -22.97722625732422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2533], device='cuda:0')), ('power', tensor([-23.3621], device='cuda:0'))])
epoch£º1065	 i:0 	 global-step:21300	 l-p:0.13155505061149597
epoch£º1065	 i:1 	 global-step:21301	 l-p:0.16256123781204224
epoch£º1065	 i:2 	 global-step:21302	 l-p:0.22128145396709442
epoch£º1065	 i:3 	 global-step:21303	 l-p:0.1257781833410263
epoch£º1065	 i:4 	 global-step:21304	 l-p:0.15605084598064423
epoch£º1065	 i:5 	 global-step:21305	 l-p:0.08545984327793121
epoch£º1065	 i:6 	 global-step:21306	 l-p:0.1259090155363083
epoch£º1065	 i:7 	 global-step:21307	 l-p:-0.18521656095981598
epoch£º1065	 i:8 	 global-step:21308	 l-p:0.12190179526805878
epoch£º1065	 i:9 	 global-step:21309	 l-p:0.13191886246204376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6486, 3.1749, 2.5623],
        [3.6486, 2.8276, 2.2824],
        [3.6486, 3.4292, 3.5506],
        [3.6486, 2.8302, 2.3117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.15422645211219788 
model_pd.l_d.mean(): -23.491573333740234 
model_pd.lagr.mean(): -23.33734703063965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1929], device='cuda:0')), ('power', tensor([-23.6845], device='cuda:0'))])
epoch£º1066	 i:0 	 global-step:21320	 l-p:0.15422645211219788
epoch£º1066	 i:1 	 global-step:21321	 l-p:0.11458973586559296
epoch£º1066	 i:2 	 global-step:21322	 l-p:-0.0695149376988411
epoch£º1066	 i:3 	 global-step:21323	 l-p:0.12125806510448456
epoch£º1066	 i:4 	 global-step:21324	 l-p:0.15618793666362762
epoch£º1066	 i:5 	 global-step:21325	 l-p:0.10758546739816666
epoch£º1066	 i:6 	 global-step:21326	 l-p:1.0977141857147217
epoch£º1066	 i:7 	 global-step:21327	 l-p:0.12568259239196777
epoch£º1066	 i:8 	 global-step:21328	 l-p:0.14707046747207642
epoch£º1066	 i:9 	 global-step:21329	 l-p:0.2254561185836792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6296, 3.6296, 3.6296],
        [3.6296, 3.6274, 3.6296],
        [3.6296, 3.6201, 3.6291],
        [3.6296, 3.1203, 2.5037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.1423051357269287 
model_pd.l_d.mean(): -23.484914779663086 
model_pd.lagr.mean(): -23.342609405517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2325], device='cuda:0')), ('power', tensor([-23.7174], device='cuda:0'))])
epoch£º1067	 i:0 	 global-step:21340	 l-p:0.1423051357269287
epoch£º1067	 i:1 	 global-step:21341	 l-p:0.1356273889541626
epoch£º1067	 i:2 	 global-step:21342	 l-p:0.14882297813892365
epoch£º1067	 i:3 	 global-step:21343	 l-p:0.010866472497582436
epoch£º1067	 i:4 	 global-step:21344	 l-p:-0.04476818069815636
epoch£º1067	 i:5 	 global-step:21345	 l-p:0.148925319314003
epoch£º1067	 i:6 	 global-step:21346	 l-p:0.15234766900539398
epoch£º1067	 i:7 	 global-step:21347	 l-p:0.39736396074295044
epoch£º1067	 i:8 	 global-step:21348	 l-p:0.1118047758936882
epoch£º1067	 i:9 	 global-step:21349	 l-p:0.15527378022670746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6102, 3.6103, 3.6102],
        [3.6102, 3.6093, 3.6102],
        [3.6102, 3.6101, 3.6102],
        [3.6102, 3.6102, 3.6103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.16700366139411926 
model_pd.l_d.mean(): -22.904909133911133 
model_pd.lagr.mean(): -22.737905502319336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2607], device='cuda:0')), ('power', tensor([-23.1656], device='cuda:0'))])
epoch£º1068	 i:0 	 global-step:21360	 l-p:0.16700366139411926
epoch£º1068	 i:1 	 global-step:21361	 l-p:0.16187621653079987
epoch£º1068	 i:2 	 global-step:21362	 l-p:0.1294417381286621
epoch£º1068	 i:3 	 global-step:21363	 l-p:0.14380280673503876
epoch£º1068	 i:4 	 global-step:21364	 l-p:-1.850644588470459
epoch£º1068	 i:5 	 global-step:21365	 l-p:0.12362243980169296
epoch£º1068	 i:6 	 global-step:21366	 l-p:0.05550386384129524
epoch£º1068	 i:7 	 global-step:21367	 l-p:0.10916385054588318
epoch£º1068	 i:8 	 global-step:21368	 l-p:0.16390758752822876
epoch£º1068	 i:9 	 global-step:21369	 l-p:0.07856322824954987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6112, 3.0723, 3.0878],
        [3.6112, 3.5864, 3.6088],
        [3.6112, 3.5729, 3.6064],
        [3.6112, 3.3399, 3.4675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.10343135893344879 
model_pd.l_d.mean(): -23.650373458862305 
model_pd.lagr.mean(): -23.54694175720215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1723], device='cuda:0')), ('power', tensor([-23.8227], device='cuda:0'))])
epoch£º1069	 i:0 	 global-step:21380	 l-p:0.10343135893344879
epoch£º1069	 i:1 	 global-step:21381	 l-p:0.6039296388626099
epoch£º1069	 i:2 	 global-step:21382	 l-p:0.1491074115037918
epoch£º1069	 i:3 	 global-step:21383	 l-p:0.004358868580311537
epoch£º1069	 i:4 	 global-step:21384	 l-p:0.13211044669151306
epoch£º1069	 i:5 	 global-step:21385	 l-p:0.12367898970842361
epoch£º1069	 i:6 	 global-step:21386	 l-p:0.1210014671087265
epoch£º1069	 i:7 	 global-step:21387	 l-p:0.07992656528949738
epoch£º1069	 i:8 	 global-step:21388	 l-p:0.16128234565258026
epoch£º1069	 i:9 	 global-step:21389	 l-p:0.13957490026950836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6270, 2.9769, 2.3397],
        [3.6270, 3.6244, 3.6269],
        [3.6270, 3.5570, 3.6138],
        [3.6270, 2.9318, 2.2903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): -0.34417590498924255 
model_pd.l_d.mean(): -22.88783073425293 
model_pd.lagr.mean(): -23.232006072998047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3407], device='cuda:0')), ('power', tensor([-23.2285], device='cuda:0'))])
epoch£º1070	 i:0 	 global-step:21400	 l-p:-0.34417590498924255
epoch£º1070	 i:1 	 global-step:21401	 l-p:0.13539384305477142
epoch£º1070	 i:2 	 global-step:21402	 l-p:0.11675955355167389
epoch£º1070	 i:3 	 global-step:21403	 l-p:0.14284037053585052
epoch£º1070	 i:4 	 global-step:21404	 l-p:0.10356434434652328
epoch£º1070	 i:5 	 global-step:21405	 l-p:0.10619004815816879
epoch£º1070	 i:6 	 global-step:21406	 l-p:0.20987464487552643
epoch£º1070	 i:7 	 global-step:21407	 l-p:0.1434640884399414
epoch£º1070	 i:8 	 global-step:21408	 l-p:0.14684025943279266
epoch£º1070	 i:9 	 global-step:21409	 l-p:0.11287456005811691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6470, 3.6470, 3.6471],
        [3.6470, 3.6139, 3.6433],
        [3.6470, 3.3402, 3.4663],
        [3.6470, 3.1146, 3.1334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.08479423075914383 
model_pd.l_d.mean(): -23.20850944519043 
model_pd.lagr.mean(): -23.123714447021484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2460], device='cuda:0')), ('power', tensor([-23.4545], device='cuda:0'))])
epoch£º1071	 i:0 	 global-step:21420	 l-p:0.08479423075914383
epoch£º1071	 i:1 	 global-step:21421	 l-p:-0.1191810593008995
epoch£º1071	 i:2 	 global-step:21422	 l-p:0.1347593069076538
epoch£º1071	 i:3 	 global-step:21423	 l-p:0.10876563936471939
epoch£º1071	 i:4 	 global-step:21424	 l-p:0.13004626333713531
epoch£º1071	 i:5 	 global-step:21425	 l-p:0.17975953221321106
epoch£º1071	 i:6 	 global-step:21426	 l-p:0.22060495615005493
epoch£º1071	 i:7 	 global-step:21427	 l-p:0.05959895998239517
epoch£º1071	 i:8 	 global-step:21428	 l-p:0.16316771507263184
epoch£º1071	 i:9 	 global-step:21429	 l-p:0.1301884800195694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6684, 3.6684, 3.6684],
        [3.6684, 3.2060, 2.5928],
        [3.6684, 3.6669, 3.6684],
        [3.6684, 2.9174, 2.6169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.22308604419231415 
model_pd.l_d.mean(): -22.9285831451416 
model_pd.lagr.mean(): -22.70549774169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3328], device='cuda:0')), ('power', tensor([-23.2614], device='cuda:0'))])
epoch£º1072	 i:0 	 global-step:21440	 l-p:0.22308604419231415
epoch£º1072	 i:1 	 global-step:21441	 l-p:0.17621921002864838
epoch£º1072	 i:2 	 global-step:21442	 l-p:0.2550286054611206
epoch£º1072	 i:3 	 global-step:21443	 l-p:0.01853020116686821
epoch£º1072	 i:4 	 global-step:21444	 l-p:0.1096864566206932
epoch£º1072	 i:5 	 global-step:21445	 l-p:0.1485508680343628
epoch£º1072	 i:6 	 global-step:21446	 l-p:0.05677919089794159
epoch£º1072	 i:7 	 global-step:21447	 l-p:0.11570077389478683
epoch£º1072	 i:8 	 global-step:21448	 l-p:0.1215200424194336
epoch£º1072	 i:9 	 global-step:21449	 l-p:0.13823655247688293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6852, 3.6450, 3.6800],
        [3.6852, 3.6729, 3.6845],
        [3.6852, 2.9171, 2.2715],
        [3.6852, 3.6562, 3.6822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.16607528924942017 
model_pd.l_d.mean(): -22.9980411529541 
model_pd.lagr.mean(): -22.831966400146484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3058], device='cuda:0')), ('power', tensor([-23.3038], device='cuda:0'))])
epoch£º1073	 i:0 	 global-step:21460	 l-p:0.16607528924942017
epoch£º1073	 i:1 	 global-step:21461	 l-p:0.12726075947284698
epoch£º1073	 i:2 	 global-step:21462	 l-p:0.11703864485025406
epoch£º1073	 i:3 	 global-step:21463	 l-p:0.11313091218471527
epoch£º1073	 i:4 	 global-step:21464	 l-p:0.11022762954235077
epoch£º1073	 i:5 	 global-step:21465	 l-p:-0.09761735796928406
epoch£º1073	 i:6 	 global-step:21466	 l-p:0.1462741196155548
epoch£º1073	 i:7 	 global-step:21467	 l-p:0.13097742199897766
epoch£º1073	 i:8 	 global-step:21468	 l-p:0.19712142646312714
epoch£º1073	 i:9 	 global-step:21469	 l-p:0.14048299193382263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6771, 3.6771, 3.6771],
        [3.6771, 2.9964, 2.8300],
        [3.6771, 3.5010, 3.6110],
        [3.6771, 2.9927, 2.8202]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): -0.07475029677152634 
model_pd.l_d.mean(): -23.0872745513916 
model_pd.lagr.mean(): -23.162025451660156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2541], device='cuda:0')), ('power', tensor([-23.3414], device='cuda:0'))])
epoch£º1074	 i:0 	 global-step:21480	 l-p:-0.07475029677152634
epoch£º1074	 i:1 	 global-step:21481	 l-p:0.12141712009906769
epoch£º1074	 i:2 	 global-step:21482	 l-p:0.1292295753955841
epoch£º1074	 i:3 	 global-step:21483	 l-p:0.1318172812461853
epoch£º1074	 i:4 	 global-step:21484	 l-p:0.13925476372241974
epoch£º1074	 i:5 	 global-step:21485	 l-p:0.2193334847688675
epoch£º1074	 i:6 	 global-step:21486	 l-p:0.08043647557497025
epoch£º1074	 i:7 	 global-step:21487	 l-p:0.14001494646072388
epoch£º1074	 i:8 	 global-step:21488	 l-p:1.4598336219787598
epoch£º1074	 i:9 	 global-step:21489	 l-p:0.17640292644500732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6606, 3.4730, 3.5866],
        [3.6606, 3.6537, 3.6603],
        [3.6606, 3.6600, 3.6606],
        [3.6606, 3.1650, 3.2131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.17354924976825714 
model_pd.l_d.mean(): -23.667116165161133 
model_pd.lagr.mean(): -23.493566513061523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1448], device='cuda:0')), ('power', tensor([-23.8119], device='cuda:0'))])
epoch£º1075	 i:0 	 global-step:21500	 l-p:0.17354924976825714
epoch£º1075	 i:1 	 global-step:21501	 l-p:0.13606925308704376
epoch£º1075	 i:2 	 global-step:21502	 l-p:0.1295885145664215
epoch£º1075	 i:3 	 global-step:21503	 l-p:0.08013787865638733
epoch£º1075	 i:4 	 global-step:21504	 l-p:0.17994536459445953
epoch£º1075	 i:5 	 global-step:21505	 l-p:0.12770958244800568
epoch£º1075	 i:6 	 global-step:21506	 l-p:0.0627683773636818
epoch£º1075	 i:7 	 global-step:21507	 l-p:0.1377945840358734
epoch£º1075	 i:8 	 global-step:21508	 l-p:0.9205740690231323
epoch£º1075	 i:9 	 global-step:21509	 l-p:0.12401231378316879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6634, 3.6284, 3.6593],
        [3.6634, 3.1023, 2.4718],
        [3.6634, 3.1241, 2.4973],
        [3.6634, 3.5988, 3.6519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.8108654618263245 
model_pd.l_d.mean(): -23.412376403808594 
model_pd.lagr.mean(): -22.601511001586914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2511], device='cuda:0')), ('power', tensor([-23.6635], device='cuda:0'))])
epoch£º1076	 i:0 	 global-step:21520	 l-p:0.8108654618263245
epoch£º1076	 i:1 	 global-step:21521	 l-p:0.07605960965156555
epoch£º1076	 i:2 	 global-step:21522	 l-p:0.13530050218105316
epoch£º1076	 i:3 	 global-step:21523	 l-p:0.12576141953468323
epoch£º1076	 i:4 	 global-step:21524	 l-p:0.12872232496738434
epoch£º1076	 i:5 	 global-step:21525	 l-p:0.10447264462709427
epoch£º1076	 i:6 	 global-step:21526	 l-p:0.1373039186000824
epoch£º1076	 i:7 	 global-step:21527	 l-p:0.12542040646076202
epoch£º1076	 i:8 	 global-step:21528	 l-p:0.1687716692686081
epoch£º1076	 i:9 	 global-step:21529	 l-p:0.130840003490448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6539, 3.0080, 2.8982],
        [3.6539, 3.0692, 3.0360],
        [3.6539, 3.6447, 3.6535],
        [3.6539, 3.6527, 3.6539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.14558981359004974 
model_pd.l_d.mean(): -23.21419334411621 
model_pd.lagr.mean(): -23.068603515625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1699], device='cuda:0')), ('power', tensor([-23.3840], device='cuda:0'))])
epoch£º1077	 i:0 	 global-step:21540	 l-p:0.14558981359004974
epoch£º1077	 i:1 	 global-step:21541	 l-p:0.24109908938407898
epoch£º1077	 i:2 	 global-step:21542	 l-p:0.12618224322795868
epoch£º1077	 i:3 	 global-step:21543	 l-p:0.11952055990695953
epoch£º1077	 i:4 	 global-step:21544	 l-p:0.2290903776884079
epoch£º1077	 i:5 	 global-step:21545	 l-p:0.09502635896205902
epoch£º1077	 i:6 	 global-step:21546	 l-p:0.1260310858488083
epoch£º1077	 i:7 	 global-step:21547	 l-p:0.13641011714935303
epoch£º1077	 i:8 	 global-step:21548	 l-p:-0.04107275232672691
epoch£º1077	 i:9 	 global-step:21549	 l-p:0.12402542680501938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6449, 2.9596, 2.7911],
        [3.6449, 3.2609, 3.3704],
        [3.6449, 3.1004, 3.1088],
        [3.6449, 3.6450, 3.6449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): 0.10821827501058578 
model_pd.l_d.mean(): -22.81146812438965 
model_pd.lagr.mean(): -22.703248977661133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3140], device='cuda:0')), ('power', tensor([-23.1254], device='cuda:0'))])
epoch£º1078	 i:0 	 global-step:21560	 l-p:0.10821827501058578
epoch£º1078	 i:1 	 global-step:21561	 l-p:0.1380317360162735
epoch£º1078	 i:2 	 global-step:21562	 l-p:0.1268385499715805
epoch£º1078	 i:3 	 global-step:21563	 l-p:0.35694190859794617
epoch£º1078	 i:4 	 global-step:21564	 l-p:0.12065894901752472
epoch£º1078	 i:5 	 global-step:21565	 l-p:0.198440283536911
epoch£º1078	 i:6 	 global-step:21566	 l-p:0.08179192245006561
epoch£º1078	 i:7 	 global-step:21567	 l-p:0.10259873420000076
epoch£º1078	 i:8 	 global-step:21568	 l-p:-0.04633346572518349
epoch£º1078	 i:9 	 global-step:21569	 l-p:0.1301826387643814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6517, 3.5789, 3.6376],
        [3.6517, 2.8560, 2.2234],
        [3.6517, 3.6432, 3.6513],
        [3.6517, 2.9342, 2.7082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.1670493632555008 
model_pd.l_d.mean(): -23.264070510864258 
model_pd.lagr.mean(): -23.097021102905273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2600], device='cuda:0')), ('power', tensor([-23.5241], device='cuda:0'))])
epoch£º1079	 i:0 	 global-step:21580	 l-p:0.1670493632555008
epoch£º1079	 i:1 	 global-step:21581	 l-p:0.19367313385009766
epoch£º1079	 i:2 	 global-step:21582	 l-p:-0.5546585917472839
epoch£º1079	 i:3 	 global-step:21583	 l-p:0.13395260274410248
epoch£º1079	 i:4 	 global-step:21584	 l-p:0.03361720219254494
epoch£º1079	 i:5 	 global-step:21585	 l-p:0.13380281627178192
epoch£º1079	 i:6 	 global-step:21586	 l-p:0.1311689168214798
epoch£º1079	 i:7 	 global-step:21587	 l-p:0.057021670043468475
epoch£º1079	 i:8 	 global-step:21588	 l-p:0.1354396641254425
epoch£º1079	 i:9 	 global-step:21589	 l-p:0.1362491250038147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6717, 3.6199, 3.6638],
        [3.6717, 3.4807, 3.5953],
        [3.6717, 3.1228, 2.4930],
        [3.6717, 2.8533, 2.2693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.11076745390892029 
model_pd.l_d.mean(): -23.323644638061523 
model_pd.lagr.mean(): -23.21287727355957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2395], device='cuda:0')), ('power', tensor([-23.5631], device='cuda:0'))])
epoch£º1080	 i:0 	 global-step:21600	 l-p:0.11076745390892029
epoch£º1080	 i:1 	 global-step:21601	 l-p:0.1125054731965065
epoch£º1080	 i:2 	 global-step:21602	 l-p:0.1887485831975937
epoch£º1080	 i:3 	 global-step:21603	 l-p:0.0670938640832901
epoch£º1080	 i:4 	 global-step:21604	 l-p:0.15206602215766907
epoch£º1080	 i:5 	 global-step:21605	 l-p:0.020741330459713936
epoch£º1080	 i:6 	 global-step:21606	 l-p:0.1326664537191391
epoch£º1080	 i:7 	 global-step:21607	 l-p:0.3325125575065613
epoch£º1080	 i:8 	 global-step:21608	 l-p:0.15037542581558228
epoch£º1080	 i:9 	 global-step:21609	 l-p:0.13519024848937988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6732, 3.6730, 3.6732],
        [3.6732, 3.6732, 3.6732],
        [3.6732, 3.6706, 3.6732],
        [3.6732, 3.6577, 3.6721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.11141444742679596 
model_pd.l_d.mean(): -22.764551162719727 
model_pd.lagr.mean(): -22.65313720703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2732], device='cuda:0')), ('power', tensor([-23.0378], device='cuda:0'))])
epoch£º1081	 i:0 	 global-step:21620	 l-p:0.11141444742679596
epoch£º1081	 i:1 	 global-step:21621	 l-p:0.1679541915655136
epoch£º1081	 i:2 	 global-step:21622	 l-p:0.12758781015872955
epoch£º1081	 i:3 	 global-step:21623	 l-p:0.07451017200946808
epoch£º1081	 i:4 	 global-step:21624	 l-p:0.15162868797779083
epoch£º1081	 i:5 	 global-step:21625	 l-p:0.23084601759910583
epoch£º1081	 i:6 	 global-step:21626	 l-p:0.05062476545572281
epoch£º1081	 i:7 	 global-step:21627	 l-p:0.12518800795078278
epoch£º1081	 i:8 	 global-step:21628	 l-p:3.297799825668335
epoch£º1081	 i:9 	 global-step:21629	 l-p:0.14132918417453766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6624, 3.6624, 3.6624],
        [3.6624, 3.4246, 3.5490],
        [3.6624, 2.8485, 2.3566],
        [3.6624, 2.8520, 2.2356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.10066446661949158 
model_pd.l_d.mean(): -22.8110408782959 
model_pd.lagr.mean(): -22.710376739501953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3098], device='cuda:0')), ('power', tensor([-23.1209], device='cuda:0'))])
epoch£º1082	 i:0 	 global-step:21640	 l-p:0.10066446661949158
epoch£º1082	 i:1 	 global-step:21641	 l-p:0.11456037312746048
epoch£º1082	 i:2 	 global-step:21642	 l-p:0.1327979862689972
epoch£º1082	 i:3 	 global-step:21643	 l-p:0.13235636055469513
epoch£º1082	 i:4 	 global-step:21644	 l-p:0.22679315507411957
epoch£º1082	 i:5 	 global-step:21645	 l-p:0.10624173283576965
epoch£º1082	 i:6 	 global-step:21646	 l-p:0.10512954741716385
epoch£º1082	 i:7 	 global-step:21647	 l-p:0.13713277876377106
epoch£º1082	 i:8 	 global-step:21648	 l-p:0.14146381616592407
epoch£º1082	 i:9 	 global-step:21649	 l-p:1.485736608505249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6622, 3.6569, 3.6621],
        [3.6622, 3.1185, 3.1267],
        [3.6622, 3.1325, 2.5074],
        [3.6622, 2.8605, 2.2327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.21727225184440613 
model_pd.l_d.mean(): -23.307708740234375 
model_pd.lagr.mean(): -23.090436935424805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2853], device='cuda:0')), ('power', tensor([-23.5930], device='cuda:0'))])
epoch£º1083	 i:0 	 global-step:21660	 l-p:0.21727225184440613
epoch£º1083	 i:1 	 global-step:21661	 l-p:0.13605962693691254
epoch£º1083	 i:2 	 global-step:21662	 l-p:0.8254972696304321
epoch£º1083	 i:3 	 global-step:21663	 l-p:0.12781599164009094
epoch£º1083	 i:4 	 global-step:21664	 l-p:0.16604383289813995
epoch£º1083	 i:5 	 global-step:21665	 l-p:0.10522056370973587
epoch£º1083	 i:6 	 global-step:21666	 l-p:0.16294333338737488
epoch£º1083	 i:7 	 global-step:21667	 l-p:0.13802769780158997
epoch£º1083	 i:8 	 global-step:21668	 l-p:-0.04178069159388542
epoch£º1083	 i:9 	 global-step:21669	 l-p:0.1489548236131668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6614, 3.1452, 2.5226],
        [3.6614, 3.6355, 3.6589],
        [3.6614, 2.9003, 2.5800],
        [3.6614, 2.8595, 2.2317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.11939357221126556 
model_pd.l_d.mean(): -23.329797744750977 
model_pd.lagr.mean(): -23.210403442382812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2816], device='cuda:0')), ('power', tensor([-23.6114], device='cuda:0'))])
epoch£º1084	 i:0 	 global-step:21680	 l-p:0.11939357221126556
epoch£º1084	 i:1 	 global-step:21681	 l-p:0.13785754144191742
epoch£º1084	 i:2 	 global-step:21682	 l-p:0.1231519877910614
epoch£º1084	 i:3 	 global-step:21683	 l-p:0.13558143377304077
epoch£º1084	 i:4 	 global-step:21684	 l-p:-0.14675089716911316
epoch£º1084	 i:5 	 global-step:21685	 l-p:0.11181227117776871
epoch£º1084	 i:6 	 global-step:21686	 l-p:0.16835609078407288
epoch£º1084	 i:7 	 global-step:21687	 l-p:0.4135293960571289
epoch£º1084	 i:8 	 global-step:21688	 l-p:0.15998069941997528
epoch£º1084	 i:9 	 global-step:21689	 l-p:0.1776576042175293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6406, 3.4119, 3.5353],
        [3.6406, 2.8354, 2.3946],
        [3.6406, 3.1384, 3.1834],
        [3.6406, 3.6356, 3.6404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.14905036985874176 
model_pd.l_d.mean(): -22.98408317565918 
model_pd.lagr.mean(): -22.835033416748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2540], device='cuda:0')), ('power', tensor([-23.2381], device='cuda:0'))])
epoch£º1085	 i:0 	 global-step:21700	 l-p:0.14905036985874176
epoch£º1085	 i:1 	 global-step:21701	 l-p:0.12664034962654114
epoch£º1085	 i:2 	 global-step:21702	 l-p:0.1395474374294281
epoch£º1085	 i:3 	 global-step:21703	 l-p:0.13102813065052032
epoch£º1085	 i:4 	 global-step:21704	 l-p:0.7769879102706909
epoch£º1085	 i:5 	 global-step:21705	 l-p:0.1587521880865097
epoch£º1085	 i:6 	 global-step:21706	 l-p:0.023115668445825577
epoch£º1085	 i:7 	 global-step:21707	 l-p:0.08969173580408096
epoch£º1085	 i:8 	 global-step:21708	 l-p:0.20121963322162628
epoch£º1085	 i:9 	 global-step:21709	 l-p:0.17528419196605682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6381, 3.4655, 3.5746],
        [3.6381, 2.9606, 2.3184],
        [3.6381, 3.6381, 3.6381],
        [3.6381, 3.6379, 3.6381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.14530305564403534 
model_pd.l_d.mean(): -23.08066749572754 
model_pd.lagr.mean(): -22.93536376953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1594], device='cuda:0')), ('power', tensor([-23.2401], device='cuda:0'))])
epoch£º1086	 i:0 	 global-step:21720	 l-p:0.14530305564403534
epoch£º1086	 i:1 	 global-step:21721	 l-p:0.132188618183136
epoch£º1086	 i:2 	 global-step:21722	 l-p:0.7327460050582886
epoch£º1086	 i:3 	 global-step:21723	 l-p:0.1020086482167244
epoch£º1086	 i:4 	 global-step:21724	 l-p:0.07878218591213226
epoch£º1086	 i:5 	 global-step:21725	 l-p:0.15022088587284088
epoch£º1086	 i:6 	 global-step:21726	 l-p:0.16561943292617798
epoch£º1086	 i:7 	 global-step:21727	 l-p:-0.07594957202672958
epoch£º1086	 i:8 	 global-step:21728	 l-p:0.12673917412757874
epoch£º1086	 i:9 	 global-step:21729	 l-p:0.151932030916214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6496, 3.6494, 3.6496],
        [3.6496, 3.5881, 3.6390],
        [3.6496, 3.6145, 3.6455],
        [3.6496, 2.8352, 2.2222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.14117839932441711 
model_pd.l_d.mean(): -22.918088912963867 
model_pd.lagr.mean(): -22.77691078186035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2905], device='cuda:0')), ('power', tensor([-23.2086], device='cuda:0'))])
epoch£º1087	 i:0 	 global-step:21740	 l-p:0.14117839932441711
epoch£º1087	 i:1 	 global-step:21741	 l-p:0.1271597295999527
epoch£º1087	 i:2 	 global-step:21742	 l-p:0.3274349570274353
epoch£º1087	 i:3 	 global-step:21743	 l-p:0.13168813288211823
epoch£º1087	 i:4 	 global-step:21744	 l-p:0.10819102078676224
epoch£º1087	 i:5 	 global-step:21745	 l-p:0.13079577684402466
epoch£º1087	 i:6 	 global-step:21746	 l-p:0.06569306552410126
epoch£º1087	 i:7 	 global-step:21747	 l-p:0.13429173827171326
epoch£º1087	 i:8 	 global-step:21748	 l-p:0.05078434199094772
epoch£º1087	 i:9 	 global-step:21749	 l-p:0.15440534055233002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6417, 3.6220, 3.6402],
        [3.6417, 2.9002, 2.2548],
        [3.6417, 3.0917, 2.4661],
        [3.6417, 2.8749, 2.2319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.13438107073307037 
model_pd.l_d.mean(): -23.26693344116211 
model_pd.lagr.mean(): -23.132553100585938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2696], device='cuda:0')), ('power', tensor([-23.5366], device='cuda:0'))])
epoch£º1088	 i:0 	 global-step:21760	 l-p:0.13438107073307037
epoch£º1088	 i:1 	 global-step:21761	 l-p:0.1342041790485382
epoch£º1088	 i:2 	 global-step:21762	 l-p:0.14697840809822083
epoch£º1088	 i:3 	 global-step:21763	 l-p:0.1014111340045929
epoch£º1088	 i:4 	 global-step:21764	 l-p:-2.1337389945983887
epoch£º1088	 i:5 	 global-step:21765	 l-p:0.1527988165616989
epoch£º1088	 i:6 	 global-step:21766	 l-p:0.08923669904470444
epoch£º1088	 i:7 	 global-step:21767	 l-p:0.17182010412216187
epoch£º1088	 i:8 	 global-step:21768	 l-p:0.053018465638160706
epoch£º1088	 i:9 	 global-step:21769	 l-p:0.10551223158836365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6347, 3.4602, 3.5700],
        [3.6347, 3.4508, 3.5636],
        [3.6347, 2.8281, 2.3871],
        [3.6347, 3.6347, 3.6347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.053864046931266785 
model_pd.l_d.mean(): -23.63413429260254 
model_pd.lagr.mean(): -23.580270767211914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2087], device='cuda:0')), ('power', tensor([-23.8428], device='cuda:0'))])
epoch£º1089	 i:0 	 global-step:21780	 l-p:0.053864046931266785
epoch£º1089	 i:1 	 global-step:21781	 l-p:0.12933361530303955
epoch£º1089	 i:2 	 global-step:21782	 l-p:0.6537134647369385
epoch£º1089	 i:3 	 global-step:21783	 l-p:0.13942308723926544
epoch£º1089	 i:4 	 global-step:21784	 l-p:0.08563995361328125
epoch£º1089	 i:5 	 global-step:21785	 l-p:0.11791881173849106
epoch£º1089	 i:6 	 global-step:21786	 l-p:0.12443573772907257
epoch£º1089	 i:7 	 global-step:21787	 l-p:0.035634201020002365
epoch£º1089	 i:8 	 global-step:21788	 l-p:0.13219678401947021
epoch£º1089	 i:9 	 global-step:21789	 l-p:0.16634760797023773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6551, 2.9582, 2.7702],
        [3.6551, 2.8633, 2.4631],
        [3.6551, 2.9643, 2.3182],
        [3.6551, 3.6525, 3.6550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.14607800543308258 
model_pd.l_d.mean(): -23.59425926208496 
model_pd.lagr.mean(): -23.44818115234375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1797], device='cuda:0')), ('power', tensor([-23.7740], device='cuda:0'))])
epoch£º1090	 i:0 	 global-step:21800	 l-p:0.14607800543308258
epoch£º1090	 i:1 	 global-step:21801	 l-p:0.21312865614891052
epoch£º1090	 i:2 	 global-step:21802	 l-p:-0.10480552166700363
epoch£º1090	 i:3 	 global-step:21803	 l-p:0.11831071227788925
epoch£º1090	 i:4 	 global-step:21804	 l-p:0.1350637972354889
epoch£º1090	 i:5 	 global-step:21805	 l-p:0.10257438570261002
epoch£º1090	 i:6 	 global-step:21806	 l-p:0.09519106894731522
epoch£º1090	 i:7 	 global-step:21807	 l-p:0.08914007246494293
epoch£º1090	 i:8 	 global-step:21808	 l-p:0.11911676824092865
epoch£º1090	 i:9 	 global-step:21809	 l-p:0.11766035854816437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6562, 2.8649, 2.2284],
        [3.6562, 2.8675, 2.4765],
        [3.6562, 2.8398, 2.2318],
        [3.6562, 3.6559, 3.6562]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.14410938322544098 
model_pd.l_d.mean(): -23.37736701965332 
model_pd.lagr.mean(): -23.233257293701172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1864], device='cuda:0')), ('power', tensor([-23.5637], device='cuda:0'))])
epoch£º1091	 i:0 	 global-step:21820	 l-p:0.14410938322544098
epoch£º1091	 i:1 	 global-step:21821	 l-p:0.134939044713974
epoch£º1091	 i:2 	 global-step:21822	 l-p:0.13441352546215057
epoch£º1091	 i:3 	 global-step:21823	 l-p:0.13926972448825836
epoch£º1091	 i:4 	 global-step:21824	 l-p:0.3431737720966339
epoch£º1091	 i:5 	 global-step:21825	 l-p:-0.05748651921749115
epoch£º1091	 i:6 	 global-step:21826	 l-p:0.09561570733785629
epoch£º1091	 i:7 	 global-step:21827	 l-p:0.19159629940986633
epoch£º1091	 i:8 	 global-step:21828	 l-p:0.09758268296718597
epoch£º1091	 i:9 	 global-step:21829	 l-p:0.11680826544761658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6482, 3.3578, 3.4852],
        [3.6482, 3.6137, 3.6442],
        [3.6482, 2.8823, 2.2382],
        [3.6482, 3.1032, 3.1120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.08452043682336807 
model_pd.l_d.mean(): -23.523244857788086 
model_pd.lagr.mean(): -23.438724517822266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2457], device='cuda:0')), ('power', tensor([-23.7690], device='cuda:0'))])
epoch£º1092	 i:0 	 global-step:21840	 l-p:0.08452043682336807
epoch£º1092	 i:1 	 global-step:21841	 l-p:0.2026485800743103
epoch£º1092	 i:2 	 global-step:21842	 l-p:0.1353899985551834
epoch£º1092	 i:3 	 global-step:21843	 l-p:0.1308036893606186
epoch£º1092	 i:4 	 global-step:21844	 l-p:0.15891602635383606
epoch£º1092	 i:5 	 global-step:21845	 l-p:0.1329311728477478
epoch£º1092	 i:6 	 global-step:21846	 l-p:0.1945229172706604
epoch£º1092	 i:7 	 global-step:21847	 l-p:0.14575962722301483
epoch£º1092	 i:8 	 global-step:21848	 l-p:-0.3106435239315033
epoch£º1092	 i:9 	 global-step:21849	 l-p:0.1250317543745041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6556, 2.8910, 2.2460],
        [3.6556, 3.6554, 3.6556],
        [3.6556, 3.6556, 3.6556],
        [3.6556, 3.6555, 3.6556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.1493195742368698 
model_pd.l_d.mean(): -23.49396514892578 
model_pd.lagr.mean(): -23.344646453857422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2223], device='cuda:0')), ('power', tensor([-23.7163], device='cuda:0'))])
epoch£º1093	 i:0 	 global-step:21860	 l-p:0.1493195742368698
epoch£º1093	 i:1 	 global-step:21861	 l-p:0.18679168820381165
epoch£º1093	 i:2 	 global-step:21862	 l-p:0.10153339058160782
epoch£º1093	 i:3 	 global-step:21863	 l-p:-0.21970602869987488
epoch£º1093	 i:4 	 global-step:21864	 l-p:0.10219938308000565
epoch£º1093	 i:5 	 global-step:21865	 l-p:0.062141235917806625
epoch£º1093	 i:6 	 global-step:21866	 l-p:0.13668833673000336
epoch£º1093	 i:7 	 global-step:21867	 l-p:0.20065249502658844
epoch£º1093	 i:8 	 global-step:21868	 l-p:0.16039195656776428
epoch£º1093	 i:9 	 global-step:21869	 l-p:0.147775799036026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6587, 3.5054, 3.6072],
        [3.6587, 3.4593, 3.5764],
        [3.6587, 3.6587, 3.6587],
        [3.6587, 3.1251, 3.1435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.14256666600704193 
model_pd.l_d.mean(): -23.68373680114746 
model_pd.lagr.mean(): -23.541170120239258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1476], device='cuda:0')), ('power', tensor([-23.8313], device='cuda:0'))])
epoch£º1094	 i:0 	 global-step:21880	 l-p:0.14256666600704193
epoch£º1094	 i:1 	 global-step:21881	 l-p:0.08806785196065903
epoch£º1094	 i:2 	 global-step:21882	 l-p:0.1826830953359604
epoch£º1094	 i:3 	 global-step:21883	 l-p:0.12975892424583435
epoch£º1094	 i:4 	 global-step:21884	 l-p:0.15184739232063293
epoch£º1094	 i:5 	 global-step:21885	 l-p:0.12642350792884827
epoch£º1094	 i:6 	 global-step:21886	 l-p:-0.32748740911483765
epoch£º1094	 i:7 	 global-step:21887	 l-p:0.1503673642873764
epoch£º1094	 i:8 	 global-step:21888	 l-p:0.09936849027872086
epoch£º1094	 i:9 	 global-step:21889	 l-p:0.28572073578834534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6519, 3.6478, 3.6518],
        [3.6519, 3.3401, 3.4660],
        [3.6519, 3.1068, 3.1151],
        [3.6519, 3.6519, 3.6519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.13895846903324127 
model_pd.l_d.mean(): -23.429319381713867 
model_pd.lagr.mean(): -23.290361404418945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2340], device='cuda:0')), ('power', tensor([-23.6633], device='cuda:0'))])
epoch£º1095	 i:0 	 global-step:21900	 l-p:0.13895846903324127
epoch£º1095	 i:1 	 global-step:21901	 l-p:0.18589511513710022
epoch£º1095	 i:2 	 global-step:21902	 l-p:0.09983127564191818
epoch£º1095	 i:3 	 global-step:21903	 l-p:0.09031348675489426
epoch£º1095	 i:4 	 global-step:21904	 l-p:0.13658642768859863
epoch£º1095	 i:5 	 global-step:21905	 l-p:0.24037763476371765
epoch£º1095	 i:6 	 global-step:21906	 l-p:0.07683874666690826
epoch£º1095	 i:7 	 global-step:21907	 l-p:-1.759764313697815
epoch£º1095	 i:8 	 global-step:21908	 l-p:0.15274256467819214
epoch£º1095	 i:9 	 global-step:21909	 l-p:0.13281719386577606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6685, 3.1355, 3.1537],
        [3.6685, 3.1239, 3.1315],
        [3.6685, 3.6683, 3.6685],
        [3.6685, 3.2231, 3.3046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.1886252760887146 
model_pd.l_d.mean(): -21.855045318603516 
model_pd.lagr.mean(): -21.666419982910156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4043], device='cuda:0')), ('power', tensor([-22.2594], device='cuda:0'))])
epoch£º1096	 i:0 	 global-step:21920	 l-p:0.1886252760887146
epoch£º1096	 i:1 	 global-step:21921	 l-p:0.37078818678855896
epoch£º1096	 i:2 	 global-step:21922	 l-p:0.14054910838603973
epoch£º1096	 i:3 	 global-step:21923	 l-p:0.1576966494321823
epoch£º1096	 i:4 	 global-step:21924	 l-p:0.077101930975914
epoch£º1096	 i:5 	 global-step:21925	 l-p:0.11151173710823059
epoch£º1096	 i:6 	 global-step:21926	 l-p:-0.12593691051006317
epoch£º1096	 i:7 	 global-step:21927	 l-p:0.13495509326457977
epoch£º1096	 i:8 	 global-step:21928	 l-p:0.1411505788564682
epoch£º1096	 i:9 	 global-step:21929	 l-p:0.14228205382823944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6912, 3.6912, 3.6912],
        [3.6912, 3.6883, 3.6912],
        [3.6912, 3.6912, 3.6912],
        [3.6912, 3.0499, 2.4032]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.16915880143642426 
model_pd.l_d.mean(): -22.936016082763672 
model_pd.lagr.mean(): -22.766857147216797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2232], device='cuda:0')), ('power', tensor([-23.1593], device='cuda:0'))])
epoch£º1097	 i:0 	 global-step:21940	 l-p:0.16915880143642426
epoch£º1097	 i:1 	 global-step:21941	 l-p:0.14160999655723572
epoch£º1097	 i:2 	 global-step:21942	 l-p:-0.15782351791858673
epoch£º1097	 i:3 	 global-step:21943	 l-p:0.12259620428085327
epoch£º1097	 i:4 	 global-step:21944	 l-p:0.12373577058315277
epoch£º1097	 i:5 	 global-step:21945	 l-p:0.12550155818462372
epoch£º1097	 i:6 	 global-step:21946	 l-p:0.21223384141921997
epoch£º1097	 i:7 	 global-step:21947	 l-p:0.12518733739852905
epoch£º1097	 i:8 	 global-step:21948	 l-p:-0.02120908722281456
epoch£º1097	 i:9 	 global-step:21949	 l-p:0.02465219981968403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6971, 3.6885, 3.6967],
        [3.6971, 3.6738, 3.6950],
        [3.6971, 2.9827, 2.7562],
        [3.6971, 3.2444, 3.3204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.1444377452135086 
model_pd.l_d.mean(): -23.666738510131836 
model_pd.lagr.mean(): -23.522300720214844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1292], device='cuda:0')), ('power', tensor([-23.7960], device='cuda:0'))])
epoch£º1098	 i:0 	 global-step:21960	 l-p:0.1444377452135086
epoch£º1098	 i:1 	 global-step:21961	 l-p:0.13937370479106903
epoch£º1098	 i:2 	 global-step:21962	 l-p:0.1423501819372177
epoch£º1098	 i:3 	 global-step:21963	 l-p:0.14250390231609344
epoch£º1098	 i:4 	 global-step:21964	 l-p:0.1289832592010498
epoch£º1098	 i:5 	 global-step:21965	 l-p:0.11327850073575974
epoch£º1098	 i:6 	 global-step:21966	 l-p:-0.019783591851592064
epoch£º1098	 i:7 	 global-step:21967	 l-p:-0.02873845584690571
epoch£º1098	 i:8 	 global-step:21968	 l-p:0.11409512907266617
epoch£º1098	 i:9 	 global-step:21969	 l-p:0.3003681004047394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6807, 2.9465, 2.2955],
        [3.6807, 3.6804, 3.6807],
        [3.6807, 3.5349, 3.6335],
        [3.6807, 3.6807, 3.6807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.052792347967624664 
model_pd.l_d.mean(): -23.51900863647461 
model_pd.lagr.mean(): -23.466217041015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2004], device='cuda:0')), ('power', tensor([-23.7194], device='cuda:0'))])
epoch£º1099	 i:0 	 global-step:21980	 l-p:0.052792347967624664
epoch£º1099	 i:1 	 global-step:21981	 l-p:0.16334718465805054
epoch£º1099	 i:2 	 global-step:21982	 l-p:0.05990063399076462
epoch£º1099	 i:3 	 global-step:21983	 l-p:0.25275084376335144
epoch£º1099	 i:4 	 global-step:21984	 l-p:0.04767608642578125
epoch£º1099	 i:5 	 global-step:21985	 l-p:0.15757712721824646
epoch£º1099	 i:6 	 global-step:21986	 l-p:0.15508700907230377
epoch£º1099	 i:7 	 global-step:21987	 l-p:0.15385770797729492
epoch£º1099	 i:8 	 global-step:21988	 l-p:0.10473867505788803
epoch£º1099	 i:9 	 global-step:21989	 l-p:0.11096873879432678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6861, 3.6861, 3.6861],
        [3.6861, 3.1934, 2.5715],
        [3.6861, 3.0150, 2.3657],
        [3.6861, 3.1341, 3.1333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): -0.1430094987154007 
model_pd.l_d.mean(): -23.419525146484375 
model_pd.lagr.mean(): -23.56253433227539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2238], device='cuda:0')), ('power', tensor([-23.6433], device='cuda:0'))])
epoch£º1100	 i:0 	 global-step:22000	 l-p:-0.1430094987154007
epoch£º1100	 i:1 	 global-step:22001	 l-p:0.2601793706417084
epoch£º1100	 i:2 	 global-step:22002	 l-p:0.10086309164762497
epoch£º1100	 i:3 	 global-step:22003	 l-p:0.14853496849536896
epoch£º1100	 i:4 	 global-step:22004	 l-p:0.12321290373802185
epoch£º1100	 i:5 	 global-step:22005	 l-p:0.03097338043153286
epoch£º1100	 i:6 	 global-step:22006	 l-p:0.17861926555633545
epoch£º1100	 i:7 	 global-step:22007	 l-p:0.14559338986873627
epoch£º1100	 i:8 	 global-step:22008	 l-p:0.17311690747737885
epoch£º1100	 i:9 	 global-step:22009	 l-p:0.13422942161560059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6855, 2.8705, 2.3636],
        [3.6855, 3.6363, 3.6783],
        [3.6855, 3.6763, 3.6851],
        [3.6855, 2.8661, 2.3142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.18081294000148773 
model_pd.l_d.mean(): -23.403600692749023 
model_pd.lagr.mean(): -23.222787857055664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2406], device='cuda:0')), ('power', tensor([-23.6442], device='cuda:0'))])
epoch£º1101	 i:0 	 global-step:22020	 l-p:0.18081294000148773
epoch£º1101	 i:1 	 global-step:22021	 l-p:0.23465517163276672
epoch£º1101	 i:2 	 global-step:22022	 l-p:0.1255783885717392
epoch£º1101	 i:3 	 global-step:22023	 l-p:0.15093110501766205
epoch£º1101	 i:4 	 global-step:22024	 l-p:0.054052140563726425
epoch£º1101	 i:5 	 global-step:22025	 l-p:-0.07706151902675629
epoch£º1101	 i:6 	 global-step:22026	 l-p:0.10386356711387634
epoch£º1101	 i:7 	 global-step:22027	 l-p:-0.02552584558725357
epoch£º1101	 i:8 	 global-step:22028	 l-p:0.11236665397882462
epoch£º1101	 i:9 	 global-step:22029	 l-p:0.17177714407444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6982, 3.1226, 3.0969],
        [3.6982, 2.8868, 2.2776],
        [3.6982, 3.6826, 3.6971],
        [3.6982, 2.9846, 2.7603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.12417062371969223 
model_pd.l_d.mean(): -22.970964431762695 
model_pd.lagr.mean(): -22.84679412841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2043], device='cuda:0')), ('power', tensor([-23.1753], device='cuda:0'))])
epoch£º1102	 i:0 	 global-step:22040	 l-p:0.12417062371969223
epoch£º1102	 i:1 	 global-step:22041	 l-p:0.1809033900499344
epoch£º1102	 i:2 	 global-step:22042	 l-p:0.13763505220413208
epoch£º1102	 i:3 	 global-step:22043	 l-p:-0.2998518645763397
epoch£º1102	 i:4 	 global-step:22044	 l-p:0.13495703041553497
epoch£º1102	 i:5 	 global-step:22045	 l-p:0.18966896831989288
epoch£º1102	 i:6 	 global-step:22046	 l-p:0.12543360888957977
epoch£º1102	 i:7 	 global-step:22047	 l-p:-0.04184672608971596
epoch£º1102	 i:8 	 global-step:22048	 l-p:0.022886700928211212
epoch£º1102	 i:9 	 global-step:22049	 l-p:0.1258314996957779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6998, 3.6595, 3.6946],
        [3.6998, 3.3775, 3.5012],
        [3.6998, 2.9737, 2.3199],
        [3.6998, 3.6998, 3.6998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.11797058582305908 
model_pd.l_d.mean(): -23.4615421295166 
model_pd.lagr.mean(): -23.343570709228516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2054], device='cuda:0')), ('power', tensor([-23.6669], device='cuda:0'))])
epoch£º1103	 i:0 	 global-step:22060	 l-p:0.11797058582305908
epoch£º1103	 i:1 	 global-step:22061	 l-p:0.12791860103607178
epoch£º1103	 i:2 	 global-step:22062	 l-p:-0.08302754908800125
epoch£º1103	 i:3 	 global-step:22063	 l-p:0.13621853291988373
epoch£º1103	 i:4 	 global-step:22064	 l-p:-1.4222880601882935
epoch£º1103	 i:5 	 global-step:22065	 l-p:0.18377535045146942
epoch£º1103	 i:6 	 global-step:22066	 l-p:0.18089035153388977
epoch£º1103	 i:7 	 global-step:22067	 l-p:0.12158087641000748
epoch£º1103	 i:8 	 global-step:22068	 l-p:-0.1365932822227478
epoch£º1103	 i:9 	 global-step:22069	 l-p:0.1432129442691803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7095, 3.1911, 2.5610],
        [3.7095, 3.5387, 3.6468],
        [3.7095, 3.7095, 3.7095],
        [3.7095, 3.7017, 3.7091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.17381401360034943 
model_pd.l_d.mean(): -23.509328842163086 
model_pd.lagr.mean(): -23.335514068603516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2045], device='cuda:0')), ('power', tensor([-23.7138], device='cuda:0'))])
epoch£º1104	 i:0 	 global-step:22080	 l-p:0.17381401360034943
epoch£º1104	 i:1 	 global-step:22081	 l-p:0.1309988796710968
epoch£º1104	 i:2 	 global-step:22082	 l-p:7.855782508850098
epoch£º1104	 i:3 	 global-step:22083	 l-p:0.35708534717559814
epoch£º1104	 i:4 	 global-step:22084	 l-p:0.14150744676589966
epoch£º1104	 i:5 	 global-step:22085	 l-p:0.13264501094818115
epoch£º1104	 i:6 	 global-step:22086	 l-p:0.14350539445877075
epoch£º1104	 i:7 	 global-step:22087	 l-p:0.11136890202760696
epoch£º1104	 i:8 	 global-step:22088	 l-p:0.1549254208803177
epoch£º1104	 i:9 	 global-step:22089	 l-p:0.12562979757785797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7169, 2.9045, 2.3092],
        [3.7169, 3.7031, 3.7160],
        [3.7169, 2.9429, 2.2955],
        [3.7169, 3.3687, 3.4871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.17053566873073578 
model_pd.l_d.mean(): -23.42113494873047 
model_pd.lagr.mean(): -23.250598907470703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2056], device='cuda:0')), ('power', tensor([-23.6267], device='cuda:0'))])
epoch£º1105	 i:0 	 global-step:22100	 l-p:0.17053566873073578
epoch£º1105	 i:1 	 global-step:22101	 l-p:0.12580527365207672
epoch£º1105	 i:2 	 global-step:22102	 l-p:-1.0483498573303223
epoch£º1105	 i:3 	 global-step:22103	 l-p:0.13043245673179626
epoch£º1105	 i:4 	 global-step:22104	 l-p:0.13213177025318146
epoch£º1105	 i:5 	 global-step:22105	 l-p:0.13288554549217224
epoch£º1105	 i:6 	 global-step:22106	 l-p:0.5942652821540833
epoch£º1105	 i:7 	 global-step:22107	 l-p:-0.361163854598999
epoch£º1105	 i:8 	 global-step:22108	 l-p:0.21811309456825256
epoch£º1105	 i:9 	 global-step:22109	 l-p:0.13413265347480774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7055, 3.0398, 2.3881],
        [3.7055, 3.7000, 3.7053],
        [3.7055, 3.6615, 3.6995],
        [3.7055, 3.6438, 3.6949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.1638036072254181 
model_pd.l_d.mean(): -23.340845108032227 
model_pd.lagr.mean(): -23.17704200744629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1922], device='cuda:0')), ('power', tensor([-23.5331], device='cuda:0'))])
epoch£º1106	 i:0 	 global-step:22120	 l-p:0.1638036072254181
epoch£º1106	 i:1 	 global-step:22121	 l-p:-0.11720558255910873
epoch£º1106	 i:2 	 global-step:22122	 l-p:0.1199248656630516
epoch£º1106	 i:3 	 global-step:22123	 l-p:0.12143674492835999
epoch£º1106	 i:4 	 global-step:22124	 l-p:-0.4190077483654022
epoch£º1106	 i:5 	 global-step:22125	 l-p:0.2095489650964737
epoch£º1106	 i:6 	 global-step:22126	 l-p:0.12912523746490479
epoch£º1106	 i:7 	 global-step:22127	 l-p:0.13214744627475739
epoch£º1106	 i:8 	 global-step:22128	 l-p:0.7341815829277039
epoch£º1106	 i:9 	 global-step:22129	 l-p:0.13673345744609833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7069, 3.0797, 2.4323],
        [3.7069, 3.7069, 3.7069],
        [3.7069, 3.5182, 3.6321],
        [3.7069, 3.7014, 3.7067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.7962058186531067 
model_pd.l_d.mean(): -23.479129791259766 
model_pd.lagr.mean(): -22.682924270629883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2141], device='cuda:0')), ('power', tensor([-23.6932], device='cuda:0'))])
epoch£º1107	 i:0 	 global-step:22140	 l-p:0.7962058186531067
epoch£º1107	 i:1 	 global-step:22141	 l-p:-0.07232029736042023
epoch£º1107	 i:2 	 global-step:22142	 l-p:0.129747211933136
epoch£º1107	 i:3 	 global-step:22143	 l-p:0.10933921486139297
epoch£º1107	 i:4 	 global-step:22144	 l-p:0.12941938638687134
epoch£º1107	 i:5 	 global-step:22145	 l-p:-0.3136358857154846
epoch£º1107	 i:6 	 global-step:22146	 l-p:0.12362098693847656
epoch£º1107	 i:7 	 global-step:22147	 l-p:0.16553129255771637
epoch£º1107	 i:8 	 global-step:22148	 l-p:0.0928879976272583
epoch£º1107	 i:9 	 global-step:22149	 l-p:0.13922670483589172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7011, 3.0316, 2.8831],
        [3.7011, 3.7011, 3.7011],
        [3.7011, 3.2012, 3.2452],
        [3.7011, 2.9235, 2.5528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.13579398393630981 
model_pd.l_d.mean(): -23.350601196289062 
model_pd.lagr.mean(): -23.214807510375977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2391], device='cuda:0')), ('power', tensor([-23.5897], device='cuda:0'))])
epoch£º1108	 i:0 	 global-step:22160	 l-p:0.13579398393630981
epoch£º1108	 i:1 	 global-step:22161	 l-p:0.1321561336517334
epoch£º1108	 i:2 	 global-step:22162	 l-p:0.1374487727880478
epoch£º1108	 i:3 	 global-step:22163	 l-p:-0.0952407494187355
epoch£º1108	 i:4 	 global-step:22164	 l-p:0.14576564729213715
epoch£º1108	 i:5 	 global-step:22165	 l-p:0.17184679210186005
epoch£º1108	 i:6 	 global-step:22166	 l-p:0.13869363069534302
epoch£º1108	 i:7 	 global-step:22167	 l-p:0.03353852033615112
epoch£º1108	 i:8 	 global-step:22168	 l-p:0.11847623437643051
epoch£º1108	 i:9 	 global-step:22169	 l-p:0.18643875420093536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6865, 3.0183, 2.3691],
        [3.6865, 3.6863, 3.6865],
        [3.6865, 3.2921, 3.3973],
        [3.6865, 3.2257, 3.2976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.1254693567752838 
model_pd.l_d.mean(): -23.356544494628906 
model_pd.lagr.mean(): -23.231075286865234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2077], device='cuda:0')), ('power', tensor([-23.5643], device='cuda:0'))])
epoch£º1109	 i:0 	 global-step:22180	 l-p:0.1254693567752838
epoch£º1109	 i:1 	 global-step:22181	 l-p:0.16375714540481567
epoch£º1109	 i:2 	 global-step:22182	 l-p:0.1600455492734909
epoch£º1109	 i:3 	 global-step:22183	 l-p:0.13231150805950165
epoch£º1109	 i:4 	 global-step:22184	 l-p:0.13426803052425385
epoch£º1109	 i:5 	 global-step:22185	 l-p:0.02219831943511963
epoch£º1109	 i:6 	 global-step:22186	 l-p:0.17740638554096222
epoch£º1109	 i:7 	 global-step:22187	 l-p:0.1335809975862503
epoch£º1109	 i:8 	 global-step:22188	 l-p:0.11817432194948196
epoch£º1109	 i:9 	 global-step:22189	 l-p:-0.038411349058151245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228]], device='cuda:0')
 pt:tensor([[3.6904, 3.3251, 3.4400],
        [3.6904, 2.8817, 2.2624],
        [3.6904, 2.9853, 2.3329],
        [3.6904, 3.0171, 2.3669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.05304098501801491 
model_pd.l_d.mean(): -23.321285247802734 
model_pd.lagr.mean(): -23.26824378967285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2686], device='cuda:0')), ('power', tensor([-23.5899], device='cuda:0'))])
epoch£º1110	 i:0 	 global-step:22200	 l-p:0.05304098501801491
epoch£º1110	 i:1 	 global-step:22201	 l-p:0.17965513467788696
epoch£º1110	 i:2 	 global-step:22202	 l-p:-0.030731577426195145
epoch£º1110	 i:3 	 global-step:22203	 l-p:0.1340802162885666
epoch£º1110	 i:4 	 global-step:22204	 l-p:-0.8169634938240051
epoch£º1110	 i:5 	 global-step:22205	 l-p:0.11546674370765686
epoch£º1110	 i:6 	 global-step:22206	 l-p:0.12617044150829315
epoch£º1110	 i:7 	 global-step:22207	 l-p:0.2131662368774414
epoch£º1110	 i:8 	 global-step:22208	 l-p:0.11537238955497742
epoch£º1110	 i:9 	 global-step:22209	 l-p:0.13091278076171875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7098, 2.9713, 2.3166],
        [3.7098, 3.6384, 3.6961],
        [3.7098, 3.7098, 3.7098],
        [3.7098, 2.9956, 2.7687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.13100773096084595 
model_pd.l_d.mean(): -22.54159164428711 
model_pd.lagr.mean(): -22.41058349609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3030], device='cuda:0')), ('power', tensor([-22.8446], device='cuda:0'))])
epoch£º1111	 i:0 	 global-step:22220	 l-p:0.13100773096084595
epoch£º1111	 i:1 	 global-step:22221	 l-p:0.6233227849006653
epoch£º1111	 i:2 	 global-step:22222	 l-p:0.13859547674655914
epoch£º1111	 i:3 	 global-step:22223	 l-p:0.14004525542259216
epoch£º1111	 i:4 	 global-step:22224	 l-p:0.12781302630901337
epoch£º1111	 i:5 	 global-step:22225	 l-p:-0.043028220534324646
epoch£º1111	 i:6 	 global-step:22226	 l-p:0.11229565739631653
epoch£º1111	 i:7 	 global-step:22227	 l-p:-0.13028684258460999
epoch£º1111	 i:8 	 global-step:22228	 l-p:0.1694742888212204
epoch£º1111	 i:9 	 global-step:22229	 l-p:0.14033734798431396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[3.6999, 2.8836, 2.3586],
        [3.6999, 2.8815, 2.3140],
        [3.6999, 3.1229, 3.0961],
        [3.6999, 2.8836, 2.3583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.1672373116016388 
model_pd.l_d.mean(): -23.09929847717285 
model_pd.lagr.mean(): -22.93206024169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2405], device='cuda:0')), ('power', tensor([-23.3398], device='cuda:0'))])
epoch£º1112	 i:0 	 global-step:22240	 l-p:0.1672373116016388
epoch£º1112	 i:1 	 global-step:22241	 l-p:0.14064271748065948
epoch£º1112	 i:2 	 global-step:22242	 l-p:0.11982721090316772
epoch£º1112	 i:3 	 global-step:22243	 l-p:0.14673861861228943
epoch£º1112	 i:4 	 global-step:22244	 l-p:0.22033360600471497
epoch£º1112	 i:5 	 global-step:22245	 l-p:-0.47631940245628357
epoch£º1112	 i:6 	 global-step:22246	 l-p:-0.24761813879013062
epoch£º1112	 i:7 	 global-step:22247	 l-p:0.1281161904335022
epoch£º1112	 i:8 	 global-step:22248	 l-p:0.12999114394187927
epoch£º1112	 i:9 	 global-step:22249	 l-p:0.12459846585988998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7020, 3.6998, 3.7019],
        [3.7020, 3.7009, 3.7020],
        [3.7020, 3.7020, 3.7020],
        [3.7020, 3.1667, 3.1814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.13548904657363892 
model_pd.l_d.mean(): -22.820241928100586 
model_pd.lagr.mean(): -22.68475341796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2744], device='cuda:0')), ('power', tensor([-23.0947], device='cuda:0'))])
epoch£º1113	 i:0 	 global-step:22260	 l-p:0.13548904657363892
epoch£º1113	 i:1 	 global-step:22261	 l-p:0.2049558013677597
epoch£º1113	 i:2 	 global-step:22262	 l-p:0.12454120069742203
epoch£º1113	 i:3 	 global-step:22263	 l-p:0.12176067382097244
epoch£º1113	 i:4 	 global-step:22264	 l-p:1.4286040823208168e-05
epoch£º1113	 i:5 	 global-step:22265	 l-p:0.13299338519573212
epoch£º1113	 i:6 	 global-step:22266	 l-p:0.11028659343719482
epoch£º1113	 i:7 	 global-step:22267	 l-p:-0.07338421046733856
epoch£º1113	 i:8 	 global-step:22268	 l-p:0.18992117047309875
epoch£º1113	 i:9 	 global-step:22269	 l-p:0.06829269975423813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6899, 3.5337, 3.6366],
        [3.6899, 2.9680, 2.7310],
        [3.6899, 2.8914, 2.4613],
        [3.6899, 3.6845, 3.6897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.12431588768959045 
model_pd.l_d.mean(): -22.29080581665039 
model_pd.lagr.mean(): -22.16649055480957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2889], device='cuda:0')), ('power', tensor([-22.5797], device='cuda:0'))])
epoch£º1114	 i:0 	 global-step:22280	 l-p:0.12431588768959045
epoch£º1114	 i:1 	 global-step:22281	 l-p:0.13884297013282776
epoch£º1114	 i:2 	 global-step:22282	 l-p:0.03973923251032829
epoch£º1114	 i:3 	 global-step:22283	 l-p:0.16413968801498413
epoch£º1114	 i:4 	 global-step:22284	 l-p:0.12266084551811218
epoch£º1114	 i:5 	 global-step:22285	 l-p:0.13089480996131897
epoch£º1114	 i:6 	 global-step:22286	 l-p:0.1958230882883072
epoch£º1114	 i:7 	 global-step:22287	 l-p:0.1382325291633606
epoch£º1114	 i:8 	 global-step:22288	 l-p:-0.10439635813236237
epoch£º1114	 i:9 	 global-step:22289	 l-p:0.20474959909915924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6877, 2.8732, 2.2641],
        [3.6877, 3.6753, 3.6870],
        [3.6877, 3.0130, 2.3629],
        [3.6877, 3.6614, 3.6852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.16760650277137756 
model_pd.l_d.mean(): -23.269054412841797 
model_pd.lagr.mean(): -23.10144805908203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2480], device='cuda:0')), ('power', tensor([-23.5171], device='cuda:0'))])
epoch£º1115	 i:0 	 global-step:22300	 l-p:0.16760650277137756
epoch£º1115	 i:1 	 global-step:22301	 l-p:-0.04167754203081131
epoch£º1115	 i:2 	 global-step:22302	 l-p:0.12808145582675934
epoch£º1115	 i:3 	 global-step:22303	 l-p:0.1794567108154297
epoch£º1115	 i:4 	 global-step:22304	 l-p:0.14611636102199554
epoch£º1115	 i:5 	 global-step:22305	 l-p:0.14290478825569153
epoch£º1115	 i:6 	 global-step:22306	 l-p:0.015736674889922142
epoch£º1115	 i:7 	 global-step:22307	 l-p:0.1046433225274086
epoch£º1115	 i:8 	 global-step:22308	 l-p:0.12387148290872574
epoch£º1115	 i:9 	 global-step:22309	 l-p:0.045106466859579086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6957, 3.2236, 3.2882],
        [3.6957, 3.1632, 3.1810],
        [3.6957, 2.8806, 2.3729],
        [3.6957, 3.6957, 3.6957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.12226215749979019 
model_pd.l_d.mean(): -23.47724151611328 
model_pd.lagr.mean(): -23.354978561401367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1799], device='cuda:0')), ('power', tensor([-23.6572], device='cuda:0'))])
epoch£º1116	 i:0 	 global-step:22320	 l-p:0.12226215749979019
epoch£º1116	 i:1 	 global-step:22321	 l-p:0.14506752789020538
epoch£º1116	 i:2 	 global-step:22322	 l-p:0.12163564562797546
epoch£º1116	 i:3 	 global-step:22323	 l-p:0.13015712797641754
epoch£º1116	 i:4 	 global-step:22324	 l-p:0.052305225282907486
epoch£º1116	 i:5 	 global-step:22325	 l-p:0.12555624544620514
epoch£º1116	 i:6 	 global-step:22326	 l-p:0.17926374077796936
epoch£º1116	 i:7 	 global-step:22327	 l-p:0.046546995639801025
epoch£º1116	 i:8 	 global-step:22328	 l-p:0.13260537385940552
epoch£º1116	 i:9 	 global-step:22329	 l-p:0.19475609064102173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6808, 3.5632, 3.6485],
        [3.6808, 3.6659, 3.6798],
        [3.6808, 3.1210, 3.1143],
        [3.6808, 3.6806, 3.6808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.1281953901052475 
model_pd.l_d.mean(): -23.603357315063477 
model_pd.lagr.mean(): -23.475162506103516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1548], device='cuda:0')), ('power', tensor([-23.7582], device='cuda:0'))])
epoch£º1117	 i:0 	 global-step:22340	 l-p:0.1281953901052475
epoch£º1117	 i:1 	 global-step:22341	 l-p:0.12080039829015732
epoch£º1117	 i:2 	 global-step:22342	 l-p:0.061065178364515305
epoch£º1117	 i:3 	 global-step:22343	 l-p:0.24793373048305511
epoch£º1117	 i:4 	 global-step:22344	 l-p:0.1407170295715332
epoch£º1117	 i:5 	 global-step:22345	 l-p:0.15481655299663544
epoch£º1117	 i:6 	 global-step:22346	 l-p:0.1899474710226059
epoch£º1117	 i:7 	 global-step:22347	 l-p:0.05749330297112465
epoch£º1117	 i:8 	 global-step:22348	 l-p:0.1378212422132492
epoch£º1117	 i:9 	 global-step:22349	 l-p:0.1304459124803543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6761, 3.6662, 3.6756],
        [3.6761, 3.3258, 3.4451],
        [3.6761, 3.5045, 3.6132],
        [3.6761, 3.6711, 3.6759]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.05776097625494003 
model_pd.l_d.mean(): -22.71474838256836 
model_pd.lagr.mean(): -22.6569881439209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2449], device='cuda:0')), ('power', tensor([-22.9596], device='cuda:0'))])
epoch£º1118	 i:0 	 global-step:22360	 l-p:0.05776097625494003
epoch£º1118	 i:1 	 global-step:22361	 l-p:0.12054760009050369
epoch£º1118	 i:2 	 global-step:22362	 l-p:0.07353375107049942
epoch£º1118	 i:3 	 global-step:22363	 l-p:0.12469189614057541
epoch£º1118	 i:4 	 global-step:22364	 l-p:0.13139519095420837
epoch£º1118	 i:5 	 global-step:22365	 l-p:0.14877304434776306
epoch£º1118	 i:6 	 global-step:22366	 l-p:0.15005236864089966
epoch£º1118	 i:7 	 global-step:22367	 l-p:0.14710292220115662
epoch£º1118	 i:8 	 global-step:22368	 l-p:0.12028905004262924
epoch£º1118	 i:9 	 global-step:22369	 l-p:1.1017401218414307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6632, 3.1469, 3.1811],
        [3.6632, 3.0169, 2.3735],
        [3.6632, 3.6537, 3.6627],
        [3.6632, 3.1848, 2.5684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 8.273938179016113 
model_pd.l_d.mean(): -22.775333404541016 
model_pd.lagr.mean(): -14.501395225524902 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3490], device='cuda:0')), ('power', tensor([-23.1243], device='cuda:0'))])
epoch£º1119	 i:0 	 global-step:22380	 l-p:8.273938179016113
epoch£º1119	 i:1 	 global-step:22381	 l-p:0.12021280825138092
epoch£º1119	 i:2 	 global-step:22382	 l-p:0.14128078520298004
epoch£º1119	 i:3 	 global-step:22383	 l-p:0.26189181208610535
epoch£º1119	 i:4 	 global-step:22384	 l-p:0.11114088445901871
epoch£º1119	 i:5 	 global-step:22385	 l-p:0.194711372256279
epoch£º1119	 i:6 	 global-step:22386	 l-p:0.055347707122564316
epoch£º1119	 i:7 	 global-step:22387	 l-p:0.13764043152332306
epoch£º1119	 i:8 	 global-step:22388	 l-p:0.14722751080989838
epoch£º1119	 i:9 	 global-step:22389	 l-p:0.13669823110103607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6550, 2.8623, 2.4668],
        [3.6550, 3.4120, 3.5376],
        [3.6550, 3.5622, 3.6337],
        [3.6550, 2.9809, 2.3357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 0.22044169902801514 
model_pd.l_d.mean(): -22.83432388305664 
model_pd.lagr.mean(): -22.613882064819336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3127], device='cuda:0')), ('power', tensor([-23.1470], device='cuda:0'))])
epoch£º1120	 i:0 	 global-step:22400	 l-p:0.22044169902801514
epoch£º1120	 i:1 	 global-step:22401	 l-p:0.15630394220352173
epoch£º1120	 i:2 	 global-step:22402	 l-p:0.1424024999141693
epoch£º1120	 i:3 	 global-step:22403	 l-p:0.12294529378414154
epoch£º1120	 i:4 	 global-step:22404	 l-p:0.1362760066986084
epoch£º1120	 i:5 	 global-step:22405	 l-p:-0.05439773574471474
epoch£º1120	 i:6 	 global-step:22406	 l-p:0.1408700793981552
epoch£º1120	 i:7 	 global-step:22407	 l-p:0.12418971955776215
epoch£º1120	 i:8 	 global-step:22408	 l-p:0.13390135765075684
epoch£º1120	 i:9 	 global-step:22409	 l-p:0.11389591544866562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6412, 2.8399, 2.2066],
        [3.6412, 2.9192, 2.2730],
        [3.6412, 3.6214, 3.6396],
        [3.6412, 3.6405, 3.6412]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): 0.10050193220376968 
model_pd.l_d.mean(): -23.75935173034668 
model_pd.lagr.mean(): -23.658849716186523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1626], device='cuda:0')), ('power', tensor([-23.9219], device='cuda:0'))])
epoch£º1121	 i:0 	 global-step:22420	 l-p:0.10050193220376968
epoch£º1121	 i:1 	 global-step:22421	 l-p:0.10744912922382355
epoch£º1121	 i:2 	 global-step:22422	 l-p:0.822995662689209
epoch£º1121	 i:3 	 global-step:22423	 l-p:0.1304796040058136
epoch£º1121	 i:4 	 global-step:22424	 l-p:0.12878943979740143
epoch£º1121	 i:5 	 global-step:22425	 l-p:0.08157751709222794
epoch£º1121	 i:6 	 global-step:22426	 l-p:0.15588578581809998
epoch£º1121	 i:7 	 global-step:22427	 l-p:0.11608263105154037
epoch£º1121	 i:8 	 global-step:22428	 l-p:0.12582212686538696
epoch£º1121	 i:9 	 global-step:22429	 l-p:0.1590466946363449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6405, 3.5605, 3.6240],
        [3.6405, 3.5787, 3.6299],
        [3.6405, 3.4712, 3.5794],
        [3.6405, 3.0787, 2.4509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.14893275499343872 
model_pd.l_d.mean(): -23.237239837646484 
model_pd.lagr.mean(): -23.088306427001953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3224], device='cuda:0')), ('power', tensor([-23.5597], device='cuda:0'))])
epoch£º1122	 i:0 	 global-step:22440	 l-p:0.14893275499343872
epoch£º1122	 i:1 	 global-step:22441	 l-p:0.10990266501903534
epoch£º1122	 i:2 	 global-step:22442	 l-p:-0.010219745337963104
epoch£º1122	 i:3 	 global-step:22443	 l-p:0.7029693722724915
epoch£º1122	 i:4 	 global-step:22444	 l-p:0.13277506828308105
epoch£º1122	 i:5 	 global-step:22445	 l-p:0.1406266689300537
epoch£º1122	 i:6 	 global-step:22446	 l-p:0.15363773703575134
epoch£º1122	 i:7 	 global-step:22447	 l-p:0.13376405835151672
epoch£º1122	 i:8 	 global-step:22448	 l-p:0.22092019021511078
epoch£º1122	 i:9 	 global-step:22449	 l-p:0.07281078398227692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6463, 2.8534, 2.4612],
        [3.6463, 2.8200, 2.2309],
        [3.6463, 3.4770, 3.5851],
        [3.6463, 3.6364, 3.6458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): 0.18757054209709167 
model_pd.l_d.mean(): -23.435749053955078 
model_pd.lagr.mean(): -23.248178482055664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2602], device='cuda:0')), ('power', tensor([-23.6959], device='cuda:0'))])
epoch£º1123	 i:0 	 global-step:22460	 l-p:0.18757054209709167
epoch£º1123	 i:1 	 global-step:22461	 l-p:0.07964220643043518
epoch£º1123	 i:2 	 global-step:22462	 l-p:0.1385316252708435
epoch£º1123	 i:3 	 global-step:22463	 l-p:0.07922183722257614
epoch£º1123	 i:4 	 global-step:22464	 l-p:0.11967083066701889
epoch£º1123	 i:5 	 global-step:22465	 l-p:-0.20605799555778503
epoch£º1123	 i:6 	 global-step:22466	 l-p:0.24194452166557312
epoch£º1123	 i:7 	 global-step:22467	 l-p:0.14828363060951233
epoch£º1123	 i:8 	 global-step:22468	 l-p:0.16005830466747284
epoch£º1123	 i:9 	 global-step:22469	 l-p:0.12425178289413452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6623, 2.8452, 2.2315],
        [3.6623, 3.4331, 3.5567],
        [3.6623, 3.0659, 3.0222],
        [3.6623, 3.4623, 3.5797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.144549161195755 
model_pd.l_d.mean(): -23.581592559814453 
model_pd.lagr.mean(): -23.437044143676758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1685], device='cuda:0')), ('power', tensor([-23.7501], device='cuda:0'))])
epoch£º1124	 i:0 	 global-step:22480	 l-p:0.144549161195755
epoch£º1124	 i:1 	 global-step:22481	 l-p:0.194618359208107
epoch£º1124	 i:2 	 global-step:22482	 l-p:0.13056063652038574
epoch£º1124	 i:3 	 global-step:22483	 l-p:0.07466098666191101
epoch£º1124	 i:4 	 global-step:22484	 l-p:0.13413015007972717
epoch£º1124	 i:5 	 global-step:22485	 l-p:0.17729274928569794
epoch£º1124	 i:6 	 global-step:22486	 l-p:0.07325200736522675
epoch£º1124	 i:7 	 global-step:22487	 l-p:-0.5272884368896484
epoch£º1124	 i:8 	 global-step:22488	 l-p:0.14844411611557007
epoch£º1124	 i:9 	 global-step:22489	 l-p:0.13892753422260284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6633, 3.4860, 3.5967],
        [3.6633, 2.9151, 2.6310],
        [3.6633, 2.9444, 2.7198],
        [3.6633, 3.6471, 3.6621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.15398617088794708 
model_pd.l_d.mean(): -23.760482788085938 
model_pd.lagr.mean(): -23.606496810913086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1816], device='cuda:0')), ('power', tensor([-23.9421], device='cuda:0'))])
epoch£º1125	 i:0 	 global-step:22500	 l-p:0.15398617088794708
epoch£º1125	 i:1 	 global-step:22501	 l-p:0.1284775584936142
epoch£º1125	 i:2 	 global-step:22502	 l-p:0.06581491976976395
epoch£º1125	 i:3 	 global-step:22503	 l-p:0.22805258631706238
epoch£º1125	 i:4 	 global-step:22504	 l-p:0.12927168607711792
epoch£º1125	 i:5 	 global-step:22505	 l-p:1.3060752153396606
epoch£º1125	 i:6 	 global-step:22506	 l-p:0.14417676627635956
epoch£º1125	 i:7 	 global-step:22507	 l-p:0.11048173904418945
epoch£º1125	 i:8 	 global-step:22508	 l-p:0.1382240653038025
epoch£º1125	 i:9 	 global-step:22509	 l-p:0.1364295333623886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6670, 3.6670, 3.6670],
        [3.6670, 3.4973, 3.6054],
        [3.6670, 3.3342, 3.4575],
        [3.6670, 2.9929, 2.8445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.15763455629348755 
model_pd.l_d.mean(): -23.649431228637695 
model_pd.lagr.mean(): -23.491796493530273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1938], device='cuda:0')), ('power', tensor([-23.8432], device='cuda:0'))])
epoch£º1126	 i:0 	 global-step:22520	 l-p:0.15763455629348755
epoch£º1126	 i:1 	 global-step:22521	 l-p:0.15451154112815857
epoch£º1126	 i:2 	 global-step:22522	 l-p:0.10159717500209808
epoch£º1126	 i:3 	 global-step:22523	 l-p:0.052930667996406555
epoch£º1126	 i:4 	 global-step:22524	 l-p:0.11616555601358414
epoch£º1126	 i:5 	 global-step:22525	 l-p:0.12895505130290985
epoch£º1126	 i:6 	 global-step:22526	 l-p:0.14360666275024414
epoch£º1126	 i:7 	 global-step:22527	 l-p:0.42177242040634155
epoch£º1126	 i:8 	 global-step:22528	 l-p:0.13002538681030273
epoch£º1126	 i:9 	 global-step:22529	 l-p:0.17125117778778076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6755, 2.9731, 2.7766],
        [3.6755, 3.6746, 3.6755],
        [3.6755, 3.6752, 3.6755],
        [3.6755, 2.8576, 2.2490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.1583860218524933 
model_pd.l_d.mean(): -23.667123794555664 
model_pd.lagr.mean(): -23.508737564086914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2344], device='cuda:0')), ('power', tensor([-23.9015], device='cuda:0'))])
epoch£º1127	 i:0 	 global-step:22540	 l-p:0.1583860218524933
epoch£º1127	 i:1 	 global-step:22541	 l-p:0.1431063413619995
epoch£º1127	 i:2 	 global-step:22542	 l-p:0.12257933616638184
epoch£º1127	 i:3 	 global-step:22543	 l-p:0.11003373563289642
epoch£º1127	 i:4 	 global-step:22544	 l-p:0.12880928814411163
epoch£º1127	 i:5 	 global-step:22545	 l-p:0.14051975309848785
epoch£º1127	 i:6 	 global-step:22546	 l-p:0.04724990203976631
epoch£º1127	 i:7 	 global-step:22547	 l-p:0.34266239404678345
epoch£º1127	 i:8 	 global-step:22548	 l-p:0.0606527142226696
epoch£º1127	 i:9 	 global-step:22549	 l-p:0.14571818709373474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6805, 3.6805, 3.6805],
        [3.6805, 3.6805, 3.6805],
        [3.6805, 3.5776, 3.6549],
        [3.6805, 2.9502, 2.7007]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.21384988725185394 
model_pd.l_d.mean(): -22.489336013793945 
model_pd.lagr.mean(): -22.27548599243164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3247], device='cuda:0')), ('power', tensor([-22.8140], device='cuda:0'))])
epoch£º1128	 i:0 	 global-step:22560	 l-p:0.21384988725185394
epoch£º1128	 i:1 	 global-step:22561	 l-p:0.2904191017150879
epoch£º1128	 i:2 	 global-step:22562	 l-p:0.10813968628644943
epoch£º1128	 i:3 	 global-step:22563	 l-p:-0.08240921050310135
epoch£º1128	 i:4 	 global-step:22564	 l-p:0.12185120582580566
epoch£º1128	 i:5 	 global-step:22565	 l-p:0.14424242079257965
epoch£º1128	 i:6 	 global-step:22566	 l-p:0.09573977440595627
epoch£º1128	 i:7 	 global-step:22567	 l-p:0.08707138150930405
epoch£º1128	 i:8 	 global-step:22568	 l-p:0.1344006359577179
epoch£º1128	 i:9 	 global-step:22569	 l-p:0.12182794511318207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6853, 3.6853, 3.6853],
        [3.6853, 3.5108, 3.6205],
        [3.6853, 3.6629, 3.6833],
        [3.6853, 3.6561, 3.6823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.17527049779891968 
model_pd.l_d.mean(): -23.009611129760742 
model_pd.lagr.mean(): -22.834341049194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2036], device='cuda:0')), ('power', tensor([-23.2132], device='cuda:0'))])
epoch£º1129	 i:0 	 global-step:22580	 l-p:0.17527049779891968
epoch£º1129	 i:1 	 global-step:22581	 l-p:0.14005541801452637
epoch£º1129	 i:2 	 global-step:22582	 l-p:0.10045647621154785
epoch£º1129	 i:3 	 global-step:22583	 l-p:0.12139836698770523
epoch£º1129	 i:4 	 global-step:22584	 l-p:0.12959197163581848
epoch£º1129	 i:5 	 global-step:22585	 l-p:0.13493964076042175
epoch£º1129	 i:6 	 global-step:22586	 l-p:0.1546376794576645
epoch£º1129	 i:7 	 global-step:22587	 l-p:0.11523200571537018
epoch£º1129	 i:8 	 global-step:22588	 l-p:0.14842981100082397
epoch£º1129	 i:9 	 global-step:22589	 l-p:0.1722375899553299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6618, 3.0946, 2.4626],
        [3.6618, 3.6618, 3.6618],
        [3.6618, 3.1164, 2.4880],
        [3.6618, 3.1267, 3.1455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.14108653366565704 
model_pd.l_d.mean(): -22.726959228515625 
model_pd.lagr.mean(): -22.585872650146484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3355], device='cuda:0')), ('power', tensor([-23.0624], device='cuda:0'))])
epoch£º1130	 i:0 	 global-step:22600	 l-p:0.14108653366565704
epoch£º1130	 i:1 	 global-step:22601	 l-p:0.09536962956190109
epoch£º1130	 i:2 	 global-step:22602	 l-p:0.12506549060344696
epoch£º1130	 i:3 	 global-step:22603	 l-p:-1.4975959062576294
epoch£º1130	 i:4 	 global-step:22604	 l-p:0.11603785306215286
epoch£º1130	 i:5 	 global-step:22605	 l-p:0.1433158814907074
epoch£º1130	 i:6 	 global-step:22606	 l-p:0.1048722118139267
epoch£º1130	 i:7 	 global-step:22607	 l-p:0.20011748373508453
epoch£º1130	 i:8 	 global-step:22608	 l-p:0.12435062229633331
epoch£º1130	 i:9 	 global-step:22609	 l-p:0.13766521215438843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6692, 3.6692, 3.6692],
        [3.6692, 3.5892, 3.6527],
        [3.6692, 2.9777, 2.3286],
        [3.6692, 3.4887, 3.6005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.14016662538051605 
model_pd.l_d.mean(): -23.127063751220703 
model_pd.lagr.mean(): -22.986896514892578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2326], device='cuda:0')), ('power', tensor([-23.3597], device='cuda:0'))])
epoch£º1131	 i:0 	 global-step:22620	 l-p:0.14016662538051605
epoch£º1131	 i:1 	 global-step:22621	 l-p:0.12180281430482864
epoch£º1131	 i:2 	 global-step:22622	 l-p:0.13001090288162231
epoch£º1131	 i:3 	 global-step:22623	 l-p:0.22139087319374084
epoch£º1131	 i:4 	 global-step:22624	 l-p:0.13115939497947693
epoch£º1131	 i:5 	 global-step:22625	 l-p:0.15077270567417145
epoch£º1131	 i:6 	 global-step:22626	 l-p:0.06252405047416687
epoch£º1131	 i:7 	 global-step:22627	 l-p:-0.6821741461753845
epoch£º1131	 i:8 	 global-step:22628	 l-p:0.15130408108234406
epoch£º1131	 i:9 	 global-step:22629	 l-p:0.12225189059972763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228]], device='cuda:0')
 pt:tensor([[3.6585, 3.1003, 2.4702],
        [3.6585, 3.1113, 3.1197],
        [3.6585, 2.8387, 2.3444],
        [3.6585, 3.0296, 2.9475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.12464719265699387 
model_pd.l_d.mean(): -23.548194885253906 
model_pd.lagr.mean(): -23.423547744750977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2022], device='cuda:0')), ('power', tensor([-23.7503], device='cuda:0'))])
epoch£º1132	 i:0 	 global-step:22640	 l-p:0.12464719265699387
epoch£º1132	 i:1 	 global-step:22641	 l-p:0.1489814668893814
epoch£º1132	 i:2 	 global-step:22642	 l-p:0.143424391746521
epoch£º1132	 i:3 	 global-step:22643	 l-p:0.13394476473331451
epoch£º1132	 i:4 	 global-step:22644	 l-p:0.15282413363456726
epoch£º1132	 i:5 	 global-step:22645	 l-p:0.6411018967628479
epoch£º1132	 i:6 	 global-step:22646	 l-p:0.13302329182624817
epoch£º1132	 i:7 	 global-step:22647	 l-p:0.10139678418636322
epoch£º1132	 i:8 	 global-step:22648	 l-p:0.04086380451917648
epoch£º1132	 i:9 	 global-step:22649	 l-p:0.12433169037103653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6386, 3.6386, 3.6386],
        [3.6386, 2.8081, 2.2407],
        [3.6386, 3.1897, 3.2721],
        [3.6386, 3.5474, 3.6180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): 8.131781578063965 
model_pd.l_d.mean(): -23.21046257019043 
model_pd.lagr.mean(): -15.078680992126465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2686], device='cuda:0')), ('power', tensor([-23.4791], device='cuda:0'))])
epoch£º1133	 i:0 	 global-step:22660	 l-p:8.131781578063965
epoch£º1133	 i:1 	 global-step:22661	 l-p:0.25362831354141235
epoch£º1133	 i:2 	 global-step:22662	 l-p:0.13682208955287933
epoch£º1133	 i:3 	 global-step:22663	 l-p:0.12735268473625183
epoch£º1133	 i:4 	 global-step:22664	 l-p:0.13667306303977966
epoch£º1133	 i:5 	 global-step:22665	 l-p:0.07399123907089233
epoch£º1133	 i:6 	 global-step:22666	 l-p:0.09290521591901779
epoch£º1133	 i:7 	 global-step:22667	 l-p:0.14394508302211761
epoch£º1133	 i:8 	 global-step:22668	 l-p:0.11717380583286285
epoch£º1133	 i:9 	 global-step:22669	 l-p:0.14676420390605927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6482, 3.1114, 3.1301],
        [3.6482, 3.1737, 3.2402],
        [3.6482, 3.4759, 3.5851],
        [3.6482, 3.2917, 3.4108]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.1501896232366562 
model_pd.l_d.mean(): -23.40195655822754 
model_pd.lagr.mean(): -23.251766204833984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2716], device='cuda:0')), ('power', tensor([-23.6736], device='cuda:0'))])
epoch£º1134	 i:0 	 global-step:22680	 l-p:0.1501896232366562
epoch£º1134	 i:1 	 global-step:22681	 l-p:0.12936054170131683
epoch£º1134	 i:2 	 global-step:22682	 l-p:0.1319984793663025
epoch£º1134	 i:3 	 global-step:22683	 l-p:0.08852861821651459
epoch£º1134	 i:4 	 global-step:22684	 l-p:0.1587820053100586
epoch£º1134	 i:5 	 global-step:22685	 l-p:0.1202002689242363
epoch£º1134	 i:6 	 global-step:22686	 l-p:0.0862625315785408
epoch£º1134	 i:7 	 global-step:22687	 l-p:0.027296394109725952
epoch£º1134	 i:8 	 global-step:22688	 l-p:0.8201859593391418
epoch£º1134	 i:9 	 global-step:22689	 l-p:0.21435672044754028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6467, 3.0961, 2.4690],
        [3.6467, 2.8422, 2.4182],
        [3.6467, 3.5510, 3.6242],
        [3.6467, 2.9236, 2.6964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): -0.0019601916428655386 
model_pd.l_d.mean(): -23.369285583496094 
model_pd.lagr.mean(): -23.371246337890625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2933], device='cuda:0')), ('power', tensor([-23.6625], device='cuda:0'))])
epoch£º1135	 i:0 	 global-step:22700	 l-p:-0.0019601916428655386
epoch£º1135	 i:1 	 global-step:22701	 l-p:0.091790571808815
epoch£º1135	 i:2 	 global-step:22702	 l-p:0.1287892609834671
epoch£º1135	 i:3 	 global-step:22703	 l-p:0.11641313135623932
epoch£º1135	 i:4 	 global-step:22704	 l-p:0.13620546460151672
epoch£º1135	 i:5 	 global-step:22705	 l-p:0.1326269656419754
epoch£º1135	 i:6 	 global-step:22706	 l-p:0.1041744202375412
epoch£º1135	 i:7 	 global-step:22707	 l-p:0.24563874304294586
epoch£º1135	 i:8 	 global-step:22708	 l-p:0.16080524027347565
epoch£º1135	 i:9 	 global-step:22709	 l-p:0.13361595571041107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6659, 3.6137, 3.6579],
        [3.6659, 3.6623, 3.6658],
        [3.6659, 3.3268, 3.4494],
        [3.6659, 2.9471, 2.7244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): 0.07135739922523499 
model_pd.l_d.mean(): -23.66813087463379 
model_pd.lagr.mean(): -23.596773147583008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1243], device='cuda:0')), ('power', tensor([-23.7924], device='cuda:0'))])
epoch£º1136	 i:0 	 global-step:22720	 l-p:0.07135739922523499
epoch£º1136	 i:1 	 global-step:22721	 l-p:0.20812717080116272
epoch£º1136	 i:2 	 global-step:22722	 l-p:0.128200963139534
epoch£º1136	 i:3 	 global-step:22723	 l-p:0.12889909744262695
epoch£º1136	 i:4 	 global-step:22724	 l-p:0.5062352418899536
epoch£º1136	 i:5 	 global-step:22725	 l-p:0.18309585750102997
epoch£º1136	 i:6 	 global-step:22726	 l-p:0.14124777913093567
epoch£º1136	 i:7 	 global-step:22727	 l-p:0.13321246206760406
epoch£º1136	 i:8 	 global-step:22728	 l-p:0.06276297569274902
epoch£º1136	 i:9 	 global-step:22729	 l-p:0.15803299844264984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6737, 3.6730, 3.6737],
        [3.6737, 3.6737, 3.6737],
        [3.6737, 3.6645, 3.6733],
        [3.6737, 2.9170, 2.6145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.47922682762145996 
model_pd.l_d.mean(): -23.349061965942383 
model_pd.lagr.mean(): -22.869834899902344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2219], device='cuda:0')), ('power', tensor([-23.5710], device='cuda:0'))])
epoch£º1137	 i:0 	 global-step:22740	 l-p:0.47922682762145996
epoch£º1137	 i:1 	 global-step:22741	 l-p:0.030118979513645172
epoch£º1137	 i:2 	 global-step:22742	 l-p:0.17383787035942078
epoch£º1137	 i:3 	 global-step:22743	 l-p:0.12836512923240662
epoch£º1137	 i:4 	 global-step:22744	 l-p:0.14983519911766052
epoch£º1137	 i:5 	 global-step:22745	 l-p:0.15148510038852692
epoch£º1137	 i:6 	 global-step:22746	 l-p:0.13695068657398224
epoch£º1137	 i:7 	 global-step:22747	 l-p:0.11300431936979294
epoch£º1137	 i:8 	 global-step:22748	 l-p:0.15323305130004883
epoch£º1137	 i:9 	 global-step:22749	 l-p:0.00618026964366436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6698, 3.6621, 3.6695],
        [3.6698, 3.6698, 3.6698],
        [3.6698, 3.5680, 3.6448],
        [3.6698, 2.9049, 2.2561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.6257979869842529 
model_pd.l_d.mean(): -22.87961769104004 
model_pd.lagr.mean(): -22.253820419311523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2543], device='cuda:0')), ('power', tensor([-23.1339], device='cuda:0'))])
epoch£º1138	 i:0 	 global-step:22760	 l-p:0.6257979869842529
epoch£º1138	 i:1 	 global-step:22761	 l-p:0.1425531506538391
epoch£º1138	 i:2 	 global-step:22762	 l-p:0.14925168454647064
epoch£º1138	 i:3 	 global-step:22763	 l-p:0.12503202259540558
epoch£º1138	 i:4 	 global-step:22764	 l-p:0.15477098524570465
epoch£º1138	 i:5 	 global-step:22765	 l-p:0.12906910479068756
epoch£º1138	 i:6 	 global-step:22766	 l-p:0.08669755607843399
epoch£º1138	 i:7 	 global-step:22767	 l-p:0.13913081586360931
epoch£º1138	 i:8 	 global-step:22768	 l-p:0.2803777754306793
epoch£º1138	 i:9 	 global-step:22769	 l-p:0.0759790912270546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6563, 3.6563, 3.6563],
        [3.6563, 3.6314, 3.6540],
        [3.6563, 2.8345, 2.3359],
        [3.6563, 3.6270, 3.6532]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.1291622519493103 
model_pd.l_d.mean(): -23.29611587524414 
model_pd.lagr.mean(): -23.166954040527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2523], device='cuda:0')), ('power', tensor([-23.5484], device='cuda:0'))])
epoch£º1139	 i:0 	 global-step:22780	 l-p:0.1291622519493103
epoch£º1139	 i:1 	 global-step:22781	 l-p:0.11862017214298248
epoch£º1139	 i:2 	 global-step:22782	 l-p:0.14367103576660156
epoch£º1139	 i:3 	 global-step:22783	 l-p:0.10413898527622223
epoch£º1139	 i:4 	 global-step:22784	 l-p:0.14289933443069458
epoch£º1139	 i:5 	 global-step:22785	 l-p:0.13205617666244507
epoch£º1139	 i:6 	 global-step:22786	 l-p:0.17721624672412872
epoch£º1139	 i:7 	 global-step:22787	 l-p:0.2680546045303345
epoch£º1139	 i:8 	 global-step:22788	 l-p:0.08327953517436981
epoch£º1139	 i:9 	 global-step:22789	 l-p:-0.08231272548437119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6582, 3.6582, 3.6583],
        [3.6582, 3.5643, 3.6365],
        [3.6582, 3.6563, 3.6582],
        [3.6582, 3.6582, 3.6583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.15202419459819794 
model_pd.l_d.mean(): -23.745359420776367 
model_pd.lagr.mean(): -23.59333610534668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1246], device='cuda:0')), ('power', tensor([-23.8699], device='cuda:0'))])
epoch£º1140	 i:0 	 global-step:22800	 l-p:0.15202419459819794
epoch£º1140	 i:1 	 global-step:22801	 l-p:0.1346711963415146
epoch£º1140	 i:2 	 global-step:22802	 l-p:0.25378990173339844
epoch£º1140	 i:3 	 global-step:22803	 l-p:0.17744381725788116
epoch£º1140	 i:4 	 global-step:22804	 l-p:0.1264331042766571
epoch£º1140	 i:5 	 global-step:22805	 l-p:0.11744503676891327
epoch£º1140	 i:6 	 global-step:22806	 l-p:0.137169748544693
epoch£º1140	 i:7 	 global-step:22807	 l-p:-0.34218084812164307
epoch£º1140	 i:8 	 global-step:22808	 l-p:0.11349814385175705
epoch£º1140	 i:9 	 global-step:22809	 l-p:0.12880292534828186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6567, 3.4033, 3.5302],
        [3.6567, 3.6554, 3.6567],
        [3.6567, 2.8626, 2.2228],
        [3.6567, 3.1809, 3.2465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): -0.17857757210731506 
model_pd.l_d.mean(): -22.642919540405273 
model_pd.lagr.mean(): -22.821496963500977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3882], device='cuda:0')), ('power', tensor([-23.0312], device='cuda:0'))])
epoch£º1141	 i:0 	 global-step:22820	 l-p:-0.17857757210731506
epoch£º1141	 i:1 	 global-step:22821	 l-p:0.16767436265945435
epoch£º1141	 i:2 	 global-step:22822	 l-p:0.13781748712062836
epoch£º1141	 i:3 	 global-step:22823	 l-p:0.0825204849243164
epoch£º1141	 i:4 	 global-step:22824	 l-p:0.12624436616897583
epoch£º1141	 i:5 	 global-step:22825	 l-p:0.14181934297084808
epoch£º1141	 i:6 	 global-step:22826	 l-p:0.14113733172416687
epoch£º1141	 i:7 	 global-step:22827	 l-p:0.38310709595680237
epoch£º1141	 i:8 	 global-step:22828	 l-p:0.09332248568534851
epoch£º1141	 i:9 	 global-step:22829	 l-p:0.07893771678209305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6587, 3.6579, 3.6587],
        [3.6587, 3.1866, 2.5715],
        [3.6587, 3.6586, 3.6587],
        [3.6587, 2.8889, 2.2418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.127306267619133 
model_pd.l_d.mean(): -23.048301696777344 
model_pd.lagr.mean(): -22.920995712280273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1934], device='cuda:0')), ('power', tensor([-23.2417], device='cuda:0'))])
epoch£º1142	 i:0 	 global-step:22840	 l-p:0.127306267619133
epoch£º1142	 i:1 	 global-step:22841	 l-p:0.09642525762319565
epoch£º1142	 i:2 	 global-step:22842	 l-p:0.18795500695705414
epoch£º1142	 i:3 	 global-step:22843	 l-p:0.13397644460201263
epoch£º1142	 i:4 	 global-step:22844	 l-p:1.6286112070083618
epoch£º1142	 i:5 	 global-step:22845	 l-p:0.11763216555118561
epoch£º1142	 i:6 	 global-step:22846	 l-p:0.13165180385112762
epoch£º1142	 i:7 	 global-step:22847	 l-p:0.21823157370090485
epoch£º1142	 i:8 	 global-step:22848	 l-p:0.047866906970739365
epoch£º1142	 i:9 	 global-step:22849	 l-p:0.1112043559551239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6748, 3.1288, 2.4980],
        [3.6748, 3.6127, 3.6641],
        [3.6748, 3.2780, 3.3840],
        [3.6748, 3.6726, 3.6748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.03909168764948845 
model_pd.l_d.mean(): -23.234729766845703 
model_pd.lagr.mean(): -23.19563865661621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1593], device='cuda:0')), ('power', tensor([-23.3941], device='cuda:0'))])
epoch£º1143	 i:0 	 global-step:22860	 l-p:0.03909168764948845
epoch£º1143	 i:1 	 global-step:22861	 l-p:0.13151434063911438
epoch£º1143	 i:2 	 global-step:22862	 l-p:0.14367979764938354
epoch£º1143	 i:3 	 global-step:22863	 l-p:0.1237092837691307
epoch£º1143	 i:4 	 global-step:22864	 l-p:0.18754903972148895
epoch£º1143	 i:5 	 global-step:22865	 l-p:0.11616816371679306
epoch£º1143	 i:6 	 global-step:22866	 l-p:0.21969760954380035
epoch£º1143	 i:7 	 global-step:22867	 l-p:0.13038279116153717
epoch£º1143	 i:8 	 global-step:22868	 l-p:0.14453594386577606
epoch£º1143	 i:9 	 global-step:22869	 l-p:0.1323782354593277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6840, 3.6840, 3.6840],
        [3.6840, 3.6840, 3.6840],
        [3.6840, 3.6836, 3.6840],
        [3.6840, 2.9584, 2.7206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.15652847290039062 
model_pd.l_d.mean(): -23.438457489013672 
model_pd.lagr.mean(): -23.28192901611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2023], device='cuda:0')), ('power', tensor([-23.6407], device='cuda:0'))])
epoch£º1144	 i:0 	 global-step:22880	 l-p:0.15652847290039062
epoch£º1144	 i:1 	 global-step:22881	 l-p:0.1419752687215805
epoch£º1144	 i:2 	 global-step:22882	 l-p:0.10072358697652817
epoch£º1144	 i:3 	 global-step:22883	 l-p:0.061595745384693146
epoch£º1144	 i:4 	 global-step:22884	 l-p:0.05015432834625244
epoch£º1144	 i:5 	 global-step:22885	 l-p:0.14104536175727844
epoch£º1144	 i:6 	 global-step:22886	 l-p:0.14385515451431274
epoch£º1144	 i:7 	 global-step:22887	 l-p:0.1818709522485733
epoch£º1144	 i:8 	 global-step:22888	 l-p:0.16152487695217133
epoch£º1144	 i:9 	 global-step:22889	 l-p:0.13461442291736603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6910, 3.5212, 3.6294],
        [3.6910, 3.4156, 3.5433],
        [3.6910, 3.6796, 3.6904],
        [3.6910, 3.6712, 3.6894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): 0.019567184150218964 
model_pd.l_d.mean(): -23.148401260375977 
model_pd.lagr.mean(): -23.128833770751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2673], device='cuda:0')), ('power', tensor([-23.4157], device='cuda:0'))])
epoch£º1145	 i:0 	 global-step:22900	 l-p:0.019567184150218964
epoch£º1145	 i:1 	 global-step:22901	 l-p:0.11277629435062408
epoch£º1145	 i:2 	 global-step:22902	 l-p:0.11900317668914795
epoch£º1145	 i:3 	 global-step:22903	 l-p:0.14216651022434235
epoch£º1145	 i:4 	 global-step:22904	 l-p:0.12629292905330658
epoch£º1145	 i:5 	 global-step:22905	 l-p:0.14188753068447113
epoch£º1145	 i:6 	 global-step:22906	 l-p:-0.017258472740650177
epoch£º1145	 i:7 	 global-step:22907	 l-p:0.2657616138458252
epoch£º1145	 i:8 	 global-step:22908	 l-p:0.12809373438358307
epoch£º1145	 i:9 	 global-step:22909	 l-p:0.13574348390102386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6837, 3.6837, 3.6837],
        [3.6837, 3.2368, 2.6234],
        [3.6837, 2.9681, 2.7498],
        [3.6837, 2.9844, 2.7952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.18932141363620758 
model_pd.l_d.mean(): -23.158796310424805 
model_pd.lagr.mean(): -22.96947479248047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3378], device='cuda:0')), ('power', tensor([-23.4966], device='cuda:0'))])
epoch£º1146	 i:0 	 global-step:22920	 l-p:0.18932141363620758
epoch£º1146	 i:1 	 global-step:22921	 l-p:0.12778693437576294
epoch£º1146	 i:2 	 global-step:22922	 l-p:0.05671338737010956
epoch£º1146	 i:3 	 global-step:22923	 l-p:0.29086288809776306
epoch£º1146	 i:4 	 global-step:22924	 l-p:0.13713566958904266
epoch£º1146	 i:5 	 global-step:22925	 l-p:0.028802823275327682
epoch£º1146	 i:6 	 global-step:22926	 l-p:0.1251886636018753
epoch£º1146	 i:7 	 global-step:22927	 l-p:0.1395033895969391
epoch£º1146	 i:8 	 global-step:22928	 l-p:0.042249955236911774
epoch£º1146	 i:9 	 global-step:22929	 l-p:0.20413734018802643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6851, 3.0405, 2.3934],
        [3.6851, 3.6842, 3.6851],
        [3.6851, 3.6832, 3.6851],
        [3.6851, 2.8660, 2.2555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.30895498394966125 
model_pd.l_d.mean(): -23.339950561523438 
model_pd.lagr.mean(): -23.030996322631836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2591], device='cuda:0')), ('power', tensor([-23.5990], device='cuda:0'))])
epoch£º1147	 i:0 	 global-step:22940	 l-p:0.30895498394966125
epoch£º1147	 i:1 	 global-step:22941	 l-p:0.06402509659528732
epoch£º1147	 i:2 	 global-step:22942	 l-p:0.1753365844488144
epoch£º1147	 i:3 	 global-step:22943	 l-p:0.10890327394008636
epoch£º1147	 i:4 	 global-step:22944	 l-p:0.1233554258942604
epoch£º1147	 i:5 	 global-step:22945	 l-p:0.14022284746170044
epoch£º1147	 i:6 	 global-step:22946	 l-p:0.12765076756477356
epoch£º1147	 i:7 	 global-step:22947	 l-p:0.12587343156337738
epoch£º1147	 i:8 	 global-step:22948	 l-p:-0.24528715014457703
epoch£º1147	 i:9 	 global-step:22949	 l-p:0.13260746002197266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6952, 3.1451, 2.5106],
        [3.6952, 3.5414, 3.6435],
        [3.6952, 3.4722, 3.5947],
        [3.6952, 3.6873, 3.6948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.11508480459451675 
model_pd.l_d.mean(): -23.01258087158203 
model_pd.lagr.mean(): -22.89749526977539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2593], device='cuda:0')), ('power', tensor([-23.2719], device='cuda:0'))])
epoch£º1148	 i:0 	 global-step:22960	 l-p:0.11508480459451675
epoch£º1148	 i:1 	 global-step:22961	 l-p:0.12253707647323608
epoch£º1148	 i:2 	 global-step:22962	 l-p:0.14369578659534454
epoch£º1148	 i:3 	 global-step:22963	 l-p:0.002738532843068242
epoch£º1148	 i:4 	 global-step:22964	 l-p:0.13883116841316223
epoch£º1148	 i:5 	 global-step:22965	 l-p:0.09965144842863083
epoch£º1148	 i:6 	 global-step:22966	 l-p:0.12335910648107529
epoch£º1148	 i:7 	 global-step:22967	 l-p:0.11525216698646545
epoch£º1148	 i:8 	 global-step:22968	 l-p:0.20518745481967926
epoch£º1148	 i:9 	 global-step:22969	 l-p:0.37017276883125305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6740, 2.8650, 2.2349],
        [3.6740, 2.8516, 2.3413],
        [3.6740, 3.4139, 3.5413],
        [3.6740, 3.4494, 3.5724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.1550009548664093 
model_pd.l_d.mean(): -23.144710540771484 
model_pd.lagr.mean(): -22.989709854125977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3046], device='cuda:0')), ('power', tensor([-23.4493], device='cuda:0'))])
epoch£º1149	 i:0 	 global-step:22980	 l-p:0.1550009548664093
epoch£º1149	 i:1 	 global-step:22981	 l-p:0.5263212323188782
epoch£º1149	 i:2 	 global-step:22982	 l-p:0.15258467197418213
epoch£º1149	 i:3 	 global-step:22983	 l-p:0.132834330201149
epoch£º1149	 i:4 	 global-step:22984	 l-p:0.03625001385807991
epoch£º1149	 i:5 	 global-step:22985	 l-p:0.12262965738773346
epoch£º1149	 i:6 	 global-step:22986	 l-p:0.13455195724964142
epoch£º1149	 i:7 	 global-step:22987	 l-p:0.1534867286682129
epoch£º1149	 i:8 	 global-step:22988	 l-p:0.10528123378753662
epoch£º1149	 i:9 	 global-step:22989	 l-p:0.2063870131969452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6583, 3.6583, 3.6583],
        [3.6583, 3.6577, 3.6583],
        [3.6583, 3.6579, 3.6583],
        [3.6583, 3.6574, 3.6583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.11463289707899094 
model_pd.l_d.mean(): -23.45125961303711 
model_pd.lagr.mean(): -23.336626052856445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1463], device='cuda:0')), ('power', tensor([-23.5975], device='cuda:0'))])
epoch£º1150	 i:0 	 global-step:23000	 l-p:0.11463289707899094
epoch£º1150	 i:1 	 global-step:23001	 l-p:0.08377160876989365
epoch£º1150	 i:2 	 global-step:23002	 l-p:0.1714395433664322
epoch£º1150	 i:3 	 global-step:23003	 l-p:0.13670535385608673
epoch£º1150	 i:4 	 global-step:23004	 l-p:0.16200801730155945
epoch£º1150	 i:5 	 global-step:23005	 l-p:0.09828568994998932
epoch£º1150	 i:6 	 global-step:23006	 l-p:-0.0903831496834755
epoch£º1150	 i:7 	 global-step:23007	 l-p:0.1381383240222931
epoch£º1150	 i:8 	 global-step:23008	 l-p:0.11024829745292664
epoch£º1150	 i:9 	 global-step:23009	 l-p:0.29763177037239075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6582, 3.6582, 3.6582],
        [3.6582, 3.6581, 3.6582],
        [3.6582, 3.4885, 3.5968],
        [3.6582, 3.0680, 3.0342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.1374654620885849 
model_pd.l_d.mean(): -23.59942054748535 
model_pd.lagr.mean(): -23.461956024169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1782], device='cuda:0')), ('power', tensor([-23.7776], device='cuda:0'))])
epoch£º1151	 i:0 	 global-step:23020	 l-p:0.1374654620885849
epoch£º1151	 i:1 	 global-step:23021	 l-p:0.19006943702697754
epoch£º1151	 i:2 	 global-step:23022	 l-p:0.08162595331668854
epoch£º1151	 i:3 	 global-step:23023	 l-p:0.0791577622294426
epoch£º1151	 i:4 	 global-step:23024	 l-p:0.12189193814992905
epoch£º1151	 i:5 	 global-step:23025	 l-p:0.13525168597698212
epoch£º1151	 i:6 	 global-step:23026	 l-p:0.21818985044956207
epoch£º1151	 i:7 	 global-step:23027	 l-p:0.08506486564874649
epoch£º1151	 i:8 	 global-step:23028	 l-p:0.12590064108371735
epoch£º1151	 i:9 	 global-step:23029	 l-p:0.4978801906108856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6754, 3.6741, 3.6754],
        [3.6754, 2.9413, 2.6898],
        [3.6754, 3.6754, 3.6754],
        [3.6754, 3.2891, 3.3991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.13388843834400177 
model_pd.l_d.mean(): -23.699148178100586 
model_pd.lagr.mean(): -23.56525993347168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1046], device='cuda:0')), ('power', tensor([-23.8037], device='cuda:0'))])
epoch£º1152	 i:0 	 global-step:23040	 l-p:0.13388843834400177
epoch£º1152	 i:1 	 global-step:23041	 l-p:0.36144712567329407
epoch£º1152	 i:2 	 global-step:23042	 l-p:-0.0231680516153574
epoch£º1152	 i:3 	 global-step:23043	 l-p:0.07272372394800186
epoch£º1152	 i:4 	 global-step:23044	 l-p:0.13159126043319702
epoch£º1152	 i:5 	 global-step:23045	 l-p:0.21102504432201385
epoch£º1152	 i:6 	 global-step:23046	 l-p:0.11357682943344116
epoch£º1152	 i:7 	 global-step:23047	 l-p:0.1381227970123291
epoch£º1152	 i:8 	 global-step:23048	 l-p:0.14370854198932648
epoch£º1152	 i:9 	 global-step:23049	 l-p:0.15605008602142334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6877, 3.6857, 3.6876],
        [3.6877, 3.5896, 3.6642],
        [3.6877, 3.4503, 3.5752],
        [3.6877, 3.6619, 3.6852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.11124904453754425 
model_pd.l_d.mean(): -23.466354370117188 
model_pd.lagr.mean(): -23.355104446411133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2197], device='cuda:0')), ('power', tensor([-23.6860], device='cuda:0'))])
epoch£º1153	 i:0 	 global-step:23060	 l-p:0.11124904453754425
epoch£º1153	 i:1 	 global-step:23061	 l-p:0.14008763432502747
epoch£º1153	 i:2 	 global-step:23062	 l-p:0.10845080763101578
epoch£º1153	 i:3 	 global-step:23063	 l-p:0.02024666778743267
epoch£º1153	 i:4 	 global-step:23064	 l-p:0.11971070617437363
epoch£º1153	 i:5 	 global-step:23065	 l-p:0.15509124100208282
epoch£º1153	 i:6 	 global-step:23066	 l-p:0.12256698310375214
epoch£º1153	 i:7 	 global-step:23067	 l-p:0.1954444795846939
epoch£º1153	 i:8 	 global-step:23068	 l-p:0.13777799904346466
epoch£º1153	 i:9 	 global-step:23069	 l-p:0.1479557752609253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6773, 3.6041, 3.6632],
        [3.6773, 3.4294, 3.5557],
        [3.6773, 3.6773, 3.6773],
        [3.6773, 3.6152, 3.6666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.13414637744426727 
model_pd.l_d.mean(): -23.603065490722656 
model_pd.lagr.mean(): -23.46891975402832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1953], device='cuda:0')), ('power', tensor([-23.7983], device='cuda:0'))])
epoch£º1154	 i:0 	 global-step:23080	 l-p:0.13414637744426727
epoch£º1154	 i:1 	 global-step:23081	 l-p:0.41662055253982544
epoch£º1154	 i:2 	 global-step:23082	 l-p:0.121884286403656
epoch£º1154	 i:3 	 global-step:23083	 l-p:0.22576892375946045
epoch£º1154	 i:4 	 global-step:23084	 l-p:0.08271146565675735
epoch£º1154	 i:5 	 global-step:23085	 l-p:0.08786117285490036
epoch£º1154	 i:6 	 global-step:23086	 l-p:0.08053289353847504
epoch£º1154	 i:7 	 global-step:23087	 l-p:0.13696278631687164
epoch£º1154	 i:8 	 global-step:23088	 l-p:0.11463672667741776
epoch£º1154	 i:9 	 global-step:23089	 l-p:0.13097111880779266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6758, 3.6671, 3.6753],
        [3.6758, 3.6758, 3.6758],
        [3.6758, 3.6758, 3.6758],
        [3.6758, 3.1587, 2.5327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.3774411678314209 
model_pd.l_d.mean(): -22.77366065979004 
model_pd.lagr.mean(): -22.39621925354004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2973], device='cuda:0')), ('power', tensor([-23.0709], device='cuda:0'))])
epoch£º1155	 i:0 	 global-step:23100	 l-p:0.3774411678314209
epoch£º1155	 i:1 	 global-step:23101	 l-p:0.1249113455414772
epoch£º1155	 i:2 	 global-step:23102	 l-p:0.11475339531898499
epoch£º1155	 i:3 	 global-step:23103	 l-p:0.16379663348197937
epoch£º1155	 i:4 	 global-step:23104	 l-p:0.13307897746562958
epoch£º1155	 i:5 	 global-step:23105	 l-p:0.139373317360878
epoch£º1155	 i:6 	 global-step:23106	 l-p:0.232512429356575
epoch£º1155	 i:7 	 global-step:23107	 l-p:0.07245157659053802
epoch£º1155	 i:8 	 global-step:23108	 l-p:0.059412308037281036
epoch£º1155	 i:9 	 global-step:23109	 l-p:0.12812183797359467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6699, 3.5862, 3.6521],
        [3.6699, 3.6699, 3.6699],
        [3.6699, 3.5965, 3.6557],
        [3.6699, 2.8582, 2.4051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.14461517333984375 
model_pd.l_d.mean(): -23.80430030822754 
model_pd.lagr.mean(): -23.659685134887695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1259], device='cuda:0')), ('power', tensor([-23.9302], device='cuda:0'))])
epoch£º1156	 i:0 	 global-step:23120	 l-p:0.14461517333984375
epoch£º1156	 i:1 	 global-step:23121	 l-p:0.1280868500471115
epoch£º1156	 i:2 	 global-step:23122	 l-p:0.09989925473928452
epoch£º1156	 i:3 	 global-step:23123	 l-p:0.1538511961698532
epoch£º1156	 i:4 	 global-step:23124	 l-p:0.12764176726341248
epoch£º1156	 i:5 	 global-step:23125	 l-p:0.14038437604904175
epoch£º1156	 i:6 	 global-step:23126	 l-p:0.343829482793808
epoch£º1156	 i:7 	 global-step:23127	 l-p:0.08214103430509567
epoch£º1156	 i:8 	 global-step:23128	 l-p:0.045825228095054626
epoch£º1156	 i:9 	 global-step:23129	 l-p:0.07563682645559311
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6498, 3.6497, 3.6498],
        [3.6498, 3.0910, 3.0911],
        [3.6498, 3.6444, 3.6496],
        [3.6498, 3.4573, 3.5731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.21111953258514404 
model_pd.l_d.mean(): -23.34214973449707 
model_pd.lagr.mean(): -23.131031036376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2698], device='cuda:0')), ('power', tensor([-23.6120], device='cuda:0'))])
epoch£º1157	 i:0 	 global-step:23140	 l-p:0.21111953258514404
epoch£º1157	 i:1 	 global-step:23141	 l-p:0.15302211046218872
epoch£º1157	 i:2 	 global-step:23142	 l-p:0.0734039843082428
epoch£º1157	 i:3 	 global-step:23143	 l-p:0.15573759377002716
epoch£º1157	 i:4 	 global-step:23144	 l-p:0.11088445782661438
epoch£º1157	 i:5 	 global-step:23145	 l-p:0.1286633014678955
epoch£º1157	 i:6 	 global-step:23146	 l-p:0.15156039595603943
epoch£º1157	 i:7 	 global-step:23147	 l-p:0.3001297414302826
epoch£º1157	 i:8 	 global-step:23148	 l-p:0.12563775479793549
epoch£º1157	 i:9 	 global-step:23149	 l-p:-0.12254906445741653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6562, 3.4322, 3.5553],
        [3.6562, 3.6561, 3.6562],
        [3.6562, 3.6562, 3.6562],
        [3.6562, 3.0565, 3.0128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.1381465643644333 
model_pd.l_d.mean(): -23.207971572875977 
model_pd.lagr.mean(): -23.06982421875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2555], device='cuda:0')), ('power', tensor([-23.4634], device='cuda:0'))])
epoch£º1158	 i:0 	 global-step:23160	 l-p:0.1381465643644333
epoch£º1158	 i:1 	 global-step:23161	 l-p:0.12410661578178406
epoch£º1158	 i:2 	 global-step:23162	 l-p:0.13031624257564545
epoch£º1158	 i:3 	 global-step:23163	 l-p:0.3977661430835724
epoch£º1158	 i:4 	 global-step:23164	 l-p:0.11862208694219589
epoch£º1158	 i:5 	 global-step:23165	 l-p:0.10038033872842789
epoch£º1158	 i:6 	 global-step:23166	 l-p:0.10219824314117432
epoch£º1158	 i:7 	 global-step:23167	 l-p:0.1499359905719757
epoch£º1158	 i:8 	 global-step:23168	 l-p:0.0838945209980011
epoch£º1158	 i:9 	 global-step:23169	 l-p:-0.03251632675528526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6541, 2.8844, 2.5615],
        [3.6541, 3.6540, 3.6541],
        [3.6541, 3.6535, 3.6541],
        [3.6541, 3.5479, 3.6273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.14506769180297852 
model_pd.l_d.mean(): -23.533748626708984 
model_pd.lagr.mean(): -23.388681411743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2068], device='cuda:0')), ('power', tensor([-23.7405], device='cuda:0'))])
epoch£º1159	 i:0 	 global-step:23180	 l-p:0.14506769180297852
epoch£º1159	 i:1 	 global-step:23181	 l-p:0.13802562654018402
epoch£º1159	 i:2 	 global-step:23182	 l-p:0.2078169584274292
epoch£º1159	 i:3 	 global-step:23183	 l-p:-0.10344316065311432
epoch£º1159	 i:4 	 global-step:23184	 l-p:0.09058160334825516
epoch£º1159	 i:5 	 global-step:23185	 l-p:0.10843586921691895
epoch£º1159	 i:6 	 global-step:23186	 l-p:0.2821904420852661
epoch£º1159	 i:7 	 global-step:23187	 l-p:0.14264021813869476
epoch£º1159	 i:8 	 global-step:23188	 l-p:0.11834786832332611
epoch£º1159	 i:9 	 global-step:23189	 l-p:0.10278985649347305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6611, 2.8456, 2.3836],
        [3.6611, 3.6511, 3.6605],
        [3.6611, 2.8310, 2.2805],
        [3.6611, 3.6609, 3.6611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.12988834083080292 
model_pd.l_d.mean(): -23.25709342956543 
model_pd.lagr.mean(): -23.12720489501953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2074], device='cuda:0')), ('power', tensor([-23.4645], device='cuda:0'))])
epoch£º1160	 i:0 	 global-step:23200	 l-p:0.12988834083080292
epoch£º1160	 i:1 	 global-step:23201	 l-p:0.2136547863483429
epoch£º1160	 i:2 	 global-step:23202	 l-p:0.14614856243133545
epoch£º1160	 i:3 	 global-step:23203	 l-p:0.06940410286188126
epoch£º1160	 i:4 	 global-step:23204	 l-p:0.14227335155010223
epoch£º1160	 i:5 	 global-step:23205	 l-p:0.198497012257576
epoch£º1160	 i:6 	 global-step:23206	 l-p:0.12337499111890793
epoch£º1160	 i:7 	 global-step:23207	 l-p:0.07555165141820908
epoch£º1160	 i:8 	 global-step:23208	 l-p:0.1596272885799408
epoch£º1160	 i:9 	 global-step:23209	 l-p:-0.34902098774909973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6622, 2.8362, 2.2314],
        [3.6622, 3.6621, 3.6622],
        [3.6622, 3.4691, 3.5850],
        [3.6622, 3.2040, 3.2814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): -0.3006220757961273 
model_pd.l_d.mean(): -22.395130157470703 
model_pd.lagr.mean(): -22.69575309753418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4317], device='cuda:0')), ('power', tensor([-22.8268], device='cuda:0'))])
epoch£º1161	 i:0 	 global-step:23220	 l-p:-0.3006220757961273
epoch£º1161	 i:1 	 global-step:23221	 l-p:0.13073444366455078
epoch£º1161	 i:2 	 global-step:23222	 l-p:0.18694359064102173
epoch£º1161	 i:3 	 global-step:23223	 l-p:0.08061262220144272
epoch£º1161	 i:4 	 global-step:23224	 l-p:0.06568697094917297
epoch£º1161	 i:5 	 global-step:23225	 l-p:0.17801395058631897
epoch£º1161	 i:6 	 global-step:23226	 l-p:0.15023566782474518
epoch£º1161	 i:7 	 global-step:23227	 l-p:0.13613711297512054
epoch£º1161	 i:8 	 global-step:23228	 l-p:0.06309825927019119
epoch£º1161	 i:9 	 global-step:23229	 l-p:0.12548233568668365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228]], device='cuda:0')
 pt:tensor([[3.6711, 2.9034, 2.2536],
        [3.6711, 3.3983, 3.5265],
        [3.6711, 3.1220, 3.1298],
        [3.6711, 2.8509, 2.3627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.15777291357517242 
model_pd.l_d.mean(): -23.367250442504883 
model_pd.lagr.mean(): -23.2094783782959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3090], device='cuda:0')), ('power', tensor([-23.6762], device='cuda:0'))])
epoch£º1162	 i:0 	 global-step:23240	 l-p:0.15777291357517242
epoch£º1162	 i:1 	 global-step:23241	 l-p:0.2039055973291397
epoch£º1162	 i:2 	 global-step:23242	 l-p:0.12212981283664703
epoch£º1162	 i:3 	 global-step:23243	 l-p:0.07803665101528168
epoch£º1162	 i:4 	 global-step:23244	 l-p:0.12227430939674377
epoch£º1162	 i:5 	 global-step:23245	 l-p:0.11978933960199356
epoch£º1162	 i:6 	 global-step:23246	 l-p:0.1679449826478958
epoch£º1162	 i:7 	 global-step:23247	 l-p:0.08403169363737106
epoch£º1162	 i:8 	 global-step:23248	 l-p:-9.531525611877441
epoch£º1162	 i:9 	 global-step:23249	 l-p:0.13081686198711395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6676, 2.8581, 2.2259],
        [3.6676, 3.6676, 3.6676],
        [3.6676, 3.6676, 3.6676],
        [3.6676, 3.1595, 2.5362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.18373213708400726 
model_pd.l_d.mean(): -23.100095748901367 
model_pd.lagr.mean(): -22.916362762451172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2472], device='cuda:0')), ('power', tensor([-23.3473], device='cuda:0'))])
epoch£º1163	 i:0 	 global-step:23260	 l-p:0.18373213708400726
epoch£º1163	 i:1 	 global-step:23261	 l-p:0.06713078916072845
epoch£º1163	 i:2 	 global-step:23262	 l-p:0.1345726102590561
epoch£º1163	 i:3 	 global-step:23263	 l-p:0.11155843734741211
epoch£º1163	 i:4 	 global-step:23264	 l-p:0.14801466464996338
epoch£º1163	 i:5 	 global-step:23265	 l-p:0.14264927804470062
epoch£º1163	 i:6 	 global-step:23266	 l-p:0.5078268051147461
epoch£º1163	 i:7 	 global-step:23267	 l-p:0.10176286101341248
epoch£º1163	 i:8 	 global-step:23268	 l-p:0.14861483871936798
epoch£º1163	 i:9 	 global-step:23269	 l-p:0.14378948509693146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1270,  0.0638,  1.0000,  0.0321,
          1.0000,  0.5026, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1497,  0.0795,  1.0000,  0.0422,
          1.0000,  0.5310, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6301, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]], device='cuda:0')
 pt:tensor([[3.6743, 3.4142, 3.5417],
        [3.6743, 3.3487, 3.4739],
        [3.6743, 3.0929, 3.0681],
        [3.6743, 3.2039, 2.5868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.13040900230407715 
model_pd.l_d.mean(): -23.210542678833008 
model_pd.lagr.mean(): -23.08013343811035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2505], device='cuda:0')), ('power', tensor([-23.4610], device='cuda:0'))])
epoch£º1164	 i:0 	 global-step:23280	 l-p:0.13040900230407715
epoch£º1164	 i:1 	 global-step:23281	 l-p:0.06343360245227814
epoch£º1164	 i:2 	 global-step:23282	 l-p:0.11481126397848129
epoch£º1164	 i:3 	 global-step:23283	 l-p:0.2328634411096573
epoch£º1164	 i:4 	 global-step:23284	 l-p:0.13073913753032684
epoch£º1164	 i:5 	 global-step:23285	 l-p:0.14879342913627625
epoch£º1164	 i:6 	 global-step:23286	 l-p:0.13221551477909088
epoch£º1164	 i:7 	 global-step:23287	 l-p:0.5930740833282471
epoch£º1164	 i:8 	 global-step:23288	 l-p:0.14216510951519012
epoch£º1164	 i:9 	 global-step:23289	 l-p:0.0638316199183464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6705, 2.8436, 2.3159],
        [3.6705, 2.8683, 2.2303],
        [3.6705, 3.2127, 3.2899],
        [3.6705, 3.6706, 3.6705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.11681091785430908 
model_pd.l_d.mean(): -23.17226219177246 
model_pd.lagr.mean(): -23.055450439453125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2892], device='cuda:0')), ('power', tensor([-23.4615], device='cuda:0'))])
epoch£º1165	 i:0 	 global-step:23300	 l-p:0.11681091785430908
epoch£º1165	 i:1 	 global-step:23301	 l-p:1.2157516479492188
epoch£º1165	 i:2 	 global-step:23302	 l-p:0.13979269564151764
epoch£º1165	 i:3 	 global-step:23303	 l-p:0.16247853636741638
epoch£º1165	 i:4 	 global-step:23304	 l-p:0.04974477365612984
epoch£º1165	 i:5 	 global-step:23305	 l-p:0.06964772194623947
epoch£º1165	 i:6 	 global-step:23306	 l-p:0.19674643874168396
epoch£º1165	 i:7 	 global-step:23307	 l-p:0.1733701527118683
epoch£º1165	 i:8 	 global-step:23308	 l-p:0.1520024836063385
epoch£º1165	 i:9 	 global-step:23309	 l-p:0.04517088830471039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6757, 3.6757, 3.6757],
        [3.6757, 3.3992, 3.5275],
        [3.6757, 3.6496, 3.6732],
        [3.6757, 3.4156, 3.5431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.19077584147453308 
model_pd.l_d.mean(): -23.489110946655273 
model_pd.lagr.mean(): -23.298336029052734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2008], device='cuda:0')), ('power', tensor([-23.6899], device='cuda:0'))])
epoch£º1166	 i:0 	 global-step:23320	 l-p:0.19077584147453308
epoch£º1166	 i:1 	 global-step:23321	 l-p:0.13923397660255432
epoch£º1166	 i:2 	 global-step:23322	 l-p:0.13349154591560364
epoch£º1166	 i:3 	 global-step:23323	 l-p:0.04018709063529968
epoch£º1166	 i:4 	 global-step:23324	 l-p:0.07818243652582169
epoch£º1166	 i:5 	 global-step:23325	 l-p:0.05126534029841423
epoch£º1166	 i:6 	 global-step:23326	 l-p:0.19138272106647491
epoch£º1166	 i:7 	 global-step:23327	 l-p:0.26942116022109985
epoch£º1166	 i:8 	 global-step:23328	 l-p:0.13849696516990662
epoch£º1166	 i:9 	 global-step:23329	 l-p:0.12781190872192383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6915, 3.4909, 3.6086],
        [3.6915, 3.6509, 3.6862],
        [3.6915, 3.1110, 3.0860],
        [3.6915, 3.2305, 3.3050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.12563060224056244 
model_pd.l_d.mean(): -23.509475708007812 
model_pd.lagr.mean(): -23.38384437561035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2766], device='cuda:0')), ('power', tensor([-23.7861], device='cuda:0'))])
epoch£º1167	 i:0 	 global-step:23340	 l-p:0.12563060224056244
epoch£º1167	 i:1 	 global-step:23341	 l-p:0.258438378572464
epoch£º1167	 i:2 	 global-step:23342	 l-p:0.1347924768924713
epoch£º1167	 i:3 	 global-step:23343	 l-p:0.14729055762290955
epoch£º1167	 i:4 	 global-step:23344	 l-p:0.06985612958669662
epoch£º1167	 i:5 	 global-step:23345	 l-p:0.12441104650497437
epoch£º1167	 i:6 	 global-step:23346	 l-p:0.12681613862514496
epoch£º1167	 i:7 	 global-step:23347	 l-p:0.12596215307712555
epoch£º1167	 i:8 	 global-step:23348	 l-p:0.014688510447740555
epoch£º1167	 i:9 	 global-step:23349	 l-p:0.07718389481306076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6862, 3.6780, 3.6858],
        [3.6862, 2.9683, 2.3142],
        [3.6862, 3.6754, 3.6856],
        [3.6862, 3.2684, 2.6604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.13632653653621674 
model_pd.l_d.mean(): -21.65374755859375 
model_pd.lagr.mean(): -21.51742172241211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4417], device='cuda:0')), ('power', tensor([-22.0955], device='cuda:0'))])
epoch£º1168	 i:0 	 global-step:23360	 l-p:0.13632653653621674
epoch£º1168	 i:1 	 global-step:23361	 l-p:0.11444167047739029
epoch£º1168	 i:2 	 global-step:23362	 l-p:0.1306600570678711
epoch£º1168	 i:3 	 global-step:23363	 l-p:0.17062588036060333
epoch£º1168	 i:4 	 global-step:23364	 l-p:0.05308236926794052
epoch£º1168	 i:5 	 global-step:23365	 l-p:0.1330435425043106
epoch£º1168	 i:6 	 global-step:23366	 l-p:0.006897635292261839
epoch£º1168	 i:7 	 global-step:23367	 l-p:0.1866171956062317
epoch£º1168	 i:8 	 global-step:23368	 l-p:0.12816806137561798
epoch£º1168	 i:9 	 global-step:23369	 l-p:0.30439749360084534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6814, 2.8730, 2.2397],
        [3.6814, 3.6773, 3.6813],
        [3.6814, 2.9131, 2.2620],
        [3.6814, 3.4807, 3.5986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.13357888162136078 
model_pd.l_d.mean(): -23.562686920166016 
model_pd.lagr.mean(): -23.429107666015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1598], device='cuda:0')), ('power', tensor([-23.7225], device='cuda:0'))])
epoch£º1169	 i:0 	 global-step:23380	 l-p:0.13357888162136078
epoch£º1169	 i:1 	 global-step:23381	 l-p:0.20420101284980774
epoch£º1169	 i:2 	 global-step:23382	 l-p:0.05190938711166382
epoch£º1169	 i:3 	 global-step:23383	 l-p:0.05215519294142723
epoch£º1169	 i:4 	 global-step:23384	 l-p:0.13263310492038727
epoch£º1169	 i:5 	 global-step:23385	 l-p:0.043240003287792206
epoch£º1169	 i:6 	 global-step:23386	 l-p:0.2984233796596527
epoch£º1169	 i:7 	 global-step:23387	 l-p:0.13849346339702606
epoch£º1169	 i:8 	 global-step:23388	 l-p:0.13119292259216309
epoch£º1169	 i:9 	 global-step:23389	 l-p:0.11758928000926971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6906, 3.6906, 3.6906],
        [3.6906, 3.0860, 3.0341],
        [3.6906, 3.1860, 3.2307],
        [3.6906, 2.8660, 2.3378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): -0.0032438850030303 
model_pd.l_d.mean(): -23.23322105407715 
model_pd.lagr.mean(): -23.236465454101562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2710], device='cuda:0')), ('power', tensor([-23.5043], device='cuda:0'))])
epoch£º1170	 i:0 	 global-step:23400	 l-p:-0.0032438850030303
epoch£º1170	 i:1 	 global-step:23401	 l-p:0.10155287384986877
epoch£º1170	 i:2 	 global-step:23402	 l-p:0.1205323189496994
epoch£º1170	 i:3 	 global-step:23403	 l-p:0.29836127161979675
epoch£º1170	 i:4 	 global-step:23404	 l-p:0.1212783232331276
epoch£º1170	 i:5 	 global-step:23405	 l-p:0.14462293684482574
epoch£º1170	 i:6 	 global-step:23406	 l-p:0.058864668011665344
epoch£º1170	 i:7 	 global-step:23407	 l-p:0.1481734812259674
epoch£º1170	 i:8 	 global-step:23408	 l-p:0.14104828238487244
epoch£º1170	 i:9 	 global-step:23409	 l-p:0.11071329563856125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6796, 3.6784, 3.6796],
        [3.6796, 2.8834, 2.2412],
        [3.6796, 2.8784, 2.2391],
        [3.6796, 3.4356, 3.5617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.13153058290481567 
model_pd.l_d.mean(): -23.442447662353516 
model_pd.lagr.mean(): -23.310916900634766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2011], device='cuda:0')), ('power', tensor([-23.6435], device='cuda:0'))])
epoch£º1171	 i:0 	 global-step:23420	 l-p:0.13153058290481567
epoch£º1171	 i:1 	 global-step:23421	 l-p:0.07058978080749512
epoch£º1171	 i:2 	 global-step:23422	 l-p:0.10800985246896744
epoch£º1171	 i:3 	 global-step:23423	 l-p:0.12265866994857788
epoch£º1171	 i:4 	 global-step:23424	 l-p:0.15028752386569977
epoch£º1171	 i:5 	 global-step:23425	 l-p:0.15359173715114594
epoch£º1171	 i:6 	 global-step:23426	 l-p:0.14335492253303528
epoch£º1171	 i:7 	 global-step:23427	 l-p:0.18707424402236938
epoch£º1171	 i:8 	 global-step:23428	 l-p:-0.6015644669532776
epoch£º1171	 i:9 	 global-step:23429	 l-p:0.10028544813394547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6644, 3.6590, 3.6642],
        [3.6644, 3.6588, 3.6642],
        [3.6644, 3.6572, 3.6641],
        [3.6644, 3.3874, 3.5159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.08094161748886108 
model_pd.l_d.mean(): -23.668283462524414 
model_pd.lagr.mean(): -23.58734130859375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1590], device='cuda:0')), ('power', tensor([-23.8273], device='cuda:0'))])
epoch£º1172	 i:0 	 global-step:23440	 l-p:0.08094161748886108
epoch£º1172	 i:1 	 global-step:23441	 l-p:0.14540459215641022
epoch£º1172	 i:2 	 global-step:23442	 l-p:0.1216508001089096
epoch£º1172	 i:3 	 global-step:23443	 l-p:-0.9506223797798157
epoch£º1172	 i:4 	 global-step:23444	 l-p:0.1410137563943863
epoch£º1172	 i:5 	 global-step:23445	 l-p:0.13131116330623627
epoch£º1172	 i:6 	 global-step:23446	 l-p:0.12525619566440582
epoch£º1172	 i:7 	 global-step:23447	 l-p:0.1541161835193634
epoch£º1172	 i:8 	 global-step:23448	 l-p:0.09377109259366989
epoch£º1172	 i:9 	 global-step:23449	 l-p:0.14319665729999542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6621, 2.9119, 2.6333],
        [3.6621, 3.6621, 3.6621],
        [3.6621, 3.0304, 2.9484],
        [3.6621, 3.6621, 3.6621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.20181076228618622 
model_pd.l_d.mean(): -23.26898765563965 
model_pd.lagr.mean(): -23.067176818847656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2253], device='cuda:0')), ('power', tensor([-23.4943], device='cuda:0'))])
epoch£º1173	 i:0 	 global-step:23460	 l-p:0.20181076228618622
epoch£º1173	 i:1 	 global-step:23461	 l-p:-0.456418514251709
epoch£º1173	 i:2 	 global-step:23462	 l-p:0.10462339222431183
epoch£º1173	 i:3 	 global-step:23463	 l-p:0.12498295307159424
epoch£º1173	 i:4 	 global-step:23464	 l-p:0.16897474229335785
epoch£º1173	 i:5 	 global-step:23465	 l-p:0.10637832432985306
epoch£º1173	 i:6 	 global-step:23466	 l-p:0.12532910704612732
epoch£º1173	 i:7 	 global-step:23467	 l-p:0.1254786252975464
epoch£º1173	 i:8 	 global-step:23468	 l-p:0.14227907359600067
epoch£º1173	 i:9 	 global-step:23469	 l-p:0.14935855567455292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6676, 3.3303, 3.4540],
        [3.6676, 3.4189, 3.5456],
        [3.6676, 2.8716, 2.4773],
        [3.6676, 2.9698, 2.3196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.1903248131275177 
model_pd.l_d.mean(): -23.0191707611084 
model_pd.lagr.mean(): -22.828845977783203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2801], device='cuda:0')), ('power', tensor([-23.2993], device='cuda:0'))])
epoch£º1174	 i:0 	 global-step:23480	 l-p:0.1903248131275177
epoch£º1174	 i:1 	 global-step:23481	 l-p:0.15001240372657776
epoch£º1174	 i:2 	 global-step:23482	 l-p:0.08244417607784271
epoch£º1174	 i:3 	 global-step:23483	 l-p:0.1245686337351799
epoch£º1174	 i:4 	 global-step:23484	 l-p:0.2562812268733978
epoch£º1174	 i:5 	 global-step:23485	 l-p:0.11890917271375656
epoch£º1174	 i:6 	 global-step:23486	 l-p:0.07955574989318848
epoch£º1174	 i:7 	 global-step:23487	 l-p:-38.422698974609375
epoch£º1174	 i:8 	 global-step:23488	 l-p:0.128844752907753
epoch£º1174	 i:9 	 global-step:23489	 l-p:0.07800399512052536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6710, 3.1991, 2.5819],
        [3.6710, 3.3493, 3.4754],
        [3.6710, 2.9645, 2.7688],
        [3.6710, 3.6708, 3.6710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.0681568905711174 
model_pd.l_d.mean(): -22.60455322265625 
model_pd.lagr.mean(): -22.536396026611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3671], device='cuda:0')), ('power', tensor([-22.9717], device='cuda:0'))])
epoch£º1175	 i:0 	 global-step:23500	 l-p:0.0681568905711174
epoch£º1175	 i:1 	 global-step:23501	 l-p:0.1534506380558014
epoch£º1175	 i:2 	 global-step:23502	 l-p:0.5586025714874268
epoch£º1175	 i:3 	 global-step:23503	 l-p:0.1149953156709671
epoch£º1175	 i:4 	 global-step:23504	 l-p:0.13675066828727722
epoch£º1175	 i:5 	 global-step:23505	 l-p:0.10843613743782043
epoch£º1175	 i:6 	 global-step:23506	 l-p:0.19235743582248688
epoch£º1175	 i:7 	 global-step:23507	 l-p:0.14538994431495667
epoch£º1175	 i:8 	 global-step:23508	 l-p:0.1517595648765564
epoch£º1175	 i:9 	 global-step:23509	 l-p:0.04194849729537964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6795, 3.1301, 3.1379],
        [3.6795, 2.9437, 2.6916],
        [3.6795, 3.6795, 3.6796],
        [3.6795, 3.1366, 3.1504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.12342607229948044 
model_pd.l_d.mean(): -23.1359806060791 
model_pd.lagr.mean(): -23.012554168701172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1676], device='cuda:0')), ('power', tensor([-23.3036], device='cuda:0'))])
epoch£º1176	 i:0 	 global-step:23520	 l-p:0.12342607229948044
epoch£º1176	 i:1 	 global-step:23521	 l-p:0.1952139437198639
epoch£º1176	 i:2 	 global-step:23522	 l-p:0.09673735499382019
epoch£º1176	 i:3 	 global-step:23523	 l-p:0.32408878207206726
epoch£º1176	 i:4 	 global-step:23524	 l-p:0.007098655682057142
epoch£º1176	 i:5 	 global-step:23525	 l-p:0.04359627515077591
epoch£º1176	 i:6 	 global-step:23526	 l-p:0.1501549482345581
epoch£º1176	 i:7 	 global-step:23527	 l-p:0.13150568306446075
epoch£º1176	 i:8 	 global-step:23528	 l-p:0.17485205829143524
epoch£º1176	 i:9 	 global-step:23529	 l-p:0.1418549120426178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6936, 3.6402, 3.6853],
        [3.6936, 3.5876, 3.6668],
        [3.6936, 3.6890, 3.6935],
        [3.6936, 3.0150, 2.3623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.1300271451473236 
model_pd.l_d.mean(): -23.41236114501953 
model_pd.lagr.mean(): -23.282333374023438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2164], device='cuda:0')), ('power', tensor([-23.6287], device='cuda:0'))])
epoch£º1177	 i:0 	 global-step:23540	 l-p:0.1300271451473236
epoch£º1177	 i:1 	 global-step:23541	 l-p:0.12330564111471176
epoch£º1177	 i:2 	 global-step:23542	 l-p:0.12866884469985962
epoch£º1177	 i:3 	 global-step:23543	 l-p:0.13055278360843658
epoch£º1177	 i:4 	 global-step:23544	 l-p:0.17702773213386536
epoch£º1177	 i:5 	 global-step:23545	 l-p:0.11760582029819489
epoch£º1177	 i:6 	 global-step:23546	 l-p:0.16278640925884247
epoch£º1177	 i:7 	 global-step:23547	 l-p:-0.016433686017990112
epoch£º1177	 i:8 	 global-step:23548	 l-p:0.01597268134355545
epoch£º1177	 i:9 	 global-step:23549	 l-p:0.13477028906345367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6987, 3.6987, 3.6987],
        [3.6987, 3.6491, 3.6914],
        [3.6987, 3.0073, 2.8329],
        [3.6987, 3.6987, 3.6987]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.14793722331523895 
model_pd.l_d.mean(): -23.742815017700195 
model_pd.lagr.mean(): -23.594877243041992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1243], device='cuda:0')), ('power', tensor([-23.8671], device='cuda:0'))])
epoch£º1178	 i:0 	 global-step:23560	 l-p:0.14793722331523895
epoch£º1178	 i:1 	 global-step:23561	 l-p:-0.02914505824446678
epoch£º1178	 i:2 	 global-step:23562	 l-p:0.16367337107658386
epoch£º1178	 i:3 	 global-step:23563	 l-p:0.1341346651315689
epoch£º1178	 i:4 	 global-step:23564	 l-p:0.20131616294384003
epoch£º1178	 i:5 	 global-step:23565	 l-p:-0.27386653423309326
epoch£º1178	 i:6 	 global-step:23566	 l-p:0.13425759971141815
epoch£º1178	 i:7 	 global-step:23567	 l-p:0.16233333945274353
epoch£º1178	 i:8 	 global-step:23568	 l-p:-0.01813400164246559
epoch£º1178	 i:9 	 global-step:23569	 l-p:0.13012661039829254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7116, 3.3599, 3.4796],
        [3.7116, 3.6764, 3.7075],
        [3.7116, 2.9484, 2.2931],
        [3.7116, 3.6495, 3.7009]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.1266995072364807 
model_pd.l_d.mean(): -23.59911346435547 
model_pd.lagr.mean(): -23.472414016723633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1602], device='cuda:0')), ('power', tensor([-23.7594], device='cuda:0'))])
epoch£º1179	 i:0 	 global-step:23580	 l-p:0.1266995072364807
epoch£º1179	 i:1 	 global-step:23581	 l-p:0.10956113785505295
epoch£º1179	 i:2 	 global-step:23582	 l-p:0.24630671739578247
epoch£º1179	 i:3 	 global-step:23583	 l-p:-0.26561152935028076
epoch£º1179	 i:4 	 global-step:23584	 l-p:0.7765141129493713
epoch£º1179	 i:5 	 global-step:23585	 l-p:0.11870753020048141
epoch£º1179	 i:6 	 global-step:23586	 l-p:0.12211337685585022
epoch£º1179	 i:7 	 global-step:23587	 l-p:0.14845819771289825
epoch£º1179	 i:8 	 global-step:23588	 l-p:0.12542958557605743
epoch£º1179	 i:9 	 global-step:23589	 l-p:-0.1336352378129959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7129, 3.6967, 3.7117],
        [3.7129, 3.6070, 3.6861],
        [3.7129, 3.5746, 3.6701],
        [3.7129, 3.6774, 3.7087]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.12767980992794037 
model_pd.l_d.mean(): -22.997879028320312 
model_pd.lagr.mean(): -22.87019920349121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1964], device='cuda:0')), ('power', tensor([-23.1943], device='cuda:0'))])
epoch£º1180	 i:0 	 global-step:23600	 l-p:0.12767980992794037
epoch£º1180	 i:1 	 global-step:23601	 l-p:0.12062118202447891
epoch£º1180	 i:2 	 global-step:23602	 l-p:0.1287255436182022
epoch£º1180	 i:3 	 global-step:23603	 l-p:0.1349567323923111
epoch£º1180	 i:4 	 global-step:23604	 l-p:0.1314411759376526
epoch£º1180	 i:5 	 global-step:23605	 l-p:0.050353288650512695
epoch£º1180	 i:6 	 global-step:23606	 l-p:0.10157687962055206
epoch£º1180	 i:7 	 global-step:23607	 l-p:0.10361876338720322
epoch£º1180	 i:8 	 global-step:23608	 l-p:0.025425085797905922
epoch£º1180	 i:9 	 global-step:23609	 l-p:0.1543639749288559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6901, 3.1716, 2.5430],
        [3.6901, 3.6746, 3.6890],
        [3.6901, 3.6494, 3.6849],
        [3.6901, 3.1352, 2.5002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.11491534858942032 
model_pd.l_d.mean(): -23.18692398071289 
model_pd.lagr.mean(): -23.07200813293457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2911], device='cuda:0')), ('power', tensor([-23.4780], device='cuda:0'))])
epoch£º1181	 i:0 	 global-step:23620	 l-p:0.11491534858942032
epoch£º1181	 i:1 	 global-step:23621	 l-p:0.14546647667884827
epoch£º1181	 i:2 	 global-step:23622	 l-p:0.06872215867042542
epoch£º1181	 i:3 	 global-step:23623	 l-p:0.20195044577121735
epoch£º1181	 i:4 	 global-step:23624	 l-p:0.14160485565662384
epoch£º1181	 i:5 	 global-step:23625	 l-p:0.16346168518066406
epoch£º1181	 i:6 	 global-step:23626	 l-p:0.11417451500892639
epoch£º1181	 i:7 	 global-step:23627	 l-p:0.23516681790351868
epoch£º1181	 i:8 	 global-step:23628	 l-p:0.1448596566915512
epoch£º1181	 i:9 	 global-step:23629	 l-p:0.14690397679805756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6743, 2.8737, 2.2330],
        [3.6743, 3.3212, 3.4416],
        [3.6743, 3.2608, 3.3611],
        [3.6743, 3.6743, 3.6743]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.12568162381649017 
model_pd.l_d.mean(): -23.557506561279297 
model_pd.lagr.mean(): -23.431825637817383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1695], device='cuda:0')), ('power', tensor([-23.7270], device='cuda:0'))])
epoch£º1182	 i:0 	 global-step:23640	 l-p:0.12568162381649017
epoch£º1182	 i:1 	 global-step:23641	 l-p:0.13405190408229828
epoch£º1182	 i:2 	 global-step:23642	 l-p:0.13259948790073395
epoch£º1182	 i:3 	 global-step:23643	 l-p:0.12088630348443985
epoch£º1182	 i:4 	 global-step:23644	 l-p:-0.47157660126686096
epoch£º1182	 i:5 	 global-step:23645	 l-p:0.14278626441955566
epoch£º1182	 i:6 	 global-step:23646	 l-p:0.28529760241508484
epoch£º1182	 i:7 	 global-step:23647	 l-p:0.10706210881471634
epoch£º1182	 i:8 	 global-step:23648	 l-p:0.13052639365196228
epoch£º1182	 i:9 	 global-step:23649	 l-p:0.15081001818180084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6563, 3.1000, 2.4698],
        [3.6563, 3.0264, 2.3852],
        [3.6563, 3.6563, 3.6563],
        [3.6563, 2.8581, 2.4635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.0661880224943161 
model_pd.l_d.mean(): -23.353071212768555 
model_pd.lagr.mean(): -23.286882400512695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2787], device='cuda:0')), ('power', tensor([-23.6318], device='cuda:0'))])
epoch£º1183	 i:0 	 global-step:23660	 l-p:0.0661880224943161
epoch£º1183	 i:1 	 global-step:23661	 l-p:0.08891191333532333
epoch£º1183	 i:2 	 global-step:23662	 l-p:0.1813424676656723
epoch£º1183	 i:3 	 global-step:23663	 l-p:0.1419605314731598
epoch£º1183	 i:4 	 global-step:23664	 l-p:0.152755007147789
epoch£º1183	 i:5 	 global-step:23665	 l-p:0.3398246765136719
epoch£º1183	 i:6 	 global-step:23666	 l-p:0.10964576154947281
epoch£º1183	 i:7 	 global-step:23667	 l-p:0.12923087179660797
epoch£º1183	 i:8 	 global-step:23668	 l-p:-0.10095395147800446
epoch£º1183	 i:9 	 global-step:23669	 l-p:0.13933558762073517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6588, 2.8864, 2.2377],
        [3.6588, 3.6438, 3.6578],
        [3.6588, 2.9511, 2.3013],
        [3.6588, 3.1516, 3.1970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.14895451068878174 
model_pd.l_d.mean(): -23.55355453491211 
model_pd.lagr.mean(): -23.404600143432617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1875], device='cuda:0')), ('power', tensor([-23.7411], device='cuda:0'))])
epoch£º1184	 i:0 	 global-step:23680	 l-p:0.14895451068878174
epoch£º1184	 i:1 	 global-step:23681	 l-p:0.14334143698215485
epoch£º1184	 i:2 	 global-step:23682	 l-p:0.16271430253982544
epoch£º1184	 i:3 	 global-step:23683	 l-p:0.08692758530378342
epoch£º1184	 i:4 	 global-step:23684	 l-p:0.0987686812877655
epoch£º1184	 i:5 	 global-step:23685	 l-p:0.13778798282146454
epoch£º1184	 i:6 	 global-step:23686	 l-p:-0.008360452018678188
epoch£º1184	 i:7 	 global-step:23687	 l-p:0.1585385799407959
epoch£º1184	 i:8 	 global-step:23688	 l-p:0.15228568017482758
epoch£º1184	 i:9 	 global-step:23689	 l-p:0.3988319933414459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6522, 2.9581, 2.7879],
        [3.6522, 3.1857, 3.2596],
        [3.6522, 3.1770, 3.2454],
        [3.6522, 3.4131, 3.5390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.1461670696735382 
model_pd.l_d.mean(): -23.511919021606445 
model_pd.lagr.mean(): -23.365751266479492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2149], device='cuda:0')), ('power', tensor([-23.7268], device='cuda:0'))])
epoch£º1185	 i:0 	 global-step:23700	 l-p:0.1461670696735382
epoch£º1185	 i:1 	 global-step:23701	 l-p:0.12306827306747437
epoch£º1185	 i:2 	 global-step:23702	 l-p:0.15067604184150696
epoch£º1185	 i:3 	 global-step:23703	 l-p:0.12135305255651474
epoch£º1185	 i:4 	 global-step:23704	 l-p:0.7205360531806946
epoch£º1185	 i:5 	 global-step:23705	 l-p:0.1310695856809616
epoch£º1185	 i:6 	 global-step:23706	 l-p:0.10741746425628662
epoch£º1185	 i:7 	 global-step:23707	 l-p:0.10290132462978363
epoch£º1185	 i:8 	 global-step:23708	 l-p:0.2061944156885147
epoch£º1185	 i:9 	 global-step:23709	 l-p:-0.006286196410655975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6524, 3.5718, 3.6358],
        [3.6524, 3.5552, 3.6294],
        [3.6524, 3.0596, 3.0263],
        [3.6524, 2.8896, 2.2405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.1949072778224945 
model_pd.l_d.mean(): -23.4807071685791 
model_pd.lagr.mean(): -23.285799026489258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2151], device='cuda:0')), ('power', tensor([-23.6958], device='cuda:0'))])
epoch£º1186	 i:0 	 global-step:23720	 l-p:0.1949072778224945
epoch£º1186	 i:1 	 global-step:23721	 l-p:0.12133411318063736
epoch£º1186	 i:2 	 global-step:23722	 l-p:0.3069232106208801
epoch£º1186	 i:3 	 global-step:23723	 l-p:0.14415211975574493
epoch£º1186	 i:4 	 global-step:23724	 l-p:-0.2353939563035965
epoch£º1186	 i:5 	 global-step:23725	 l-p:0.09358090162277222
epoch£º1186	 i:6 	 global-step:23726	 l-p:0.06737399846315384
epoch£º1186	 i:7 	 global-step:23727	 l-p:0.1329021453857422
epoch£º1186	 i:8 	 global-step:23728	 l-p:0.11889416724443436
epoch£º1186	 i:9 	 global-step:23729	 l-p:0.10001377761363983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6769, 3.3775, 3.5057],
        [3.6769, 3.0258, 2.3786],
        [3.6769, 3.5221, 3.6249],
        [3.6769, 3.6742, 3.6768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.4744827449321747 
model_pd.l_d.mean(): -23.485122680664062 
model_pd.lagr.mean(): -23.010639190673828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2135], device='cuda:0')), ('power', tensor([-23.6986], device='cuda:0'))])
epoch£º1187	 i:0 	 global-step:23740	 l-p:0.4744827449321747
epoch£º1187	 i:1 	 global-step:23741	 l-p:0.1322070211172104
epoch£º1187	 i:2 	 global-step:23742	 l-p:0.11498929560184479
epoch£º1187	 i:3 	 global-step:23743	 l-p:0.13807620108127594
epoch£º1187	 i:4 	 global-step:23744	 l-p:0.1209801435470581
epoch£º1187	 i:5 	 global-step:23745	 l-p:0.1347387135028839
epoch£º1187	 i:6 	 global-step:23746	 l-p:-0.018512198701500893
epoch£º1187	 i:7 	 global-step:23747	 l-p:0.0690029188990593
epoch£º1187	 i:8 	 global-step:23748	 l-p:0.24705679714679718
epoch£º1187	 i:9 	 global-step:23749	 l-p:0.14804582297801971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6786, 3.5028, 3.6134],
        [3.6786, 3.6786, 3.6786],
        [3.6786, 3.2423, 2.6310],
        [3.6786, 3.5947, 3.6608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.12631194293498993 
model_pd.l_d.mean(): -23.446557998657227 
model_pd.lagr.mean(): -23.32024574279785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1387], device='cuda:0')), ('power', tensor([-23.5853], device='cuda:0'))])
epoch£º1188	 i:0 	 global-step:23760	 l-p:0.12631194293498993
epoch£º1188	 i:1 	 global-step:23761	 l-p:0.14521954953670502
epoch£º1188	 i:2 	 global-step:23762	 l-p:0.11182421445846558
epoch£º1188	 i:3 	 global-step:23763	 l-p:0.21996192634105682
epoch£º1188	 i:4 	 global-step:23764	 l-p:0.061284735798835754
epoch£º1188	 i:5 	 global-step:23765	 l-p:0.9016020894050598
epoch£º1188	 i:6 	 global-step:23766	 l-p:0.11111502349376678
epoch£º1188	 i:7 	 global-step:23767	 l-p:0.15407496690750122
epoch£º1188	 i:8 	 global-step:23768	 l-p:0.07263782620429993
epoch£º1188	 i:9 	 global-step:23769	 l-p:0.1189611405134201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6767, 3.4032, 3.5317],
        [3.6767, 3.2177, 3.2951],
        [3.6767, 3.6767, 3.6767],
        [3.6767, 3.6733, 3.6766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.06855404376983643 
model_pd.l_d.mean(): -23.7706241607666 
model_pd.lagr.mean(): -23.702070236206055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0967], device='cuda:0')), ('power', tensor([-23.8674], device='cuda:0'))])
epoch£º1189	 i:0 	 global-step:23780	 l-p:0.06855404376983643
epoch£º1189	 i:1 	 global-step:23781	 l-p:0.4649888575077057
epoch£º1189	 i:2 	 global-step:23782	 l-p:0.1472075879573822
epoch£º1189	 i:3 	 global-step:23783	 l-p:0.023480424657464027
epoch£º1189	 i:4 	 global-step:23784	 l-p:0.1391284316778183
epoch£º1189	 i:5 	 global-step:23785	 l-p:0.12346524745225906
epoch£º1189	 i:6 	 global-step:23786	 l-p:0.15425848960876465
epoch£º1189	 i:7 	 global-step:23787	 l-p:0.18508869409561157
epoch£º1189	 i:8 	 global-step:23788	 l-p:0.1184619814157486
epoch£º1189	 i:9 	 global-step:23789	 l-p:0.09269923716783524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6802, 3.6802, 3.6802],
        [3.6802, 3.6454, 3.6761],
        [3.6802, 3.6802, 3.6802],
        [3.6802, 3.6507, 3.6771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.16642898321151733 
model_pd.l_d.mean(): -22.999040603637695 
model_pd.lagr.mean(): -22.832611083984375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2471], device='cuda:0')), ('power', tensor([-23.2461], device='cuda:0'))])
epoch£º1190	 i:0 	 global-step:23800	 l-p:0.16642898321151733
epoch£º1190	 i:1 	 global-step:23801	 l-p:0.13209176063537598
epoch£º1190	 i:2 	 global-step:23802	 l-p:0.16162753105163574
epoch£º1190	 i:3 	 global-step:23803	 l-p:0.14928391575813293
epoch£º1190	 i:4 	 global-step:23804	 l-p:0.08629743754863739
epoch£º1190	 i:5 	 global-step:23805	 l-p:0.16918142139911652
epoch£º1190	 i:6 	 global-step:23806	 l-p:0.12993323802947998
epoch£º1190	 i:7 	 global-step:23807	 l-p:0.1305037885904312
epoch£º1190	 i:8 	 global-step:23808	 l-p:-0.03145458176732063
epoch£º1190	 i:9 	 global-step:23809	 l-p:0.40189802646636963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6788, 3.1969, 2.5765],
        [3.6788, 3.6787, 3.6788],
        [3.6788, 3.5983, 3.6622],
        [3.6788, 3.2159, 3.2909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.1702500730752945 
model_pd.l_d.mean(): -22.72441864013672 
model_pd.lagr.mean(): -22.554168701171875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3349], device='cuda:0')), ('power', tensor([-23.0593], device='cuda:0'))])
epoch£º1191	 i:0 	 global-step:23820	 l-p:0.1702500730752945
epoch£º1191	 i:1 	 global-step:23821	 l-p:0.07657550275325775
epoch£º1191	 i:2 	 global-step:23822	 l-p:0.042783286422491074
epoch£º1191	 i:3 	 global-step:23823	 l-p:0.14930006861686707
epoch£º1191	 i:4 	 global-step:23824	 l-p:0.15899859368801117
epoch£º1191	 i:5 	 global-step:23825	 l-p:0.03836920112371445
epoch£º1191	 i:6 	 global-step:23826	 l-p:0.14270640909671783
epoch£º1191	 i:7 	 global-step:23827	 l-p:0.23589028418064117
epoch£º1191	 i:8 	 global-step:23828	 l-p:0.13678619265556335
epoch£º1191	 i:9 	 global-step:23829	 l-p:0.14898374676704407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6957, 3.5484, 3.6480],
        [3.6957, 3.5229, 3.6324],
        [3.6957, 3.3787, 3.5051],
        [3.6957, 3.6866, 3.6953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.12659378349781036 
model_pd.l_d.mean(): -23.01031494140625 
model_pd.lagr.mean(): -22.88372039794922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2632], device='cuda:0')), ('power', tensor([-23.2735], device='cuda:0'))])
epoch£º1192	 i:0 	 global-step:23840	 l-p:0.12659378349781036
epoch£º1192	 i:1 	 global-step:23841	 l-p:0.14423656463623047
epoch£º1192	 i:2 	 global-step:23842	 l-p:0.12996207177639008
epoch£º1192	 i:3 	 global-step:23843	 l-p:0.13925138115882874
epoch£º1192	 i:4 	 global-step:23844	 l-p:0.012705988250672817
epoch£º1192	 i:5 	 global-step:23845	 l-p:0.1514398753643036
epoch£º1192	 i:6 	 global-step:23846	 l-p:0.11628104746341705
epoch£º1192	 i:7 	 global-step:23847	 l-p:0.17154952883720398
epoch£º1192	 i:8 	 global-step:23848	 l-p:0.06053610518574715
epoch£º1192	 i:9 	 global-step:23849	 l-p:0.17563460767269135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6890, 2.8972, 2.5131],
        [3.6890, 3.5853, 3.6633],
        [3.6890, 3.6804, 3.6886],
        [3.6890, 3.6478, 3.6837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.11857320368289948 
model_pd.l_d.mean(): -23.291723251342773 
model_pd.lagr.mean(): -23.17314910888672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2846], device='cuda:0')), ('power', tensor([-23.5763], device='cuda:0'))])
epoch£º1193	 i:0 	 global-step:23860	 l-p:0.11857320368289948
epoch£º1193	 i:1 	 global-step:23861	 l-p:0.09892232716083527
epoch£º1193	 i:2 	 global-step:23862	 l-p:0.1607484370470047
epoch£º1193	 i:3 	 global-step:23863	 l-p:-0.006601319182664156
epoch£º1193	 i:4 	 global-step:23864	 l-p:0.1255389302968979
epoch£º1193	 i:5 	 global-step:23865	 l-p:0.14456577599048615
epoch£º1193	 i:6 	 global-step:23866	 l-p:0.042724888771772385
epoch£º1193	 i:7 	 global-step:23867	 l-p:0.14503738284111023
epoch£º1193	 i:8 	 global-step:23868	 l-p:0.12377132475376129
epoch£º1193	 i:9 	 global-step:23869	 l-p:0.23075363039970398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6971, 2.8706, 2.3408],
        [3.6971, 3.6951, 3.6971],
        [3.6971, 2.8884, 2.2525],
        [3.6971, 2.9299, 2.6084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.11776421964168549 
model_pd.l_d.mean(): -23.492219924926758 
model_pd.lagr.mean(): -23.37445640563965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1985], device='cuda:0')), ('power', tensor([-23.6907], device='cuda:0'))])
epoch£º1194	 i:0 	 global-step:23880	 l-p:0.11776421964168549
epoch£º1194	 i:1 	 global-step:23881	 l-p:0.04916997253894806
epoch£º1194	 i:2 	 global-step:23882	 l-p:0.12950314581394196
epoch£º1194	 i:3 	 global-step:23883	 l-p:0.14179940521717072
epoch£º1194	 i:4 	 global-step:23884	 l-p:0.12768125534057617
epoch£º1194	 i:5 	 global-step:23885	 l-p:0.1986992210149765
epoch£º1194	 i:6 	 global-step:23886	 l-p:0.030024657025933266
epoch£º1194	 i:7 	 global-step:23887	 l-p:0.15458498895168304
epoch£º1194	 i:8 	 global-step:23888	 l-p:0.1317664533853531
epoch£º1194	 i:9 	 global-step:23889	 l-p:0.10576565563678741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6931, 3.6838, 3.6927],
        [3.6931, 3.6924, 3.6931],
        [3.6931, 3.4325, 3.5603],
        [3.6931, 3.0935, 3.0492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.1874459832906723 
model_pd.l_d.mean(): -23.485836029052734 
model_pd.lagr.mean(): -23.298389434814453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1942], device='cuda:0')), ('power', tensor([-23.6800], device='cuda:0'))])
epoch£º1195	 i:0 	 global-step:23900	 l-p:0.1874459832906723
epoch£º1195	 i:1 	 global-step:23901	 l-p:-0.05381302163004875
epoch£º1195	 i:2 	 global-step:23902	 l-p:0.10197960585355759
epoch£º1195	 i:3 	 global-step:23903	 l-p:0.13584508001804352
epoch£º1195	 i:4 	 global-step:23904	 l-p:0.12178964167833328
epoch£º1195	 i:5 	 global-step:23905	 l-p:-0.1035398617386818
epoch£º1195	 i:6 	 global-step:23906	 l-p:0.13274940848350525
epoch£º1195	 i:7 	 global-step:23907	 l-p:0.17722108960151672
epoch£º1195	 i:8 	 global-step:23908	 l-p:0.1261926293373108
epoch£º1195	 i:9 	 global-step:23909	 l-p:0.1471484899520874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7056, 3.5511, 3.6537],
        [3.7056, 3.6483, 3.6963],
        [3.7056, 3.1303, 2.4893],
        [3.7056, 3.0290, 2.3743]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): -0.5204382538795471 
model_pd.l_d.mean(): -23.341976165771484 
model_pd.lagr.mean(): -23.862415313720703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2268], device='cuda:0')), ('power', tensor([-23.5687], device='cuda:0'))])
epoch£º1196	 i:0 	 global-step:23920	 l-p:-0.5204382538795471
epoch£º1196	 i:1 	 global-step:23921	 l-p:0.13037905097007751
epoch£º1196	 i:2 	 global-step:23922	 l-p:0.15724965929985046
epoch£º1196	 i:3 	 global-step:23923	 l-p:0.1431383639574051
epoch£º1196	 i:4 	 global-step:23924	 l-p:0.18660815060138702
epoch£º1196	 i:5 	 global-step:23925	 l-p:0.173982635140419
epoch£º1196	 i:6 	 global-step:23926	 l-p:0.13318896293640137
epoch£º1196	 i:7 	 global-step:23927	 l-p:0.12159647047519684
epoch£º1196	 i:8 	 global-step:23928	 l-p:0.12753498554229736
epoch£º1196	 i:9 	 global-step:23929	 l-p:0.0018203782383352518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7047, 3.0742, 2.9912],
        [3.7047, 3.5472, 3.6510],
        [3.7047, 3.2037, 2.5760],
        [3.7047, 2.8790, 2.2811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.13946861028671265 
model_pd.l_d.mean(): -23.2913875579834 
model_pd.lagr.mean(): -23.151918411254883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2696], device='cuda:0')), ('power', tensor([-23.5610], device='cuda:0'))])
epoch£º1197	 i:0 	 global-step:23940	 l-p:0.13946861028671265
epoch£º1197	 i:1 	 global-step:23941	 l-p:0.17137601971626282
epoch£º1197	 i:2 	 global-step:23942	 l-p:0.059727173298597336
epoch£º1197	 i:3 	 global-step:23943	 l-p:0.1264006793498993
epoch£º1197	 i:4 	 global-step:23944	 l-p:0.12816932797431946
epoch£º1197	 i:5 	 global-step:23945	 l-p:0.12934881448745728
epoch£º1197	 i:6 	 global-step:23946	 l-p:0.13759316504001617
epoch£º1197	 i:7 	 global-step:23947	 l-p:0.1293068677186966
epoch£º1197	 i:8 	 global-step:23948	 l-p:-0.05868721753358841
epoch£º1197	 i:9 	 global-step:23949	 l-p:0.050232913345098495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6965, 2.9536, 2.2972],
        [3.6965, 3.6965, 3.6965],
        [3.6965, 3.6886, 3.6961],
        [3.6965, 3.6659, 3.6932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): -0.154126837849617 
model_pd.l_d.mean(): -23.180387496948242 
model_pd.lagr.mean(): -23.334514617919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2362], device='cuda:0')), ('power', tensor([-23.4166], device='cuda:0'))])
epoch£º1198	 i:0 	 global-step:23960	 l-p:-0.154126837849617
epoch£º1198	 i:1 	 global-step:23961	 l-p:0.18609870970249176
epoch£º1198	 i:2 	 global-step:23962	 l-p:0.060569532215595245
epoch£º1198	 i:3 	 global-step:23963	 l-p:0.13693374395370483
epoch£º1198	 i:4 	 global-step:23964	 l-p:0.12555846571922302
epoch£º1198	 i:5 	 global-step:23965	 l-p:0.11913345009088516
epoch£º1198	 i:6 	 global-step:23966	 l-p:0.1228850707411766
epoch£º1198	 i:7 	 global-step:23967	 l-p:0.10564174503087997
epoch£º1198	 i:8 	 global-step:23968	 l-p:0.15645653009414673
epoch£º1198	 i:9 	 global-step:23969	 l-p:0.23884528875350952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6951, 3.6925, 3.6951],
        [3.6951, 3.6951, 3.6951],
        [3.6951, 3.6935, 3.6951],
        [3.6951, 3.6951, 3.6951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.09849242866039276 
model_pd.l_d.mean(): -22.739580154418945 
model_pd.lagr.mean(): -22.641088485717773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3069], device='cuda:0')), ('power', tensor([-23.0465], device='cuda:0'))])
epoch£º1199	 i:0 	 global-step:23980	 l-p:0.09849242866039276
epoch£º1199	 i:1 	 global-step:23981	 l-p:0.059828467667102814
epoch£º1199	 i:2 	 global-step:23982	 l-p:0.15874895453453064
epoch£º1199	 i:3 	 global-step:23983	 l-p:0.1404239982366562
epoch£º1199	 i:4 	 global-step:23984	 l-p:0.1292446255683899
epoch£º1199	 i:5 	 global-step:23985	 l-p:0.1394907534122467
epoch£º1199	 i:6 	 global-step:23986	 l-p:0.18909701704978943
epoch£º1199	 i:7 	 global-step:23987	 l-p:0.20730933547019958
epoch£º1199	 i:8 	 global-step:23988	 l-p:0.13341055810451508
epoch£º1199	 i:9 	 global-step:23989	 l-p:0.02325611002743244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6860, 3.5356, 3.6366],
        [3.6860, 3.6859, 3.6860],
        [3.6860, 3.2083, 3.2740],
        [3.6860, 3.1948, 3.2513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.14934048056602478 
model_pd.l_d.mean(): -23.35770034790039 
model_pd.lagr.mean(): -23.20836067199707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2161], device='cuda:0')), ('power', tensor([-23.5738], device='cuda:0'))])
epoch£º1200	 i:0 	 global-step:24000	 l-p:0.14934048056602478
epoch£º1200	 i:1 	 global-step:24001	 l-p:0.11514875292778015
epoch£º1200	 i:2 	 global-step:24002	 l-p:0.1947188526391983
epoch£º1200	 i:3 	 global-step:24003	 l-p:0.05238405615091324
epoch£º1200	 i:4 	 global-step:24004	 l-p:0.13379515707492828
epoch£º1200	 i:5 	 global-step:24005	 l-p:0.12811121344566345
epoch£º1200	 i:6 	 global-step:24006	 l-p:0.1523628830909729
epoch£º1200	 i:7 	 global-step:24007	 l-p:0.16665557026863098
epoch£º1200	 i:8 	 global-step:24008	 l-p:0.10155194997787476
epoch£º1200	 i:9 	 global-step:24009	 l-p:0.1343669891357422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6837, 3.6837, 3.6837],
        [3.6837, 3.6391, 3.6776],
        [3.6837, 2.8906, 2.5062],
        [3.6837, 2.9096, 2.5758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.1362616866827011 
model_pd.l_d.mean(): -22.625864028930664 
model_pd.lagr.mean(): -22.48960304260254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3078], device='cuda:0')), ('power', tensor([-22.9337], device='cuda:0'))])
epoch£º1201	 i:0 	 global-step:24020	 l-p:0.1362616866827011
epoch£º1201	 i:1 	 global-step:24021	 l-p:0.2107723206281662
epoch£º1201	 i:2 	 global-step:24022	 l-p:0.13846473395824432
epoch£º1201	 i:3 	 global-step:24023	 l-p:0.10435140132904053
epoch£º1201	 i:4 	 global-step:24024	 l-p:0.07691772282123566
epoch£º1201	 i:5 	 global-step:24025	 l-p:0.05788854509592056
epoch£º1201	 i:6 	 global-step:24026	 l-p:0.12110163271427155
epoch£º1201	 i:7 	 global-step:24027	 l-p:0.5071720480918884
epoch£º1201	 i:8 	 global-step:24028	 l-p:0.1934533268213272
epoch£º1201	 i:9 	 global-step:24029	 l-p:0.06277145445346832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6773, 3.5193, 3.6235],
        [3.6773, 3.0294, 2.9269],
        [3.6773, 3.6773, 3.6773],
        [3.6773, 3.1388, 2.5081]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.1440741866827011 
model_pd.l_d.mean(): -23.112529754638672 
model_pd.lagr.mean(): -22.968456268310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2852], device='cuda:0')), ('power', tensor([-23.3978], device='cuda:0'))])
epoch£º1202	 i:0 	 global-step:24040	 l-p:0.1440741866827011
epoch£º1202	 i:1 	 global-step:24041	 l-p:0.12871013581752777
epoch£º1202	 i:2 	 global-step:24042	 l-p:0.14275182783603668
epoch£º1202	 i:3 	 global-step:24043	 l-p:0.13416795432567596
epoch£º1202	 i:4 	 global-step:24044	 l-p:0.43219101428985596
epoch£º1202	 i:5 	 global-step:24045	 l-p:0.10359270125627518
epoch£º1202	 i:6 	 global-step:24046	 l-p:0.1245652362704277
epoch£º1202	 i:7 	 global-step:24047	 l-p:0.16885824501514435
epoch£º1202	 i:8 	 global-step:24048	 l-p:0.12763534486293793
epoch£º1202	 i:9 	 global-step:24049	 l-p:0.0748400017619133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6787, 3.6353, 3.6729],
        [3.6787, 3.6781, 3.6787],
        [3.6787, 2.8567, 2.2347],
        [3.6787, 2.8489, 2.3192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): 0.08153537660837173 
model_pd.l_d.mean(): -23.475692749023438 
model_pd.lagr.mean(): -23.39415740966797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2673], device='cuda:0')), ('power', tensor([-23.7430], device='cuda:0'))])
epoch£º1203	 i:0 	 global-step:24060	 l-p:0.08153537660837173
epoch£º1203	 i:1 	 global-step:24061	 l-p:0.18104921281337738
epoch£º1203	 i:2 	 global-step:24062	 l-p:0.06305962055921555
epoch£º1203	 i:3 	 global-step:24063	 l-p:0.13010023534297943
epoch£º1203	 i:4 	 global-step:24064	 l-p:0.12144206464290619
epoch£º1203	 i:5 	 global-step:24065	 l-p:0.14059457182884216
epoch£º1203	 i:6 	 global-step:24066	 l-p:0.328351765871048
epoch£º1203	 i:7 	 global-step:24067	 l-p:0.13025079667568207
epoch£º1203	 i:8 	 global-step:24068	 l-p:0.191981241106987
epoch£º1203	 i:9 	 global-step:24069	 l-p:0.07099629938602448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6833, 3.6669, 3.6821],
        [3.6833, 3.6296, 3.6750],
        [3.6833, 3.0228, 2.9018],
        [3.6833, 3.3684, 3.4955]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.08173738420009613 
model_pd.l_d.mean(): -23.631450653076172 
model_pd.lagr.mean(): -23.549713134765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1726], device='cuda:0')), ('power', tensor([-23.8040], device='cuda:0'))])
epoch£º1204	 i:0 	 global-step:24080	 l-p:0.08173738420009613
epoch£º1204	 i:1 	 global-step:24081	 l-p:0.06189241260290146
epoch£º1204	 i:2 	 global-step:24082	 l-p:0.03559572994709015
epoch£º1204	 i:3 	 global-step:24083	 l-p:0.2893276810646057
epoch£º1204	 i:4 	 global-step:24084	 l-p:0.12150687724351883
epoch£º1204	 i:5 	 global-step:24085	 l-p:0.16477912664413452
epoch£º1204	 i:6 	 global-step:24086	 l-p:0.21334131062030792
epoch£º1204	 i:7 	 global-step:24087	 l-p:0.120367631316185
epoch£º1204	 i:8 	 global-step:24088	 l-p:0.09865670651197433
epoch£º1204	 i:9 	 global-step:24089	 l-p:0.1442929059267044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6911, 2.8634, 2.3375],
        [3.6911, 3.5925, 3.6675],
        [3.6911, 3.0418, 2.3922],
        [3.6911, 2.8605, 2.2880]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.2507684528827667 
model_pd.l_d.mean(): -23.533981323242188 
model_pd.lagr.mean(): -23.283212661743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2277], device='cuda:0')), ('power', tensor([-23.7617], device='cuda:0'))])
epoch£º1205	 i:0 	 global-step:24100	 l-p:0.2507684528827667
epoch£º1205	 i:1 	 global-step:24101	 l-p:0.13177934288978577
epoch£º1205	 i:2 	 global-step:24102	 l-p:0.12389983236789703
epoch£º1205	 i:3 	 global-step:24103	 l-p:0.14456894993782043
epoch£º1205	 i:4 	 global-step:24104	 l-p:0.17155060172080994
epoch£º1205	 i:5 	 global-step:24105	 l-p:0.040398817509412766
epoch£º1205	 i:6 	 global-step:24106	 l-p:0.06334903836250305
epoch£º1205	 i:7 	 global-step:24107	 l-p:0.12954679131507874
epoch£º1205	 i:8 	 global-step:24108	 l-p:0.15311390161514282
epoch£º1205	 i:9 	 global-step:24109	 l-p:0.053283289074897766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6890, 3.6890, 3.6890],
        [3.6890, 2.9355, 2.6489],
        [3.6890, 3.5946, 3.6671],
        [3.6890, 3.0823, 3.0309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.1453864872455597 
model_pd.l_d.mean(): -23.634140014648438 
model_pd.lagr.mean(): -23.488754272460938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1516], device='cuda:0')), ('power', tensor([-23.7857], device='cuda:0'))])
epoch£º1206	 i:0 	 global-step:24120	 l-p:0.1453864872455597
epoch£º1206	 i:1 	 global-step:24121	 l-p:0.12312259525060654
epoch£º1206	 i:2 	 global-step:24122	 l-p:0.2113957554101944
epoch£º1206	 i:3 	 global-step:24123	 l-p:0.10853908956050873
epoch£º1206	 i:4 	 global-step:24124	 l-p:0.03864065185189247
epoch£º1206	 i:5 	 global-step:24125	 l-p:0.14391358196735382
epoch£º1206	 i:6 	 global-step:24126	 l-p:0.15225735306739807
epoch£º1206	 i:7 	 global-step:24127	 l-p:-0.0255351010710001
epoch£º1206	 i:8 	 global-step:24128	 l-p:0.2879062294960022
epoch£º1206	 i:9 	 global-step:24129	 l-p:0.14379344880580902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6855, 3.3274, 3.4469],
        [3.6855, 2.8688, 2.4050],
        [3.6855, 3.6833, 3.6855],
        [3.6855, 2.9626, 2.7386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.3100975453853607 
model_pd.l_d.mean(): -23.316362380981445 
model_pd.lagr.mean(): -23.00626564025879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2680], device='cuda:0')), ('power', tensor([-23.5843], device='cuda:0'))])
epoch£º1207	 i:0 	 global-step:24140	 l-p:0.3100975453853607
epoch£º1207	 i:1 	 global-step:24141	 l-p:0.06852246820926666
epoch£º1207	 i:2 	 global-step:24142	 l-p:0.11582526564598083
epoch£º1207	 i:3 	 global-step:24143	 l-p:0.13562552630901337
epoch£º1207	 i:4 	 global-step:24144	 l-p:0.14355315268039703
epoch£º1207	 i:5 	 global-step:24145	 l-p:0.06111777946352959
epoch£º1207	 i:6 	 global-step:24146	 l-p:0.2072441428899765
epoch£º1207	 i:7 	 global-step:24147	 l-p:0.08117309212684631
epoch£º1207	 i:8 	 global-step:24148	 l-p:0.11281854659318924
epoch£º1207	 i:9 	 global-step:24149	 l-p:0.11709541082382202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6854, 2.8645, 2.2413],
        [3.6854, 3.4307, 3.5582],
        [3.6854, 3.6854, 3.6854],
        [3.6854, 3.3922, 3.5207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.03910503163933754 
model_pd.l_d.mean(): -23.260229110717773 
model_pd.lagr.mean(): -23.22112464904785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2811], device='cuda:0')), ('power', tensor([-23.5413], device='cuda:0'))])
epoch£º1208	 i:0 	 global-step:24160	 l-p:0.03910503163933754
epoch£º1208	 i:1 	 global-step:24161	 l-p:0.19655384123325348
epoch£º1208	 i:2 	 global-step:24162	 l-p:0.1195426732301712
epoch£º1208	 i:3 	 global-step:24163	 l-p:0.06951434165239334
epoch£º1208	 i:4 	 global-step:24164	 l-p:0.06204051896929741
epoch£º1208	 i:5 	 global-step:24165	 l-p:0.14797812700271606
epoch£º1208	 i:6 	 global-step:24166	 l-p:0.2728710472583771
epoch£º1208	 i:7 	 global-step:24167	 l-p:0.1568543165922165
epoch£º1208	 i:8 	 global-step:24168	 l-p:0.12903131544589996
epoch£º1208	 i:9 	 global-step:24169	 l-p:0.10488320887088776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6933, 3.5147, 3.6263],
        [3.6933, 3.6708, 3.6914],
        [3.6933, 3.6933, 3.6933],
        [3.6933, 3.1584, 2.5260]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.13148118555545807 
model_pd.l_d.mean(): -22.94347381591797 
model_pd.lagr.mean(): -22.811992645263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1999], device='cuda:0')), ('power', tensor([-23.1434], device='cuda:0'))])
epoch£º1209	 i:0 	 global-step:24180	 l-p:0.13148118555545807
epoch£º1209	 i:1 	 global-step:24181	 l-p:-0.030270377174019814
epoch£º1209	 i:2 	 global-step:24182	 l-p:0.14056038856506348
epoch£º1209	 i:3 	 global-step:24183	 l-p:0.17100417613983154
epoch£º1209	 i:4 	 global-step:24184	 l-p:-0.003192853881046176
epoch£º1209	 i:5 	 global-step:24185	 l-p:0.11085545271635056
epoch£º1209	 i:6 	 global-step:24186	 l-p:0.10386623442173004
epoch£º1209	 i:7 	 global-step:24187	 l-p:0.14937274158000946
epoch£º1209	 i:8 	 global-step:24188	 l-p:0.1755548119544983
epoch£º1209	 i:9 	 global-step:24189	 l-p:0.23191694915294647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6945, 3.6914, 3.6944],
        [3.6945, 3.1154, 2.4753],
        [3.6945, 3.6849, 3.6940],
        [3.6945, 3.5984, 3.6719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.02461111918091774 
model_pd.l_d.mean(): -23.217992782592773 
model_pd.lagr.mean(): -23.193382263183594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2854], device='cuda:0')), ('power', tensor([-23.5034], device='cuda:0'))])
epoch£º1210	 i:0 	 global-step:24200	 l-p:0.02461111918091774
epoch£º1210	 i:1 	 global-step:24201	 l-p:0.12311594933271408
epoch£º1210	 i:2 	 global-step:24202	 l-p:0.12071362882852554
epoch£º1210	 i:3 	 global-step:24203	 l-p:0.25658702850341797
epoch£º1210	 i:4 	 global-step:24204	 l-p:0.019344069063663483
epoch£º1210	 i:5 	 global-step:24205	 l-p:0.16983073949813843
epoch£º1210	 i:6 	 global-step:24206	 l-p:0.174615278840065
epoch£º1210	 i:7 	 global-step:24207	 l-p:-0.039973240345716476
epoch£º1210	 i:8 	 global-step:24208	 l-p:0.16015315055847168
epoch£º1210	 i:9 	 global-step:24209	 l-p:0.11474688351154327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7003, 3.7003, 3.7003],
        [3.7003, 3.6744, 3.6978],
        [3.7003, 3.7003, 3.7003],
        [3.7003, 3.7002, 3.7003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.14941968023777008 
model_pd.l_d.mean(): -23.53843879699707 
model_pd.lagr.mean(): -23.389019012451172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2053], device='cuda:0')), ('power', tensor([-23.7438], device='cuda:0'))])
epoch£º1211	 i:0 	 global-step:24220	 l-p:0.14941968023777008
epoch£º1211	 i:1 	 global-step:24221	 l-p:-0.21571499109268188
epoch£º1211	 i:2 	 global-step:24222	 l-p:0.12842774391174316
epoch£º1211	 i:3 	 global-step:24223	 l-p:0.11449343711137772
epoch£º1211	 i:4 	 global-step:24224	 l-p:0.0454859584569931
epoch£º1211	 i:5 	 global-step:24225	 l-p:0.13410985469818115
epoch£º1211	 i:6 	 global-step:24226	 l-p:0.12605471909046173
epoch£º1211	 i:7 	 global-step:24227	 l-p:0.29299700260162354
epoch£º1211	 i:8 	 global-step:24228	 l-p:0.11731195449829102
epoch£º1211	 i:9 	 global-step:24229	 l-p:0.11992795765399933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6980, 3.6869, 3.6974],
        [3.6980, 3.6947, 3.6979],
        [3.6980, 3.3617, 3.4855],
        [3.6980, 3.6979, 3.6980]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.11823290586471558 
model_pd.l_d.mean(): -22.742599487304688 
model_pd.lagr.mean(): -22.624366760253906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3012], device='cuda:0')), ('power', tensor([-23.0437], device='cuda:0'))])
epoch£º1212	 i:0 	 global-step:24240	 l-p:0.11823290586471558
epoch£º1212	 i:1 	 global-step:24241	 l-p:0.1518675982952118
epoch£º1212	 i:2 	 global-step:24242	 l-p:0.13668952882289886
epoch£º1212	 i:3 	 global-step:24243	 l-p:0.25982582569122314
epoch£º1212	 i:4 	 global-step:24244	 l-p:0.04809458926320076
epoch£º1212	 i:5 	 global-step:24245	 l-p:0.156584694981575
epoch£º1212	 i:6 	 global-step:24246	 l-p:0.15980014204978943
epoch£º1212	 i:7 	 global-step:24247	 l-p:0.048909395933151245
epoch£º1212	 i:8 	 global-step:24248	 l-p:0.09773118793964386
epoch£º1212	 i:9 	 global-step:24249	 l-p:-0.0033461665734648705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6928, 3.6925, 3.6928],
        [3.6928, 3.6926, 3.6928],
        [3.6928, 3.5337, 3.6382],
        [3.6928, 3.4437, 3.5706]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): -0.06946901977062225 
model_pd.l_d.mean(): -23.415916442871094 
model_pd.lagr.mean(): -23.48538589477539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2165], device='cuda:0')), ('power', tensor([-23.6324], device='cuda:0'))])
epoch£º1213	 i:0 	 global-step:24260	 l-p:-0.06946901977062225
epoch£º1213	 i:1 	 global-step:24261	 l-p:0.13101790845394135
epoch£º1213	 i:2 	 global-step:24262	 l-p:0.1694888323545456
epoch£º1213	 i:3 	 global-step:24263	 l-p:0.00994184985756874
epoch£º1213	 i:4 	 global-step:24264	 l-p:0.17251139879226685
epoch£º1213	 i:5 	 global-step:24265	 l-p:0.12075051665306091
epoch£º1213	 i:6 	 global-step:24266	 l-p:0.2444097399711609
epoch£º1213	 i:7 	 global-step:24267	 l-p:0.12993498146533966
epoch£º1213	 i:8 	 global-step:24268	 l-p:0.12750615179538727
epoch£º1213	 i:9 	 global-step:24269	 l-p:0.1585633009672165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6951, 3.3983, 3.5266],
        [3.6951, 2.8669, 2.2690],
        [3.6951, 3.5757, 3.6622],
        [3.6951, 3.5496, 3.6485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.13495859503746033 
model_pd.l_d.mean(): -23.46992301940918 
model_pd.lagr.mean(): -23.334964752197266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2039], device='cuda:0')), ('power', tensor([-23.6738], device='cuda:0'))])
epoch£º1214	 i:0 	 global-step:24280	 l-p:0.13495859503746033
epoch£º1214	 i:1 	 global-step:24281	 l-p:0.01722109690308571
epoch£º1214	 i:2 	 global-step:24282	 l-p:0.1681288331747055
epoch£º1214	 i:3 	 global-step:24283	 l-p:-0.0009685230324976146
epoch£º1214	 i:4 	 global-step:24284	 l-p:0.13286267220973969
epoch£º1214	 i:5 	 global-step:24285	 l-p:0.12616463005542755
epoch£º1214	 i:6 	 global-step:24286	 l-p:0.2824517488479614
epoch£º1214	 i:7 	 global-step:24287	 l-p:0.13722756505012512
epoch£º1214	 i:8 	 global-step:24288	 l-p:0.11203446239233017
epoch£º1214	 i:9 	 global-step:24289	 l-p:0.0693821981549263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6933, 2.8677, 2.2575],
        [3.6933, 2.8733, 2.2497],
        [3.6933, 3.1915, 3.2401],
        [3.6933, 3.6933, 3.6933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.12917791306972504 
model_pd.l_d.mean(): -23.595666885375977 
model_pd.lagr.mean(): -23.466489791870117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1894], device='cuda:0')), ('power', tensor([-23.7850], device='cuda:0'))])
epoch£º1215	 i:0 	 global-step:24300	 l-p:0.12917791306972504
epoch£º1215	 i:1 	 global-step:24301	 l-p:-0.07280095666646957
epoch£º1215	 i:2 	 global-step:24302	 l-p:0.11692500859498978
epoch£º1215	 i:3 	 global-step:24303	 l-p:0.1317603886127472
epoch£º1215	 i:4 	 global-step:24304	 l-p:0.18566392362117767
epoch£º1215	 i:5 	 global-step:24305	 l-p:0.1618584394454956
epoch£º1215	 i:6 	 global-step:24306	 l-p:0.14321444928646088
epoch£º1215	 i:7 	 global-step:24307	 l-p:0.14722289144992828
epoch£º1215	 i:8 	 global-step:24308	 l-p:0.03141846880316734
epoch£º1215	 i:9 	 global-step:24309	 l-p:0.25638025999069214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6906, 2.8700, 2.2466],
        [3.6906, 2.9332, 2.2780],
        [3.6906, 3.5119, 3.6235],
        [3.6906, 3.6899, 3.6906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.12103286385536194 
model_pd.l_d.mean(): -22.843544006347656 
model_pd.lagr.mean(): -22.722511291503906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2102], device='cuda:0')), ('power', tensor([-23.0538], device='cuda:0'))])
epoch£º1216	 i:0 	 global-step:24320	 l-p:0.12103286385536194
epoch£º1216	 i:1 	 global-step:24321	 l-p:0.13038764894008636
epoch£º1216	 i:2 	 global-step:24322	 l-p:0.14426809549331665
epoch£º1216	 i:3 	 global-step:24323	 l-p:0.12801192700862885
epoch£º1216	 i:4 	 global-step:24324	 l-p:0.18263962864875793
epoch£º1216	 i:5 	 global-step:24325	 l-p:0.20420867204666138
epoch£º1216	 i:6 	 global-step:24326	 l-p:0.12086658179759979
epoch£º1216	 i:7 	 global-step:24327	 l-p:0.05492762476205826
epoch£º1216	 i:8 	 global-step:24328	 l-p:0.14619240164756775
epoch£º1216	 i:9 	 global-step:24329	 l-p:0.08552133291959763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6871, 3.6815, 3.6869],
        [3.6871, 2.9943, 2.8235],
        [3.6871, 2.8755, 2.2399],
        [3.6871, 3.3159, 3.4320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.13145938515663147 
model_pd.l_d.mean(): -22.865297317504883 
model_pd.lagr.mean(): -22.733837127685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2917], device='cuda:0')), ('power', tensor([-23.1570], device='cuda:0'))])
epoch£º1217	 i:0 	 global-step:24340	 l-p:0.13145938515663147
epoch£º1217	 i:1 	 global-step:24341	 l-p:0.1295202672481537
epoch£º1217	 i:2 	 global-step:24342	 l-p:0.13424667716026306
epoch£º1217	 i:3 	 global-step:24343	 l-p:0.15590839087963104
epoch£º1217	 i:4 	 global-step:24344	 l-p:0.21830804646015167
epoch£º1217	 i:5 	 global-step:24345	 l-p:0.1255635768175125
epoch£º1217	 i:6 	 global-step:24346	 l-p:0.14283806085586548
epoch£º1217	 i:7 	 global-step:24347	 l-p:0.12784093618392944
epoch£º1217	 i:8 	 global-step:24348	 l-p:0.13660623133182526
epoch£º1217	 i:9 	 global-step:24349	 l-p:0.07995285838842392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6811, 2.8533, 2.2440],
        [3.6811, 2.9431, 2.6916],
        [3.6811, 3.6811, 3.6811],
        [3.6811, 3.4587, 3.5817]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.10783392935991287 
model_pd.l_d.mean(): -23.12360191345215 
model_pd.lagr.mean(): -23.01576805114746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2524], device='cuda:0')), ('power', tensor([-23.3760], device='cuda:0'))])
epoch£º1218	 i:0 	 global-step:24360	 l-p:0.10783392935991287
epoch£º1218	 i:1 	 global-step:24361	 l-p:0.14880520105361938
epoch£º1218	 i:2 	 global-step:24362	 l-p:0.14870792627334595
epoch£º1218	 i:3 	 global-step:24363	 l-p:0.13276119530200958
epoch£º1218	 i:4 	 global-step:24364	 l-p:0.1391526311635971
epoch£º1218	 i:5 	 global-step:24365	 l-p:0.5065436959266663
epoch£º1218	 i:6 	 global-step:24366	 l-p:0.14252015948295593
epoch£º1218	 i:7 	 global-step:24367	 l-p:0.21100576221942902
epoch£º1218	 i:8 	 global-step:24368	 l-p:0.12968549132347107
epoch£º1218	 i:9 	 global-step:24369	 l-p:0.143724724650383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6707, 2.9067, 2.6020],
        [3.6707, 3.5125, 3.6168],
        [3.6707, 3.5983, 3.6569],
        [3.6707, 3.6707, 3.6707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.12460291385650635 
model_pd.l_d.mean(): -22.78272819519043 
model_pd.lagr.mean(): -22.658124923706055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2605], device='cuda:0')), ('power', tensor([-23.0432], device='cuda:0'))])
epoch£º1219	 i:0 	 global-step:24380	 l-p:0.12460291385650635
epoch£º1219	 i:1 	 global-step:24381	 l-p:0.13176743686199188
epoch£º1219	 i:2 	 global-step:24382	 l-p:-1.0761516094207764
epoch£º1219	 i:3 	 global-step:24383	 l-p:0.14009641110897064
epoch£º1219	 i:4 	 global-step:24384	 l-p:0.24185584485530853
epoch£º1219	 i:5 	 global-step:24385	 l-p:0.14640045166015625
epoch£º1219	 i:6 	 global-step:24386	 l-p:0.10496214777231216
epoch£º1219	 i:7 	 global-step:24387	 l-p:0.11973164975643158
epoch£º1219	 i:8 	 global-step:24388	 l-p:0.1541731059551239
epoch£º1219	 i:9 	 global-step:24389	 l-p:0.07921520620584488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6642, 3.6641, 3.6642],
        [3.6642, 3.0764, 2.4396],
        [3.6642, 2.8769, 2.2292],
        [3.6642, 2.8394, 2.2176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.13621973991394043 
model_pd.l_d.mean(): -23.64879608154297 
model_pd.lagr.mean(): -23.512577056884766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1817], device='cuda:0')), ('power', tensor([-23.8305], device='cuda:0'))])
epoch£º1220	 i:0 	 global-step:24400	 l-p:0.13621973991394043
epoch£º1220	 i:1 	 global-step:24401	 l-p:0.13306821882724762
epoch£º1220	 i:2 	 global-step:24402	 l-p:0.28474998474121094
epoch£º1220	 i:3 	 global-step:24403	 l-p:0.09455844759941101
epoch£º1220	 i:4 	 global-step:24404	 l-p:0.12179575860500336
epoch£º1220	 i:5 	 global-step:24405	 l-p:0.12022162973880768
epoch£º1220	 i:6 	 global-step:24406	 l-p:-0.15355463325977325
epoch£º1220	 i:7 	 global-step:24407	 l-p:0.12714815139770508
epoch£º1220	 i:8 	 global-step:24408	 l-p:0.12849250435829163
epoch£º1220	 i:9 	 global-step:24409	 l-p:0.08025854080915451
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6636, 3.6616, 3.6635],
        [3.6636, 3.6635, 3.6636],
        [3.6636, 2.9125, 2.2603],
        [3.6636, 2.9045, 2.6130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): -0.24149863421916962 
model_pd.l_d.mean(): -23.46169662475586 
model_pd.lagr.mean(): -23.703195571899414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2577], device='cuda:0')), ('power', tensor([-23.7194], device='cuda:0'))])
epoch£º1221	 i:0 	 global-step:24420	 l-p:-0.24149863421916962
epoch£º1221	 i:1 	 global-step:24421	 l-p:0.07171624898910522
epoch£º1221	 i:2 	 global-step:24422	 l-p:0.13570445775985718
epoch£º1221	 i:3 	 global-step:24423	 l-p:0.148200124502182
epoch£º1221	 i:4 	 global-step:24424	 l-p:0.12903784215450287
epoch£º1221	 i:5 	 global-step:24425	 l-p:0.13607382774353027
epoch£º1221	 i:6 	 global-step:24426	 l-p:0.14721174538135529
epoch£º1221	 i:7 	 global-step:24427	 l-p:0.15624186396598816
epoch£º1221	 i:8 	 global-step:24428	 l-p:0.25368160009384155
epoch£º1221	 i:9 	 global-step:24429	 l-p:0.11923876404762268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6648, 3.6648, 3.6648],
        [3.6648, 3.5607, 3.6389],
        [3.6648, 3.6341, 3.6615],
        [3.6648, 3.6638, 3.6648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.1215851753950119 
model_pd.l_d.mean(): -23.660133361816406 
model_pd.lagr.mean(): -23.53854751586914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1557], device='cuda:0')), ('power', tensor([-23.8159], device='cuda:0'))])
epoch£º1222	 i:0 	 global-step:24440	 l-p:0.1215851753950119
epoch£º1222	 i:1 	 global-step:24441	 l-p:0.13312192261219025
epoch£º1222	 i:2 	 global-step:24442	 l-p:0.13686765730381012
epoch£º1222	 i:3 	 global-step:24443	 l-p:0.2676931321620941
epoch£º1222	 i:4 	 global-step:24444	 l-p:0.0709061250090599
epoch£º1222	 i:5 	 global-step:24445	 l-p:-0.17455615103244781
epoch£º1222	 i:6 	 global-step:24446	 l-p:0.12632523477077484
epoch£º1222	 i:7 	 global-step:24447	 l-p:0.14120100438594818
epoch£º1222	 i:8 	 global-step:24448	 l-p:0.19027628004550934
epoch£º1222	 i:9 	 global-step:24449	 l-p:0.1289421170949936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6611, 3.5673, 3.6396],
        [3.6611, 3.6360, 3.6588],
        [3.6611, 3.5103, 3.6116],
        [3.6611, 3.2069, 3.2886]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.11722211539745331 
model_pd.l_d.mean(): -22.87103843688965 
model_pd.lagr.mean(): -22.753816604614258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2666], device='cuda:0')), ('power', tensor([-23.1376], device='cuda:0'))])
epoch£º1223	 i:0 	 global-step:24460	 l-p:0.11722211539745331
epoch£º1223	 i:1 	 global-step:24461	 l-p:-0.0900602862238884
epoch£º1223	 i:2 	 global-step:24462	 l-p:0.13570262491703033
epoch£º1223	 i:3 	 global-step:24463	 l-p:0.10133566707372665
epoch£º1223	 i:4 	 global-step:24464	 l-p:0.07770132273435593
epoch£º1223	 i:5 	 global-step:24465	 l-p:0.32688820362091064
epoch£º1223	 i:6 	 global-step:24466	 l-p:0.11981851607561111
epoch£º1223	 i:7 	 global-step:24467	 l-p:0.10059358179569244
epoch£º1223	 i:8 	 global-step:24468	 l-p:0.20904940366744995
epoch£º1223	 i:9 	 global-step:24469	 l-p:0.13010330498218536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6597, 3.5071, 3.6092],
        [3.6597, 3.6597, 3.6597],
        [3.6597, 3.1799, 2.5621],
        [3.6597, 2.8857, 2.5611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.14143165946006775 
model_pd.l_d.mean(): -22.681623458862305 
model_pd.lagr.mean(): -22.540191650390625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2769], device='cuda:0')), ('power', tensor([-22.9585], device='cuda:0'))])
epoch£º1224	 i:0 	 global-step:24480	 l-p:0.14143165946006775
epoch£º1224	 i:1 	 global-step:24481	 l-p:-0.10871714353561401
epoch£º1224	 i:2 	 global-step:24482	 l-p:0.13103295862674713
epoch£º1224	 i:3 	 global-step:24483	 l-p:0.285459965467453
epoch£º1224	 i:4 	 global-step:24484	 l-p:0.15126565098762512
epoch£º1224	 i:5 	 global-step:24485	 l-p:0.08903041481971741
epoch£º1224	 i:6 	 global-step:24486	 l-p:0.13664820790290833
epoch£º1224	 i:7 	 global-step:24487	 l-p:0.08442344516515732
epoch£º1224	 i:8 	 global-step:24488	 l-p:0.17919857800006866
epoch£º1224	 i:9 	 global-step:24489	 l-p:0.1319703459739685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6643, 3.6229, 3.6589],
        [3.6643, 3.6639, 3.6643],
        [3.6643, 3.6306, 3.6605],
        [3.6643, 3.6643, 3.6643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): -0.24504569172859192 
model_pd.l_d.mean(): -23.44346809387207 
model_pd.lagr.mean(): -23.688514709472656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1798], device='cuda:0')), ('power', tensor([-23.6233], device='cuda:0'))])
epoch£º1225	 i:0 	 global-step:24500	 l-p:-0.24504569172859192
epoch£º1225	 i:1 	 global-step:24501	 l-p:0.11114318668842316
epoch£º1225	 i:2 	 global-step:24502	 l-p:0.13631406426429749
epoch£º1225	 i:3 	 global-step:24503	 l-p:0.12508317828178406
epoch£º1225	 i:4 	 global-step:24504	 l-p:0.058362655341625214
epoch£º1225	 i:5 	 global-step:24505	 l-p:0.1235976591706276
epoch£º1225	 i:6 	 global-step:24506	 l-p:0.14399923384189606
epoch£º1225	 i:7 	 global-step:24507	 l-p:0.2530777156352997
epoch£º1225	 i:8 	 global-step:24508	 l-p:0.1471824198961258
epoch£º1225	 i:9 	 global-step:24509	 l-p:0.1428806185722351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6680, 3.4127, 3.5406],
        [3.6680, 2.8421, 2.3515],
        [3.6680, 3.6680, 3.6680],
        [3.6680, 3.6679, 3.6680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.18189239501953125 
model_pd.l_d.mean(): -22.661476135253906 
model_pd.lagr.mean(): -22.479583740234375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3382], device='cuda:0')), ('power', tensor([-22.9997], device='cuda:0'))])
epoch£º1226	 i:0 	 global-step:24520	 l-p:0.18189239501953125
epoch£º1226	 i:1 	 global-step:24521	 l-p:0.15632465481758118
epoch£º1226	 i:2 	 global-step:24522	 l-p:0.10860678553581238
epoch£º1226	 i:3 	 global-step:24523	 l-p:0.15244971215724945
epoch£º1226	 i:4 	 global-step:24524	 l-p:-0.5590065717697144
epoch£º1226	 i:5 	 global-step:24525	 l-p:0.13871198892593384
epoch£º1226	 i:6 	 global-step:24526	 l-p:0.03795522451400757
epoch£º1226	 i:7 	 global-step:24527	 l-p:0.2329310178756714
epoch£º1226	 i:8 	 global-step:24528	 l-p:0.14356465637683868
epoch£º1226	 i:9 	 global-step:24529	 l-p:0.07453712075948715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6669, 3.6570, 3.6664],
        [3.6669, 3.6648, 3.6668],
        [3.6669, 3.4954, 3.6047],
        [3.6669, 3.6669, 3.6669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.1751006692647934 
model_pd.l_d.mean(): -23.294429779052734 
model_pd.lagr.mean(): -23.11932945251465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2604], device='cuda:0')), ('power', tensor([-23.5548], device='cuda:0'))])
epoch£º1227	 i:0 	 global-step:24540	 l-p:0.1751006692647934
epoch£º1227	 i:1 	 global-step:24541	 l-p:0.15579523146152496
epoch£º1227	 i:2 	 global-step:24542	 l-p:0.11174533516168594
epoch£º1227	 i:3 	 global-step:24543	 l-p:0.13768431544303894
epoch£º1227	 i:4 	 global-step:24544	 l-p:0.02653578296303749
epoch£º1227	 i:5 	 global-step:24545	 l-p:0.24179524183273315
epoch£º1227	 i:6 	 global-step:24546	 l-p:0.08478406071662903
epoch£º1227	 i:7 	 global-step:24547	 l-p:0.13674752414226532
epoch£º1227	 i:8 	 global-step:24548	 l-p:0.1331402212381363
epoch£º1227	 i:9 	 global-step:24549	 l-p:-35.85050582885742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6715, 2.8537, 2.2226],
        [3.6715, 3.6515, 3.6699],
        [3.6715, 3.6659, 3.6713],
        [3.6715, 2.8471, 2.2253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.19045674800872803 
model_pd.l_d.mean(): -23.481937408447266 
model_pd.lagr.mean(): -23.291481018066406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1964], device='cuda:0')), ('power', tensor([-23.6784], device='cuda:0'))])
epoch£º1228	 i:0 	 global-step:24560	 l-p:0.19045674800872803
epoch£º1228	 i:1 	 global-step:24561	 l-p:0.1861516684293747
epoch£º1228	 i:2 	 global-step:24562	 l-p:0.08146369457244873
epoch£º1228	 i:3 	 global-step:24563	 l-p:0.1231970563530922
epoch£º1228	 i:4 	 global-step:24564	 l-p:0.12306974083185196
epoch£º1228	 i:5 	 global-step:24565	 l-p:0.15643830597400665
epoch£º1228	 i:6 	 global-step:24566	 l-p:0.5895482301712036
epoch£º1228	 i:7 	 global-step:24567	 l-p:0.1548873782157898
epoch£º1228	 i:8 	 global-step:24568	 l-p:0.10447884351015091
epoch£º1228	 i:9 	 global-step:24569	 l-p:0.12588843703269958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6751, 3.5788, 3.6525],
        [3.6751, 3.6610, 3.6742],
        [3.6751, 2.9366, 2.6859],
        [3.6751, 2.9808, 2.8100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.16628123819828033 
model_pd.l_d.mean(): -22.806598663330078 
model_pd.lagr.mean(): -22.640317916870117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2361], device='cuda:0')), ('power', tensor([-23.0427], device='cuda:0'))])
epoch£º1229	 i:0 	 global-step:24580	 l-p:0.16628123819828033
epoch£º1229	 i:1 	 global-step:24581	 l-p:0.1117042675614357
epoch£º1229	 i:2 	 global-step:24582	 l-p:0.12496006488800049
epoch£º1229	 i:3 	 global-step:24583	 l-p:0.13596636056900024
epoch£º1229	 i:4 	 global-step:24584	 l-p:0.1406635195016861
epoch£º1229	 i:5 	 global-step:24585	 l-p:0.11270647495985031
epoch£º1229	 i:6 	 global-step:24586	 l-p:0.12848526239395142
epoch£º1229	 i:7 	 global-step:24587	 l-p:-12.292402267456055
epoch£º1229	 i:8 	 global-step:24588	 l-p:0.13543014228343964
epoch£º1229	 i:9 	 global-step:24589	 l-p:0.06045951694250107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6683, 3.5655, 3.6430],
        [3.6683, 3.6525, 3.6672],
        [3.6683, 3.6673, 3.6683],
        [3.6683, 2.8628, 2.2217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.19601507484912872 
model_pd.l_d.mean(): -23.749393463134766 
model_pd.lagr.mean(): -23.55337905883789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1420], device='cuda:0')), ('power', tensor([-23.8914], device='cuda:0'))])
epoch£º1230	 i:0 	 global-step:24600	 l-p:0.19601507484912872
epoch£º1230	 i:1 	 global-step:24601	 l-p:0.14220696687698364
epoch£º1230	 i:2 	 global-step:24602	 l-p:0.10627821087837219
epoch£º1230	 i:3 	 global-step:24603	 l-p:0.12437538802623749
epoch£º1230	 i:4 	 global-step:24604	 l-p:-0.44767555594444275
epoch£º1230	 i:5 	 global-step:24605	 l-p:0.07079160213470459
epoch£º1230	 i:6 	 global-step:24606	 l-p:0.1322341412305832
epoch£º1230	 i:7 	 global-step:24607	 l-p:0.26309290528297424
epoch£º1230	 i:8 	 global-step:24608	 l-p:0.0636797770857811
epoch£º1230	 i:9 	 global-step:24609	 l-p:0.139272078871727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6678, 2.8340, 2.2924],
        [3.6678, 3.6677, 3.6678],
        [3.6678, 3.6678, 3.6679],
        [3.6678, 2.8604, 2.2205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.1744678020477295 
model_pd.l_d.mean(): -23.58114242553711 
model_pd.lagr.mean(): -23.406675338745117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1864], device='cuda:0')), ('power', tensor([-23.7676], device='cuda:0'))])
epoch£º1231	 i:0 	 global-step:24620	 l-p:0.1744678020477295
epoch£º1231	 i:1 	 global-step:24621	 l-p:0.13117077946662903
epoch£º1231	 i:2 	 global-step:24622	 l-p:0.14839796721935272
epoch£º1231	 i:3 	 global-step:24623	 l-p:0.1360693871974945
epoch£º1231	 i:4 	 global-step:24624	 l-p:0.16542647778987885
epoch£º1231	 i:5 	 global-step:24625	 l-p:0.1447158008813858
epoch£º1231	 i:6 	 global-step:24626	 l-p:0.1212778091430664
epoch£º1231	 i:7 	 global-step:24627	 l-p:0.2509142756462097
epoch£º1231	 i:8 	 global-step:24628	 l-p:0.08989511430263519
epoch£º1231	 i:9 	 global-step:24629	 l-p:-0.4558679163455963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6662, 2.8357, 2.3222],
        [3.6662, 3.6367, 3.6631],
        [3.6662, 2.8505, 2.4039],
        [3.6662, 3.0721, 3.0381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.1512279063463211 
model_pd.l_d.mean(): -23.677759170532227 
model_pd.lagr.mean(): -23.526531219482422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1797], device='cuda:0')), ('power', tensor([-23.8575], device='cuda:0'))])
epoch£º1232	 i:0 	 global-step:24640	 l-p:0.1512279063463211
epoch£º1232	 i:1 	 global-step:24641	 l-p:0.10984741151332855
epoch£º1232	 i:2 	 global-step:24642	 l-p:0.13061325252056122
epoch£º1232	 i:3 	 global-step:24643	 l-p:0.20463821291923523
epoch£º1232	 i:4 	 global-step:24644	 l-p:-0.42067548632621765
epoch£º1232	 i:5 	 global-step:24645	 l-p:0.13642306625843048
epoch£º1232	 i:6 	 global-step:24646	 l-p:0.11606693267822266
epoch£º1232	 i:7 	 global-step:24647	 l-p:0.13368408381938934
epoch£º1232	 i:8 	 global-step:24648	 l-p:0.13541942834854126
epoch£º1232	 i:9 	 global-step:24649	 l-p:0.10888034850358963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6675, 3.0331, 2.9508],
        [3.6675, 2.8669, 2.2233],
        [3.6675, 3.6675, 3.6675],
        [3.6675, 3.6512, 3.6664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 0.05710918456315994 
model_pd.l_d.mean(): -23.535362243652344 
model_pd.lagr.mean(): -23.478252410888672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1719], device='cuda:0')), ('power', tensor([-23.7073], device='cuda:0'))])
epoch£º1233	 i:0 	 global-step:24660	 l-p:0.05710918456315994
epoch£º1233	 i:1 	 global-step:24661	 l-p:0.11473604291677475
epoch£º1233	 i:2 	 global-step:24662	 l-p:0.1988309621810913
epoch£º1233	 i:3 	 global-step:24663	 l-p:0.08696305751800537
epoch£º1233	 i:4 	 global-step:24664	 l-p:-0.45555391907691956
epoch£º1233	 i:5 	 global-step:24665	 l-p:0.1527346968650818
epoch£º1233	 i:6 	 global-step:24666	 l-p:0.16526474058628082
epoch£º1233	 i:7 	 global-step:24667	 l-p:0.11499787867069244
epoch£º1233	 i:8 	 global-step:24668	 l-p:0.14057514071464539
epoch£º1233	 i:9 	 global-step:24669	 l-p:0.18032315373420715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6672, 3.6642, 3.6671],
        [3.6672, 3.6671, 3.6672],
        [3.6672, 3.6672, 3.6672],
        [3.6672, 3.0817, 3.0571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.12368008494377136 
model_pd.l_d.mean(): -23.607799530029297 
model_pd.lagr.mean(): -23.484119415283203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1248], device='cuda:0')), ('power', tensor([-23.7326], device='cuda:0'))])
epoch£º1234	 i:0 	 global-step:24680	 l-p:0.12368008494377136
epoch£º1234	 i:1 	 global-step:24681	 l-p:0.15316921472549438
epoch£º1234	 i:2 	 global-step:24682	 l-p:0.11910063773393631
epoch£º1234	 i:3 	 global-step:24683	 l-p:0.14817993342876434
epoch£º1234	 i:4 	 global-step:24684	 l-p:0.10005559772253036
epoch£º1234	 i:5 	 global-step:24685	 l-p:0.1615176796913147
epoch£º1234	 i:6 	 global-step:24686	 l-p:0.16565540432929993
epoch£º1234	 i:7 	 global-step:24687	 l-p:0.07819311320781708
epoch£º1234	 i:8 	 global-step:24688	 l-p:-0.27230164408683777
epoch£º1234	 i:9 	 global-step:24689	 l-p:0.26235097646713257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6655, 3.5851, 3.6490],
        [3.6655, 3.6600, 3.6653],
        [3.6655, 3.6008, 3.6541],
        [3.6655, 3.6505, 3.6645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.1302686482667923 
model_pd.l_d.mean(): -23.58200454711914 
model_pd.lagr.mean(): -23.451736450195312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1436], device='cuda:0')), ('power', tensor([-23.7256], device='cuda:0'))])
epoch£º1235	 i:0 	 global-step:24700	 l-p:0.1302686482667923
epoch£º1235	 i:1 	 global-step:24701	 l-p:0.13629892468452454
epoch£º1235	 i:2 	 global-step:24702	 l-p:0.16217918694019318
epoch£º1235	 i:3 	 global-step:24703	 l-p:0.14163744449615479
epoch£º1235	 i:4 	 global-step:24704	 l-p:-0.28123876452445984
epoch£º1235	 i:5 	 global-step:24705	 l-p:0.08458522707223892
epoch£º1235	 i:6 	 global-step:24706	 l-p:0.19919955730438232
epoch£º1235	 i:7 	 global-step:24707	 l-p:0.1745651513338089
epoch£º1235	 i:8 	 global-step:24708	 l-p:0.1223137229681015
epoch£º1235	 i:9 	 global-step:24709	 l-p:0.11668231338262558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6673, 3.0732, 3.0392],
        [3.6673, 3.3308, 3.4555],
        [3.6673, 3.3441, 3.4708],
        [3.6673, 3.6673, 3.6673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.2567768692970276 
model_pd.l_d.mean(): -23.390737533569336 
model_pd.lagr.mean(): -23.133960723876953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2585], device='cuda:0')), ('power', tensor([-23.6492], device='cuda:0'))])
epoch£º1236	 i:0 	 global-step:24720	 l-p:0.2567768692970276
epoch£º1236	 i:1 	 global-step:24721	 l-p:0.13256937265396118
epoch£º1236	 i:2 	 global-step:24722	 l-p:0.15037629008293152
epoch£º1236	 i:3 	 global-step:24723	 l-p:0.12129203230142593
epoch£º1236	 i:4 	 global-step:24724	 l-p:0.1323469877243042
epoch£º1236	 i:5 	 global-step:24725	 l-p:0.09897688031196594
epoch£º1236	 i:6 	 global-step:24726	 l-p:0.12964807450771332
epoch£º1236	 i:7 	 global-step:24727	 l-p:0.07155458629131317
epoch£º1236	 i:8 	 global-step:24728	 l-p:0.16065311431884766
epoch£º1236	 i:9 	 global-step:24729	 l-p:-0.5016863346099854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6673, 3.6673, 3.6673],
        [3.6673, 3.6673, 3.6673],
        [3.6673, 3.6664, 3.6673],
        [3.6673, 3.5831, 3.6494]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): 0.07429322600364685 
model_pd.l_d.mean(): -23.19285774230957 
model_pd.lagr.mean(): -23.11856460571289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1981], device='cuda:0')), ('power', tensor([-23.3909], device='cuda:0'))])
epoch£º1237	 i:0 	 global-step:24740	 l-p:0.07429322600364685
epoch£º1237	 i:1 	 global-step:24741	 l-p:0.16069653630256653
epoch£º1237	 i:2 	 global-step:24742	 l-p:0.1337878257036209
epoch£º1237	 i:3 	 global-step:24743	 l-p:0.0673869177699089
epoch£º1237	 i:4 	 global-step:24744	 l-p:0.1360587328672409
epoch£º1237	 i:5 	 global-step:24745	 l-p:0.13521133363246918
epoch£º1237	 i:6 	 global-step:24746	 l-p:0.08809105306863785
epoch£º1237	 i:7 	 global-step:24747	 l-p:0.17368443310260773
epoch£º1237	 i:8 	 global-step:24748	 l-p:-0.39591503143310547
epoch£º1237	 i:9 	 global-step:24749	 l-p:0.29284849762916565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6677, 2.9269, 2.6739],
        [3.6677, 3.0823, 3.0578],
        [3.6677, 3.5480, 3.6347],
        [3.6677, 3.5832, 3.6497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.12948785722255707 
model_pd.l_d.mean(): -23.84027099609375 
model_pd.lagr.mean(): -23.710783004760742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0846], device='cuda:0')), ('power', tensor([-23.9249], device='cuda:0'))])
epoch£º1238	 i:0 	 global-step:24760	 l-p:0.12948785722255707
epoch£º1238	 i:1 	 global-step:24761	 l-p:0.17444062232971191
epoch£º1238	 i:2 	 global-step:24762	 l-p:0.1444094181060791
epoch£º1238	 i:3 	 global-step:24763	 l-p:0.10828346014022827
epoch£º1238	 i:4 	 global-step:24764	 l-p:-1.2768328189849854
epoch£º1238	 i:5 	 global-step:24765	 l-p:0.08838734775781631
epoch£º1238	 i:6 	 global-step:24766	 l-p:0.16550737619400024
epoch£º1238	 i:7 	 global-step:24767	 l-p:0.11942464858293533
epoch£º1238	 i:8 	 global-step:24768	 l-p:0.23011068999767303
epoch£º1238	 i:9 	 global-step:24769	 l-p:0.06296993792057037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6739, 3.6731, 3.6739],
        [3.6739, 3.6599, 3.6730],
        [3.6739, 3.0146, 2.3663],
        [3.6739, 3.6500, 3.6718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.22303178906440735 
model_pd.l_d.mean(): -23.765850067138672 
model_pd.lagr.mean(): -23.542818069458008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1257], device='cuda:0')), ('power', tensor([-23.8915], device='cuda:0'))])
epoch£º1239	 i:0 	 global-step:24780	 l-p:0.22303178906440735
epoch£º1239	 i:1 	 global-step:24781	 l-p:0.13691942393779755
epoch£º1239	 i:2 	 global-step:24782	 l-p:0.7044705748558044
epoch£º1239	 i:3 	 global-step:24783	 l-p:0.07746197283267975
epoch£º1239	 i:4 	 global-step:24784	 l-p:0.10259266942739487
epoch£º1239	 i:5 	 global-step:24785	 l-p:0.1723351776599884
epoch£º1239	 i:6 	 global-step:24786	 l-p:0.05358140170574188
epoch£º1239	 i:7 	 global-step:24787	 l-p:0.12291686981916428
epoch£º1239	 i:8 	 global-step:24788	 l-p:0.1719241440296173
epoch£º1239	 i:9 	 global-step:24789	 l-p:0.06897082924842834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6859, 3.2325, 2.6163],
        [3.6859, 3.1539, 2.5229],
        [3.6859, 3.4547, 3.5793],
        [3.6859, 2.8542, 2.2628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.11864537745714188 
model_pd.l_d.mean(): -23.2751522064209 
model_pd.lagr.mean(): -23.15650749206543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2530], device='cuda:0')), ('power', tensor([-23.5281], device='cuda:0'))])
epoch£º1240	 i:0 	 global-step:24800	 l-p:0.11864537745714188
epoch£º1240	 i:1 	 global-step:24801	 l-p:0.06604930013418198
epoch£º1240	 i:2 	 global-step:24802	 l-p:0.29374048113822937
epoch£º1240	 i:3 	 global-step:24803	 l-p:0.06858083605766296
epoch£º1240	 i:4 	 global-step:24804	 l-p:0.12330605834722519
epoch£º1240	 i:5 	 global-step:24805	 l-p:-0.024892505258321762
epoch£º1240	 i:6 	 global-step:24806	 l-p:0.1276669055223465
epoch£º1240	 i:7 	 global-step:24807	 l-p:0.1742611974477768
epoch£º1240	 i:8 	 global-step:24808	 l-p:0.1763167679309845
epoch£º1240	 i:9 	 global-step:24809	 l-p:0.13252423703670502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7003, 3.4555, 3.5819],
        [3.7003, 3.0532, 2.9503],
        [3.7003, 3.2813, 3.3795],
        [3.7003, 3.5739, 3.6640]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): -0.08798215538263321 
model_pd.l_d.mean(): -22.330955505371094 
model_pd.lagr.mean(): -22.41893768310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3492], device='cuda:0')), ('power', tensor([-22.6801], device='cuda:0'))])
epoch£º1241	 i:0 	 global-step:24820	 l-p:-0.08798215538263321
epoch£º1241	 i:1 	 global-step:24821	 l-p:0.1499514877796173
epoch£º1241	 i:2 	 global-step:24822	 l-p:0.03771178051829338
epoch£º1241	 i:3 	 global-step:24823	 l-p:0.12530939280986786
epoch£º1241	 i:4 	 global-step:24824	 l-p:0.15403427183628082
epoch£º1241	 i:5 	 global-step:24825	 l-p:0.14260689914226532
epoch£º1241	 i:6 	 global-step:24826	 l-p:0.12025249004364014
epoch£º1241	 i:7 	 global-step:24827	 l-p:0.2408890426158905
epoch£º1241	 i:8 	 global-step:24828	 l-p:-0.0662589743733406
epoch£º1241	 i:9 	 global-step:24829	 l-p:0.14730915427207947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7070, 2.9136, 2.2649],
        [3.7070, 2.9784, 2.3198],
        [3.7070, 3.7070, 3.7070],
        [3.7070, 3.6993, 3.7067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.17048576474189758 
model_pd.l_d.mean(): -23.282756805419922 
model_pd.lagr.mean(): -23.11227035522461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2638], device='cuda:0')), ('power', tensor([-23.5465], device='cuda:0'))])
epoch£º1242	 i:0 	 global-step:24840	 l-p:0.17048576474189758
epoch£º1242	 i:1 	 global-step:24841	 l-p:0.10709556192159653
epoch£º1242	 i:2 	 global-step:24842	 l-p:-0.0060748145915567875
epoch£º1242	 i:3 	 global-step:24843	 l-p:0.22433744370937347
epoch£º1242	 i:4 	 global-step:24844	 l-p:0.1212477907538414
epoch£º1242	 i:5 	 global-step:24845	 l-p:0.13418255746364594
epoch£º1242	 i:6 	 global-step:24846	 l-p:-0.7025468945503235
epoch£º1242	 i:7 	 global-step:24847	 l-p:0.13377684354782104
epoch£º1242	 i:8 	 global-step:24848	 l-p:-0.09933695942163467
epoch£º1242	 i:9 	 global-step:24849	 l-p:0.13419850170612335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7103, 3.7096, 3.7103],
        [3.7103, 3.5582, 3.6599],
        [3.7103, 3.4612, 3.5880],
        [3.7103, 3.0389, 2.8996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): -0.9593522548675537 
model_pd.l_d.mean(): -23.362871170043945 
model_pd.lagr.mean(): -24.322223663330078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2064], device='cuda:0')), ('power', tensor([-23.5693], device='cuda:0'))])
epoch£º1243	 i:0 	 global-step:24860	 l-p:-0.9593522548675537
epoch£º1243	 i:1 	 global-step:24861	 l-p:-0.025025557726621628
epoch£º1243	 i:2 	 global-step:24862	 l-p:0.17172546684741974
epoch£º1243	 i:3 	 global-step:24863	 l-p:0.10512115061283112
epoch£º1243	 i:4 	 global-step:24864	 l-p:0.13103872537612915
epoch£º1243	 i:5 	 global-step:24865	 l-p:-0.17204833030700684
epoch£º1243	 i:6 	 global-step:24866	 l-p:0.13035206496715546
epoch£º1243	 i:7 	 global-step:24867	 l-p:0.17250999808311462
epoch£º1243	 i:8 	 global-step:24868	 l-p:0.18802376091480255
epoch£º1243	 i:9 	 global-step:24869	 l-p:0.13503989577293396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7144, 3.5819, 3.6749],
        [3.7144, 3.4216, 3.5499],
        [3.7144, 3.2386, 3.3047],
        [3.7144, 3.7144, 3.7144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.14306430518627167 
model_pd.l_d.mean(): -23.346174240112305 
model_pd.lagr.mean(): -23.203109741210938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1495], device='cuda:0')), ('power', tensor([-23.4957], device='cuda:0'))])
epoch£º1244	 i:0 	 global-step:24880	 l-p:0.14306430518627167
epoch£º1244	 i:1 	 global-step:24881	 l-p:0.17548514902591705
epoch£º1244	 i:2 	 global-step:24882	 l-p:-0.24329827725887299
epoch£º1244	 i:3 	 global-step:24883	 l-p:0.1297782063484192
epoch£º1244	 i:4 	 global-step:24884	 l-p:0.11868537962436676
epoch£º1244	 i:5 	 global-step:24885	 l-p:1.115689992904663
epoch£º1244	 i:6 	 global-step:24886	 l-p:0.13133136928081512
epoch£º1244	 i:7 	 global-step:24887	 l-p:-0.0910019502043724
epoch£º1244	 i:8 	 global-step:24888	 l-p:0.18411318957805634
epoch£º1244	 i:9 	 global-step:24889	 l-p:0.13228467106819153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6075,  0.5145,  1.0000,  0.4357,
          1.0000,  0.8469, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[3.7159, 2.9211, 2.2729],
        [3.7159, 2.9088, 2.2694],
        [3.7159, 3.0245, 2.3664],
        [3.7159, 2.8898, 2.3587]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): -0.5683746933937073 
model_pd.l_d.mean(): -23.53412628173828 
model_pd.lagr.mean(): -24.102500915527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2044], device='cuda:0')), ('power', tensor([-23.7386], device='cuda:0'))])
epoch£º1245	 i:0 	 global-step:24900	 l-p:-0.5683746933937073
epoch£º1245	 i:1 	 global-step:24901	 l-p:0.14076071977615356
epoch£º1245	 i:2 	 global-step:24902	 l-p:0.09578926116228104
epoch£º1245	 i:3 	 global-step:24903	 l-p:0.17293021082878113
epoch£º1245	 i:4 	 global-step:24904	 l-p:0.1405799835920334
epoch£º1245	 i:5 	 global-step:24905	 l-p:0.14083801209926605
epoch£º1245	 i:6 	 global-step:24906	 l-p:0.11838241666555405
epoch£º1245	 i:7 	 global-step:24907	 l-p:1.490836501121521
epoch£º1245	 i:8 	 global-step:24908	 l-p:0.13926000893115997
epoch£º1245	 i:9 	 global-step:24909	 l-p:0.12085312604904175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7124, 2.8858, 2.3549],
        [3.7124, 3.7124, 3.7124],
        [3.7124, 3.6319, 3.6958],
        [3.7124, 3.7124, 3.7124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.14242005348205566 
model_pd.l_d.mean(): -23.292760848999023 
model_pd.lagr.mean(): -23.150341033935547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2963], device='cuda:0')), ('power', tensor([-23.5891], device='cuda:0'))])
epoch£º1246	 i:0 	 global-step:24920	 l-p:0.14242005348205566
epoch£º1246	 i:1 	 global-step:24921	 l-p:0.13413479924201965
epoch£º1246	 i:2 	 global-step:24922	 l-p:-0.0922660231590271
epoch£º1246	 i:3 	 global-step:24923	 l-p:0.13143931329250336
epoch£º1246	 i:4 	 global-step:24924	 l-p:0.1217498779296875
epoch£º1246	 i:5 	 global-step:24925	 l-p:0.19410289824008942
epoch£º1246	 i:6 	 global-step:24926	 l-p:0.11499740183353424
epoch£º1246	 i:7 	 global-step:24927	 l-p:-0.27723923325538635
epoch£º1246	 i:8 	 global-step:24928	 l-p:0.000396890623960644
epoch£º1246	 i:9 	 global-step:24929	 l-p:0.1381901204586029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7065, 2.9996, 2.8029],
        [3.7065, 3.7065, 3.7065],
        [3.7065, 3.3655, 3.4886],
        [3.7065, 3.0549, 2.4023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): -0.005170660093426704 
model_pd.l_d.mean(): -23.192298889160156 
model_pd.lagr.mean(): -23.19746971130371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2164], device='cuda:0')), ('power', tensor([-23.4087], device='cuda:0'))])
epoch£º1247	 i:0 	 global-step:24940	 l-p:-0.005170660093426704
epoch£º1247	 i:1 	 global-step:24941	 l-p:0.10450517386198044
epoch£º1247	 i:2 	 global-step:24942	 l-p:-0.2712576389312744
epoch£º1247	 i:3 	 global-step:24943	 l-p:0.16488125920295715
epoch£º1247	 i:4 	 global-step:24944	 l-p:0.21957826614379883
epoch£º1247	 i:5 	 global-step:24945	 l-p:0.11985330283641815
epoch£º1247	 i:6 	 global-step:24946	 l-p:0.13213957846164703
epoch£º1247	 i:7 	 global-step:24947	 l-p:0.13142181932926178
epoch£º1247	 i:8 	 global-step:24948	 l-p:0.1352781355381012
epoch£º1247	 i:9 	 global-step:24949	 l-p:0.015078144147992134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7064, 3.5353, 3.6443],
        [3.7064, 3.7064, 3.7064],
        [3.7064, 3.0769, 2.9969],
        [3.7064, 2.9816, 2.7523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.1331060379743576 
model_pd.l_d.mean(): -23.806934356689453 
model_pd.lagr.mean(): -23.673828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1096], device='cuda:0')), ('power', tensor([-23.9165], device='cuda:0'))])
epoch£º1248	 i:0 	 global-step:24960	 l-p:0.1331060379743576
epoch£º1248	 i:1 	 global-step:24961	 l-p:0.14070120453834534
epoch£º1248	 i:2 	 global-step:24962	 l-p:0.12383434921503067
epoch£º1248	 i:3 	 global-step:24963	 l-p:0.18436606228351593
epoch£º1248	 i:4 	 global-step:24964	 l-p:0.11826179921627045
epoch£º1248	 i:5 	 global-step:24965	 l-p:0.08186107128858566
epoch£º1248	 i:6 	 global-step:24966	 l-p:0.1282188892364502
epoch£º1248	 i:7 	 global-step:24967	 l-p:0.13895754516124725
epoch£º1248	 i:8 	 global-step:24968	 l-p:0.13401052355766296
epoch£º1248	 i:9 	 global-step:24969	 l-p:-0.20765420794487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7023, 2.9334, 2.6111],
        [3.7023, 3.7023, 3.7023],
        [3.7023, 2.8862, 2.4218],
        [3.7023, 3.7023, 3.7023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.12097367644309998 
model_pd.l_d.mean(): -23.361392974853516 
model_pd.lagr.mean(): -23.240419387817383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2282], device='cuda:0')), ('power', tensor([-23.5895], device='cuda:0'))])
epoch£º1249	 i:0 	 global-step:24980	 l-p:0.12097367644309998
epoch£º1249	 i:1 	 global-step:24981	 l-p:0.14740824699401855
epoch£º1249	 i:2 	 global-step:24982	 l-p:0.18726739287376404
epoch£º1249	 i:3 	 global-step:24983	 l-p:0.12497127056121826
epoch£º1249	 i:4 	 global-step:24984	 l-p:-0.03355923667550087
epoch£º1249	 i:5 	 global-step:24985	 l-p:0.14100077748298645
epoch£º1249	 i:6 	 global-step:24986	 l-p:-0.27715954184532166
epoch£º1249	 i:7 	 global-step:24987	 l-p:0.15170598030090332
epoch£º1249	 i:8 	 global-step:24988	 l-p:0.19913429021835327
epoch£º1249	 i:9 	 global-step:24989	 l-p:0.1366967409849167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1828,  0.1038,  1.0000,  0.0589,
          1.0000,  0.5675, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1353,  0.0695,  1.0000,  0.0357,
          1.0000,  0.5134, 31.6228]], device='cuda:0')
 pt:tensor([[3.7053, 3.2864, 3.3845],
        [3.7053, 3.2276, 3.2932],
        [3.7053, 3.4057, 3.5339],
        [3.7053, 3.4201, 3.5488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.1301470398902893 
model_pd.l_d.mean(): -23.417556762695312 
model_pd.lagr.mean(): -23.28740882873535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1889], device='cuda:0')), ('power', tensor([-23.6065], device='cuda:0'))])
epoch£º1250	 i:0 	 global-step:25000	 l-p:0.1301470398902893
epoch£º1250	 i:1 	 global-step:25001	 l-p:0.13272437453269958
epoch£º1250	 i:2 	 global-step:25002	 l-p:0.13305902481079102
epoch£º1250	 i:3 	 global-step:25003	 l-p:-0.008772530592978
epoch£º1250	 i:4 	 global-step:25004	 l-p:0.1664687693119049
epoch£º1250	 i:5 	 global-step:25005	 l-p:0.1527741551399231
epoch£º1250	 i:6 	 global-step:25006	 l-p:0.22775283455848694
epoch£º1250	 i:7 	 global-step:25007	 l-p:0.10744865238666534
epoch£º1250	 i:8 	 global-step:25008	 l-p:-0.11443863809108734
epoch£º1250	 i:9 	 global-step:25009	 l-p:0.04278860613703728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7026, 3.0725, 2.9923],
        [3.7026, 3.7011, 3.7026],
        [3.7026, 3.7026, 3.7027],
        [3.7026, 3.2835, 3.3818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.14759914577007294 
model_pd.l_d.mean(): -23.223718643188477 
model_pd.lagr.mean(): -23.076120376586914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2385], device='cuda:0')), ('power', tensor([-23.4622], device='cuda:0'))])
epoch£º1251	 i:0 	 global-step:25020	 l-p:0.14759914577007294
epoch£º1251	 i:1 	 global-step:25021	 l-p:0.12949812412261963
epoch£º1251	 i:2 	 global-step:25022	 l-p:0.13866257667541504
epoch£º1251	 i:3 	 global-step:25023	 l-p:0.024167785421013832
epoch£º1251	 i:4 	 global-step:25024	 l-p:0.14608317613601685
epoch£º1251	 i:5 	 global-step:25025	 l-p:0.10741666704416275
epoch£º1251	 i:6 	 global-step:25026	 l-p:0.1941918581724167
epoch£º1251	 i:7 	 global-step:25027	 l-p:0.14741063117980957
epoch£º1251	 i:8 	 global-step:25028	 l-p:0.17753185331821442
epoch£º1251	 i:9 	 global-step:25029	 l-p:-0.2529388666152954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7031, 3.4178, 3.5465],
        [3.7031, 2.8834, 2.2580],
        [3.7031, 3.7030, 3.7031],
        [3.7031, 2.8838, 2.4051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.130916029214859 
model_pd.l_d.mean(): -23.252103805541992 
model_pd.lagr.mean(): -23.121187210083008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2631], device='cuda:0')), ('power', tensor([-23.5152], device='cuda:0'))])
epoch£º1252	 i:0 	 global-step:25040	 l-p:0.130916029214859
epoch£º1252	 i:1 	 global-step:25041	 l-p:0.22273823618888855
epoch£º1252	 i:2 	 global-step:25042	 l-p:0.13393540680408478
epoch£º1252	 i:3 	 global-step:25043	 l-p:-0.010774554684758186
epoch£º1252	 i:4 	 global-step:25044	 l-p:-0.1468295007944107
epoch£º1252	 i:5 	 global-step:25045	 l-p:0.15260905027389526
epoch£º1252	 i:6 	 global-step:25046	 l-p:0.16276134550571442
epoch£º1252	 i:7 	 global-step:25047	 l-p:0.1467479169368744
epoch£º1252	 i:8 	 global-step:25048	 l-p:0.00037365435855463147
epoch£º1252	 i:9 	 global-step:25049	 l-p:0.11928645521402359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7084, 3.7084, 3.7084],
        [3.7084, 3.7073, 3.7083],
        [3.7084, 3.7084, 3.7084],
        [3.7084, 3.7084, 3.7084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.1228385791182518 
model_pd.l_d.mean(): -23.278776168823242 
model_pd.lagr.mean(): -23.15593719482422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1485], device='cuda:0')), ('power', tensor([-23.4273], device='cuda:0'))])
epoch£º1253	 i:0 	 global-step:25060	 l-p:0.1228385791182518
epoch£º1253	 i:1 	 global-step:25061	 l-p:0.20526772737503052
epoch£º1253	 i:2 	 global-step:25062	 l-p:0.12721560895442963
epoch£º1253	 i:3 	 global-step:25063	 l-p:0.1392836570739746
epoch£º1253	 i:4 	 global-step:25064	 l-p:-0.04977882653474808
epoch£º1253	 i:5 	 global-step:25065	 l-p:0.11123688519001007
epoch£º1253	 i:6 	 global-step:25066	 l-p:-0.03303325176239014
epoch£º1253	 i:7 	 global-step:25067	 l-p:-0.4371177554130554
epoch£º1253	 i:8 	 global-step:25068	 l-p:0.1095767393708229
epoch£º1253	 i:9 	 global-step:25069	 l-p:0.19419853389263153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7096, 3.1641, 2.5272],
        [3.7096, 3.3686, 3.4917],
        [3.7096, 3.7040, 3.7094],
        [3.7096, 2.9876, 2.7631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.11034036427736282 
model_pd.l_d.mean(): -22.74087142944336 
model_pd.lagr.mean(): -22.630531311035156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2776], device='cuda:0')), ('power', tensor([-23.0185], device='cuda:0'))])
epoch£º1254	 i:0 	 global-step:25080	 l-p:0.11034036427736282
epoch£º1254	 i:1 	 global-step:25081	 l-p:0.22766047716140747
epoch£º1254	 i:2 	 global-step:25082	 l-p:-0.7365341186523438
epoch£º1254	 i:3 	 global-step:25083	 l-p:-0.2592393755912781
epoch£º1254	 i:4 	 global-step:25084	 l-p:0.09808865934610367
epoch£º1254	 i:5 	 global-step:25085	 l-p:0.1193169578909874
epoch£º1254	 i:6 	 global-step:25086	 l-p:0.15979865193367004
epoch£º1254	 i:7 	 global-step:25087	 l-p:0.16749191284179688
epoch£º1254	 i:8 	 global-step:25088	 l-p:0.15677624940872192
epoch£º1254	 i:9 	 global-step:25089	 l-p:0.12627840042114258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7119, 3.3540, 3.4733],
        [3.7119, 3.0203, 2.8491],
        [3.7119, 3.4574, 3.5848],
        [3.7119, 3.7119, 3.7119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.17396457493305206 
model_pd.l_d.mean(): -22.687637329101562 
model_pd.lagr.mean(): -22.513671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3049], device='cuda:0')), ('power', tensor([-22.9926], device='cuda:0'))])
epoch£º1255	 i:0 	 global-step:25100	 l-p:0.17396457493305206
epoch£º1255	 i:1 	 global-step:25101	 l-p:0.13017278909683228
epoch£º1255	 i:2 	 global-step:25102	 l-p:-0.12188979983329773
epoch£º1255	 i:3 	 global-step:25103	 l-p:0.15753626823425293
epoch£º1255	 i:4 	 global-step:25104	 l-p:-1.843913197517395
epoch£º1255	 i:5 	 global-step:25105	 l-p:0.14352074265480042
epoch£º1255	 i:6 	 global-step:25106	 l-p:0.14579473435878754
epoch£º1255	 i:7 	 global-step:25107	 l-p:0.1650109440088272
epoch£º1255	 i:8 	 global-step:25108	 l-p:0.1305304914712906
epoch£º1255	 i:9 	 global-step:25109	 l-p:-0.06009945645928383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7126, 3.5199, 3.6358],
        [3.7126, 3.1064, 3.0546],
        [3.7126, 3.1815, 2.5467],
        [3.7126, 3.6971, 3.7116]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.13932988047599792 
model_pd.l_d.mean(): -23.497730255126953 
model_pd.lagr.mean(): -23.358400344848633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1681], device='cuda:0')), ('power', tensor([-23.6659], device='cuda:0'))])
epoch£º1256	 i:0 	 global-step:25120	 l-p:0.13932988047599792
epoch£º1256	 i:1 	 global-step:25121	 l-p:0.20560675859451294
epoch£º1256	 i:2 	 global-step:25122	 l-p:0.1405107080936432
epoch£º1256	 i:3 	 global-step:25123	 l-p:-0.1826716959476471
epoch£º1256	 i:4 	 global-step:25124	 l-p:0.13542750477790833
epoch£º1256	 i:5 	 global-step:25125	 l-p:0.13827210664749146
epoch£º1256	 i:6 	 global-step:25126	 l-p:5.732212543487549
epoch£º1256	 i:7 	 global-step:25127	 l-p:0.12730662524700165
epoch£º1256	 i:8 	 global-step:25128	 l-p:0.1343405544757843
epoch£º1256	 i:9 	 global-step:25129	 l-p:-0.01024668663740158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7124, 3.0409, 2.9015],
        [3.7124, 2.9205, 2.2705],
        [3.7124, 3.7117, 3.7124],
        [3.7124, 3.7124, 3.7124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): -0.05260853469371796 
model_pd.l_d.mean(): -23.43151092529297 
model_pd.lagr.mean(): -23.484119415283203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2116], device='cuda:0')), ('power', tensor([-23.6431], device='cuda:0'))])
epoch£º1257	 i:0 	 global-step:25140	 l-p:-0.05260853469371796
epoch£º1257	 i:1 	 global-step:25141	 l-p:0.13494981825351715
epoch£º1257	 i:2 	 global-step:25142	 l-p:0.16792134940624237
epoch£º1257	 i:3 	 global-step:25143	 l-p:0.1449301838874817
epoch£º1257	 i:4 	 global-step:25144	 l-p:0.1507028490304947
epoch£º1257	 i:5 	 global-step:25145	 l-p:-0.20260296761989594
epoch£º1257	 i:6 	 global-step:25146	 l-p:0.11718273162841797
epoch£º1257	 i:7 	 global-step:25147	 l-p:0.17864570021629333
epoch£º1257	 i:8 	 global-step:25148	 l-p:1.4002223014831543
epoch£º1257	 i:9 	 global-step:25149	 l-p:0.13053259253501892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7161, 3.7154, 3.7161],
        [3.7161, 3.7161, 3.7161],
        [3.7161, 3.1665, 3.1743],
        [3.7161, 2.9093, 2.4744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.1319517344236374 
model_pd.l_d.mean(): -23.69114112854004 
model_pd.lagr.mean(): -23.559188842773438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1512], device='cuda:0')), ('power', tensor([-23.8423], device='cuda:0'))])
epoch£º1258	 i:0 	 global-step:25160	 l-p:0.1319517344236374
epoch£º1258	 i:1 	 global-step:25161	 l-p:0.17526771128177643
epoch£º1258	 i:2 	 global-step:25162	 l-p:0.14340659976005554
epoch£º1258	 i:3 	 global-step:25163	 l-p:0.3871818482875824
epoch£º1258	 i:4 	 global-step:25164	 l-p:0.11746710538864136
epoch£º1258	 i:5 	 global-step:25165	 l-p:0.1585383117198944
epoch£º1258	 i:6 	 global-step:25166	 l-p:-0.035774387419223785
epoch£º1258	 i:7 	 global-step:25167	 l-p:0.10687818378210068
epoch£º1258	 i:8 	 global-step:25168	 l-p:0.12116382271051407
epoch£º1258	 i:9 	 global-step:25169	 l-p:0.12020494043827057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7151, 3.7072, 3.7147],
        [3.7151, 3.1085, 3.0561],
        [3.7151, 3.7075, 3.7147],
        [3.7151, 2.8975, 2.2706]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.17179280519485474 
model_pd.l_d.mean(): -23.197954177856445 
model_pd.lagr.mean(): -23.026161193847656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2684], device='cuda:0')), ('power', tensor([-23.4663], device='cuda:0'))])
epoch£º1259	 i:0 	 global-step:25180	 l-p:0.17179280519485474
epoch£º1259	 i:1 	 global-step:25181	 l-p:0.13826975226402283
epoch£º1259	 i:2 	 global-step:25182	 l-p:0.11465103179216385
epoch£º1259	 i:3 	 global-step:25183	 l-p:-0.06307754665613174
epoch£º1259	 i:4 	 global-step:25184	 l-p:0.2058081030845642
epoch£º1259	 i:5 	 global-step:25185	 l-p:-0.196585550904274
epoch£º1259	 i:6 	 global-step:25186	 l-p:3.020401954650879
epoch£º1259	 i:7 	 global-step:25187	 l-p:0.1287023276090622
epoch£º1259	 i:8 	 global-step:25188	 l-p:0.13717864453792572
epoch£º1259	 i:9 	 global-step:25189	 l-p:0.1257946938276291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7144, 3.7066, 3.7141],
        [3.7144, 3.1494, 3.1424],
        [3.7144, 3.7144, 3.7144],
        [3.7144, 3.5529, 3.6583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): -0.058113325387239456 
model_pd.l_d.mean(): -23.4573974609375 
model_pd.lagr.mean(): -23.51551055908203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1910], device='cuda:0')), ('power', tensor([-23.6484], device='cuda:0'))])
epoch£º1260	 i:0 	 global-step:25200	 l-p:-0.058113325387239456
epoch£º1260	 i:1 	 global-step:25201	 l-p:0.13250790536403656
epoch£º1260	 i:2 	 global-step:25202	 l-p:0.1425454467535019
epoch£º1260	 i:3 	 global-step:25203	 l-p:0.11439786851406097
epoch£º1260	 i:4 	 global-step:25204	 l-p:0.13097405433654785
epoch£º1260	 i:5 	 global-step:25205	 l-p:0.12307731062173843
epoch£º1260	 i:6 	 global-step:25206	 l-p:0.1443430483341217
epoch£º1260	 i:7 	 global-step:25207	 l-p:0.1460581123828888
epoch£º1260	 i:8 	 global-step:25208	 l-p:-0.5609058737754822
epoch£º1260	 i:9 	 global-step:25209	 l-p:-0.084126316010952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7093, 3.7093, 3.7093],
        [3.7093, 3.7091, 3.7093],
        [3.7093, 3.6787, 3.7060],
        [3.7093, 3.7006, 3.7089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): -0.5107644200325012 
model_pd.l_d.mean(): -23.678482055664062 
model_pd.lagr.mean(): -24.189247131347656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1501], device='cuda:0')), ('power', tensor([-23.8285], device='cuda:0'))])
epoch£º1261	 i:0 	 global-step:25220	 l-p:-0.5107644200325012
epoch£º1261	 i:1 	 global-step:25221	 l-p:-0.05456213653087616
epoch£º1261	 i:2 	 global-step:25222	 l-p:0.15478354692459106
epoch£º1261	 i:3 	 global-step:25223	 l-p:0.09992998093366623
epoch£º1261	 i:4 	 global-step:25224	 l-p:0.16430135071277618
epoch£º1261	 i:5 	 global-step:25225	 l-p:0.1434168517589569
epoch£º1261	 i:6 	 global-step:25226	 l-p:0.1964845210313797
epoch£º1261	 i:7 	 global-step:25227	 l-p:0.11390960216522217
epoch£º1261	 i:8 	 global-step:25228	 l-p:0.13066503405570984
epoch£º1261	 i:9 	 global-step:25229	 l-p:-0.041548699140548706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7123, 2.9938, 2.3345],
        [3.7123, 3.6677, 3.7062],
        [3.7123, 2.9927, 2.3333],
        [3.7123, 2.9188, 2.2693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.14670330286026 
model_pd.l_d.mean(): -22.956436157226562 
model_pd.lagr.mean(): -22.80973243713379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2313], device='cuda:0')), ('power', tensor([-23.1877], device='cuda:0'))])
epoch£º1262	 i:0 	 global-step:25240	 l-p:0.14670330286026
epoch£º1262	 i:1 	 global-step:25241	 l-p:0.14094345271587372
epoch£º1262	 i:2 	 global-step:25242	 l-p:-0.11125463247299194
epoch£º1262	 i:3 	 global-step:25243	 l-p:0.12921687960624695
epoch£º1262	 i:4 	 global-step:25244	 l-p:0.10179179906845093
epoch£º1262	 i:5 	 global-step:25245	 l-p:-0.03771359845995903
epoch£º1262	 i:6 	 global-step:25246	 l-p:0.20080159604549408
epoch£º1262	 i:7 	 global-step:25247	 l-p:0.18517909944057465
epoch£º1262	 i:8 	 global-step:25248	 l-p:-1.2029168605804443
epoch£º1262	 i:9 	 global-step:25249	 l-p:0.13853929936885834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7124, 3.0742, 2.4223],
        [3.7124, 3.6231, 3.6926],
        [3.7124, 3.5305, 3.6432],
        [3.7124, 3.3948, 3.5214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): -0.2819274961948395 
model_pd.l_d.mean(): -22.53949546813965 
model_pd.lagr.mean(): -22.821422576904297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2666], device='cuda:0')), ('power', tensor([-22.8061], device='cuda:0'))])
epoch£º1263	 i:0 	 global-step:25260	 l-p:-0.2819274961948395
epoch£º1263	 i:1 	 global-step:25261	 l-p:-16.589754104614258
epoch£º1263	 i:2 	 global-step:25262	 l-p:0.1308140754699707
epoch£º1263	 i:3 	 global-step:25263	 l-p:0.10068511962890625
epoch£º1263	 i:4 	 global-step:25264	 l-p:0.1528368890285492
epoch£º1263	 i:5 	 global-step:25265	 l-p:0.15755504369735718
epoch£º1263	 i:6 	 global-step:25266	 l-p:0.13592565059661865
epoch£º1263	 i:7 	 global-step:25267	 l-p:0.09984348714351654
epoch£º1263	 i:8 	 global-step:25268	 l-p:0.1951083093881607
epoch£º1263	 i:9 	 global-step:25269	 l-p:0.14141909778118134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7135, 2.9404, 2.6059],
        [3.7135, 3.2702, 3.3562],
        [3.7135, 3.7135, 3.7135],
        [3.7135, 3.7135, 3.7135]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.16276659071445465 
model_pd.l_d.mean(): -22.69938850402832 
model_pd.lagr.mean(): -22.53662109375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2674], device='cuda:0')), ('power', tensor([-22.9668], device='cuda:0'))])
epoch£º1264	 i:0 	 global-step:25280	 l-p:0.16276659071445465
epoch£º1264	 i:1 	 global-step:25281	 l-p:0.21210666000843048
epoch£º1264	 i:2 	 global-step:25282	 l-p:0.1271614283323288
epoch£º1264	 i:3 	 global-step:25283	 l-p:0.13632598519325256
epoch£º1264	 i:4 	 global-step:25284	 l-p:0.12202902883291245
epoch£º1264	 i:5 	 global-step:25285	 l-p:-0.06967946141958237
epoch£º1264	 i:6 	 global-step:25286	 l-p:0.14649896323680878
epoch£º1264	 i:7 	 global-step:25287	 l-p:-1.3649876117706299
epoch£º1264	 i:8 	 global-step:25288	 l-p:0.12644240260124207
epoch£º1264	 i:9 	 global-step:25289	 l-p:0.15829433500766754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7103, 3.7010, 3.7099],
        [3.7103, 3.2848, 3.3799],
        [3.7103, 3.6111, 3.6865],
        [3.7103, 3.7103, 3.7103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.10223134607076645 
model_pd.l_d.mean(): -22.809585571289062 
model_pd.lagr.mean(): -22.707353591918945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2711], device='cuda:0')), ('power', tensor([-23.0807], device='cuda:0'))])
epoch£º1265	 i:0 	 global-step:25300	 l-p:0.10223134607076645
epoch£º1265	 i:1 	 global-step:25301	 l-p:0.12961183488368988
epoch£º1265	 i:2 	 global-step:25302	 l-p:0.13557536900043488
epoch£º1265	 i:3 	 global-step:25303	 l-p:0.13500039279460907
epoch£º1265	 i:4 	 global-step:25304	 l-p:0.1592223197221756
epoch£º1265	 i:5 	 global-step:25305	 l-p:-0.033292416483163834
epoch£º1265	 i:6 	 global-step:25306	 l-p:0.028810758143663406
epoch£º1265	 i:7 	 global-step:25307	 l-p:0.15904347598552704
epoch£º1265	 i:8 	 global-step:25308	 l-p:-0.15427584946155548
epoch£º1265	 i:9 	 global-step:25309	 l-p:0.191476970911026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1995,  0.1166,  1.0000,  0.0681,
          1.0000,  0.5843, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2321,  0.1426,  1.0000,  0.0876,
          1.0000,  0.6145, 31.6228]], device='cuda:0')
 pt:tensor([[3.7057, 3.2422, 3.3171],
        [3.7057, 3.4058, 3.5341],
        [3.7057, 3.2817, 3.3777],
        [3.7057, 3.1624, 3.1771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.14230409264564514 
model_pd.l_d.mean(): -23.131460189819336 
model_pd.lagr.mean(): -22.98915672302246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2240], device='cuda:0')), ('power', tensor([-23.3555], device='cuda:0'))])
epoch£º1266	 i:0 	 global-step:25320	 l-p:0.14230409264564514
epoch£º1266	 i:1 	 global-step:25321	 l-p:0.13122589886188507
epoch£º1266	 i:2 	 global-step:25322	 l-p:0.12997135519981384
epoch£º1266	 i:3 	 global-step:25323	 l-p:0.11262530833482742
epoch£º1266	 i:4 	 global-step:25324	 l-p:-0.026792187243700027
epoch£º1266	 i:5 	 global-step:25325	 l-p:0.0479869544506073
epoch£º1266	 i:6 	 global-step:25326	 l-p:0.15776729583740234
epoch£º1266	 i:7 	 global-step:25327	 l-p:-0.05579890310764313
epoch£º1266	 i:8 	 global-step:25328	 l-p:0.12545441091060638
epoch£º1266	 i:9 	 global-step:25329	 l-p:0.18742971122264862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7047, 3.1129, 3.0787],
        [3.7047, 3.7037, 3.7047],
        [3.7047, 3.6932, 3.7041],
        [3.7047, 3.7047, 3.7047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.11428776383399963 
model_pd.l_d.mean(): -23.682031631469727 
model_pd.lagr.mean(): -23.5677433013916 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1663], device='cuda:0')), ('power', tensor([-23.8484], device='cuda:0'))])
epoch£º1267	 i:0 	 global-step:25340	 l-p:0.11428776383399963
epoch£º1267	 i:1 	 global-step:25341	 l-p:0.1197780966758728
epoch£º1267	 i:2 	 global-step:25342	 l-p:0.12403174489736557
epoch£º1267	 i:3 	 global-step:25343	 l-p:0.05561506748199463
epoch£º1267	 i:4 	 global-step:25344	 l-p:-0.12181120365858078
epoch£º1267	 i:5 	 global-step:25345	 l-p:0.1348380148410797
epoch£º1267	 i:6 	 global-step:25346	 l-p:0.13466809689998627
epoch£º1267	 i:7 	 global-step:25347	 l-p:0.11498551070690155
epoch£º1267	 i:8 	 global-step:25348	 l-p:0.2140805721282959
epoch£º1267	 i:9 	 global-step:25349	 l-p:0.037010133266448975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7051, 3.7021, 3.7050],
        [3.7051, 3.3092, 3.4171],
        [3.7051, 3.7051, 3.7051],
        [3.7051, 3.1662, 3.1847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.12407468259334564 
model_pd.l_d.mean(): -22.8848819732666 
model_pd.lagr.mean(): -22.760807037353516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1935], device='cuda:0')), ('power', tensor([-23.0784], device='cuda:0'))])
epoch£º1268	 i:0 	 global-step:25360	 l-p:0.12407468259334564
epoch£º1268	 i:1 	 global-step:25361	 l-p:0.12511961162090302
epoch£º1268	 i:2 	 global-step:25362	 l-p:-0.15637362003326416
epoch£º1268	 i:3 	 global-step:25363	 l-p:0.14272679388523102
epoch£º1268	 i:4 	 global-step:25364	 l-p:0.1630941778421402
epoch£º1268	 i:5 	 global-step:25365	 l-p:0.10384009778499603
epoch£º1268	 i:6 	 global-step:25366	 l-p:0.14010512828826904
epoch£º1268	 i:7 	 global-step:25367	 l-p:0.14501558244228363
epoch£º1268	 i:8 	 global-step:25368	 l-p:0.12875060737133026
epoch£º1268	 i:9 	 global-step:25369	 l-p:-0.03991309925913811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7063, 3.3523, 3.4729],
        [3.7063, 3.6922, 3.7054],
        [3.7063, 3.7063, 3.7063],
        [3.7063, 3.7062, 3.7063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.06326597183942795 
model_pd.l_d.mean(): -23.339130401611328 
model_pd.lagr.mean(): -23.275863647460938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2051], device='cuda:0')), ('power', tensor([-23.5443], device='cuda:0'))])
epoch£º1269	 i:0 	 global-step:25380	 l-p:0.06326597183942795
epoch£º1269	 i:1 	 global-step:25381	 l-p:0.11862847954034805
epoch£º1269	 i:2 	 global-step:25382	 l-p:0.17262837290763855
epoch£º1269	 i:3 	 global-step:25383	 l-p:-0.4542537331581116
epoch£º1269	 i:4 	 global-step:25384	 l-p:-0.023853683844208717
epoch£º1269	 i:5 	 global-step:25385	 l-p:0.14257831871509552
epoch£º1269	 i:6 	 global-step:25386	 l-p:0.1331581026315689
epoch£º1269	 i:7 	 global-step:25387	 l-p:0.13291265070438385
epoch£º1269	 i:8 	 global-step:25388	 l-p:0.12432882189750671
epoch£º1269	 i:9 	 global-step:25389	 l-p:0.128453329205513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7137, 3.0629, 2.4091],
        [3.7137, 3.7116, 3.7137],
        [3.7137, 2.9526, 2.6474],
        [3.7137, 2.9216, 2.5363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.16834783554077148 
model_pd.l_d.mean(): -23.478391647338867 
model_pd.lagr.mean(): -23.310043334960938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1945], device='cuda:0')), ('power', tensor([-23.6728], device='cuda:0'))])
epoch£º1270	 i:0 	 global-step:25400	 l-p:0.16834783554077148
epoch£º1270	 i:1 	 global-step:25401	 l-p:0.13349932432174683
epoch£º1270	 i:2 	 global-step:25402	 l-p:0.18227127194404602
epoch£º1270	 i:3 	 global-step:25403	 l-p:0.13757240772247314
epoch£º1270	 i:4 	 global-step:25404	 l-p:-4.658257484436035
epoch£º1270	 i:5 	 global-step:25405	 l-p:-0.13573288917541504
epoch£º1270	 i:6 	 global-step:25406	 l-p:0.1430313140153885
epoch£º1270	 i:7 	 global-step:25407	 l-p:0.12253178656101227
epoch£º1270	 i:8 	 global-step:25408	 l-p:0.10376383364200592
epoch£º1270	 i:9 	 global-step:25409	 l-p:0.12214931845664978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7114, 2.9825, 2.3230],
        [3.7114, 2.8885, 2.2701],
        [3.7114, 3.6142, 3.6884],
        [3.7114, 3.0192, 2.8478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.12508319318294525 
model_pd.l_d.mean(): -22.819583892822266 
model_pd.lagr.mean(): -22.694499969482422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2497], device='cuda:0')), ('power', tensor([-23.0693], device='cuda:0'))])
epoch£º1271	 i:0 	 global-step:25420	 l-p:0.12508319318294525
epoch£º1271	 i:1 	 global-step:25421	 l-p:0.138021320104599
epoch£º1271	 i:2 	 global-step:25422	 l-p:0.1326030045747757
epoch£º1271	 i:3 	 global-step:25423	 l-p:0.11598168313503265
epoch£º1271	 i:4 	 global-step:25424	 l-p:-0.029991133138537407
epoch£º1271	 i:5 	 global-step:25425	 l-p:0.13911576569080353
epoch£º1271	 i:6 	 global-step:25426	 l-p:0.15534959733486176
epoch£º1271	 i:7 	 global-step:25427	 l-p:0.21499527990818024
epoch£º1271	 i:8 	 global-step:25428	 l-p:0.2179362028837204
epoch£º1271	 i:9 	 global-step:25429	 l-p:-0.1598893702030182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228]], device='cuda:0')
 pt:tensor([[3.6988, 2.8717, 2.2602],
        [3.6988, 2.9083, 2.5329],
        [3.6988, 2.8772, 2.3938],
        [3.6988, 3.1184, 2.4771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): -0.05123593658208847 
model_pd.l_d.mean(): -23.238832473754883 
model_pd.lagr.mean(): -23.290067672729492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2944], device='cuda:0')), ('power', tensor([-23.5332], device='cuda:0'))])
epoch£º1272	 i:0 	 global-step:25440	 l-p:-0.05123593658208847
epoch£º1272	 i:1 	 global-step:25441	 l-p:-0.05764804407954216
epoch£º1272	 i:2 	 global-step:25442	 l-p:0.12035909295082092
epoch£º1272	 i:3 	 global-step:25443	 l-p:0.19387756288051605
epoch£º1272	 i:4 	 global-step:25444	 l-p:0.20236317813396454
epoch£º1272	 i:5 	 global-step:25445	 l-p:0.11107059568166733
epoch£º1272	 i:6 	 global-step:25446	 l-p:0.1311124861240387
epoch£º1272	 i:7 	 global-step:25447	 l-p:0.1686287224292755
epoch£º1272	 i:8 	 global-step:25448	 l-p:0.1319267749786377
epoch£º1272	 i:9 	 global-step:25449	 l-p:0.15275509655475616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7009, 2.9406, 2.6406],
        [3.7009, 3.5982, 3.6756],
        [3.7009, 3.7002, 3.7009],
        [3.7009, 3.6925, 3.7005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): -0.07194021344184875 
model_pd.l_d.mean(): -23.105764389038086 
model_pd.lagr.mean(): -23.177703857421875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2955], device='cuda:0')), ('power', tensor([-23.4013], device='cuda:0'))])
epoch£º1273	 i:0 	 global-step:25460	 l-p:-0.07194021344184875
epoch£º1273	 i:1 	 global-step:25461	 l-p:0.15623162686824799
epoch£º1273	 i:2 	 global-step:25462	 l-p:0.15179844200611115
epoch£º1273	 i:3 	 global-step:25463	 l-p:0.14053446054458618
epoch£º1273	 i:4 	 global-step:25464	 l-p:0.1084684282541275
epoch£º1273	 i:5 	 global-step:25465	 l-p:0.19305577874183655
epoch£º1273	 i:6 	 global-step:25466	 l-p:0.19000858068466187
epoch£º1273	 i:7 	 global-step:25467	 l-p:0.12362092733383179
epoch£º1273	 i:8 	 global-step:25468	 l-p:-0.0779411792755127
epoch£º1273	 i:9 	 global-step:25469	 l-p:0.13834130764007568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7013, 3.7013, 3.7013],
        [3.7013, 3.3471, 3.4677],
        [3.7013, 3.2751, 3.3705],
        [3.7013, 3.6973, 3.7011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.05231648311018944 
model_pd.l_d.mean(): -23.217851638793945 
model_pd.lagr.mean(): -23.16553497314453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2083], device='cuda:0')), ('power', tensor([-23.4262], device='cuda:0'))])
epoch£º1274	 i:0 	 global-step:25480	 l-p:0.05231648311018944
epoch£º1274	 i:1 	 global-step:25481	 l-p:-0.07588060200214386
epoch£º1274	 i:2 	 global-step:25482	 l-p:0.014658927917480469
epoch£º1274	 i:3 	 global-step:25483	 l-p:0.09674955159425735
epoch£º1274	 i:4 	 global-step:25484	 l-p:0.1334974765777588
epoch£º1274	 i:5 	 global-step:25485	 l-p:0.12429896742105484
epoch£º1274	 i:6 	 global-step:25486	 l-p:0.2656444013118744
epoch£º1274	 i:7 	 global-step:25487	 l-p:0.13321112096309662
epoch£º1274	 i:8 	 global-step:25488	 l-p:0.15249387919902802
epoch£º1274	 i:9 	 global-step:25489	 l-p:0.12532679736614227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7035, 3.4584, 3.5850],
        [3.7035, 3.6999, 3.7034],
        [3.7035, 3.7023, 3.7035],
        [3.7035, 3.2252, 2.6015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.14881153404712677 
model_pd.l_d.mean(): -23.31380844116211 
model_pd.lagr.mean(): -23.164997100830078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2350], device='cuda:0')), ('power', tensor([-23.5488], device='cuda:0'))])
epoch£º1275	 i:0 	 global-step:25500	 l-p:0.14881153404712677
epoch£º1275	 i:1 	 global-step:25501	 l-p:0.005272478796541691
epoch£º1275	 i:2 	 global-step:25502	 l-p:0.1959095001220703
epoch£º1275	 i:3 	 global-step:25503	 l-p:0.12860365211963654
epoch£º1275	 i:4 	 global-step:25504	 l-p:0.11442498862743378
epoch£º1275	 i:5 	 global-step:25505	 l-p:-0.14456982910633087
epoch£º1275	 i:6 	 global-step:25506	 l-p:0.020643215626478195
epoch£º1275	 i:7 	 global-step:25507	 l-p:0.13226495683193207
epoch£º1275	 i:8 	 global-step:25508	 l-p:0.17469793558120728
epoch£º1275	 i:9 	 global-step:25509	 l-p:0.1314099133014679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7071, 3.7062, 3.7071],
        [3.7071, 2.8824, 2.2667],
        [3.7071, 2.8970, 2.2581],
        [3.7071, 3.0121, 2.8371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): -0.0006047868519090116 
model_pd.l_d.mean(): -23.5085506439209 
model_pd.lagr.mean(): -23.5091552734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2230], device='cuda:0')), ('power', tensor([-23.7315], device='cuda:0'))])
epoch£º1276	 i:0 	 global-step:25520	 l-p:-0.0006047868519090116
epoch£º1276	 i:1 	 global-step:25521	 l-p:0.13054734468460083
epoch£º1276	 i:2 	 global-step:25522	 l-p:0.1332097202539444
epoch£º1276	 i:3 	 global-step:25523	 l-p:0.21064700186252594
epoch£º1276	 i:4 	 global-step:25524	 l-p:-0.6420329213142395
epoch£º1276	 i:5 	 global-step:25525	 l-p:0.11034996062517166
epoch£º1276	 i:6 	 global-step:25526	 l-p:0.10966107249259949
epoch£º1276	 i:7 	 global-step:25527	 l-p:0.14955927431583405
epoch£º1276	 i:8 	 global-step:25528	 l-p:-0.15521861612796783
epoch£º1276	 i:9 	 global-step:25529	 l-p:0.1464943289756775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7148, 2.8850, 2.3258],
        [3.7148, 3.7121, 3.7147],
        [3.7148, 3.1730, 2.5359],
        [3.7148, 3.7138, 3.7147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.14730426669120789 
model_pd.l_d.mean(): -23.46176528930664 
model_pd.lagr.mean(): -23.31446075439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2811], device='cuda:0')), ('power', tensor([-23.7429], device='cuda:0'))])
epoch£º1277	 i:0 	 global-step:25540	 l-p:0.14730426669120789
epoch£º1277	 i:1 	 global-step:25541	 l-p:0.11631856858730316
epoch£º1277	 i:2 	 global-step:25542	 l-p:0.13528664410114288
epoch£º1277	 i:3 	 global-step:25543	 l-p:0.16044801473617554
epoch£º1277	 i:4 	 global-step:25544	 l-p:-0.21501611173152924
epoch£º1277	 i:5 	 global-step:25545	 l-p:2.2336745262145996
epoch£º1277	 i:6 	 global-step:25546	 l-p:-0.08617391437292099
epoch£º1277	 i:7 	 global-step:25547	 l-p:0.15142454206943512
epoch£º1277	 i:8 	 global-step:25548	 l-p:0.19169166684150696
epoch£º1277	 i:9 	 global-step:25549	 l-p:0.14115548133850098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7204, 3.7204, 3.7204],
        [3.7204, 3.0018, 2.3412],
        [3.7204, 2.9410, 2.2854],
        [3.7204, 3.6938, 3.7178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.22007901966571808 
model_pd.l_d.mean(): -23.48938751220703 
model_pd.lagr.mean(): -23.26930809020996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1631], device='cuda:0')), ('power', tensor([-23.6525], device='cuda:0'))])
epoch£º1278	 i:0 	 global-step:25560	 l-p:0.22007901966571808
epoch£º1278	 i:1 	 global-step:25561	 l-p:0.1346668004989624
epoch£º1278	 i:2 	 global-step:25562	 l-p:0.1093524917960167
epoch£º1278	 i:3 	 global-step:25563	 l-p:0.4122243821620941
epoch£º1278	 i:4 	 global-step:25564	 l-p:0.12815065681934357
epoch£º1278	 i:5 	 global-step:25565	 l-p:0.14025138318538666
epoch£º1278	 i:6 	 global-step:25566	 l-p:2.143017530441284
epoch£º1278	 i:7 	 global-step:25567	 l-p:0.12553729116916656
epoch£º1278	 i:8 	 global-step:25568	 l-p:0.1325315535068512
epoch£º1278	 i:9 	 global-step:25569	 l-p:-0.687804102897644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7250, 3.5498, 3.6602],
        [3.7250, 3.7246, 3.7250],
        [3.7250, 2.9038, 2.2848],
        [3.7250, 3.7249, 3.7250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.11578758805990219 
model_pd.l_d.mean(): -22.03233528137207 
model_pd.lagr.mean(): -21.916547775268555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4258], device='cuda:0')), ('power', tensor([-22.4581], device='cuda:0'))])
epoch£º1279	 i:0 	 global-step:25580	 l-p:0.11578758805990219
epoch£º1279	 i:1 	 global-step:25581	 l-p:0.12634529173374176
epoch£º1279	 i:2 	 global-step:25582	 l-p:1.3281841278076172
epoch£º1279	 i:3 	 global-step:25583	 l-p:0.11699619144201279
epoch£º1279	 i:4 	 global-step:25584	 l-p:0.232960507273674
epoch£º1279	 i:5 	 global-step:25585	 l-p:0.0896853432059288
epoch£º1279	 i:6 	 global-step:25586	 l-p:0.18002374470233917
epoch£º1279	 i:7 	 global-step:25587	 l-p:-0.9098979830741882
epoch£º1279	 i:8 	 global-step:25588	 l-p:0.12982209026813507
epoch£º1279	 i:9 	 global-step:25589	 l-p:0.1234370768070221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7258, 2.9583, 2.6357],
        [3.7258, 3.4263, 3.5544],
        [3.7258, 3.7207, 3.7256],
        [3.7258, 3.7228, 3.7257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): -1.1425960063934326 
model_pd.l_d.mean(): -23.06151008605957 
model_pd.lagr.mean(): -24.204105377197266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2519], device='cuda:0')), ('power', tensor([-23.3134], device='cuda:0'))])
epoch£º1280	 i:0 	 global-step:25600	 l-p:-1.1425960063934326
epoch£º1280	 i:1 	 global-step:25601	 l-p:0.15127061307430267
epoch£º1280	 i:2 	 global-step:25602	 l-p:0.12361498922109604
epoch£º1280	 i:3 	 global-step:25603	 l-p:0.1201832965016365
epoch£º1280	 i:4 	 global-step:25604	 l-p:0.10845384001731873
epoch£º1280	 i:5 	 global-step:25605	 l-p:2.6351912021636963
epoch£º1280	 i:6 	 global-step:25606	 l-p:0.13132089376449585
epoch£º1280	 i:7 	 global-step:25607	 l-p:0.1693970412015915
epoch£º1280	 i:8 	 global-step:25608	 l-p:0.4583984315395355
epoch£º1280	 i:9 	 global-step:25609	 l-p:0.11911994963884354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7210, 3.6823, 3.7162],
        [3.7210, 3.1299, 3.0955],
        [3.7210, 3.4588, 3.5868],
        [3.7210, 3.0914, 3.0108]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.20564262568950653 
model_pd.l_d.mean(): -23.49161148071289 
model_pd.lagr.mean(): -23.285968780517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1543], device='cuda:0')), ('power', tensor([-23.6459], device='cuda:0'))])
epoch£º1281	 i:0 	 global-step:25620	 l-p:0.20564262568950653
epoch£º1281	 i:1 	 global-step:25621	 l-p:0.12262004613876343
epoch£º1281	 i:2 	 global-step:25622	 l-p:0.12481687217950821
epoch£º1281	 i:3 	 global-step:25623	 l-p:0.13317447900772095
epoch£º1281	 i:4 	 global-step:25624	 l-p:0.6301655173301697
epoch£º1281	 i:5 	 global-step:25625	 l-p:0.12226223945617676
epoch£º1281	 i:6 	 global-step:25626	 l-p:0.1709340214729309
epoch£º1281	 i:7 	 global-step:25627	 l-p:0.12187229096889496
epoch£º1281	 i:8 	 global-step:25628	 l-p:-0.5496959686279297
epoch£º1281	 i:9 	 global-step:25629	 l-p:0.12944985926151276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7159, 3.7157, 3.7159],
        [3.7159, 3.7118, 3.7158],
        [3.7159, 3.2072, 3.2507],
        [3.7159, 2.9154, 2.2689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.08890537172555923 
model_pd.l_d.mean(): -23.403657913208008 
model_pd.lagr.mean(): -23.31475257873535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2075], device='cuda:0')), ('power', tensor([-23.6111], device='cuda:0'))])
epoch£º1282	 i:0 	 global-step:25640	 l-p:0.08890537172555923
epoch£º1282	 i:1 	 global-step:25641	 l-p:-0.04751849174499512
epoch£º1282	 i:2 	 global-step:25642	 l-p:0.14093343913555145
epoch£º1282	 i:3 	 global-step:25643	 l-p:-0.15394258499145508
epoch£º1282	 i:4 	 global-step:25644	 l-p:0.1892876923084259
epoch£º1282	 i:5 	 global-step:25645	 l-p:0.12559261918067932
epoch£º1282	 i:6 	 global-step:25646	 l-p:0.15643629431724548
epoch£º1282	 i:7 	 global-step:25647	 l-p:-5.141554355621338
epoch£º1282	 i:8 	 global-step:25648	 l-p:0.1551470309495926
epoch£º1282	 i:9 	 global-step:25649	 l-p:0.12469647824764252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7136, 3.0188, 2.8434],
        [3.7136, 3.7124, 3.7136],
        [3.7136, 3.7095, 3.7135],
        [3.7136, 3.7136, 3.7136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.20881912112236023 
model_pd.l_d.mean(): -23.27054214477539 
model_pd.lagr.mean(): -23.061723709106445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1976], device='cuda:0')), ('power', tensor([-23.4682], device='cuda:0'))])
epoch£º1283	 i:0 	 global-step:25660	 l-p:0.20881912112236023
epoch£º1283	 i:1 	 global-step:25661	 l-p:-10.1616849899292
epoch£º1283	 i:2 	 global-step:25662	 l-p:0.1415255069732666
epoch£º1283	 i:3 	 global-step:25663	 l-p:-0.041233185678720474
epoch£º1283	 i:4 	 global-step:25664	 l-p:0.1347992867231369
epoch£º1283	 i:5 	 global-step:25665	 l-p:-0.159857839345932
epoch£º1283	 i:6 	 global-step:25666	 l-p:0.12098217755556107
epoch£º1283	 i:7 	 global-step:25667	 l-p:0.10540591180324554
epoch£º1283	 i:8 	 global-step:25668	 l-p:0.1353539079427719
epoch£º1283	 i:9 	 global-step:25669	 l-p:0.13214880228042603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7146, 3.0325, 2.3751],
        [3.7146, 3.7093, 3.7145],
        [3.7146, 3.5566, 3.6608],
        [3.7146, 3.7146, 3.7146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.19088372588157654 
model_pd.l_d.mean(): -23.686904907226562 
model_pd.lagr.mean(): -23.496021270751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1591], device='cuda:0')), ('power', tensor([-23.8460], device='cuda:0'))])
epoch£º1284	 i:0 	 global-step:25680	 l-p:0.19088372588157654
epoch£º1284	 i:1 	 global-step:25681	 l-p:3.880070686340332
epoch£º1284	 i:2 	 global-step:25682	 l-p:0.12153896689414978
epoch£º1284	 i:3 	 global-step:25683	 l-p:0.12099277228116989
epoch£º1284	 i:4 	 global-step:25684	 l-p:-0.18164202570915222
epoch£º1284	 i:5 	 global-step:25685	 l-p:0.14238090813159943
epoch£º1284	 i:6 	 global-step:25686	 l-p:-0.022471599280834198
epoch£º1284	 i:7 	 global-step:25687	 l-p:0.13253097236156464
epoch£º1284	 i:8 	 global-step:25688	 l-p:0.15011006593704224
epoch£º1284	 i:9 	 global-step:25689	 l-p:0.10772861540317535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7140, 2.8857, 2.2860],
        [3.7140, 3.7041, 3.7135],
        [3.7140, 3.1305, 3.1051],
        [3.7140, 2.9198, 2.2697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.12909124791622162 
model_pd.l_d.mean(): -23.596105575561523 
model_pd.lagr.mean(): -23.46701431274414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1855], device='cuda:0')), ('power', tensor([-23.7817], device='cuda:0'))])
epoch£º1285	 i:0 	 global-step:25700	 l-p:0.12909124791622162
epoch£º1285	 i:1 	 global-step:25701	 l-p:-0.026616372168064117
epoch£º1285	 i:2 	 global-step:25702	 l-p:0.169447660446167
epoch£º1285	 i:3 	 global-step:25703	 l-p:83.5895004272461
epoch£º1285	 i:4 	 global-step:25704	 l-p:-0.1897326558828354
epoch£º1285	 i:5 	 global-step:25705	 l-p:0.12246599048376083
epoch£º1285	 i:6 	 global-step:25706	 l-p:0.13970322906970978
epoch£º1285	 i:7 	 global-step:25707	 l-p:0.20124123990535736
epoch£º1285	 i:8 	 global-step:25708	 l-p:0.13604077696800232
epoch£º1285	 i:9 	 global-step:25709	 l-p:0.11571650952100754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7189, 3.0890, 3.0085],
        [3.7189, 3.6383, 3.7023],
        [3.7189, 3.2211, 2.5914],
        [3.7189, 3.0813, 2.4283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): -0.05155175179243088 
model_pd.l_d.mean(): -22.927780151367188 
model_pd.lagr.mean(): -22.979331970214844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2616], device='cuda:0')), ('power', tensor([-23.1894], device='cuda:0'))])
epoch£º1286	 i:0 	 global-step:25720	 l-p:-0.05155175179243088
epoch£º1286	 i:1 	 global-step:25721	 l-p:0.10924080014228821
epoch£º1286	 i:2 	 global-step:25722	 l-p:0.15551286935806274
epoch£º1286	 i:3 	 global-step:25723	 l-p:0.1527939736843109
epoch£º1286	 i:4 	 global-step:25724	 l-p:0.1293576955795288
epoch£º1286	 i:5 	 global-step:25725	 l-p:0.1355733722448349
epoch£º1286	 i:6 	 global-step:25726	 l-p:0.132897287607193
epoch£º1286	 i:7 	 global-step:25727	 l-p:0.11596820503473282
epoch£º1286	 i:8 	 global-step:25728	 l-p:0.11783834546804428
epoch£º1286	 i:9 	 global-step:25729	 l-p:-0.00506281852722168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7159, 2.9491, 2.2910],
        [3.7159, 2.9931, 2.7681],
        [3.7159, 3.7158, 3.7159],
        [3.7159, 3.7159, 3.7159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.18845218420028687 
model_pd.l_d.mean(): -23.466936111450195 
model_pd.lagr.mean(): -23.278484344482422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2553], device='cuda:0')), ('power', tensor([-23.7222], device='cuda:0'))])
epoch£º1287	 i:0 	 global-step:25740	 l-p:0.18845218420028687
epoch£º1287	 i:1 	 global-step:25741	 l-p:0.11313363909721375
epoch£º1287	 i:2 	 global-step:25742	 l-p:0.13028138875961304
epoch£º1287	 i:3 	 global-step:25743	 l-p:0.1413765549659729
epoch£º1287	 i:4 	 global-step:25744	 l-p:-0.14372463524341583
epoch£º1287	 i:5 	 global-step:25745	 l-p:-2.6207640171051025
epoch£º1287	 i:6 	 global-step:25746	 l-p:0.10898921638727188
epoch£º1287	 i:7 	 global-step:25747	 l-p:0.12041177600622177
epoch£º1287	 i:8 	 global-step:25748	 l-p:0.15955959260463715
epoch£º1287	 i:9 	 global-step:25749	 l-p:-0.008140897378325462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7117, 3.7117, 3.7117],
        [3.7117, 3.7115, 3.7117],
        [3.7117, 3.0792, 2.9962],
        [3.7117, 2.9140, 2.2657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.14129768311977386 
model_pd.l_d.mean(): -23.15709114074707 
model_pd.lagr.mean(): -23.015792846679688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2269], device='cuda:0')), ('power', tensor([-23.3840], device='cuda:0'))])
epoch£º1288	 i:0 	 global-step:25760	 l-p:0.14129768311977386
epoch£º1288	 i:1 	 global-step:25761	 l-p:-0.08333459496498108
epoch£º1288	 i:2 	 global-step:25762	 l-p:0.1250607818365097
epoch£º1288	 i:3 	 global-step:25763	 l-p:0.13863413035869598
epoch£º1288	 i:4 	 global-step:25764	 l-p:0.09154794365167618
epoch£º1288	 i:5 	 global-step:25765	 l-p:0.19668415188789368
epoch£º1288	 i:6 	 global-step:25766	 l-p:0.1251593381166458
epoch£º1288	 i:7 	 global-step:25767	 l-p:0.12412800639867783
epoch£º1288	 i:8 	 global-step:25768	 l-p:-0.3192744255065918
epoch£º1288	 i:9 	 global-step:25769	 l-p:0.1435869336128235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7071, 3.0649, 2.4132],
        [3.7071, 3.5315, 3.6421],
        [3.7071, 3.7071, 3.7071],
        [3.7071, 3.6268, 3.6905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): -0.2367757111787796 
model_pd.l_d.mean(): -23.3414363861084 
model_pd.lagr.mean(): -23.57821273803711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1549], device='cuda:0')), ('power', tensor([-23.4963], device='cuda:0'))])
epoch£º1289	 i:0 	 global-step:25780	 l-p:-0.2367757111787796
epoch£º1289	 i:1 	 global-step:25781	 l-p:0.07067843526601791
epoch£º1289	 i:2 	 global-step:25782	 l-p:0.022753600031137466
epoch£º1289	 i:3 	 global-step:25783	 l-p:0.1490909904241562
epoch£º1289	 i:4 	 global-step:25784	 l-p:0.12065322697162628
epoch£º1289	 i:5 	 global-step:25785	 l-p:0.16224318742752075
epoch£º1289	 i:6 	 global-step:25786	 l-p:0.12057391554117203
epoch£º1289	 i:7 	 global-step:25787	 l-p:0.15361547470092773
epoch£º1289	 i:8 	 global-step:25788	 l-p:0.13438400626182556
epoch£º1289	 i:9 	 global-step:25789	 l-p:0.10252029448747635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7070, 3.7070, 3.7070],
        [3.7070, 3.2471, 3.3244],
        [3.7070, 3.3029, 3.4079],
        [3.7070, 3.5377, 3.6461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.14850275218486786 
model_pd.l_d.mean(): -22.976537704467773 
model_pd.lagr.mean(): -22.828035354614258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2770], device='cuda:0')), ('power', tensor([-23.2536], device='cuda:0'))])
epoch£º1290	 i:0 	 global-step:25800	 l-p:0.14850275218486786
epoch£º1290	 i:1 	 global-step:25801	 l-p:0.15562009811401367
epoch£º1290	 i:2 	 global-step:25802	 l-p:0.13582667708396912
epoch£º1290	 i:3 	 global-step:25803	 l-p:0.1208171471953392
epoch£º1290	 i:4 	 global-step:25804	 l-p:0.16176177561283112
epoch£º1290	 i:5 	 global-step:25805	 l-p:0.12794014811515808
epoch£º1290	 i:6 	 global-step:25806	 l-p:0.015656517818570137
epoch£º1290	 i:7 	 global-step:25807	 l-p:0.15702137351036072
epoch£º1290	 i:8 	 global-step:25808	 l-p:-0.0697689801454544
epoch£º1290	 i:9 	 global-step:25809	 l-p:0.1384100764989853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6977, 3.2300, 3.3031],
        [3.6977, 3.2050, 3.2615],
        [3.6977, 2.9884, 2.7914],
        [3.6977, 3.6976, 3.6977]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.12869371473789215 
model_pd.l_d.mean(): -23.73787498474121 
model_pd.lagr.mean(): -23.609180450439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1414], device='cuda:0')), ('power', tensor([-23.8792], device='cuda:0'))])
epoch£º1291	 i:0 	 global-step:25820	 l-p:0.12869371473789215
epoch£º1291	 i:1 	 global-step:25821	 l-p:0.12796814739704132
epoch£º1291	 i:2 	 global-step:25822	 l-p:0.12578153610229492
epoch£º1291	 i:3 	 global-step:25823	 l-p:0.28556177020072937
epoch£º1291	 i:4 	 global-step:25824	 l-p:0.19479474425315857
epoch£º1291	 i:5 	 global-step:25825	 l-p:0.1349523365497589
epoch£º1291	 i:6 	 global-step:25826	 l-p:0.11868836730718613
epoch£º1291	 i:7 	 global-step:25827	 l-p:0.12226781249046326
epoch£º1291	 i:8 	 global-step:25828	 l-p:-0.03807687759399414
epoch£º1291	 i:9 	 global-step:25829	 l-p:0.06628292798995972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6907, 3.6901, 3.6907],
        [3.6907, 3.2367, 2.6194],
        [3.6907, 3.6886, 3.6907],
        [3.6907, 3.5381, 3.6402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.18079277873039246 
model_pd.l_d.mean(): -22.53825569152832 
model_pd.lagr.mean(): -22.357463836669922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2726], device='cuda:0')), ('power', tensor([-22.8108], device='cuda:0'))])
epoch£º1292	 i:0 	 global-step:25840	 l-p:0.18079277873039246
epoch£º1292	 i:1 	 global-step:25841	 l-p:0.1989208608865738
epoch£º1292	 i:2 	 global-step:25842	 l-p:0.13849036395549774
epoch£º1292	 i:3 	 global-step:25843	 l-p:0.2690231204032898
epoch£º1292	 i:4 	 global-step:25844	 l-p:0.034768033772706985
epoch£º1292	 i:5 	 global-step:25845	 l-p:-0.0018065260956063867
epoch£º1292	 i:6 	 global-step:25846	 l-p:0.054731156677007675
epoch£º1292	 i:7 	 global-step:25847	 l-p:0.1268651932477951
epoch£º1292	 i:8 	 global-step:25848	 l-p:0.11761213093996048
epoch£º1292	 i:9 	 global-step:25849	 l-p:0.1264113038778305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6984, 3.6909, 3.6981],
        [3.6984, 2.9926, 2.8018],
        [3.6984, 3.3820, 3.5093],
        [3.6984, 3.4732, 3.5969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.20959651470184326 
model_pd.l_d.mean(): -23.38614845275879 
model_pd.lagr.mean(): -23.176551818847656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2242], device='cuda:0')), ('power', tensor([-23.6103], device='cuda:0'))])
epoch£º1293	 i:0 	 global-step:25860	 l-p:0.20959651470184326
epoch£º1293	 i:1 	 global-step:25861	 l-p:0.026707075536251068
epoch£º1293	 i:2 	 global-step:25862	 l-p:-0.010673627257347107
epoch£º1293	 i:3 	 global-step:25863	 l-p:0.14460808038711548
epoch£º1293	 i:4 	 global-step:25864	 l-p:0.13630180060863495
epoch£º1293	 i:5 	 global-step:25865	 l-p:0.15627923607826233
epoch£º1293	 i:6 	 global-step:25866	 l-p:0.11581739038228989
epoch£º1293	 i:7 	 global-step:25867	 l-p:0.049078043550252914
epoch£º1293	 i:8 	 global-step:25868	 l-p:0.14741861820220947
epoch£º1293	 i:9 	 global-step:25869	 l-p:0.12876328825950623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7022, 3.7006, 3.7022],
        [3.7022, 3.6105, 3.6814],
        [3.7022, 2.9804, 2.7601],
        [3.7022, 3.7014, 3.7022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): -0.1626245528459549 
model_pd.l_d.mean(): -22.79326820373535 
model_pd.lagr.mean(): -22.95589256286621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2886], device='cuda:0')), ('power', tensor([-23.0818], device='cuda:0'))])
epoch£º1294	 i:0 	 global-step:25880	 l-p:-0.1626245528459549
epoch£º1294	 i:1 	 global-step:25881	 l-p:0.1349353939294815
epoch£º1294	 i:2 	 global-step:25882	 l-p:0.11015388369560242
epoch£º1294	 i:3 	 global-step:25883	 l-p:0.14827115833759308
epoch£º1294	 i:4 	 global-step:25884	 l-p:0.12198727577924728
epoch£º1294	 i:5 	 global-step:25885	 l-p:0.23548361659049988
epoch£º1294	 i:6 	 global-step:25886	 l-p:0.050129592418670654
epoch£º1294	 i:7 	 global-step:25887	 l-p:0.1203588917851448
epoch£º1294	 i:8 	 global-step:25888	 l-p:0.14074113965034485
epoch£º1294	 i:9 	 global-step:25889	 l-p:0.16293559968471527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6967, 3.6925, 3.6965],
        [3.6967, 3.6716, 3.6943],
        [3.6967, 3.6029, 3.6751],
        [3.6967, 3.0157, 2.3611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): -0.0025718212127685547 
model_pd.l_d.mean(): -23.45530891418457 
model_pd.lagr.mean(): -23.4578800201416 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2065], device='cuda:0')), ('power', tensor([-23.6618], device='cuda:0'))])
epoch£º1295	 i:0 	 global-step:25900	 l-p:-0.0025718212127685547
epoch£º1295	 i:1 	 global-step:25901	 l-p:0.04757289960980415
epoch£º1295	 i:2 	 global-step:25902	 l-p:0.20459018647670746
epoch£º1295	 i:3 	 global-step:25903	 l-p:0.12420304119586945
epoch£º1295	 i:4 	 global-step:25904	 l-p:0.11655505746603012
epoch£º1295	 i:5 	 global-step:25905	 l-p:0.10452989488840103
epoch£º1295	 i:6 	 global-step:25906	 l-p:0.14887966215610504
epoch£º1295	 i:7 	 global-step:25907	 l-p:0.24356521666049957
epoch£º1295	 i:8 	 global-step:25908	 l-p:0.060344599187374115
epoch£º1295	 i:9 	 global-step:25909	 l-p:0.1338529735803604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6957, 3.5336, 3.6395],
        [3.6957, 3.5690, 3.6593],
        [3.6957, 3.6957, 3.6957],
        [3.6957, 3.6600, 3.6915]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): 0.061609748750925064 
model_pd.l_d.mean(): -22.844755172729492 
model_pd.lagr.mean(): -22.783145904541016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2776], device='cuda:0')), ('power', tensor([-23.1224], device='cuda:0'))])
epoch£º1296	 i:0 	 global-step:25920	 l-p:0.061609748750925064
epoch£º1296	 i:1 	 global-step:25921	 l-p:0.07625970989465714
epoch£º1296	 i:2 	 global-step:25922	 l-p:0.2481738179922104
epoch£º1296	 i:3 	 global-step:25923	 l-p:0.03644391521811485
epoch£º1296	 i:4 	 global-step:25924	 l-p:0.1487298458814621
epoch£º1296	 i:5 	 global-step:25925	 l-p:0.14901654422283173
epoch£º1296	 i:6 	 global-step:25926	 l-p:0.11868596822023392
epoch£º1296	 i:7 	 global-step:25927	 l-p:0.11808430403470993
epoch£º1296	 i:8 	 global-step:25928	 l-p:0.11646664142608643
epoch£º1296	 i:9 	 global-step:25929	 l-p:0.10990811884403229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4712,  0.3667,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2304,  0.1412,  1.0000,  0.0866,
          1.0000,  0.6130, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2326,  0.1431,  1.0000,  0.0880,
          1.0000,  0.6150, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]], device='cuda:0')
 pt:tensor([[3.6983, 2.8674, 2.3357],
        [3.6983, 3.1578, 3.1762],
        [3.6983, 3.1525, 3.1662],
        [3.6983, 2.9599, 2.7085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.120228610932827 
model_pd.l_d.mean(): -22.509897232055664 
model_pd.lagr.mean(): -22.38966941833496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2877], device='cuda:0')), ('power', tensor([-22.7976], device='cuda:0'))])
epoch£º1297	 i:0 	 global-step:25940	 l-p:0.120228610932827
epoch£º1297	 i:1 	 global-step:25941	 l-p:-0.016408758237957954
epoch£º1297	 i:2 	 global-step:25942	 l-p:0.051914412528276443
epoch£º1297	 i:3 	 global-step:25943	 l-p:0.14017991721630096
epoch£º1297	 i:4 	 global-step:25944	 l-p:0.1563825160264969
epoch£º1297	 i:5 	 global-step:25945	 l-p:0.1329909861087799
epoch£º1297	 i:6 	 global-step:25946	 l-p:0.1539161205291748
epoch£º1297	 i:7 	 global-step:25947	 l-p:0.20735225081443787
epoch£º1297	 i:8 	 global-step:25948	 l-p:0.13436436653137207
epoch£º1297	 i:9 	 global-step:25949	 l-p:0.11289531737565994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6938, 3.6935, 3.6938],
        [3.6938, 2.8746, 2.4092],
        [3.6938, 2.8659, 2.2508],
        [3.6938, 2.8933, 2.4930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.24250690639019012 
model_pd.l_d.mean(): -23.486841201782227 
model_pd.lagr.mean(): -23.244335174560547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2362], device='cuda:0')), ('power', tensor([-23.7230], device='cuda:0'))])
epoch£º1298	 i:0 	 global-step:25960	 l-p:0.24250690639019012
epoch£º1298	 i:1 	 global-step:25961	 l-p:0.07408816367387772
epoch£º1298	 i:2 	 global-step:25962	 l-p:0.15559695661067963
epoch£º1298	 i:3 	 global-step:25963	 l-p:0.03866415098309517
epoch£º1298	 i:4 	 global-step:25964	 l-p:0.09261386841535568
epoch£º1298	 i:5 	 global-step:25965	 l-p:0.03678242489695549
epoch£º1298	 i:6 	 global-step:25966	 l-p:0.16367480158805847
epoch£º1298	 i:7 	 global-step:25967	 l-p:0.13055531680583954
epoch£º1298	 i:8 	 global-step:25968	 l-p:0.14504513144493103
epoch£º1298	 i:9 	 global-step:25969	 l-p:0.1420624852180481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6951, 3.6940, 3.6951],
        [3.6951, 2.9390, 2.6515],
        [3.6951, 3.6951, 3.6951],
        [3.6951, 3.6359, 3.6853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): 0.059148091822862625 
model_pd.l_d.mean(): -23.75617027282715 
model_pd.lagr.mean(): -23.697021484375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1456], device='cuda:0')), ('power', tensor([-23.9018], device='cuda:0'))])
epoch£º1299	 i:0 	 global-step:25980	 l-p:0.059148091822862625
epoch£º1299	 i:1 	 global-step:25981	 l-p:0.03423498943448067
epoch£º1299	 i:2 	 global-step:25982	 l-p:0.15082181990146637
epoch£º1299	 i:3 	 global-step:25983	 l-p:0.13487352430820465
epoch£º1299	 i:4 	 global-step:25984	 l-p:0.16619403660297394
epoch£º1299	 i:5 	 global-step:25985	 l-p:0.11913716048002243
epoch£º1299	 i:6 	 global-step:25986	 l-p:0.11995775997638702
epoch£º1299	 i:7 	 global-step:25987	 l-p:0.2515733540058136
epoch£º1299	 i:8 	 global-step:25988	 l-p:-0.009813747368752956
epoch£º1299	 i:9 	 global-step:25989	 l-p:0.18570350110530853
