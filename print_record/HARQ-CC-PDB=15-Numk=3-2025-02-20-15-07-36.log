
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.22045108675956726 
model_pd.l_d.mean(): -22.75084114074707 
model_pd.lagr.mean(): -22.5303897857666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6202], device='cuda:0')), ('power', tensor([-23.3711], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.22045108675956726
epoch£º0	 i:1 	 global-step:1	 l-p:0.1639728546142578
epoch£º0	 i:2 	 global-step:2	 l-p:0.20958468317985535
epoch£º0	 i:3 	 global-step:3	 l-p:0.18547755479812622
epoch£º0	 i:4 	 global-step:4	 l-p:0.1604577600955963
epoch£º0	 i:5 	 global-step:5	 l-p:0.13323137164115906
epoch£º0	 i:6 	 global-step:6	 l-p:0.07205227017402649
epoch£º0	 i:7 	 global-step:7	 l-p:0.15227164328098297
epoch£º0	 i:8 	 global-step:8	 l-p:0.12444312125444412
epoch£º0	 i:9 	 global-step:9	 l-p:0.1525035947561264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5290, 3.5294, 3.5290],
        [3.5290, 3.7409, 3.6980],
        [3.5290, 3.5896, 3.5508],
        [3.5290, 4.5221, 5.2100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.03510548174381256 
model_pd.l_d.mean(): -22.22926139831543 
model_pd.lagr.mean(): -22.194156646728516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1471], device='cuda:0')), ('power', tensor([-22.3764], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.03510548174381256
epoch£º1	 i:1 	 global-step:21	 l-p:0.0948038101196289
epoch£º1	 i:2 	 global-step:22	 l-p:0.1295902580022812
epoch£º1	 i:3 	 global-step:23	 l-p:0.1265886425971985
epoch£º1	 i:4 	 global-step:24	 l-p:0.08247099071741104
epoch£º1	 i:5 	 global-step:25	 l-p:0.10980378836393356
epoch£º1	 i:6 	 global-step:26	 l-p:0.14614053070545197
epoch£º1	 i:7 	 global-step:27	 l-p:0.1311836540699005
epoch£º1	 i:8 	 global-step:28	 l-p:0.11177258938550949
epoch£º1	 i:9 	 global-step:29	 l-p:0.09871816635131836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0404, 4.8941, 5.3155],
        [4.0404, 4.1189, 4.0708],
        [4.0404, 4.6872, 4.8975],
        [4.0404, 4.1062, 4.0631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.1317572444677353 
model_pd.l_d.mean(): -22.885700225830078 
model_pd.lagr.mean(): -22.753942489624023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0763], device='cuda:0')), ('power', tensor([-22.8094], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.1317572444677353
epoch£º2	 i:1 	 global-step:41	 l-p:0.08083644509315491
epoch£º2	 i:2 	 global-step:42	 l-p:0.13022761046886444
epoch£º2	 i:3 	 global-step:43	 l-p:0.11213386058807373
epoch£º2	 i:4 	 global-step:44	 l-p:0.12493521720170975
epoch£º2	 i:5 	 global-step:45	 l-p:0.1048060953617096
epoch£º2	 i:6 	 global-step:46	 l-p:0.10844957828521729
epoch£º2	 i:7 	 global-step:47	 l-p:0.11355182528495789
epoch£º2	 i:8 	 global-step:48	 l-p:0.1038631796836853
epoch£º2	 i:9 	 global-step:49	 l-p:0.12123554199934006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6315, 4.3173, 4.6195],
        [3.6315, 3.6643, 3.6394],
        [3.6315, 3.6995, 3.6575],
        [3.6315, 4.6156, 5.2709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.27104756236076355 
model_pd.l_d.mean(): -21.906286239624023 
model_pd.lagr.mean(): -21.635238647460938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1621], device='cuda:0')), ('power', tensor([-22.0684], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.27104756236076355
epoch£º3	 i:1 	 global-step:61	 l-p:0.27006858587265015
epoch£º3	 i:2 	 global-step:62	 l-p:0.19370543956756592
epoch£º3	 i:3 	 global-step:63	 l-p:0.12462924420833588
epoch£º3	 i:4 	 global-step:64	 l-p:0.11662871390581131
epoch£º3	 i:5 	 global-step:65	 l-p:0.11948096752166748
epoch£º3	 i:6 	 global-step:66	 l-p:0.14479844272136688
epoch£º3	 i:7 	 global-step:67	 l-p:0.15526239573955536
epoch£º3	 i:8 	 global-step:68	 l-p:0.11071538925170898
epoch£º3	 i:9 	 global-step:69	 l-p:0.1126028522849083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5472, 4.0326, 4.1544],
        [3.5472, 4.1388, 4.3576],
        [3.5472, 4.5717, 5.3009],
        [3.5472, 3.6072, 3.5688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.15943466126918793 
model_pd.l_d.mean(): -22.1464900970459 
model_pd.lagr.mean(): -21.9870548248291 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1523], device='cuda:0')), ('power', tensor([-22.2988], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.15943466126918793
epoch£º4	 i:1 	 global-step:81	 l-p:0.12336204946041107
epoch£º4	 i:2 	 global-step:82	 l-p:0.40479815006256104
epoch£º4	 i:3 	 global-step:83	 l-p:0.11930130422115326
epoch£º4	 i:4 	 global-step:84	 l-p:0.12040646374225616
epoch£º4	 i:5 	 global-step:85	 l-p:0.14234112203121185
epoch£º4	 i:6 	 global-step:86	 l-p:0.13757319748401642
epoch£º4	 i:7 	 global-step:87	 l-p:0.32293879985809326
epoch£º4	 i:8 	 global-step:88	 l-p:0.13615918159484863
epoch£º4	 i:9 	 global-step:89	 l-p:0.09223649650812149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8067, 3.8069, 3.8067],
        [3.8067, 3.8116, 3.8071],
        [3.8067, 3.8805, 3.8356],
        [3.8067, 3.9124, 3.8589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.12029188126325607 
model_pd.l_d.mean(): -22.832006454467773 
model_pd.lagr.mean(): -22.711713790893555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0265], device='cuda:0')), ('power', tensor([-22.8585], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.12029188126325607
epoch£º5	 i:1 	 global-step:101	 l-p:6.539961814880371
epoch£º5	 i:2 	 global-step:102	 l-p:0.11233143508434296
epoch£º5	 i:3 	 global-step:103	 l-p:0.12942193448543549
epoch£º5	 i:4 	 global-step:104	 l-p:0.07733569294214249
epoch£º5	 i:5 	 global-step:105	 l-p:0.12809383869171143
epoch£º5	 i:6 	 global-step:106	 l-p:-1.7804425954818726
epoch£º5	 i:7 	 global-step:107	 l-p:0.11401429772377014
epoch£º5	 i:8 	 global-step:108	 l-p:0.12316782772541046
epoch£º5	 i:9 	 global-step:109	 l-p:0.11050175130367279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8387, 3.8597, 3.8424],
        [3.8387, 3.8387, 3.8387],
        [3.8387, 4.8962, 5.6053],
        [3.8387, 3.8423, 3.8389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.14074261486530304 
model_pd.l_d.mean(): -23.183931350708008 
model_pd.lagr.mean(): -23.043188095092773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0408], device='cuda:0')), ('power', tensor([-23.1431], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.14074261486530304
epoch£º6	 i:1 	 global-step:121	 l-p:0.11582386493682861
epoch£º6	 i:2 	 global-step:122	 l-p:0.10622213035821915
epoch£º6	 i:3 	 global-step:123	 l-p:0.13054074347019196
epoch£º6	 i:4 	 global-step:124	 l-p:0.10712093114852905
epoch£º6	 i:5 	 global-step:125	 l-p:0.009321996942162514
epoch£º6	 i:6 	 global-step:126	 l-p:0.12978659570217133
epoch£º6	 i:7 	 global-step:127	 l-p:0.6742303371429443
epoch£º6	 i:8 	 global-step:128	 l-p:0.098542720079422
epoch£º6	 i:9 	 global-step:129	 l-p:0.11275807023048401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6214, 4.0770, 4.1697],
        [3.6214, 3.6655, 3.6344],
        [3.6214, 3.6214, 3.6214],
        [3.6214, 3.6364, 3.6236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.6808287501335144 
model_pd.l_d.mean(): -23.140625 
model_pd.lagr.mean(): -22.459796905517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0511], device='cuda:0')), ('power', tensor([-23.1917], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.6808287501335144
epoch£º7	 i:1 	 global-step:141	 l-p:0.12045914679765701
epoch£º7	 i:2 	 global-step:142	 l-p:0.10574367642402649
epoch£º7	 i:3 	 global-step:143	 l-p:0.13485625386238098
epoch£º7	 i:4 	 global-step:144	 l-p:0.14500769972801208
epoch£º7	 i:5 	 global-step:145	 l-p:0.10494812577962875
epoch£º7	 i:6 	 global-step:146	 l-p:0.11308516561985016
epoch£º7	 i:7 	 global-step:147	 l-p:0.11201997846364975
epoch£º7	 i:8 	 global-step:148	 l-p:0.10918053984642029
epoch£º7	 i:9 	 global-step:149	 l-p:0.11247791349887848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7490, 3.7490, 3.7490],
        [3.7490, 3.7491, 3.7490],
        [3.7490, 4.3568, 4.5700],
        [3.7490, 3.8584, 3.8052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.1556144654750824 
model_pd.l_d.mean(): -21.972938537597656 
model_pd.lagr.mean(): -21.817323684692383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0890], device='cuda:0')), ('power', tensor([-22.0620], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.1556144654750824
epoch£º8	 i:1 	 global-step:161	 l-p:0.12010688334703445
epoch£º8	 i:2 	 global-step:162	 l-p:0.08678347617387772
epoch£º8	 i:3 	 global-step:163	 l-p:0.1384551227092743
epoch£º8	 i:4 	 global-step:164	 l-p:0.1076338067650795
epoch£º8	 i:5 	 global-step:165	 l-p:0.12447140365839005
epoch£º8	 i:6 	 global-step:166	 l-p:0.14984378218650818
epoch£º8	 i:7 	 global-step:167	 l-p:0.1341535598039627
epoch£º8	 i:8 	 global-step:168	 l-p:0.08105441927909851
epoch£º8	 i:9 	 global-step:169	 l-p:0.14001484215259552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7623, 3.7637, 3.7624],
        [3.7623, 3.7638, 3.7624],
        [3.7623, 3.8714, 3.8183],
        [3.7623, 3.7777, 3.7646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.09200488775968552 
model_pd.l_d.mean(): -21.241451263427734 
model_pd.lagr.mean(): -21.149446487426758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1146], device='cuda:0')), ('power', tensor([-21.3561], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.09200488775968552
epoch£º9	 i:1 	 global-step:181	 l-p:0.11670836061239243
epoch£º9	 i:2 	 global-step:182	 l-p:0.11232366412878036
epoch£º9	 i:3 	 global-step:183	 l-p:0.08288080990314484
epoch£º9	 i:4 	 global-step:184	 l-p:0.1212141215801239
epoch£º9	 i:5 	 global-step:185	 l-p:0.11359874159097672
epoch£º9	 i:6 	 global-step:186	 l-p:0.1159854382276535
epoch£º9	 i:7 	 global-step:187	 l-p:0.125969797372818
epoch£º9	 i:8 	 global-step:188	 l-p:0.06590621918439865
epoch£º9	 i:9 	 global-step:189	 l-p:0.12519225478172302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7771, 4.6898, 5.2332],
        [3.7771, 4.4007, 4.6276],
        [3.7771, 3.7809, 3.7774],
        [3.7771, 3.7771, 3.7771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.178880512714386 
model_pd.l_d.mean(): -22.927595138549805 
model_pd.lagr.mean(): -22.748714447021484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0218], device='cuda:0')), ('power', tensor([-22.9494], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.178880512714386
epoch£º10	 i:1 	 global-step:201	 l-p:0.09280985593795776
epoch£º10	 i:2 	 global-step:202	 l-p:0.06468698382377625
epoch£º10	 i:3 	 global-step:203	 l-p:0.12416332215070724
epoch£º10	 i:4 	 global-step:204	 l-p:-0.49287569522857666
epoch£º10	 i:5 	 global-step:205	 l-p:0.12360356748104095
epoch£º10	 i:6 	 global-step:206	 l-p:0.09681141376495361
epoch£º10	 i:7 	 global-step:207	 l-p:0.11734693497419357
epoch£º10	 i:8 	 global-step:208	 l-p:0.12650653719902039
epoch£º10	 i:9 	 global-step:209	 l-p:0.12664130330085754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8724, 4.2737, 4.3100],
        [3.8724, 4.8797, 5.5226],
        [3.8724, 4.4213, 4.5703],
        [3.8724, 3.8903, 3.8753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.11920811235904694 
model_pd.l_d.mean(): -22.82210350036621 
model_pd.lagr.mean(): -22.702896118164062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0043], device='cuda:0')), ('power', tensor([-22.8178], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.11920811235904694
epoch£º11	 i:1 	 global-step:221	 l-p:0.11746802181005478
epoch£º11	 i:2 	 global-step:222	 l-p:0.11574047058820724
epoch£º11	 i:3 	 global-step:223	 l-p:0.10371655970811844
epoch£º11	 i:4 	 global-step:224	 l-p:0.14266350865364075
epoch£º11	 i:5 	 global-step:225	 l-p:0.11525001376867294
epoch£º11	 i:6 	 global-step:226	 l-p:0.0851227268576622
epoch£º11	 i:7 	 global-step:227	 l-p:0.11964728683233261
epoch£º11	 i:8 	 global-step:228	 l-p:0.20507822930812836
epoch£º11	 i:9 	 global-step:229	 l-p:0.11984646320343018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7241, 4.0126, 3.9949],
        [3.7241, 3.7505, 3.7297],
        [3.7241, 3.7465, 3.7284],
        [3.7241, 3.8464, 3.7927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.24274016916751862 
model_pd.l_d.mean(): -22.336225509643555 
model_pd.lagr.mean(): -22.09348487854004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1062], device='cuda:0')), ('power', tensor([-22.4424], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.24274016916751862
epoch£º12	 i:1 	 global-step:241	 l-p:0.11948980391025543
epoch£º12	 i:2 	 global-step:242	 l-p:0.12386123090982437
epoch£º12	 i:3 	 global-step:243	 l-p:0.14190731942653656
epoch£º12	 i:4 	 global-step:244	 l-p:0.16068167984485626
epoch£º12	 i:5 	 global-step:245	 l-p:0.09152315557003021
epoch£º12	 i:6 	 global-step:246	 l-p:0.11633595079183578
epoch£º12	 i:7 	 global-step:247	 l-p:0.11325560510158539
epoch£º12	 i:8 	 global-step:248	 l-p:0.13303764164447784
epoch£º12	 i:9 	 global-step:249	 l-p:0.04745364189147949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7689, 4.6085, 5.0698],
        [3.7689, 3.7689, 3.7689],
        [3.7689, 3.7713, 3.7690],
        [3.7689, 4.0600, 4.0418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.11455263942480087 
model_pd.l_d.mean(): -21.925798416137695 
model_pd.lagr.mean(): -21.81124496459961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0802], device='cuda:0')), ('power', tensor([-22.0060], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.11455263942480087
epoch£º13	 i:1 	 global-step:261	 l-p:0.14189593493938446
epoch£º13	 i:2 	 global-step:262	 l-p:0.11534196883440018
epoch£º13	 i:3 	 global-step:263	 l-p:0.11341245472431183
epoch£º13	 i:4 	 global-step:264	 l-p:0.11746606230735779
epoch£º13	 i:5 	 global-step:265	 l-p:0.1418507695198059
epoch£º13	 i:6 	 global-step:266	 l-p:0.07032261043787003
epoch£º13	 i:7 	 global-step:267	 l-p:0.2094326764345169
epoch£º13	 i:8 	 global-step:268	 l-p:0.12021089345216751
epoch£º13	 i:9 	 global-step:269	 l-p:0.10982891172170639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7609, 3.7609, 3.7609],
        [3.7609, 4.4219, 4.6915],
        [3.7609, 3.7861, 3.7660],
        [3.7609, 4.3735, 4.5955]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.1144104078412056 
model_pd.l_d.mean(): -21.86316680908203 
model_pd.lagr.mean(): -21.748756408691406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0713], device='cuda:0')), ('power', tensor([-21.9345], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.1144104078412056
epoch£º14	 i:1 	 global-step:281	 l-p:0.1151927039027214
epoch£º14	 i:2 	 global-step:282	 l-p:0.11810092628002167
epoch£º14	 i:3 	 global-step:283	 l-p:0.10340379178524017
epoch£º14	 i:4 	 global-step:284	 l-p:0.09407667815685272
epoch£º14	 i:5 	 global-step:285	 l-p:0.07596393674612045
epoch£º14	 i:6 	 global-step:286	 l-p:0.2759498953819275
epoch£º14	 i:7 	 global-step:287	 l-p:0.18174690008163452
epoch£º14	 i:8 	 global-step:288	 l-p:0.1544661670923233
epoch£º14	 i:9 	 global-step:289	 l-p:0.12279889732599258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8352, 3.8591, 3.8399],
        [3.8352, 3.8380, 3.8353],
        [3.8352, 3.9672, 3.9118],
        [3.8352, 3.8433, 3.8360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.13173112273216248 
model_pd.l_d.mean(): -23.132572174072266 
model_pd.lagr.mean(): -23.00084114074707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0307], device='cuda:0')), ('power', tensor([-23.1019], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.13173112273216248
epoch£º15	 i:1 	 global-step:301	 l-p:0.1263171285390854
epoch£º15	 i:2 	 global-step:302	 l-p:0.13736757636070251
epoch£º15	 i:3 	 global-step:303	 l-p:0.1172536313533783
epoch£º15	 i:4 	 global-step:304	 l-p:0.12032552808523178
epoch£º15	 i:5 	 global-step:305	 l-p:0.1289338320493698
epoch£º15	 i:6 	 global-step:306	 l-p:0.09493388235569
epoch£º15	 i:7 	 global-step:307	 l-p:1.0166088342666626
epoch£º15	 i:8 	 global-step:308	 l-p:0.10418987274169922
epoch£º15	 i:9 	 global-step:309	 l-p:0.042320504784584045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8626, 3.8640, 3.8627],
        [3.8626, 3.8824, 3.8661],
        [3.8626, 3.8629, 3.8626],
        [3.8626, 4.1328, 4.1029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.10998685657978058 
model_pd.l_d.mean(): -22.141254425048828 
model_pd.lagr.mean(): -22.031267166137695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0100], device='cuda:0')), ('power', tensor([-22.1513], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.10998685657978058
epoch£º16	 i:1 	 global-step:321	 l-p:0.3281247913837433
epoch£º16	 i:2 	 global-step:322	 l-p:0.11119907349348068
epoch£º16	 i:3 	 global-step:323	 l-p:0.10360807925462723
epoch£º16	 i:4 	 global-step:324	 l-p:0.14509296417236328
epoch£º16	 i:5 	 global-step:325	 l-p:0.3995974361896515
epoch£º16	 i:6 	 global-step:326	 l-p:0.11005137860774994
epoch£º16	 i:7 	 global-step:327	 l-p:0.09145954251289368
epoch£º16	 i:8 	 global-step:328	 l-p:0.11583070456981659
epoch£º16	 i:9 	 global-step:329	 l-p:0.09215143322944641
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9204, 4.6263, 4.9230],
        [3.9204, 4.1597, 4.1172],
        [3.9204, 3.9204, 3.9204],
        [3.9204, 4.6170, 4.9044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.1116262823343277 
model_pd.l_d.mean(): -22.87198829650879 
model_pd.lagr.mean(): -22.76036262512207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0310], device='cuda:0')), ('power', tensor([-22.8410], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.1116262823343277
epoch£º17	 i:1 	 global-step:341	 l-p:0.2632147967815399
epoch£º17	 i:2 	 global-step:342	 l-p:0.10875475406646729
epoch£º17	 i:3 	 global-step:343	 l-p:0.11330139636993408
epoch£º17	 i:4 	 global-step:344	 l-p:0.09059928357601166
epoch£º17	 i:5 	 global-step:345	 l-p:0.128964364528656
epoch£º17	 i:6 	 global-step:346	 l-p:0.16306734085083008
epoch£º17	 i:7 	 global-step:347	 l-p:0.10663885623216629
epoch£º17	 i:8 	 global-step:348	 l-p:0.11521047353744507
epoch£º17	 i:9 	 global-step:349	 l-p:0.11583171784877777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7934, 3.8300, 3.8029],
        [3.7934, 3.7936, 3.7934],
        [3.7934, 3.8782, 3.8312],
        [3.7934, 3.8028, 3.7944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.086534783244133 
model_pd.l_d.mean(): -22.798383712768555 
model_pd.lagr.mean(): -22.711849212646484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0313], device='cuda:0')), ('power', tensor([-22.8297], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.086534783244133
epoch£º18	 i:1 	 global-step:361	 l-p:0.11233292520046234
epoch£º18	 i:2 	 global-step:362	 l-p:0.1652822196483612
epoch£º18	 i:3 	 global-step:363	 l-p:0.12352415919303894
epoch£º18	 i:4 	 global-step:364	 l-p:0.10213090479373932
epoch£º18	 i:5 	 global-step:365	 l-p:0.11971529573202133
epoch£º18	 i:6 	 global-step:366	 l-p:0.12316494435071945
epoch£º18	 i:7 	 global-step:367	 l-p:0.10506784915924072
epoch£º18	 i:8 	 global-step:368	 l-p:0.14713457226753235
epoch£º18	 i:9 	 global-step:369	 l-p:0.08811256289482117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7012, 3.8280, 3.7756],
        [3.7012, 3.7229, 3.7054],
        [3.7012, 3.7012, 3.7012],
        [3.7012, 3.8518, 3.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.11581721901893616 
model_pd.l_d.mean(): -23.166126251220703 
model_pd.lagr.mean(): -23.050308227539062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0085], device='cuda:0')), ('power', tensor([-23.1746], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.11581721901893616
epoch£º19	 i:1 	 global-step:381	 l-p:0.1455712616443634
epoch£º19	 i:2 	 global-step:382	 l-p:0.1435130536556244
epoch£º19	 i:3 	 global-step:383	 l-p:-0.03202364221215248
epoch£º19	 i:4 	 global-step:384	 l-p:0.1257089376449585
epoch£º19	 i:5 	 global-step:385	 l-p:0.1252790242433548
epoch£º19	 i:6 	 global-step:386	 l-p:0.13288156688213348
epoch£º19	 i:7 	 global-step:387	 l-p:0.15560060739517212
epoch£º19	 i:8 	 global-step:388	 l-p:0.07486925274133682
epoch£º19	 i:9 	 global-step:389	 l-p:0.11703819036483765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8219, 3.8239, 3.8220],
        [3.8219, 4.1326, 4.1248],
        [3.8219, 3.8531, 3.8293],
        [3.8219, 3.8669, 3.8353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.1507832556962967 
model_pd.l_d.mean(): -23.117788314819336 
model_pd.lagr.mean(): -22.967004776000977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0226], device='cuda:0')), ('power', tensor([-23.0952], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.1507832556962967
epoch£º20	 i:1 	 global-step:401	 l-p:0.13313618302345276
epoch£º20	 i:2 	 global-step:402	 l-p:0.10315736383199692
epoch£º20	 i:3 	 global-step:403	 l-p:0.11281018704175949
epoch£º20	 i:4 	 global-step:404	 l-p:0.1133965402841568
epoch£º20	 i:5 	 global-step:405	 l-p:0.034690011292696
epoch£º20	 i:6 	 global-step:406	 l-p:0.11853243410587311
epoch£º20	 i:7 	 global-step:407	 l-p:-0.5708696246147156
epoch£º20	 i:8 	 global-step:408	 l-p:0.11285185813903809
epoch£º20	 i:9 	 global-step:409	 l-p:0.12351884692907333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8516, 4.8987, 5.6068],
        [3.8516, 3.8974, 3.8653],
        [3.8516, 3.8516, 3.8516],
        [3.8516, 3.9260, 3.8821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.11030703037977219 
model_pd.l_d.mean(): -22.995887756347656 
model_pd.lagr.mean(): -22.88558006286621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0155], device='cuda:0')), ('power', tensor([-22.9804], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.11030703037977219
epoch£º21	 i:1 	 global-step:421	 l-p:0.029302053153514862
epoch£º21	 i:2 	 global-step:422	 l-p:0.10432840138673782
epoch£º21	 i:3 	 global-step:423	 l-p:0.08593262732028961
epoch£º21	 i:4 	 global-step:424	 l-p:0.1349625438451767
epoch£º21	 i:5 	 global-step:425	 l-p:0.17801806330680847
epoch£º21	 i:6 	 global-step:426	 l-p:0.10819258540868759
epoch£º21	 i:7 	 global-step:427	 l-p:0.14280179142951965
epoch£º21	 i:8 	 global-step:428	 l-p:0.14503158628940582
epoch£º21	 i:9 	 global-step:429	 l-p:0.10710395872592926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8264, 3.9014, 3.8575],
        [3.8264, 4.0448, 4.0013],
        [3.8264, 3.8281, 3.8265],
        [3.8264, 3.8499, 3.8311]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.13284410536289215 
model_pd.l_d.mean(): -23.111663818359375 
model_pd.lagr.mean(): -22.978818893432617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0248], device='cuda:0')), ('power', tensor([-23.0869], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.13284410536289215
epoch£º22	 i:1 	 global-step:441	 l-p:0.09535232186317444
epoch£º22	 i:2 	 global-step:442	 l-p:0.1197023019194603
epoch£º22	 i:3 	 global-step:443	 l-p:0.11250710487365723
epoch£º22	 i:4 	 global-step:444	 l-p:0.11690180748701096
epoch£º22	 i:5 	 global-step:445	 l-p:0.14205002784729004
epoch£º22	 i:6 	 global-step:446	 l-p:0.12790365517139435
epoch£º22	 i:7 	 global-step:447	 l-p:0.0947822704911232
epoch£º22	 i:8 	 global-step:448	 l-p:0.13422928750514984
epoch£º22	 i:9 	 global-step:449	 l-p:0.14300885796546936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7791, 4.7956, 5.4815],
        [3.7791, 4.1736, 4.2199],
        [3.7791, 3.7795, 3.7791],
        [3.7791, 4.4408, 4.7168]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.10782168805599213 
model_pd.l_d.mean(): -22.796955108642578 
model_pd.lagr.mean(): -22.689132690429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0335], device='cuda:0')), ('power', tensor([-22.8304], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.10782168805599213
epoch£º23	 i:1 	 global-step:461	 l-p:0.06418613344430923
epoch£º23	 i:2 	 global-step:462	 l-p:0.1517195850610733
epoch£º23	 i:3 	 global-step:463	 l-p:0.13085387647151947
epoch£º23	 i:4 	 global-step:464	 l-p:0.10266976803541183
epoch£º23	 i:5 	 global-step:465	 l-p:0.13040301203727722
epoch£º23	 i:6 	 global-step:466	 l-p:-0.30316174030303955
epoch£º23	 i:7 	 global-step:467	 l-p:0.10455472022294998
epoch£º23	 i:8 	 global-step:468	 l-p:0.10430856794118881
epoch£º23	 i:9 	 global-step:469	 l-p:0.10917924344539642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9055, 4.2512, 4.2594],
        [3.9055, 4.1244, 4.0792],
        [3.9055, 3.9675, 3.9280],
        [3.9055, 3.9055, 3.9055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.11318070441484451 
model_pd.l_d.mean(): -23.413278579711914 
model_pd.lagr.mean(): -23.300098419189453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0989], device='cuda:0')), ('power', tensor([-23.3144], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.11318070441484451
epoch£º24	 i:1 	 global-step:481	 l-p:0.12502042949199677
epoch£º24	 i:2 	 global-step:482	 l-p:0.11172784864902496
epoch£º24	 i:3 	 global-step:483	 l-p:0.13888698816299438
epoch£º24	 i:4 	 global-step:484	 l-p:0.055436596274375916
epoch£º24	 i:5 	 global-step:485	 l-p:0.5427950024604797
epoch£º24	 i:6 	 global-step:486	 l-p:0.12046092003583908
epoch£º24	 i:7 	 global-step:487	 l-p:0.11295974254608154
epoch£º24	 i:8 	 global-step:488	 l-p:0.11951185762882233
epoch£º24	 i:9 	 global-step:489	 l-p:0.12337250262498856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8888, 3.8888, 3.8888],
        [3.8888, 3.8888, 3.8888],
        [3.8888, 3.8888, 3.8888],
        [3.8888, 4.7421, 5.2104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.08396521210670471 
model_pd.l_d.mean(): -21.67487335205078 
model_pd.lagr.mean(): -21.59090805053711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0988], device='cuda:0')), ('power', tensor([-21.7737], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.08396521210670471
epoch£º25	 i:1 	 global-step:501	 l-p:0.11542725563049316
epoch£º25	 i:2 	 global-step:502	 l-p:0.12104303389787674
epoch£º25	 i:3 	 global-step:503	 l-p:0.20850367844104767
epoch£º25	 i:4 	 global-step:504	 l-p:0.08302400261163712
epoch£º25	 i:5 	 global-step:505	 l-p:0.15853676199913025
epoch£º25	 i:6 	 global-step:506	 l-p:0.121332086622715
epoch£º25	 i:7 	 global-step:507	 l-p:0.10520562529563904
epoch£º25	 i:8 	 global-step:508	 l-p:0.13596241176128387
epoch£º25	 i:9 	 global-step:509	 l-p:0.11568319797515869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7649, 3.9050, 3.8526],
        [3.7649, 4.2053, 4.2875],
        [3.7649, 3.8085, 3.7780],
        [3.7649, 4.1448, 4.1840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.13035157322883606 
model_pd.l_d.mean(): -23.18290901184082 
model_pd.lagr.mean(): -23.05255699157715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0053], device='cuda:0')), ('power', tensor([-23.1776], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.13035157322883606
epoch£º26	 i:1 	 global-step:521	 l-p:0.11276904493570328
epoch£º26	 i:2 	 global-step:522	 l-p:0.13189496099948883
epoch£º26	 i:3 	 global-step:523	 l-p:-0.32749778032302856
epoch£º26	 i:4 	 global-step:524	 l-p:0.08744321763515472
epoch£º26	 i:5 	 global-step:525	 l-p:0.11349992454051971
epoch£º26	 i:6 	 global-step:526	 l-p:0.0933571383357048
epoch£º26	 i:7 	 global-step:527	 l-p:0.13619104027748108
epoch£º26	 i:8 	 global-step:528	 l-p:0.11190716177225113
epoch£º26	 i:9 	 global-step:529	 l-p:0.15822234749794006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8244, 3.9108, 3.8641],
        [3.8244, 4.0707, 4.0379],
        [3.8244, 3.8244, 3.8244],
        [3.8244, 3.8263, 3.8245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.06126974895596504 
model_pd.l_d.mean(): -21.645605087280273 
model_pd.lagr.mean(): -21.584335327148438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0769], device='cuda:0')), ('power', tensor([-21.7225], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.06126974895596504
epoch£º27	 i:1 	 global-step:541	 l-p:0.14141997694969177
epoch£º27	 i:2 	 global-step:542	 l-p:0.12238094210624695
epoch£º27	 i:3 	 global-step:543	 l-p:0.12276656180620193
epoch£º27	 i:4 	 global-step:544	 l-p:0.12213040888309479
epoch£º27	 i:5 	 global-step:545	 l-p:0.07949163019657135
epoch£º27	 i:6 	 global-step:546	 l-p:0.12185618281364441
epoch£º27	 i:7 	 global-step:547	 l-p:0.12261312454938889
epoch£º27	 i:8 	 global-step:548	 l-p:-0.017786148935556412
epoch£º27	 i:9 	 global-step:549	 l-p:0.11871631443500519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8125, 3.9606, 3.9081],
        [3.8125, 4.2561, 4.3379],
        [3.8125, 3.8484, 3.8220],
        [3.8125, 4.7859, 5.4145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.14631764590740204 
model_pd.l_d.mean(): -22.71399688720703 
model_pd.lagr.mean(): -22.567678451538086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0332], device='cuda:0')), ('power', tensor([-22.7472], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.14631764590740204
epoch£º28	 i:1 	 global-step:561	 l-p:0.11901560425758362
epoch£º28	 i:2 	 global-step:562	 l-p:0.10430892556905746
epoch£º28	 i:3 	 global-step:563	 l-p:0.2630615234375
epoch£º28	 i:4 	 global-step:564	 l-p:0.08318990468978882
epoch£º28	 i:5 	 global-step:565	 l-p:0.11559724807739258
epoch£º28	 i:6 	 global-step:566	 l-p:0.14759427309036255
epoch£º28	 i:7 	 global-step:567	 l-p:0.13099528849124908
epoch£º28	 i:8 	 global-step:568	 l-p:0.08372478932142258
epoch£º28	 i:9 	 global-step:569	 l-p:0.11788736283779144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8309, 3.8309, 3.8309],
        [3.8309, 3.9781, 3.9253],
        [3.8309, 4.2974, 4.3960],
        [3.8309, 3.8339, 3.8310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.12106287479400635 
model_pd.l_d.mean(): -22.579269409179688 
model_pd.lagr.mean(): -22.458206176757812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0230], device='cuda:0')), ('power', tensor([-22.6023], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.12106287479400635
epoch£º29	 i:1 	 global-step:581	 l-p:0.12872837483882904
epoch£º29	 i:2 	 global-step:582	 l-p:0.11865052580833435
epoch£º29	 i:3 	 global-step:583	 l-p:0.05787281319499016
epoch£º29	 i:4 	 global-step:584	 l-p:0.16666166484355927
epoch£º29	 i:5 	 global-step:585	 l-p:0.1135220155119896
epoch£º29	 i:6 	 global-step:586	 l-p:0.08371298015117645
epoch£º29	 i:7 	 global-step:587	 l-p:0.11896385997533798
epoch£º29	 i:8 	 global-step:588	 l-p:0.09421852231025696
epoch£º29	 i:9 	 global-step:589	 l-p:0.1074386015534401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9097, 3.9109, 3.9097],
        [3.9097, 4.9010, 5.5344],
        [3.9097, 4.0728, 4.0196],
        [3.9097, 4.8098, 5.3329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.22506043314933777 
model_pd.l_d.mean(): -22.054780960083008 
model_pd.lagr.mean(): -21.829721450805664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0631], device='cuda:0')), ('power', tensor([-22.1178], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.22506043314933777
epoch£º30	 i:1 	 global-step:601	 l-p:-0.13526320457458496
epoch£º30	 i:2 	 global-step:602	 l-p:0.09466850012540817
epoch£º30	 i:3 	 global-step:603	 l-p:0.1222430095076561
epoch£º30	 i:4 	 global-step:604	 l-p:0.11623939126729965
epoch£º30	 i:5 	 global-step:605	 l-p:0.12858650088310242
epoch£º30	 i:6 	 global-step:606	 l-p:0.11233633011579514
epoch£º30	 i:7 	 global-step:607	 l-p:0.13015976548194885
epoch£º30	 i:8 	 global-step:608	 l-p:0.12818163633346558
epoch£º30	 i:9 	 global-step:609	 l-p:0.10266117006540298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7536, 3.7536, 3.7536],
        [3.7536, 3.7595, 3.7541],
        [3.7536, 4.7448, 5.4102],
        [3.7536, 3.7941, 3.7654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.12072723358869553 
model_pd.l_d.mean(): -21.506969451904297 
model_pd.lagr.mean(): -21.386241912841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1577], device='cuda:0')), ('power', tensor([-21.6647], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.12072723358869553
epoch£º31	 i:1 	 global-step:621	 l-p:0.1181817576289177
epoch£º31	 i:2 	 global-step:622	 l-p:0.15118977427482605
epoch£º31	 i:3 	 global-step:623	 l-p:0.11457697302103043
epoch£º31	 i:4 	 global-step:624	 l-p:0.12782210111618042
epoch£º31	 i:5 	 global-step:625	 l-p:0.13005787134170532
epoch£º31	 i:6 	 global-step:626	 l-p:0.11422628909349442
epoch£º31	 i:7 	 global-step:627	 l-p:-0.9493773579597473
epoch£º31	 i:8 	 global-step:628	 l-p:0.12521494925022125
epoch£º31	 i:9 	 global-step:629	 l-p:0.13826774060726166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5207, 4.4885, 5.1745],
        [3.5207, 4.1494, 4.4335],
        [3.5207, 3.5589, 3.5320],
        [3.5207, 3.5217, 3.5207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.16069087386131287 
model_pd.l_d.mean(): -22.784683227539062 
model_pd.lagr.mean(): -22.623992919921875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1105], device='cuda:0')), ('power', tensor([-22.8951], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.16069087386131287
epoch£º32	 i:1 	 global-step:641	 l-p:0.12936924397945404
epoch£º32	 i:2 	 global-step:642	 l-p:0.20866912603378296
epoch£º32	 i:3 	 global-step:643	 l-p:0.34462082386016846
epoch£º32	 i:4 	 global-step:644	 l-p:0.10323848575353622
epoch£º32	 i:5 	 global-step:645	 l-p:0.11943995207548141
epoch£º32	 i:6 	 global-step:646	 l-p:0.12306150048971176
epoch£º32	 i:7 	 global-step:647	 l-p:0.12167158722877502
epoch£º32	 i:8 	 global-step:648	 l-p:-0.054741792380809784
epoch£º32	 i:9 	 global-step:649	 l-p:0.20301751792430878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7209, 3.7211, 3.7209],
        [3.7209, 3.8943, 3.8474],
        [3.7209, 3.7214, 3.7209],
        [3.7209, 3.8945, 3.8475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): -0.26013514399528503 
model_pd.l_d.mean(): -23.05179786682129 
model_pd.lagr.mean(): -23.311933517456055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0309], device='cuda:0')), ('power', tensor([-23.0827], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:-0.26013514399528503
epoch£º33	 i:1 	 global-step:661	 l-p:0.11726973950862885
epoch£º33	 i:2 	 global-step:662	 l-p:0.11145034432411194
epoch£º33	 i:3 	 global-step:663	 l-p:0.10965083539485931
epoch£º33	 i:4 	 global-step:664	 l-p:0.1062401756644249
epoch£º33	 i:5 	 global-step:665	 l-p:0.12351621687412262
epoch£º33	 i:6 	 global-step:666	 l-p:0.11971019208431244
epoch£º33	 i:7 	 global-step:667	 l-p:0.11743810027837753
epoch£º33	 i:8 	 global-step:668	 l-p:-0.05081113055348396
epoch£º33	 i:9 	 global-step:669	 l-p:0.12435620278120041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8476, 4.2929, 4.3763],
        [3.8476, 3.8478, 3.8476],
        [3.8476, 4.0555, 4.0122],
        [3.8476, 3.8888, 3.8596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.11409057676792145 
model_pd.l_d.mean(): -23.04611587524414 
model_pd.lagr.mean(): -22.932025909423828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0165], device='cuda:0')), ('power', tensor([-23.0296], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.11409057676792145
epoch£º34	 i:1 	 global-step:681	 l-p:0.10812783986330032
epoch£º34	 i:2 	 global-step:682	 l-p:0.14481966197490692
epoch£º34	 i:3 	 global-step:683	 l-p:0.1425226628780365
epoch£º34	 i:4 	 global-step:684	 l-p:0.10835044831037521
epoch£º34	 i:5 	 global-step:685	 l-p:0.1300515979528427
epoch£º34	 i:6 	 global-step:686	 l-p:0.15122847259044647
epoch£º34	 i:7 	 global-step:687	 l-p:0.1077546775341034
epoch£º34	 i:8 	 global-step:688	 l-p:0.12234282493591309
epoch£º34	 i:9 	 global-step:689	 l-p:0.15532977879047394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7232, 4.0434, 4.0530],
        [3.7232, 3.7232, 3.7232],
        [3.7232, 3.8159, 3.7695],
        [3.7232, 4.6790, 5.3086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): -0.24691440165042877 
model_pd.l_d.mean(): -22.73963737487793 
model_pd.lagr.mean(): -22.98655128479004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0696], device='cuda:0')), ('power', tensor([-22.8093], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:-0.24691440165042877
epoch£º35	 i:1 	 global-step:701	 l-p:0.10261870175600052
epoch£º35	 i:2 	 global-step:702	 l-p:0.18373744189739227
epoch£º35	 i:3 	 global-step:703	 l-p:0.12559746205806732
epoch£º35	 i:4 	 global-step:704	 l-p:0.11742275208234787
epoch£º35	 i:5 	 global-step:705	 l-p:0.11477992683649063
epoch£º35	 i:6 	 global-step:706	 l-p:0.11720805615186691
epoch£º35	 i:7 	 global-step:707	 l-p:0.13229089975357056
epoch£º35	 i:8 	 global-step:708	 l-p:0.07798740267753601
epoch£º35	 i:9 	 global-step:709	 l-p:0.11868617683649063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7253, 3.7253, 3.7253],
        [3.7253, 3.7253, 3.7253],
        [3.7253, 4.1784, 4.2795],
        [3.7253, 3.7578, 3.7337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.15492233633995056 
model_pd.l_d.mean(): -22.156057357788086 
model_pd.lagr.mean(): -22.001134872436523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0909], device='cuda:0')), ('power', tensor([-22.2469], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.15492233633995056
epoch£º36	 i:1 	 global-step:721	 l-p:0.13484208285808563
epoch£º36	 i:2 	 global-step:722	 l-p:0.1104709804058075
epoch£º36	 i:3 	 global-step:723	 l-p:0.17624343931674957
epoch£º36	 i:4 	 global-step:724	 l-p:0.36026641726493835
epoch£º36	 i:5 	 global-step:725	 l-p:0.07968902587890625
epoch£º36	 i:6 	 global-step:726	 l-p:0.07121488451957703
epoch£º36	 i:7 	 global-step:727	 l-p:0.10512111335992813
epoch£º36	 i:8 	 global-step:728	 l-p:0.12607644498348236
epoch£º36	 i:9 	 global-step:729	 l-p:0.12072385847568512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9075, 3.9121, 3.9079],
        [3.9075, 4.8953, 5.5294],
        [3.9075, 4.2071, 4.1965],
        [3.9075, 4.2835, 4.3156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): -0.0015855311648920178 
model_pd.l_d.mean(): -21.81189727783203 
model_pd.lagr.mean(): -21.8134822845459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0701], device='cuda:0')), ('power', tensor([-21.8820], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:-0.0015855311648920178
epoch£º37	 i:1 	 global-step:741	 l-p:0.111959308385849
epoch£º37	 i:2 	 global-step:742	 l-p:0.1149226725101471
epoch£º37	 i:3 	 global-step:743	 l-p:0.12536212801933289
epoch£º37	 i:4 	 global-step:744	 l-p:0.1428229808807373
epoch£º37	 i:5 	 global-step:745	 l-p:0.11150531470775604
epoch£º37	 i:6 	 global-step:746	 l-p:0.04162627458572388
epoch£º37	 i:7 	 global-step:747	 l-p:0.130502387881279
epoch£º37	 i:8 	 global-step:748	 l-p:0.11133796721696854
epoch£º37	 i:9 	 global-step:749	 l-p:0.14288125932216644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7668, 3.8453, 3.8020],
        [3.7668, 3.8134, 3.7819],
        [3.7668, 3.7848, 3.7700],
        [3.7668, 3.8113, 3.7808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.1421010047197342 
model_pd.l_d.mean(): -22.71839714050293 
model_pd.lagr.mean(): -22.576295852661133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0538], device='cuda:0')), ('power', tensor([-22.7722], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.1421010047197342
epoch£º38	 i:1 	 global-step:761	 l-p:0.10551171749830246
epoch£º38	 i:2 	 global-step:762	 l-p:0.0914447009563446
epoch£º38	 i:3 	 global-step:763	 l-p:0.18325693905353546
epoch£º38	 i:4 	 global-step:764	 l-p:0.11349000036716461
epoch£º38	 i:5 	 global-step:765	 l-p:0.14166273176670074
epoch£º38	 i:6 	 global-step:766	 l-p:0.11542630940675735
epoch£º38	 i:7 	 global-step:767	 l-p:0.20301854610443115
epoch£º38	 i:8 	 global-step:768	 l-p:0.10499855130910873
epoch£º38	 i:9 	 global-step:769	 l-p:0.1295129358768463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8366, 3.8366, 3.8366],
        [3.8366, 4.6339, 5.0573],
        [3.8366, 3.8366, 3.8366],
        [3.8366, 3.8381, 3.8367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.12102552503347397 
model_pd.l_d.mean(): -23.30300521850586 
model_pd.lagr.mean(): -23.18198013305664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0512], device='cuda:0')), ('power', tensor([-23.2518], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.12102552503347397
epoch£º39	 i:1 	 global-step:781	 l-p:0.12058276683092117
epoch£º39	 i:2 	 global-step:782	 l-p:0.1149575412273407
epoch£º39	 i:3 	 global-step:783	 l-p:0.15337005257606506
epoch£º39	 i:4 	 global-step:784	 l-p:0.11790306866168976
epoch£º39	 i:5 	 global-step:785	 l-p:0.12010353058576584
epoch£º39	 i:6 	 global-step:786	 l-p:0.08843135833740234
epoch£º39	 i:7 	 global-step:787	 l-p:0.1995786875486374
epoch£º39	 i:8 	 global-step:788	 l-p:0.16204413771629333
epoch£º39	 i:9 	 global-step:789	 l-p:0.00573762645944953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7402, 3.7404, 3.7402],
        [3.7402, 3.7531, 3.7421],
        [3.7402, 3.8186, 3.7757],
        [3.7402, 3.7454, 3.7407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.19147498905658722 
model_pd.l_d.mean(): -23.213722229003906 
model_pd.lagr.mean(): -23.022247314453125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0037], device='cuda:0')), ('power', tensor([-23.2174], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.19147498905658722
epoch£º40	 i:1 	 global-step:801	 l-p:0.13156385719776154
epoch£º40	 i:2 	 global-step:802	 l-p:0.04751966521143913
epoch£º40	 i:3 	 global-step:803	 l-p:0.10202814638614655
epoch£º40	 i:4 	 global-step:804	 l-p:-0.15430477261543274
epoch£º40	 i:5 	 global-step:805	 l-p:0.12505953013896942
epoch£º40	 i:6 	 global-step:806	 l-p:0.0973452627658844
epoch£º40	 i:7 	 global-step:807	 l-p:0.11132413893938065
epoch£º40	 i:8 	 global-step:808	 l-p:0.07735826820135117
epoch£º40	 i:9 	 global-step:809	 l-p:0.10063301026821136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0686, 4.0707, 4.0687],
        [4.0686, 4.0689, 4.0686],
        [4.0686, 4.2129, 4.1576],
        [4.0686, 4.1586, 4.1101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.1037677749991417 
model_pd.l_d.mean(): -23.05397605895996 
model_pd.lagr.mean(): -22.95020866394043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1045], device='cuda:0')), ('power', tensor([-22.9495], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.1037677749991417
epoch£º41	 i:1 	 global-step:821	 l-p:0.09601061791181564
epoch£º41	 i:2 	 global-step:822	 l-p:0.10072757303714752
epoch£º41	 i:3 	 global-step:823	 l-p:0.110803984105587
epoch£º41	 i:4 	 global-step:824	 l-p:-1.6964073181152344
epoch£º41	 i:5 	 global-step:825	 l-p:0.1250707358121872
epoch£º41	 i:6 	 global-step:826	 l-p:0.10846734046936035
epoch£º41	 i:7 	 global-step:827	 l-p:15.631793022155762
epoch£º41	 i:8 	 global-step:828	 l-p:0.116426482796669
epoch£º41	 i:9 	 global-step:829	 l-p:0.11794544011354446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8171, 4.0243, 3.9836],
        [3.8171, 3.8171, 3.8171],
        [3.8171, 4.8333, 5.5240],
        [3.8171, 3.8286, 3.8187]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.0838419497013092 
model_pd.l_d.mean(): -22.12687873840332 
model_pd.lagr.mean(): -22.04303741455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0560], device='cuda:0')), ('power', tensor([-22.1828], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.0838419497013092
epoch£º42	 i:1 	 global-step:841	 l-p:0.18224452435970306
epoch£º42	 i:2 	 global-step:842	 l-p:0.11273950338363647
epoch£º42	 i:3 	 global-step:843	 l-p:0.10488660633563995
epoch£º42	 i:4 	 global-step:844	 l-p:0.10099857300519943
epoch£º42	 i:5 	 global-step:845	 l-p:0.18178443610668182
epoch£º42	 i:6 	 global-step:846	 l-p:0.1187385842204094
epoch£º42	 i:7 	 global-step:847	 l-p:0.18738913536071777
epoch£º42	 i:8 	 global-step:848	 l-p:0.16545073688030243
epoch£º42	 i:9 	 global-step:849	 l-p:0.09371131658554077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[3.7835, 4.6866, 5.2442],
        [3.7835, 4.1896, 4.2533],
        [3.7835, 4.0071, 3.9727],
        [3.7835, 4.6417, 5.1458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.13140055537223816 
model_pd.l_d.mean(): -22.729978561401367 
model_pd.lagr.mean(): -22.59857749938965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0448], device='cuda:0')), ('power', tensor([-22.7748], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.13140055537223816
epoch£º43	 i:1 	 global-step:861	 l-p:0.17105907201766968
epoch£º43	 i:2 	 global-step:862	 l-p:0.1130843535065651
epoch£º43	 i:3 	 global-step:863	 l-p:0.12290703505277634
epoch£º43	 i:4 	 global-step:864	 l-p:0.11126062273979187
epoch£º43	 i:5 	 global-step:865	 l-p:0.1094709187746048
epoch£º43	 i:6 	 global-step:866	 l-p:0.10022168606519699
epoch£º43	 i:7 	 global-step:867	 l-p:0.12813672423362732
epoch£º43	 i:8 	 global-step:868	 l-p:-0.023166598752141
epoch£º43	 i:9 	 global-step:869	 l-p:0.07132253050804138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8882, 3.9825, 3.9349],
        [3.8882, 3.8934, 3.8886],
        [3.8882, 3.8899, 3.8882],
        [3.8882, 3.8882, 3.8882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.11000131070613861 
model_pd.l_d.mean(): -23.07328224182129 
model_pd.lagr.mean(): -22.963281631469727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0450], device='cuda:0')), ('power', tensor([-23.0283], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.11000131070613861
epoch£º44	 i:1 	 global-step:881	 l-p:-0.3348621726036072
epoch£º44	 i:2 	 global-step:882	 l-p:0.11299213021993637
epoch£º44	 i:3 	 global-step:883	 l-p:0.10209381580352783
epoch£º44	 i:4 	 global-step:884	 l-p:0.1590714007616043
epoch£º44	 i:5 	 global-step:885	 l-p:0.09092264622449875
epoch£º44	 i:6 	 global-step:886	 l-p:0.12280004471540451
epoch£º44	 i:7 	 global-step:887	 l-p:0.11172182112932205
epoch£º44	 i:8 	 global-step:888	 l-p:0.11702680587768555
epoch£º44	 i:9 	 global-step:889	 l-p:0.09869647026062012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8422, 3.8422, 3.8422],
        [3.8422, 3.8429, 3.8422],
        [3.8422, 4.0147, 3.9667],
        [3.8422, 3.8422, 3.8422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.044700250029563904 
model_pd.l_d.mean(): -23.363054275512695 
model_pd.lagr.mean(): -23.3183536529541 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0587], device='cuda:0')), ('power', tensor([-23.3044], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.044700250029563904
epoch£º45	 i:1 	 global-step:901	 l-p:0.13416510820388794
epoch£º45	 i:2 	 global-step:902	 l-p:0.092950738966465
epoch£º45	 i:3 	 global-step:903	 l-p:0.12208959460258484
epoch£º45	 i:4 	 global-step:904	 l-p:0.099577397108078
epoch£º45	 i:5 	 global-step:905	 l-p:0.1276690810918808
epoch£º45	 i:6 	 global-step:906	 l-p:0.10010621696710587
epoch£º45	 i:7 	 global-step:907	 l-p:0.10176084190607071
epoch£º45	 i:8 	 global-step:908	 l-p:0.10965083539485931
epoch£º45	 i:9 	 global-step:909	 l-p:0.14283500611782074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7962, 3.7992, 3.7964],
        [3.7962, 3.8153, 3.7998],
        [3.7962, 3.7962, 3.7962],
        [3.7962, 3.8149, 3.7997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.16943465173244476 
model_pd.l_d.mean(): -22.587942123413086 
model_pd.lagr.mean(): -22.418506622314453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0454], device='cuda:0')), ('power', tensor([-22.6333], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.16943465173244476
epoch£º46	 i:1 	 global-step:921	 l-p:0.1420370191335678
epoch£º46	 i:2 	 global-step:922	 l-p:0.12390657514333725
epoch£º46	 i:3 	 global-step:923	 l-p:0.0898049995303154
epoch£º46	 i:4 	 global-step:924	 l-p:0.09120146185159683
epoch£º46	 i:5 	 global-step:925	 l-p:0.10180902481079102
epoch£º46	 i:6 	 global-step:926	 l-p:0.11855341494083405
epoch£º46	 i:7 	 global-step:927	 l-p:0.13569220900535583
epoch£º46	 i:8 	 global-step:928	 l-p:0.14352048933506012
epoch£º46	 i:9 	 global-step:929	 l-p:0.1139896959066391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7894, 3.8302, 3.8017],
        [3.7894, 3.8674, 3.8246],
        [3.7894, 3.7907, 3.7894],
        [3.7894, 4.0407, 4.0172]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.20906531810760498 
model_pd.l_d.mean(): -21.77979850769043 
model_pd.lagr.mean(): -21.57073402404785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0723], device='cuda:0')), ('power', tensor([-21.8521], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.20906531810760498
epoch£º47	 i:1 	 global-step:941	 l-p:0.0858241468667984
epoch£º47	 i:2 	 global-step:942	 l-p:0.11831152439117432
epoch£º47	 i:3 	 global-step:943	 l-p:0.14644789695739746
epoch£º47	 i:4 	 global-step:944	 l-p:0.1046125739812851
epoch£º47	 i:5 	 global-step:945	 l-p:0.1474158614873886
epoch£º47	 i:6 	 global-step:946	 l-p:0.122512087225914
epoch£º47	 i:7 	 global-step:947	 l-p:0.08189529925584793
epoch£º47	 i:8 	 global-step:948	 l-p:0.11358892917633057
epoch£º47	 i:9 	 global-step:949	 l-p:0.14250615239143372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7932, 4.0455, 4.0225],
        [3.7932, 3.9276, 3.8781],
        [3.7932, 3.7932, 3.7932],
        [3.7932, 3.9646, 3.9183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.1185728907585144 
model_pd.l_d.mean(): -23.37205696105957 
model_pd.lagr.mean(): -23.25348472595215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0387], device='cuda:0')), ('power', tensor([-23.3333], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.1185728907585144
epoch£º48	 i:1 	 global-step:961	 l-p:0.11271413415670395
epoch£º48	 i:2 	 global-step:962	 l-p:0.10798364132642746
epoch£º48	 i:3 	 global-step:963	 l-p:-0.08620741963386536
epoch£º48	 i:4 	 global-step:964	 l-p:0.08909527212381363
epoch£º48	 i:5 	 global-step:965	 l-p:0.15329377353191376
epoch£º48	 i:6 	 global-step:966	 l-p:0.12722085416316986
epoch£º48	 i:7 	 global-step:967	 l-p:0.1159682348370552
epoch£º48	 i:8 	 global-step:968	 l-p:0.10683931410312653
epoch£º48	 i:9 	 global-step:969	 l-p:0.132141575217247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7386, 3.7626, 3.7440],
        [3.7386, 3.7396, 3.7387],
        [3.7386, 3.8496, 3.8020],
        [3.7386, 3.7429, 3.7390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.1284058541059494 
model_pd.l_d.mean(): -23.414899826049805 
model_pd.lagr.mean(): -23.2864933013916 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0257], device='cuda:0')), ('power', tensor([-23.3892], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.1284058541059494
epoch£º49	 i:1 	 global-step:981	 l-p:0.11511088162660599
epoch£º49	 i:2 	 global-step:982	 l-p:0.10643676668405533
epoch£º49	 i:3 	 global-step:983	 l-p:0.12362699210643768
epoch£º49	 i:4 	 global-step:984	 l-p:0.11067847907543182
epoch£º49	 i:5 	 global-step:985	 l-p:-0.11602476984262466
epoch£º49	 i:6 	 global-step:986	 l-p:0.15530110895633698
epoch£º49	 i:7 	 global-step:987	 l-p:0.1275750696659088
epoch£º49	 i:8 	 global-step:988	 l-p:0.017898159101605415
epoch£º49	 i:9 	 global-step:989	 l-p:0.09462225437164307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4581, 4.0017, 4.2186],
        [3.4581, 3.4582, 3.4581],
        [3.4581, 3.6645, 3.6386],
        [3.4581, 3.4593, 3.4582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.10187213867902756 
model_pd.l_d.mean(): -22.903209686279297 
model_pd.lagr.mean(): -22.80133819580078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1509], device='cuda:0')), ('power', tensor([-23.0541], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.10187213867902756
epoch£º50	 i:1 	 global-step:1001	 l-p:-0.06384213268756866
epoch£º50	 i:2 	 global-step:1002	 l-p:0.09116873890161514
epoch£º50	 i:3 	 global-step:1003	 l-p:0.08424262702465057
epoch£º50	 i:4 	 global-step:1004	 l-p:0.1033417135477066
epoch£º50	 i:5 	 global-step:1005	 l-p:0.11412210017442703
epoch£º50	 i:6 	 global-step:1006	 l-p:0.2148556411266327
epoch£º50	 i:7 	 global-step:1007	 l-p:0.12553183734416962
epoch£º50	 i:8 	 global-step:1008	 l-p:0.22202028334140778
epoch£º50	 i:9 	 global-step:1009	 l-p:0.10828077048063278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
