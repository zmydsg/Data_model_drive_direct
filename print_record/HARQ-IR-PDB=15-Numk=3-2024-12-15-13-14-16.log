
bounds:tensor([-2.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.6167, 2.6493],
        [2.3753, 2.3852, 2.3768],
        [2.3753, 2.9405, 3.3198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.2962360382080078 
model_pd.l_d.mean(): -22.299562454223633 
model_pd.lagr.mean(): -22.003326416015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0013], device='cuda:0')), ('power', tensor([0.9988], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2784], device='cuda:0')), ('power', tensor([-23.5779], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.2962360382080078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.4887, 2.4992, 2.4903],
        [2.4887, 2.7447, 2.7786],
        [2.4887, 3.0904, 3.4927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.23835885524749756 
model_pd.l_d.mean(): -22.318408966064453 
model_pd.lagr.mean(): -22.080049514770508 
model_pd.lambdas: dict_items([('pout', tensor([1.0025], device='cuda:0')), ('power', tensor([0.9976], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2162], device='cuda:0')), ('power', tensor([-23.5640], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.23835885524749756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6034, 2.8739, 2.9093],
        [2.6034, 3.2418, 3.6673],
        [2.6034, 2.6143, 2.6050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.2052627056837082 
model_pd.l_d.mean(): -22.326507568359375 
model_pd.lagr.mean(): -22.121244430541992 
model_pd.lambdas: dict_items([('pout', tensor([1.0037], device='cuda:0')), ('power', tensor([0.9965], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1563], device='cuda:0')), ('power', tensor([-23.5412], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.2052627056837082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7192, 2.7307, 2.7210],
        [2.7192, 3.0046, 3.0413],
        [2.7192, 3.3949, 3.8439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.1833677738904953 
model_pd.l_d.mean(): -22.32489585876465 
model_pd.lagr.mean(): -22.14152717590332 
model_pd.lambdas: dict_items([('pout', tensor([1.0047], device='cuda:0')), ('power', tensor([0.9953], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0985], device='cuda:0')), ('power', tensor([-23.5105], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.1833677738904953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8363, 3.1366, 3.1748],
        [2.8363, 2.8484, 2.8381],
        [2.8363, 3.5496, 4.0223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.167565256357193 
model_pd.l_d.mean(): -22.314435958862305 
model_pd.lagr.mean(): -22.14687156677246 
model_pd.lambdas: dict_items([('pout', tensor([1.0058], device='cuda:0')), ('power', tensor([0.9941], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0425], device='cuda:0')), ('power', tensor([-23.4724], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.167565256357193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9546, 3.7059, 4.2025],
        [2.9546, 2.9673, 2.9565],
        [2.9546, 3.2701, 3.3097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.15548895299434662 
model_pd.l_d.mean(): -22.295896530151367 
model_pd.lagr.mean(): -22.14040756225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0068], device='cuda:0')), ('power', tensor([0.9929], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9883], device='cuda:0')), ('power', tensor([-23.4277], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.15548895299434662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0704, 3.8589, 4.3787],
        [3.0704, 3.4006, 3.4416],
        [3.0704, 3.0836, 3.0724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.14615343511104584 
model_pd.l_d.mean(): -22.26999282836914 
model_pd.lagr.mean(): -22.12384033203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0077], device='cuda:0')), ('power', tensor([0.9918], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9373], device='cuda:0')), ('power', tensor([-23.3786], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.14615343511104584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1790, 4.0022, 4.5439],
        [3.1790, 3.1927, 3.1811],
        [3.1790, 3.5230, 3.5653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.1389622539281845 
model_pd.l_d.mean(): -22.23820686340332 
model_pd.lagr.mean(): -22.099245071411133 
model_pd.lambdas: dict_items([('pout', tensor([1.0086], device='cuda:0')), ('power', tensor([0.9906], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8913], device='cuda:0')), ('power', tensor([-23.3282], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.1389622539281845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2756, 3.2898, 3.2777],
        [3.2756, 3.6317, 3.6751],
        [3.2756, 4.1295, 4.6905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.13351857662200928 
model_pd.l_d.mean(): -22.20266342163086 
model_pd.lagr.mean(): -22.06914520263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0095], device='cuda:0')), ('power', tensor([0.9894], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8517], device='cuda:0')), ('power', tensor([-23.2803], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.13351857662200928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3558, 3.3704, 3.3580],
        [3.3558, 3.7220, 3.7663],
        [3.3558, 4.2352, 4.8121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.129533052444458 
model_pd.l_d.mean(): -22.165742874145508 
model_pd.lagr.mean(): -22.036209106445312 
model_pd.lambdas: dict_items([('pout', tensor([1.0103], device='cuda:0')), ('power', tensor([0.9883], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8197], device='cuda:0')), ('power', tensor([-23.2385], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.129533052444458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4170, 4.3155, 4.9043],
        [3.4170, 3.7906, 3.8356],
        [3.4170, 3.4318, 3.4192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.12677164375782013 
model_pd.l_d.mean(): -22.129589080810547 
model_pd.lagr.mean(): -22.002817153930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0111], device='cuda:0')), ('power', tensor([0.9871], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7959], device='cuda:0')), ('power', tensor([-23.2055], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.12677164375782013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4582, 3.4732, 3.4604],
        [3.4582, 4.3693, 4.9659],
        [3.4582, 3.8367, 3.8821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.12503284215927124 
model_pd.l_d.mean(): -22.095691680908203 
model_pd.lagr.mean(): -21.970659255981445 
model_pd.lambdas: dict_items([('pout', tensor([1.0119], device='cuda:0')), ('power', tensor([0.9860], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7801], device='cuda:0')), ('power', tensor([-23.1830], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.12503284215927124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4801, 3.8611, 3.9066],
        [3.4801, 3.4952, 3.4824],
        [3.4801, 4.3977, 4.9981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12414436042308807 
model_pd.l_d.mean(): -22.064760208129883 
model_pd.lagr.mean(): -21.940616607666016 
model_pd.lambdas: dict_items([('pout', tensor([1.0126], device='cuda:0')), ('power', tensor([0.9848], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7718], device='cuda:0')), ('power', tensor([-23.1710], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.12414436042308807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4848, 3.8659, 3.9114],
        [3.4848, 4.4032, 5.0040],
        [3.4848, 3.4999, 3.4871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.12396429479122162 
model_pd.l_d.mean(): -22.036840438842773 
model_pd.lagr.mean(): -21.91287612915039 
model_pd.lambdas: dict_items([('pout', tensor([1.0134], device='cuda:0')), ('power', tensor([0.9836], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7701], device='cuda:0')), ('power', tensor([-23.1687], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.12396429479122162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4748, 3.8541, 3.8993],
        [3.4748, 4.3892, 4.9872],
        [3.4748, 3.4897, 3.4770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.12437987327575684 
model_pd.l_d.mean(): -22.011518478393555 
model_pd.lagr.mean(): -21.88713836669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0142], device='cuda:0')), ('power', tensor([0.9825], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7741], device='cuda:0')), ('power', tensor([-23.1749], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.12437987327575684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4526, 3.8286, 3.8734],
        [3.4526, 3.4674, 3.4548],
        [3.4526, 4.3591, 4.9520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.12530231475830078 
model_pd.l_d.mean(): -21.98814582824707 
model_pd.lagr.mean(): -21.862842559814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0150], device='cuda:0')), ('power', tensor([0.9813], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7827], device='cuda:0')), ('power', tensor([-23.1880], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.12530231475830078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4209, 3.4356, 3.4231],
        [3.4209, 4.3165, 4.9024],
        [3.4209, 3.7925, 3.8368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.12665939331054688 
model_pd.l_d.mean(): -21.965991973876953 
model_pd.lagr.mean(): -21.839332580566406 
model_pd.lambdas: dict_items([('pout', tensor([1.0157], device='cuda:0')), ('power', tensor([0.9802], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7950], device='cuda:0')), ('power', tensor([-23.2061], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.12665939331054688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3823, 4.2648, 4.8422],
        [3.3823, 3.3968, 3.3845],
        [3.3823, 3.7485, 3.7922]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.12838752567768097 
model_pd.l_d.mean(): -21.9443359375 
model_pd.lagr.mean(): -21.815948486328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0166], device='cuda:0')), ('power', tensor([0.9790], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8101], device='cuda:0')), ('power', tensor([-23.2278], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.12838752567768097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3392, 3.6994, 3.7424],
        [3.3392, 4.2070, 4.7750],
        [3.3392, 3.3535, 3.3414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.1304234117269516 
model_pd.l_d.mean(): -21.922565460205078 
model_pd.lagr.mean(): -21.792142868041992 
model_pd.lambdas: dict_items([('pout', tensor([1.0174], device='cuda:0')), ('power', tensor([0.9778], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8272], device='cuda:0')), ('power', tensor([-23.2516], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.1304234117269516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2939, 3.3079, 3.2960],
        [3.2939, 4.1464, 4.7045],
        [3.2939, 3.6478, 3.6901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.13269619643688202 
model_pd.l_d.mean(): -21.900177001953125 
model_pd.lagr.mean(): -21.767480850219727 
model_pd.lambdas: dict_items([('pout', tensor([1.0182], device='cuda:0')), ('power', tensor([0.9767], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8454], device='cuda:0')), ('power', tensor([-23.2760], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.13269619643688202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2486, 4.0857, 4.6339],
        [3.2486, 3.2624, 3.2507],
        [3.2486, 3.5962, 3.6378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.13512009382247925 
model_pd.l_d.mean(): -21.876850128173828 
model_pd.lagr.mean(): -21.741729736328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0191], device='cuda:0')), ('power', tensor([0.9755], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8639], device='cuda:0')), ('power', tensor([-23.2998], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.13512009382247925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2054, 4.0277, 4.5665],
        [3.2054, 3.5469, 3.5879],
        [3.2054, 3.2189, 3.2074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.1375894546508789 
model_pd.l_d.mean(): -21.852426528930664 
model_pd.lagr.mean(): -21.71483612060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0200], device='cuda:0')), ('power', tensor([0.9744], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8818], device='cuda:0')), ('power', tensor([-23.3221], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.1375894546508789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1662, 3.5022, 3.5425],
        [3.1662, 3.9750, 4.5051],
        [3.1662, 3.1795, 3.1682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.13997799158096313 
model_pd.l_d.mean(): -21.826929092407227 
model_pd.lagr.mean(): -21.68695068359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0209], device='cuda:0')), ('power', tensor([0.9732], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8982], device='cuda:0')), ('power', tensor([-23.3418], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.13997799158096313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1327, 3.1458, 3.1347],
        [3.1327, 3.9299, 4.4524],
        [3.1327, 3.4638, 3.5036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.14214476943016052 
model_pd.l_d.mean(): -21.800514221191406 
model_pd.lagr.mean(): -21.658369064331055 
model_pd.lambdas: dict_items([('pout', tensor([1.0218], device='cuda:0')), ('power', tensor([0.9720], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9125], device='cuda:0')), ('power', tensor([-23.3584], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.14214476943016052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1063, 3.1192, 3.1082],
        [3.1063, 3.8940, 4.4103],
        [3.1063, 3.4334, 3.4727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.14394859969615936 
model_pd.l_d.mean(): -21.7734317779541 
model_pd.lagr.mean(): -21.62948226928711 
model_pd.lambdas: dict_items([('pout', tensor([1.0227], device='cuda:0')), ('power', tensor([0.9708], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9239], device='cuda:0')), ('power', tensor([-23.3715], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.14394859969615936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0878, 3.4120, 3.4509],
        [3.0878, 3.8686, 4.3803],
        [3.0878, 3.1006, 3.0897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.14526864886283875 
model_pd.l_d.mean(): -21.74595832824707 
model_pd.lagr.mean(): -21.600688934326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0236], device='cuda:0')), ('power', tensor([0.9697], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9320], device='cuda:0')), ('power', tensor([-23.3807], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.14526864886283875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0776, 3.8542, 4.3630],
        [3.0776, 3.3999, 3.4385],
        [3.0776, 3.0903, 3.0795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.14602482318878174 
model_pd.l_d.mean(): -21.718324661254883 
model_pd.lagr.mean(): -21.57229995727539 
model_pd.lambdas: dict_items([('pout', tensor([1.0246], device='cuda:0')), ('power', tensor([0.9685], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9365], device='cuda:0')), ('power', tensor([-23.3861], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.14602482318878174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0757, 3.0884, 3.0776],
        [3.0757, 3.8506, 4.3581],
        [3.0757, 3.3971, 3.4355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.1461920142173767 
model_pd.l_d.mean(): -21.6906681060791 
model_pd.lagr.mean(): -21.544475555419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0255], device='cuda:0')), ('power', tensor([0.9673], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9375], device='cuda:0')), ('power', tensor([-23.3877], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.1461920142173767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0814, 3.4030, 3.4413],
        [3.0814, 3.0941, 3.0834],
        [3.0814, 3.8572, 4.3650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.14580240845680237 
model_pd.l_d.mean(): -21.663042068481445 
model_pd.lagr.mean(): -21.517240524291992 
model_pd.lambdas: dict_items([('pout', tensor([1.0265], device='cuda:0')), ('power', tensor([0.9662], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9352], device='cuda:0')), ('power', tensor([-23.3859], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.14580240845680237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0940, 3.1067, 3.0959],
        [3.0940, 3.8728, 4.3821],
        [3.0940, 3.4165, 3.4547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.14493556320667267 
model_pd.l_d.mean(): -21.63540267944336 
model_pd.lagr.mean(): -21.490467071533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0274], device='cuda:0')), ('power', tensor([0.9650], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9299], device='cuda:0')), ('power', tensor([-23.3809], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.14493556320667267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1121, 3.8957, 4.4078],
        [3.1121, 3.1249, 3.1141],
        [3.1121, 3.4363, 3.4746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.14370198547840118 
model_pd.l_d.mean(): -21.60765838623047 
model_pd.lagr.mean(): -21.463956832885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0283], device='cuda:0')), ('power', tensor([0.9638], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9223], device='cuda:0')), ('power', tensor([-23.3733], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.14370198547840118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1345, 3.1473, 3.1364],
        [3.1345, 3.9242, 4.4398],
        [3.1345, 3.4608, 3.4992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.14222481846809387 
model_pd.l_d.mean(): -21.57971954345703 
model_pd.lagr.mean(): -21.4374942779541 
model_pd.lambdas: dict_items([('pout', tensor([1.0292], device='cuda:0')), ('power', tensor([0.9627], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9130], device='cuda:0')), ('power', tensor([-23.3636], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.14222481846809387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1596, 3.9563, 4.4760],
        [3.1596, 3.1725, 3.1615],
        [3.1596, 3.4885, 3.5269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.14062467217445374 
model_pd.l_d.mean(): -21.551517486572266 
model_pd.lagr.mean(): -21.410892486572266 
model_pd.lambdas: dict_items([('pout', tensor([1.0301], device='cuda:0')), ('power', tensor([0.9615], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9026], device='cuda:0')), ('power', tensor([-23.3524], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.14062467217445374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1859, 3.9900, 4.5141],
        [3.1859, 3.1989, 3.1879],
        [3.1859, 3.5175, 3.5561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.13900938630104065 
model_pd.l_d.mean(): -21.523038864135742 
model_pd.lagr.mean(): -21.384029388427734 
model_pd.lambdas: dict_items([('pout', tensor([1.0310], device='cuda:0')), ('power', tensor([0.9603], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8917], device='cuda:0')), ('power', tensor([-23.3403], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.13900938630104065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2121, 4.0236, 4.5520],
        [3.2121, 3.2252, 3.2141],
        [3.2121, 3.5464, 3.5851]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.1374688744544983 
model_pd.l_d.mean(): -21.494325637817383 
model_pd.lagr.mean(): -21.356857299804688 
model_pd.lambdas: dict_items([('pout', tensor([1.0319], device='cuda:0')), ('power', tensor([0.9592], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8810], device='cuda:0')), ('power', tensor([-23.3281], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.1374688744544983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2368, 3.5735, 3.6123],
        [3.2368, 3.2499, 3.2388],
        [3.2368, 4.0550, 4.5874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.13607344031333923 
model_pd.l_d.mean(): -21.465478897094727 
model_pd.lagr.mean(): -21.329404830932617 
model_pd.lambdas: dict_items([('pout', tensor([1.0328], device='cuda:0')), ('power', tensor([0.9580], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8711], device='cuda:0')), ('power', tensor([-23.3165], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.13607344031333923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2587, 3.2719, 3.2607],
        [3.2587, 4.0830, 4.6188],
        [3.2587, 3.5976, 3.6364]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.13487419486045837 
model_pd.l_d.mean(): -21.436626434326172 
model_pd.lagr.mean(): -21.3017520904541 
model_pd.lambdas: dict_items([('pout', tensor([1.0336], device='cuda:0')), ('power', tensor([0.9568], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8622], device='cuda:0')), ('power', tensor([-23.3060], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.13487419486045837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2770, 4.1061, 4.6446],
        [3.2770, 3.6175, 3.6564],
        [3.2770, 3.2903, 3.2790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.13390503823757172 
model_pd.l_d.mean(): -21.407917022705078 
model_pd.lagr.mean(): -21.274011611938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0345], device='cuda:0')), ('power', tensor([0.9557], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8550], device='cuda:0')), ('power', tensor([-23.2973], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.13390503823757172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2911, 4.1235, 4.6639],
        [3.2911, 3.3044, 3.2931],
        [3.2911, 3.6327, 3.6715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.13318486511707306 
model_pd.l_d.mean(): -21.379474639892578 
model_pd.lagr.mean(): -21.24629020690918 
model_pd.lambdas: dict_items([('pout', tensor([1.0353], device='cuda:0')), ('power', tensor([0.9545], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8495], device='cuda:0')), ('power', tensor([-23.2908], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.13318486511707306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3004, 4.1348, 4.6760],
        [3.3004, 3.6425, 3.6812],
        [3.3004, 3.3137, 3.3024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.13271963596343994 
model_pd.l_d.mean(): -21.351398468017578 
model_pd.lagr.mean(): -21.218679428100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0362], device='cuda:0')), ('power', tensor([0.9533], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8459], device='cuda:0')), ('power', tensor([-23.2866], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.13271963596343994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3049, 3.3182, 3.3069],
        [3.3049, 3.6469, 3.6855],
        [3.3049, 4.1397, 4.6809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.13250428438186646 
model_pd.l_d.mean(): -21.323726654052734 
model_pd.lagr.mean(): -21.19122314453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0370], device='cuda:0')), ('power', tensor([0.9522], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8442], device='cuda:0')), ('power', tensor([-23.2850], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.13250428438186646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3049, 4.1386, 4.6788],
        [3.3049, 3.3182, 3.3069],
        [3.3049, 3.6462, 3.6846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.13252416253089905 
model_pd.l_d.mean(): -21.29646110534668 
model_pd.lagr.mean(): -21.163936614990234 
model_pd.lambdas: dict_items([('pout', tensor([1.0379], device='cuda:0')), ('power', tensor([0.9510], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8444], device='cuda:0')), ('power', tensor([-23.2858], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.13252416253089905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3008, 3.3140, 3.3028],
        [3.3008, 3.6409, 3.6790],
        [3.3008, 4.1321, 4.6704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.132756307721138 
model_pd.l_d.mean(): -21.269542694091797 
model_pd.lagr.mean(): -21.13678550720215 
model_pd.lambdas: dict_items([('pout', tensor([1.0387], device='cuda:0')), ('power', tensor([0.9498], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8462], device='cuda:0')), ('power', tensor([-23.2887], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.132756307721138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2932, 4.1210, 4.6568],
        [3.2932, 3.3063, 3.2952],
        [3.2932, 3.6317, 3.6695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.13317006826400757 
model_pd.l_d.mean(): -21.24288558959961 
model_pd.lagr.mean(): -21.109716415405273 
model_pd.lambdas: dict_items([('pout', tensor([1.0396], device='cuda:0')), ('power', tensor([0.9487], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8494], device='cuda:0')), ('power', tensor([-23.2935], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.13317006826400757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2830, 3.2961, 3.2850],
        [3.2830, 3.6195, 3.6570],
        [3.2830, 4.1064, 4.6392]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.13372814655303955 
model_pd.l_d.mean(): -21.216384887695312 
model_pd.lagr.mean(): -21.082656860351562 
model_pd.lambdas: dict_items([('pout', tensor([1.0404], device='cuda:0')), ('power', tensor([0.9475], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8537], device='cuda:0')), ('power', tensor([-23.2996], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.13372814655303955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2711, 3.6054, 3.6425],
        [3.2711, 3.2840, 3.2731],
        [3.2711, 4.0896, 4.6190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.13438712060451508 
model_pd.l_d.mean(): -21.189943313598633 
model_pd.lagr.mean(): -21.05555534362793 
model_pd.lambdas: dict_items([('pout', tensor([1.0413], device='cuda:0')), ('power', tensor([0.9464], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8587], device='cuda:0')), ('power', tensor([-23.3066], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.13438712060451508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2585, 4.0718, 4.5976],
        [3.2585, 3.5904, 3.6272],
        [3.2585, 3.2713, 3.2604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.1350988745689392 
model_pd.l_d.mean(): -21.163461685180664 
model_pd.lagr.mean(): -21.028362274169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0421], device='cuda:0')), ('power', tensor([0.9452], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8640], device='cuda:0')), ('power', tensor([-23.3139], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.1350988745689392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2461, 4.0542, 4.5766],
        [3.2461, 3.5757, 3.6122],
        [3.2461, 3.2588, 3.2480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.13581280410289764 
model_pd.l_d.mean(): -21.136871337890625 
model_pd.lagr.mean(): -21.00105857849121 
model_pd.lambdas: dict_items([('pout', tensor([1.0430], device='cuda:0')), ('power', tensor([0.9440], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8693], device='cuda:0')), ('power', tensor([-23.3212], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.13581280410289764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2347, 3.5622, 3.5983],
        [3.2347, 3.2474, 3.2366],
        [3.2347, 4.0381, 4.5572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.13647828996181488 
model_pd.l_d.mean(): -21.110139846801758 
model_pd.lagr.mean(): -20.973661422729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0439], device='cuda:0')), ('power', tensor([0.9429], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8741], device='cuda:0')), ('power', tensor([-23.3278], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.13647828996181488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2253, 3.2378, 3.2271],
        [3.2253, 4.0245, 4.5406],
        [3.2253, 3.5508, 3.5866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.13704848289489746 
model_pd.l_d.mean(): -21.083240509033203 
model_pd.lagr.mean(): -20.946191787719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0448], device='cuda:0')), ('power', tensor([0.9417], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8782], device='cuda:0')), ('power', tensor([-23.3334], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.13704848289489746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2182, 4.0140, 4.5277],
        [3.2182, 3.5422, 3.5776],
        [3.2182, 3.2307, 3.2201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.13748404383659363 
model_pd.l_d.mean(): -21.05617904663086 
model_pd.lagr.mean(): -20.9186954498291 
model_pd.lambdas: dict_items([('pout', tensor([1.0456], device='cuda:0')), ('power', tensor([0.9405], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8813], device='cuda:0')), ('power', tensor([-23.3378], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.13748404383659363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2140, 4.0073, 4.5190],
        [3.2140, 3.2264, 3.2159],
        [3.2140, 3.5367, 3.5719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.1377568244934082 
model_pd.l_d.mean(): -21.02896499633789 
model_pd.lagr.mean(): -20.89120864868164 
model_pd.lambdas: dict_items([('pout', tensor([1.0465], device='cuda:0')), ('power', tensor([0.9394], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8832], device='cuda:0')), ('power', tensor([-23.3408], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.1377568244934082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2128, 4.0045, 4.5149],
        [3.2128, 3.5346, 3.5696],
        [3.2128, 3.2252, 3.2147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.13785269856452942 
model_pd.l_d.mean(): -21.001619338989258 
model_pd.lagr.mean(): -20.863765716552734 
model_pd.lambdas: dict_items([('pout', tensor([1.0474], device='cuda:0')), ('power', tensor([0.9382], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8839], device='cuda:0')), ('power', tensor([-23.3423], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.13785269856452942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2146, 4.0057, 4.5153],
        [3.2146, 3.5359, 3.5706],
        [3.2146, 3.2270, 3.2165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.1377720832824707 
model_pd.l_d.mean(): -20.974153518676758 
model_pd.lagr.mean(): -20.836381912231445 
model_pd.lambdas: dict_items([('pout', tensor([1.0483], device='cuda:0')), ('power', tensor([0.9370], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8833], device='cuda:0')), ('power', tensor([-23.3423], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.1377720832824707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2192, 3.5402, 3.5748],
        [3.2192, 3.2315, 3.2210],
        [3.2192, 4.0105, 4.5199]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.13753004372119904 
model_pd.l_d.mean(): -20.94658088684082 
model_pd.lagr.mean(): -20.809051513671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0492], device='cuda:0')), ('power', tensor([0.9358], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8816], device='cuda:0')), ('power', tensor([-23.3409], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.13753004372119904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2261, 3.2384, 3.2279],
        [3.2261, 3.5472, 3.5816],
        [3.2261, 4.0183, 4.5280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.1371532380580902 
model_pd.l_d.mean(): -20.91890525817871 
model_pd.lagr.mean(): -20.78175163269043 
model_pd.lambdas: dict_items([('pout', tensor([1.0501], device='cuda:0')), ('power', tensor([0.9347], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8790], device='cuda:0')), ('power', tensor([-23.3383], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.1371532380580902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2347, 3.2470, 3.2366],
        [3.2347, 3.5562, 3.5904],
        [3.2347, 4.0285, 4.5388]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.1366768479347229 
model_pd.l_d.mean(): -20.891141891479492 
model_pd.lagr.mean(): -20.754465103149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0509], device='cuda:0')), ('power', tensor([0.9335], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8756], device='cuda:0')), ('power', tensor([-23.3347], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.1366768479347229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2446, 3.5664, 3.6006],
        [3.2446, 3.2568, 3.2464],
        [3.2446, 4.0403, 4.5513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.13614079356193542 
model_pd.l_d.mean(): -20.863319396972656 
model_pd.lagr.mean(): -20.7271785736084 
model_pd.lambdas: dict_items([('pout', tensor([1.0518], device='cuda:0')), ('power', tensor([0.9323], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8718], device='cuda:0')), ('power', tensor([-23.3306], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.13614079356193542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2549, 3.5772, 3.6112],
        [3.2549, 4.0526, 4.5645],
        [3.2549, 3.2671, 3.2567]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.13558606803417206 
model_pd.l_d.mean(): -20.835453033447266 
model_pd.lagr.mean(): -20.699867248535156 
model_pd.lambdas: dict_items([('pout', tensor([1.0527], device='cuda:0')), ('power', tensor([0.9312], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8678], device='cuda:0')), ('power', tensor([-23.3262], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.13558606803417206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2649, 3.5877, 3.6216],
        [3.2649, 4.0646, 4.5774],
        [3.2649, 3.2772, 3.2668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.1350511759519577 
model_pd.l_d.mean(): -20.80758285522461 
model_pd.lagr.mean(): -20.672531127929688 
model_pd.lambdas: dict_items([('pout', tensor([1.0535], device='cuda:0')), ('power', tensor([0.9300], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8639], device='cuda:0')), ('power', tensor([-23.3219], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.1350511759519577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2742, 3.2864, 3.2760],
        [3.2742, 3.5973, 3.6310],
        [3.2742, 4.0755, 4.5890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.1345692276954651 
model_pd.l_d.mean(): -20.77975082397461 
model_pd.lagr.mean(): -20.64518165588379 
model_pd.lambdas: dict_items([('pout', tensor([1.0544], device='cuda:0')), ('power', tensor([0.9289], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8603], device='cuda:0')), ('power', tensor([-23.3180], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.1345692276954651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2821, 3.2943, 3.2839],
        [3.2821, 3.6053, 3.6389],
        [3.2821, 4.0847, 4.5986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.13416635990142822 
model_pd.l_d.mean(): -20.751983642578125 
model_pd.lagr.mean(): -20.617816925048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0552], device='cuda:0')), ('power', tensor([0.9277], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8573], device='cuda:0')), ('power', tensor([-23.3147], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.13416635990142822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2882, 4.0915, 4.6054],
        [3.2882, 3.3004, 3.2900],
        [3.2882, 3.6114, 3.6447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.13386055827140808 
model_pd.l_d.mean(): -20.724328994750977 
model_pd.lagr.mean(): -20.590469360351562 
model_pd.lambdas: dict_items([('pout', tensor([1.0561], device='cuda:0')), ('power', tensor([0.9265], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8550], device='cuda:0')), ('power', tensor([-23.3124], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.13386055827140808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2924, 3.6153, 3.6484],
        [3.2924, 3.3046, 3.2942],
        [3.2924, 4.0957, 4.6093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.1336609572172165 
model_pd.l_d.mean(): -20.696802139282227 
model_pd.lagr.mean(): -20.563140869140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0570], device='cuda:0')), ('power', tensor([0.9254], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8535], device='cuda:0')), ('power', tensor([-23.3111], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.1336609572172165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2946, 4.0973, 4.6102],
        [3.2946, 3.3068, 3.2964],
        [3.2946, 3.6169, 3.6498]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.13356870412826538 
model_pd.l_d.mean(): -20.669414520263672 
model_pd.lagr.mean(): -20.535846710205078 
model_pd.lambdas: dict_items([('pout', tensor([1.0578], device='cuda:0')), ('power', tensor([0.9242], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8528], device='cuda:0')), ('power', tensor([-23.3109], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.13356870412826538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2949, 4.0964, 4.6081],
        [3.2949, 3.3070, 3.2967],
        [3.2949, 3.6164, 3.6490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.13357704877853394 
model_pd.l_d.mean(): -20.642147064208984 
model_pd.lagr.mean(): -20.508569717407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0587], device='cuda:0')), ('power', tensor([0.9230], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8529], device='cuda:0')), ('power', tensor([-23.3117], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.13357704877853394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2936, 3.6140, 3.6464],
        [3.2936, 3.3056, 3.2954],
        [3.2936, 4.0934, 4.6036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.13367216289043427 
model_pd.l_d.mean(): -20.614990234375 
model_pd.lagr.mean(): -20.4813175201416 
model_pd.lambdas: dict_items([('pout', tensor([1.0595], device='cuda:0')), ('power', tensor([0.9219], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8537], device='cuda:0')), ('power', tensor([-23.3133], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.13367216289043427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2910, 3.3030, 3.2928],
        [3.2910, 3.6103, 3.6423],
        [3.2910, 4.0887, 4.5971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.1338346302509308 
model_pd.l_d.mean(): -20.58789825439453 
model_pd.lagr.mean(): -20.454063415527344 
model_pd.lambdas: dict_items([('pout', tensor([1.0604], device='cuda:0')), ('power', tensor([0.9207], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8549], device='cuda:0')), ('power', tensor([-23.3156], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.1338346302509308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2877, 4.0829, 4.5895],
        [3.2877, 3.6056, 3.6373],
        [3.2877, 3.2996, 3.2894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.13404080271720886 
model_pd.l_d.mean(): -20.560848236083984 
model_pd.lagr.mean(): -20.426807403564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0612], device='cuda:0')), ('power', tensor([0.9195], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8565], device='cuda:0')), ('power', tensor([-23.3184], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.13404080271720886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2840, 4.0768, 4.5813],
        [3.2840, 3.2958, 3.2857],
        [3.2840, 3.6006, 3.6320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.13426457345485687 
model_pd.l_d.mean(): -20.533796310424805 
model_pd.lagr.mean(): -20.399532318115234 
model_pd.lambdas: dict_items([('pout', tensor([1.0621], device='cuda:0')), ('power', tensor([0.9184], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8582], device='cuda:0')), ('power', tensor([-23.3213], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.13426457345485687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2805, 3.2923, 3.2822],
        [3.2805, 4.0709, 4.5735],
        [3.2805, 3.5959, 3.6269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.1344795823097229 
model_pd.l_d.mean(): -20.506717681884766 
model_pd.lagr.mean(): -20.372238159179688 
model_pd.lambdas: dict_items([('pout', tensor([1.0629], device='cuda:0')), ('power', tensor([0.9172], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8598], device='cuda:0')), ('power', tensor([-23.3241], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.1344795823097229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2776, 4.0658, 4.5666],
        [3.2776, 3.5918, 3.6225],
        [3.2776, 3.2894, 3.2794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.134661465883255 
model_pd.l_d.mean(): -20.479583740234375 
model_pd.lagr.mean(): -20.34492301940918 
model_pd.lambdas: dict_items([('pout', tensor([1.0638], device='cuda:0')), ('power', tensor([0.9160], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8612], device='cuda:0')), ('power', tensor([-23.3266], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.134661465883255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2758, 3.2874, 3.2775],
        [3.2758, 3.5888, 3.6193],
        [3.2758, 4.0620, 4.5613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.1347900778055191 
model_pd.l_d.mean(): -20.452383041381836 
model_pd.lagr.mean(): -20.31759262084961 
model_pd.lambdas: dict_items([('pout', tensor([1.0647], device='cuda:0')), ('power', tensor([0.9149], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8621], device='cuda:0')), ('power', tensor([-23.3285], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.1347900778055191
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2751, 3.5873, 3.6174],
        [3.2751, 4.0599, 4.5578],
        [3.2751, 3.2867, 3.2768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.13485121726989746 
model_pd.l_d.mean(): -20.425111770629883 
model_pd.lagr.mean(): -20.290260314941406 
model_pd.lambdas: dict_items([('pout', tensor([1.0655], device='cuda:0')), ('power', tensor([0.9137], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8626], device='cuda:0')), ('power', tensor([-23.3298], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.13485121726989746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2758, 3.5872, 3.6171],
        [3.2758, 3.2874, 3.2775],
        [3.2758, 4.0595, 4.5564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.1348378211259842 
model_pd.l_d.mean(): -20.39776039123535 
model_pd.lagr.mean(): -20.262922286987305 
model_pd.lambdas: dict_items([('pout', tensor([1.0664], device='cuda:0')), ('power', tensor([0.9125], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8625], device='cuda:0')), ('power', tensor([-23.3304], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.1348378211259842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2779, 3.2894, 3.2796],
        [3.2779, 3.5887, 3.6183],
        [3.2779, 4.0610, 4.5570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.13475048542022705 
model_pd.l_d.mean(): -20.37034034729004 
model_pd.lagr.mean(): -20.2355899810791 
model_pd.lambdas: dict_items([('pout', tensor([1.0673], device='cuda:0')), ('power', tensor([0.9114], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8619], device='cuda:0')), ('power', tensor([-23.3302], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.13475048542022705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2812, 3.5915, 3.6209],
        [3.2812, 4.0640, 4.5594],
        [3.2812, 3.2926, 3.2829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.1345970332622528 
model_pd.l_d.mean(): -20.34285545349121 
model_pd.lagr.mean(): -20.2082576751709 
model_pd.lambdas: dict_items([('pout', tensor([1.0681], device='cuda:0')), ('power', tensor([0.9102], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8608], device='cuda:0')), ('power', tensor([-23.3294], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.1345970332622528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2854, 4.0683, 4.5634],
        [3.2854, 3.5954, 3.6246],
        [3.2854, 3.2968, 3.2871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.13439126312732697 
model_pd.l_d.mean(): -20.3153133392334 
model_pd.lagr.mean(): -20.18092155456543 
model_pd.lambdas: dict_items([('pout', tensor([1.0690], device='cuda:0')), ('power', tensor([0.9090], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8593], device='cuda:0')), ('power', tensor([-23.3281], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.13439126312732697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2903, 4.0735, 4.5683],
        [3.2903, 3.3017, 3.2919],
        [3.2903, 3.6001, 3.6290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.1341513842344284 
model_pd.l_d.mean(): -20.28773307800293 
model_pd.lagr.mean(): -20.153581619262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0698], device='cuda:0')), ('power', tensor([0.9079], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8575], device='cuda:0')), ('power', tensor([-23.3264], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.1341513842344284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2954, 4.0790, 4.5737],
        [3.2954, 3.6050, 3.6338],
        [3.2954, 3.3068, 3.2971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.13389772176742554 
model_pd.l_d.mean(): -20.260129928588867 
model_pd.lagr.mean(): -20.126232147216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0707], device='cuda:0')), ('power', tensor([0.9067], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8556], device='cuda:0')), ('power', tensor([-23.3246], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.13389772176742554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3005, 3.6099, 3.6384],
        [3.3005, 4.0845, 4.5789],
        [3.3005, 3.3118, 3.3022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.13365058600902557 
model_pd.l_d.mean(): -20.23253059387207 
model_pd.lagr.mean(): -20.098880767822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0715], device='cuda:0')), ('power', tensor([0.9055], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8538], device='cuda:0')), ('power', tensor([-23.3227], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.13365058600902557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3051, 3.3164, 3.3068],
        [3.3051, 3.6143, 3.6426],
        [3.3051, 4.0893, 4.5836]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.13342832028865814 
model_pd.l_d.mean(): -20.204946517944336 
model_pd.lagr.mean(): -20.071517944335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0724], device='cuda:0')), ('power', tensor([0.9044], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8521], device='cuda:0')), ('power', tensor([-23.3211], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.13342832028865814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3090, 3.3203, 3.3107],
        [3.3090, 4.0932, 4.5871],
        [3.3090, 3.6178, 3.6459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.1332455575466156 
model_pd.l_d.mean(): -20.17740249633789 
model_pd.lagr.mean(): -20.044157028198242 
model_pd.lambdas: dict_items([('pout', tensor([1.0732], device='cuda:0')), ('power', tensor([0.9032], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8507], device='cuda:0')), ('power', tensor([-23.3199], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.1332455575466156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3120, 4.0959, 4.5892],
        [3.3120, 3.6204, 3.6482],
        [3.3120, 3.3232, 3.3136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.13311195373535156 
model_pd.l_d.mean(): -20.149904251098633 
model_pd.lagr.mean(): -20.01679229736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0741], device='cuda:0')), ('power', tensor([0.9020], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8497], device='cuda:0')), ('power', tensor([-23.3192], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.13311195373535156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3139, 3.6217, 3.6493],
        [3.3139, 3.3251, 3.3155],
        [3.3139, 4.0973, 4.5898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.1330338418483734 
model_pd.l_d.mean(): -20.122461318969727 
model_pd.lagr.mean(): -19.98942756652832 
model_pd.lambdas: dict_items([('pout', tensor([1.0749], device='cuda:0')), ('power', tensor([0.9009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8491], device='cuda:0')), ('power', tensor([-23.3190], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.1330338418483734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3147, 4.0972, 4.5888],
        [3.3147, 3.3259, 3.3164],
        [3.3147, 3.6219, 3.6492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.13301128149032593 
model_pd.l_d.mean(): -20.095075607299805 
model_pd.lagr.mean(): -19.962064743041992 
model_pd.lambdas: dict_items([('pout', tensor([1.0758], device='cuda:0')), ('power', tensor([0.8997], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8490], device='cuda:0')), ('power', tensor([-23.3194], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.13301128149032593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3146, 4.0959, 4.5864],
        [3.3146, 3.3258, 3.3163],
        [3.3146, 3.6210, 3.6481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.13303782045841217 
model_pd.l_d.mean(): -20.06773567199707 
model_pd.lagr.mean(): -19.9346981048584 
model_pd.lambdas: dict_items([('pout', tensor([1.0766], device='cuda:0')), ('power', tensor([0.8985], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8492], device='cuda:0')), ('power', tensor([-23.3203], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.13303782045841217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3139, 3.3249, 3.3155],
        [3.3139, 4.0937, 4.5829],
        [3.3139, 3.6194, 3.6462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.133101224899292 
model_pd.l_d.mean(): -20.040430068969727 
model_pd.lagr.mean(): -19.907329559326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0775], device='cuda:0')), ('power', tensor([0.8974], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8497], device='cuda:0')), ('power', tensor([-23.3216], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.133101224899292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3127, 4.0910, 4.5788],
        [3.3127, 3.3237, 3.3143],
        [3.3127, 3.6173, 3.6438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.13318543136119843 
model_pd.l_d.mean(): -20.013137817382812 
model_pd.lagr.mean(): -19.87995147705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0783], device='cuda:0')), ('power', tensor([0.8962], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8504], device='cuda:0')), ('power', tensor([-23.3231], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.13318543136119843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3115, 4.0883, 4.5747],
        [3.3115, 3.3224, 3.3131],
        [3.3115, 3.6151, 3.6414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.1332722008228302 
model_pd.l_d.mean(): -19.98583984375 
model_pd.lagr.mean(): -19.852567672729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0792], device='cuda:0')), ('power', tensor([0.8950], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8510], device='cuda:0')), ('power', tensor([-23.3246], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.1332722008228302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3105, 3.3215, 3.3121],
        [3.3105, 3.6133, 3.6393],
        [3.3105, 4.0859, 4.5711]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.1333436369895935 
model_pd.l_d.mean(): -19.958515167236328 
model_pd.lagr.mean(): -19.825172424316406 
model_pd.lambdas: dict_items([('pout', tensor([1.0800], device='cuda:0')), ('power', tensor([0.8939], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8516], device='cuda:0')), ('power', tensor([-23.3260], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.1333436369895935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3102, 3.6121, 3.6379],
        [3.3102, 4.0843, 4.5683],
        [3.3102, 3.3211, 3.3118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.13338443636894226 
model_pd.l_d.mean(): -19.9311580657959 
model_pd.lagr.mean(): -19.797773361206055 
model_pd.lambdas: dict_items([('pout', tensor([1.0809], device='cuda:0')), ('power', tensor([0.8927], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8519], device='cuda:0')), ('power', tensor([-23.3270], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.13338443636894226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3106, 4.0837, 4.5667],
        [3.3106, 3.3214, 3.3122],
        [3.3106, 3.6118, 3.6373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.13338389992713928 
model_pd.l_d.mean(): -19.903751373291016 
model_pd.lagr.mean(): -19.770366668701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0818], device='cuda:0')), ('power', tensor([0.8915], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8519], device='cuda:0')), ('power', tensor([-23.3276], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.13338389992713928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3119, 4.0842, 4.5665],
        [3.3119, 3.6125, 3.6378],
        [3.3119, 3.3227, 3.3135]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.1333369016647339 
model_pd.l_d.mean(): -19.876296997070312 
model_pd.lagr.mean(): -19.74295997619629 
model_pd.lambdas: dict_items([('pout', tensor([1.0826], device='cuda:0')), ('power', tensor([0.8904], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8516], device='cuda:0')), ('power', tensor([-23.3277], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.1333369016647339
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3141, 3.3248, 3.3156],
        [3.3141, 4.0859, 4.5675],
        [3.3141, 3.6142, 3.6392]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.1332445740699768 
model_pd.l_d.mean(): -19.848791122436523 
model_pd.lagr.mean(): -19.715545654296875 
model_pd.lambdas: dict_items([('pout', tensor([1.0835], device='cuda:0')), ('power', tensor([0.8892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8509], device='cuda:0')), ('power', tensor([-23.3274], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.1332445740699768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3169, 3.6167, 3.6415],
        [3.3169, 4.0886, 4.5697],
        [3.3169, 3.3277, 3.3185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.13311414420604706 
model_pd.l_d.mean(): -19.821252822875977 
model_pd.lagr.mean(): -19.688138961791992 
model_pd.lambdas: dict_items([('pout', tensor([1.0843], device='cuda:0')), ('power', tensor([0.8880], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8499], device='cuda:0')), ('power', tensor([-23.3266], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.13311414420604706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3203, 3.3310, 3.3219],
        [3.3203, 3.6197, 3.6443],
        [3.3203, 4.0919, 4.5726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.13295751810073853 
model_pd.l_d.mean(): -19.793684005737305 
model_pd.lagr.mean(): -19.66072654724121 
model_pd.lambdas: dict_items([('pout', tensor([1.0852], device='cuda:0')), ('power', tensor([0.8869], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8488], device='cuda:0')), ('power', tensor([-23.3256], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.13295751810073853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3239, 3.3346, 3.3255],
        [3.3239, 4.0956, 4.5759],
        [3.3239, 3.6230, 3.6474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.13278931379318237 
model_pd.l_d.mean(): -19.766101837158203 
model_pd.lagr.mean(): -19.633312225341797 
model_pd.lambdas: dict_items([('pout', tensor([1.0860], device='cuda:0')), ('power', tensor([0.8857], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8475], device='cuda:0')), ('power', tensor([-23.3244], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.13278931379318237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3275, 4.0991, 4.5791],
        [3.3275, 3.6263, 3.6504],
        [3.3275, 3.3381, 3.3290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.13262589275836945 
model_pd.l_d.mean(): -19.738515853881836 
model_pd.lagr.mean(): -19.60589027404785 
model_pd.lambdas: dict_items([('pout', tensor([1.0868], device='cuda:0')), ('power', tensor([0.8845], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8462], device='cuda:0')), ('power', tensor([-23.3233], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.13262589275836945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3306, 4.1022, 4.5817],
        [3.3306, 3.6290, 3.6530],
        [3.3306, 3.3412, 3.3321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.13248391449451447 
model_pd.l_d.mean(): -19.71094512939453 
model_pd.lagr.mean(): -19.578460693359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0877], device='cuda:0')), ('power', tensor([0.8834], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8452], device='cuda:0')), ('power', tensor([-23.3223], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.13248391449451447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3331, 3.6311, 3.6549],
        [3.3331, 3.3436, 3.3346],
        [3.3331, 4.1044, 4.5834]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.13237467408180237 
model_pd.l_d.mean(): -19.68340301513672 
model_pd.lagr.mean(): -19.551029205322266 
model_pd.lambdas: dict_items([('pout', tensor([1.0885], device='cuda:0')), ('power', tensor([0.8822], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8443], device='cuda:0')), ('power', tensor([-23.3217], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.13237467408180237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3348, 4.1056, 4.5840],
        [3.3348, 3.6324, 3.6559],
        [3.3348, 3.3454, 3.3364]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.13230329751968384 
model_pd.l_d.mean(): -19.655899047851562 
model_pd.lagr.mean(): -19.523595809936523 
model_pd.lambdas: dict_items([('pout', tensor([1.0894], device='cuda:0')), ('power', tensor([0.8810], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8438], device='cuda:0')), ('power', tensor([-23.3215], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.13230329751968384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3359, 4.1060, 4.5835],
        [3.3359, 3.6329, 3.6562],
        [3.3359, 3.3464, 3.3374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.13226868212223053 
model_pd.l_d.mean(): -19.6284236907959 
model_pd.lagr.mean(): -19.49615478515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0902], device='cuda:0')), ('power', tensor([0.8799], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8436], device='cuda:0')), ('power', tensor([-23.3217], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.13226868212223053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3364, 3.3468, 3.3379],
        [3.3364, 4.1056, 4.5822],
        [3.3364, 3.6327, 3.6558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.13226386904716492 
model_pd.l_d.mean(): -19.60097885131836 
model_pd.lagr.mean(): -19.46871566772461 
model_pd.lambdas: dict_items([('pout', tensor([1.0911], device='cuda:0')), ('power', tensor([0.8787], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8435], device='cuda:0')), ('power', tensor([-23.3222], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.13226386904716492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3365, 4.1047, 4.5804],
        [3.3365, 3.3469, 3.3380],
        [3.3365, 3.6321, 3.6550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.13227719068527222 
model_pd.l_d.mean(): -19.573545455932617 
model_pd.lagr.mean(): -19.441268920898438 
model_pd.lambdas: dict_items([('pout', tensor([1.0919], device='cuda:0')), ('power', tensor([0.8775], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8437], device='cuda:0')), ('power', tensor([-23.3229], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.13227719068527222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3365, 3.6314, 3.6540],
        [3.3365, 4.1036, 4.5783],
        [3.3365, 3.3468, 3.3380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.13229703903198242 
model_pd.l_d.mean(): -19.54611587524414 
model_pd.lagr.mean(): -19.413818359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0928], device='cuda:0')), ('power', tensor([0.8764], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8438], device='cuda:0')), ('power', tensor([-23.3237], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.13229703903198242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3366, 4.1027, 4.5765],
        [3.3366, 3.6309, 3.6532],
        [3.3366, 3.3469, 3.3381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.1323111653327942 
model_pd.l_d.mean(): -19.5186710357666 
model_pd.lagr.mean(): -19.38636016845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0936], device='cuda:0')), ('power', tensor([0.8752], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8439], device='cuda:0')), ('power', tensor([-23.3244], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.1323111653327942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3370, 4.1022, 4.5751],
        [3.3370, 3.3473, 3.3385],
        [3.3370, 3.6306, 3.6528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.13230863213539124 
model_pd.l_d.mean(): -19.491207122802734 
model_pd.lagr.mean(): -19.358898162841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0944], device='cuda:0')), ('power', tensor([0.8740], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8439], device='cuda:0')), ('power', tensor([-23.3249], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.13230863213539124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3379, 3.3481, 3.3394],
        [3.3379, 3.6310, 3.6529],
        [3.3379, 4.1024, 4.5745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.13228176534175873 
model_pd.l_d.mean(): -19.463712692260742 
model_pd.lagr.mean(): -19.331430435180664 
model_pd.lambdas: dict_items([('pout', tensor([1.0953], device='cuda:0')), ('power', tensor([0.8729], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8438], device='cuda:0')), ('power', tensor([-23.3251], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.13228176534175873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3393, 4.1033, 4.5747],
        [3.3393, 3.3495, 3.3408],
        [3.3393, 3.6319, 3.6536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.13222698867321014 
model_pd.l_d.mean(): -19.436189651489258 
model_pd.lagr.mean(): -19.30396270751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0961], device='cuda:0')), ('power', tensor([0.8717], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8433], device='cuda:0')), ('power', tensor([-23.3250], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.13222698867321014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3413, 3.3514, 3.3427],
        [3.3413, 3.6334, 3.6550],
        [3.3413, 4.1048, 4.5757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.13214533030986786 
model_pd.l_d.mean(): -19.408641815185547 
model_pd.lagr.mean(): -19.27649688720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0970], device='cuda:0')), ('power', tensor([0.8705], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8427], device='cuda:0')), ('power', tensor([-23.3247], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.13214533030986786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3436, 3.6354, 3.6568],
        [3.3436, 3.3538, 3.3451],
        [3.3436, 4.1069, 4.5773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.1320418417453766 
model_pd.l_d.mean(): -19.38106918334961 
model_pd.lagr.mean(): -19.249027252197266 
model_pd.lambdas: dict_items([('pout', tensor([1.0978], device='cuda:0')), ('power', tensor([0.8694], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8419], device='cuda:0')), ('power', tensor([-23.3241], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.1320418417453766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3461, 3.6376, 3.6588],
        [3.3461, 4.1092, 4.5792],
        [3.3461, 3.3563, 3.3476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.13193129003047943 
model_pd.l_d.mean(): -19.353487014770508 
model_pd.lagr.mean(): -19.221555709838867 
model_pd.lambdas: dict_items([('pout', tensor([1.0987], device='cuda:0')), ('power', tensor([0.8682], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8411], device='cuda:0')), ('power', tensor([-23.3234], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.13193129003047943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3485, 3.6396, 3.6605],
        [3.3485, 3.3586, 3.3499],
        [3.3485, 4.1113, 4.5808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.13183002173900604 
model_pd.l_d.mean(): -19.325910568237305 
model_pd.lagr.mean(): -19.194080352783203 
model_pd.lambdas: dict_items([('pout', tensor([1.0995], device='cuda:0')), ('power', tensor([0.8670], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8403], device='cuda:0')), ('power', tensor([-23.3228], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.13183002173900604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3504, 3.6411, 3.6619],
        [3.3504, 3.3605, 3.3519],
        [3.3504, 4.1129, 4.5818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.13174980878829956 
model_pd.l_d.mean(): -19.298355102539062 
model_pd.lagr.mean(): -19.16660499572754 
model_pd.lambdas: dict_items([('pout', tensor([1.1003], device='cuda:0')), ('power', tensor([0.8659], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8397], device='cuda:0')), ('power', tensor([-23.3224], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.13174980878829956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3518, 3.3619, 3.3533],
        [3.3518, 3.6421, 3.6626],
        [3.3518, 4.1137, 4.5820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.13169610500335693 
model_pd.l_d.mean(): -19.270822525024414 
model_pd.lagr.mean(): -19.13912582397461 
model_pd.lambdas: dict_items([('pout', tensor([1.1012], device='cuda:0')), ('power', tensor([0.8647], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8393], device='cuda:0')), ('power', tensor([-23.3223], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.13169610500335693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3528, 3.6425, 3.6628],
        [3.3528, 4.1140, 4.5815],
        [3.3528, 3.3628, 3.3542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.13166533410549164 
model_pd.l_d.mean(): -19.243322372436523 
model_pd.lagr.mean(): -19.111656188964844 
model_pd.lambdas: dict_items([('pout', tensor([1.1020], device='cuda:0')), ('power', tensor([0.8635], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8391], device='cuda:0')), ('power', tensor([-23.3225], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.13166533410549164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3535, 3.6426, 3.6627],
        [3.3535, 3.3634, 3.3549],
        [3.3535, 4.1139, 4.5806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.13164909183979034 
model_pd.l_d.mean(): -19.21583366394043 
model_pd.lagr.mean(): -19.084184646606445 
model_pd.lambdas: dict_items([('pout', tensor([1.1029], device='cuda:0')), ('power', tensor([0.8624], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8389], device='cuda:0')), ('power', tensor([-23.3228], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.13164909183979034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3540, 4.1136, 4.5795],
        [3.3540, 3.3639, 3.3554],
        [3.3540, 3.6425, 3.6625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.13164043426513672 
model_pd.l_d.mean(): -19.188335418701172 
model_pd.lagr.mean(): -19.05669403076172 
model_pd.lambdas: dict_items([('pout', tensor([1.1037], device='cuda:0')), ('power', tensor([0.8612], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8389], device='cuda:0')), ('power', tensor([-23.3232], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.13164043426513672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3544, 4.1133, 4.5784],
        [3.3544, 3.6425, 3.6622],
        [3.3544, 3.3644, 3.3559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.13163387775421143 
model_pd.l_d.mean(): -19.160823822021484 
model_pd.lagr.mean(): -19.029190063476562 
model_pd.lambdas: dict_items([('pout', tensor([1.1045], device='cuda:0')), ('power', tensor([0.8600], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8389], device='cuda:0')), ('power', tensor([-23.3236], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.13163387775421143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3550, 4.1131, 4.5774],
        [3.3550, 3.6425, 3.6621],
        [3.3550, 3.3649, 3.3564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.13162213563919067 
model_pd.l_d.mean(): -19.13329315185547 
model_pd.lagr.mean(): -19.001670837402344 
model_pd.lambdas: dict_items([('pout', tensor([1.1054], device='cuda:0')), ('power', tensor([0.8589], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8388], device='cuda:0')), ('power', tensor([-23.3239], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.13162213563919067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3558, 3.6429, 3.6623],
        [3.3558, 3.3657, 3.3573],
        [3.3558, 4.1133, 4.5770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.13159602880477905 
model_pd.l_d.mean(): -19.10573959350586 
model_pd.lagr.mean(): -18.974143981933594 
model_pd.lambdas: dict_items([('pout', tensor([1.1062], device='cuda:0')), ('power', tensor([0.8577], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8386], device='cuda:0')), ('power', tensor([-23.3241], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.13159602880477905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3571, 3.6437, 3.6630],
        [3.3571, 4.1141, 4.5772],
        [3.3571, 3.3669, 3.3585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.13154849410057068 
model_pd.l_d.mean(): -19.078157424926758 
model_pd.lagr.mean(): -18.946609497070312 
model_pd.lambdas: dict_items([('pout', tensor([1.1070], device='cuda:0')), ('power', tensor([0.8566], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8382], device='cuda:0')), ('power', tensor([-23.3240], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.13154849410057068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3588, 3.3686, 3.3602],
        [3.3588, 3.6451, 3.6642],
        [3.3588, 4.1155, 4.5781]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.13147592544555664 
model_pd.l_d.mean(): -19.050556182861328 
model_pd.lagr.mean(): -18.91908073425293 
model_pd.lambdas: dict_items([('pout', tensor([1.1079], device='cuda:0')), ('power', tensor([0.8554], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8377], device='cuda:0')), ('power', tensor([-23.3237], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.13147592544555664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3611, 4.1176, 4.5798],
        [3.3611, 3.6471, 3.6660],
        [3.3611, 3.3708, 3.3625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.13137879967689514 
model_pd.l_d.mean(): -19.022930145263672 
model_pd.lagr.mean(): -18.891551971435547 
model_pd.lambdas: dict_items([('pout', tensor([1.1087], device='cuda:0')), ('power', tensor([0.8542], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8369], device='cuda:0')), ('power', tensor([-23.3230], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.13137879967689514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3637, 4.1202, 4.5822],
        [3.3637, 3.6495, 3.6682],
        [3.3637, 3.3734, 3.3651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.13126195967197418 
model_pd.l_d.mean(): -18.995290756225586 
model_pd.lagr.mean(): -18.864028930664062 
model_pd.lambdas: dict_items([('pout', tensor([1.1096], device='cuda:0')), ('power', tensor([0.8531], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8360], device='cuda:0')), ('power', tensor([-23.3221], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.13126195967197418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3665, 4.1231, 4.5848],
        [3.3665, 3.6521, 3.6707],
        [3.3665, 3.3763, 3.3679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.1311338245868683 
model_pd.l_d.mean(): -18.96764373779297 
model_pd.lagr.mean(): -18.836509704589844 
model_pd.lambdas: dict_items([('pout', tensor([1.1104], device='cuda:0')), ('power', tensor([0.8519], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8350], device='cuda:0')), ('power', tensor([-23.3211], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.1311338245868683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3694, 3.3791, 3.3708],
        [3.3694, 4.1260, 4.5876],
        [3.3694, 3.6548, 3.6733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.13100406527519226 
model_pd.l_d.mean(): -18.93999481201172 
model_pd.lagr.mean(): -18.808990478515625 
model_pd.lambdas: dict_items([('pout', tensor([1.1112], device='cuda:0')), ('power', tensor([0.8507], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8340], device='cuda:0')), ('power', tensor([-23.3201], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.13100406527519226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3722, 3.3819, 3.3736],
        [3.3722, 3.6573, 3.6756],
        [3.3722, 4.1288, 4.5901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.13088175654411316 
model_pd.l_d.mean(): -18.9123592376709 
model_pd.lagr.mean(): -18.781476974487305 
model_pd.lambdas: dict_items([('pout', tensor([1.1121], device='cuda:0')), ('power', tensor([0.8496], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8330], device='cuda:0')), ('power', tensor([-23.3191], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.13088175654411316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3747, 3.3843, 3.3761],
        [3.3747, 4.1312, 4.5921],
        [3.3747, 3.6595, 3.6777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.13077329099178314 
model_pd.l_d.mean(): -18.884742736816406 
model_pd.lagr.mean(): -18.753969192504883 
model_pd.lambdas: dict_items([('pout', tensor([1.1129], device='cuda:0')), ('power', tensor([0.8484], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8322], device='cuda:0')), ('power', tensor([-23.3183], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.13077329099178314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3768, 3.6614, 3.6794],
        [3.3768, 3.3865, 3.3782],
        [3.3768, 4.1332, 4.5937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.13068178296089172 
model_pd.l_d.mean(): -18.857145309448242 
model_pd.lagr.mean(): -18.726463317871094 
model_pd.lambdas: dict_items([('pout', tensor([1.1137], device='cuda:0')), ('power', tensor([0.8472], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8314], device='cuda:0')), ('power', tensor([-23.3177], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.13068178296089172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3787, 3.6628, 3.6807],
        [3.3787, 4.1347, 4.5948],
        [3.3787, 3.3883, 3.3800]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.1306069791316986 
model_pd.l_d.mean(): -18.829570770263672 
model_pd.lagr.mean(): -18.698963165283203 
model_pd.lambdas: dict_items([('pout', tensor([1.1146], device='cuda:0')), ('power', tensor([0.8461], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8309], device='cuda:0')), ('power', tensor([-23.3173], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.1306069791316986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3802, 4.1358, 4.5954],
        [3.3802, 3.3898, 3.3816],
        [3.3802, 3.6640, 3.6816]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.13054519891738892 
model_pd.l_d.mean(): -18.8020076751709 
model_pd.lagr.mean(): -18.671463012695312 
model_pd.lambdas: dict_items([('pout', tensor([1.1154], device='cuda:0')), ('power', tensor([0.8449], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8304], device='cuda:0')), ('power', tensor([-23.3170], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.13054519891738892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3817, 3.3912, 3.3830],
        [3.3817, 4.1368, 4.5958],
        [3.3817, 3.6650, 3.6825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.13049060106277466 
model_pd.l_d.mean(): -18.774457931518555 
model_pd.lagr.mean(): -18.643966674804688 
model_pd.lambdas: dict_items([('pout', tensor([1.1162], device='cuda:0')), ('power', tensor([0.8437], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8300], device='cuda:0')), ('power', tensor([-23.3168], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.13049060106277466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3831, 3.6660, 3.6833],
        [3.3831, 3.3926, 3.3845],
        [3.3831, 4.1378, 4.5962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13043639063835144 
model_pd.l_d.mean(): -18.74690818786621 
model_pd.lagr.mean(): -18.616472244262695 
model_pd.lambdas: dict_items([('pout', tensor([1.1170], device='cuda:0')), ('power', tensor([0.8426], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8295], device='cuda:0')), ('power', tensor([-23.3167], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.13043639063835144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3846, 4.1389, 4.5968],
        [3.3846, 3.3941, 3.3860],
        [3.3846, 3.6671, 3.6842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.130376398563385 
model_pd.l_d.mean(): -18.719358444213867 
model_pd.lagr.mean(): -18.58898162841797 
model_pd.lambdas: dict_items([('pout', tensor([1.1179], device='cuda:0')), ('power', tensor([0.8414], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8291], device='cuda:0')), ('power', tensor([-23.3164], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.130376398563385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3864, 3.6685, 3.6854],
        [3.3864, 4.1403, 4.5977],
        [3.3864, 3.3958, 3.3878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.13030585646629333 
model_pd.l_d.mean(): -18.69179916381836 
model_pd.lagr.mean(): -18.561492919921875 
model_pd.lambdas: dict_items([('pout', tensor([1.1187], device='cuda:0')), ('power', tensor([0.8402], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8285], device='cuda:0')), ('power', tensor([-23.3161], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.13030585646629333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3884, 3.3978, 3.3898],
        [3.3884, 3.6701, 3.6869],
        [3.3884, 4.1420, 4.5990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.1302226036787033 
model_pd.l_d.mean(): -18.664228439331055 
model_pd.lagr.mean(): -18.534006118774414 
model_pd.lambdas: dict_items([('pout', tensor([1.1195], device='cuda:0')), ('power', tensor([0.8391], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8279], device='cuda:0')), ('power', tensor([-23.3155], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.1302226036787033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3907, 3.4001, 3.3920],
        [3.3907, 3.6721, 3.6886],
        [3.3907, 4.1441, 4.6007]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.1301271915435791 
model_pd.l_d.mean(): -18.636648178100586 
model_pd.lagr.mean(): -18.506521224975586 
model_pd.lambdas: dict_items([('pout', tensor([1.1204], device='cuda:0')), ('power', tensor([0.8379], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8271], device='cuda:0')), ('power', tensor([-23.3149], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.1301271915435791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3932, 4.1464, 4.6026],
        [3.3932, 3.4025, 3.3945],
        [3.3932, 3.6742, 3.6906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.13002270460128784 
model_pd.l_d.mean(): -18.609066009521484 
model_pd.lagr.mean(): -18.47904396057129 
model_pd.lambdas: dict_items([('pout', tensor([1.1212], device='cuda:0')), ('power', tensor([0.8367], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8263], device='cuda:0')), ('power', tensor([-23.3141], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.13002270460128784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3957, 3.6765, 3.6927],
        [3.3957, 4.1489, 4.6048],
        [3.3957, 3.4051, 3.3971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.12991368770599365 
model_pd.l_d.mean(): -18.581480026245117 
model_pd.lagr.mean(): -18.451566696166992 
model_pd.lambdas: dict_items([('pout', tensor([1.1220], device='cuda:0')), ('power', tensor([0.8356], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8254], device='cuda:0')), ('power', tensor([-23.3133], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.12991368770599365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3983, 3.4076, 3.3996],
        [3.3983, 3.6788, 3.6948],
        [3.3983, 4.1513, 4.6069]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.12980493903160095 
model_pd.l_d.mean(): -18.553897857666016 
model_pd.lagr.mean(): -18.42409324645996 
model_pd.lambdas: dict_items([('pout', tensor([1.1228], device='cuda:0')), ('power', tensor([0.8344], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8245], device='cuda:0')), ('power', tensor([-23.3124], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.12980493903160095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4008, 4.1537, 4.6088],
        [3.4008, 3.6809, 3.6968],
        [3.4008, 3.4101, 3.4021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.1297006458044052 
model_pd.l_d.mean(): -18.526323318481445 
model_pd.lagr.mean(): -18.396623611450195 
model_pd.lambdas: dict_items([('pout', tensor([1.1237], device='cuda:0')), ('power', tensor([0.8332], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8237], device='cuda:0')), ('power', tensor([-23.3116], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.1297006458044052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4031, 4.1558, 4.6106],
        [3.4031, 3.4124, 3.4044],
        [3.4031, 3.6829, 3.6986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.12960359454154968 
model_pd.l_d.mean(): -18.498754501342773 
model_pd.lagr.mean(): -18.369150161743164 
model_pd.lambdas: dict_items([('pout', tensor([1.1245], device='cuda:0')), ('power', tensor([0.8321], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8229], device='cuda:0')), ('power', tensor([-23.3109], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.12960359454154968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4053, 4.1578, 4.6121],
        [3.4053, 3.4145, 3.4066],
        [3.4053, 3.6847, 3.7003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.12951470911502838 
model_pd.l_d.mean(): -18.47120475769043 
model_pd.lagr.mean(): -18.341690063476562 
model_pd.lambdas: dict_items([('pout', tensor([1.1253], device='cuda:0')), ('power', tensor([0.8309], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8222], device='cuda:0')), ('power', tensor([-23.3103], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.12951470911502838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4073, 4.1595, 4.6134],
        [3.4073, 3.6864, 3.7017],
        [3.4073, 3.4165, 3.4086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.12943284213542938 
model_pd.l_d.mean(): -18.44365882873535 
model_pd.lagr.mean(): -18.314226150512695 
model_pd.lambdas: dict_items([('pout', tensor([1.1261], device='cuda:0')), ('power', tensor([0.8297], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8216], device='cuda:0')), ('power', tensor([-23.3098], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.12943284213542938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4093, 3.6879, 3.7031],
        [3.4093, 4.1611, 4.6145],
        [3.4093, 3.4185, 3.4106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.12935541570186615 
model_pd.l_d.mean(): -18.416120529174805 
model_pd.lagr.mean(): -18.286766052246094 
model_pd.lambdas: dict_items([('pout', tensor([1.1269], device='cuda:0')), ('power', tensor([0.8286], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8209], device='cuda:0')), ('power', tensor([-23.3093], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.12935541570186615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4112, 4.1627, 4.6156],
        [3.4112, 3.4204, 3.4125],
        [3.4112, 3.6895, 3.7044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.12927894294261932 
model_pd.l_d.mean(): -18.388586044311523 
model_pd.lagr.mean(): -18.259307861328125 
model_pd.lambdas: dict_items([('pout', tensor([1.1278], device='cuda:0')), ('power', tensor([0.8274], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8203], device='cuda:0')), ('power', tensor([-23.3089], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.12927894294261932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4132, 3.4223, 3.4145],
        [3.4132, 4.1644, 4.6168],
        [3.4132, 3.6911, 3.7058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.12919992208480835 
model_pd.l_d.mean(): -18.361047744750977 
model_pd.lagr.mean(): -18.231847763061523 
model_pd.lambdas: dict_items([('pout', tensor([1.1286], device='cuda:0')), ('power', tensor([0.8262], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8197], device='cuda:0')), ('power', tensor([-23.3084], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.12919992208480835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4153, 4.1662, 4.6182],
        [3.4153, 3.4244, 3.4166],
        [3.4153, 3.6928, 3.7073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.12911555171012878 
model_pd.l_d.mean(): -18.333513259887695 
model_pd.lagr.mean(): -18.204397201538086 
model_pd.lambdas: dict_items([('pout', tensor([1.1294], device='cuda:0')), ('power', tensor([0.8251], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8190], device='cuda:0')), ('power', tensor([-23.3078], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.12911555171012878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4176, 4.1682, 4.6198],
        [3.4176, 3.6947, 3.7090],
        [3.4176, 3.4266, 3.4189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.12902414798736572 
model_pd.l_d.mean(): -18.305971145629883 
model_pd.lagr.mean(): -18.17694664001465 
model_pd.lambdas: dict_items([('pout', tensor([1.1302], device='cuda:0')), ('power', tensor([0.8239], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8183], device='cuda:0')), ('power', tensor([-23.3071], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.12902414798736572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4200, 4.1704, 4.6216],
        [3.4200, 3.4290, 3.4213],
        [3.4200, 3.6967, 3.7109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.12892574071884155 
model_pd.l_d.mean(): -18.278423309326172 
model_pd.lagr.mean(): -18.149497985839844 
model_pd.lambdas: dict_items([('pout', tensor([1.1310], device='cuda:0')), ('power', tensor([0.8227], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8175], device='cuda:0')), ('power', tensor([-23.3064], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.12892574071884155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4225, 4.1728, 4.6237],
        [3.4225, 3.4315, 3.4238],
        [3.4225, 3.6989, 3.7129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.12882156670093536 
model_pd.l_d.mean(): -18.25087547302246 
model_pd.lagr.mean(): -18.122053146362305 
model_pd.lambdas: dict_items([('pout', tensor([1.1319], device='cuda:0')), ('power', tensor([0.8216], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8166], device='cuda:0')), ('power', tensor([-23.3056], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.12882156670093536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4252, 3.7012, 3.7150],
        [3.4252, 4.1753, 4.6258],
        [3.4252, 3.4341, 3.4264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.12871374189853668 
model_pd.l_d.mean(): -18.223323822021484 
model_pd.lagr.mean(): -18.0946102142334 
model_pd.lambdas: dict_items([('pout', tensor([1.1327], device='cuda:0')), ('power', tensor([0.8204], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8158], device='cuda:0')), ('power', tensor([-23.3047], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.12871374189853668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4278, 3.7036, 3.7172],
        [3.4278, 4.1779, 4.6280],
        [3.4278, 3.4367, 3.4291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.12860453128814697 
model_pd.l_d.mean(): -18.195775985717773 
model_pd.lagr.mean(): -18.067171096801758 
model_pd.lambdas: dict_items([('pout', tensor([1.1335], device='cuda:0')), ('power', tensor([0.8192], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8149], device='cuda:0')), ('power', tensor([-23.3038], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.12860453128814697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4305, 4.1804, 4.6302],
        [3.4305, 3.7059, 3.7193],
        [3.4305, 3.4394, 3.4317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.12849636375904083 
model_pd.l_d.mean(): -18.168228149414062 
model_pd.lagr.mean(): -18.039731979370117 
model_pd.lambdas: dict_items([('pout', tensor([1.1343], device='cuda:0')), ('power', tensor([0.8181], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8140], device='cuda:0')), ('power', tensor([-23.3029], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.12849636375904083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4331, 3.7081, 3.7214],
        [3.4331, 4.1828, 4.6323],
        [3.4331, 3.4419, 3.4343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.12839102745056152 
model_pd.l_d.mean(): -18.140689849853516 
model_pd.lagr.mean(): -18.012298583984375 
model_pd.lambdas: dict_items([('pout', tensor([1.1351], device='cuda:0')), ('power', tensor([0.8169], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8131], device='cuda:0')), ('power', tensor([-23.3020], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.12839102745056152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4356, 3.7103, 3.7234],
        [3.4356, 4.1852, 4.6342],
        [3.4356, 3.4444, 3.4368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.12828950583934784 
model_pd.l_d.mean(): -18.113155364990234 
model_pd.lagr.mean(): -17.984865188598633 
model_pd.lambdas: dict_items([('pout', tensor([1.1359], device='cuda:0')), ('power', tensor([0.8158], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8123], device='cuda:0')), ('power', tensor([-23.3012], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.12828950583934784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4380, 3.7124, 3.7252],
        [3.4380, 3.4468, 3.4393],
        [3.4380, 4.1874, 4.6361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.12819187343120575 
model_pd.l_d.mean(): -18.085630416870117 
model_pd.lagr.mean(): -17.957439422607422 
model_pd.lambdas: dict_items([('pout', tensor([1.1367], device='cuda:0')), ('power', tensor([0.8146], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8115], device='cuda:0')), ('power', tensor([-23.3004], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.12819187343120575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4404, 3.7144, 3.7271],
        [3.4404, 3.4492, 3.4416],
        [3.4404, 4.1896, 4.6378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.128097265958786 
model_pd.l_d.mean(): -18.058109283447266 
model_pd.lagr.mean(): -17.930011749267578 
model_pd.lambdas: dict_items([('pout', tensor([1.1375], device='cuda:0')), ('power', tensor([0.8134], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8107], device='cuda:0')), ('power', tensor([-23.2997], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.128097265958786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4428, 3.4515, 3.4440],
        [3.4428, 4.1917, 4.6395],
        [3.4428, 3.7164, 3.7288]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.12800408899784088 
model_pd.l_d.mean(): -18.030593872070312 
model_pd.lagr.mean(): -17.902589797973633 
model_pd.lambdas: dict_items([('pout', tensor([1.1384], device='cuda:0')), ('power', tensor([0.8123], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8099], device='cuda:0')), ('power', tensor([-23.2990], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.12800408899784088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4451, 3.4538, 3.4464],
        [3.4451, 3.7183, 3.7306],
        [3.4451, 4.1939, 4.6413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.1279107928276062 
model_pd.l_d.mean(): -18.003080368041992 
model_pd.lagr.mean(): -17.87516975402832 
model_pd.lambdas: dict_items([('pout', tensor([1.1392], device='cuda:0')), ('power', tensor([0.8111], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8092], device='cuda:0')), ('power', tensor([-23.2983], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.1279107928276062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4475, 3.4562, 3.4488],
        [3.4475, 4.1961, 4.6430],
        [3.4475, 3.7204, 3.7325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.12781581282615662 
model_pd.l_d.mean(): -17.975566864013672 
model_pd.lagr.mean(): -17.84775161743164 
model_pd.lambdas: dict_items([('pout', tensor([1.1400], device='cuda:0')), ('power', tensor([0.8099], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8084], device='cuda:0')), ('power', tensor([-23.2975], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.12781581282615662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4500, 3.4587, 3.4512],
        [3.4500, 3.7225, 3.7344],
        [3.4500, 4.1983, 4.6449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.12771821022033691 
model_pd.l_d.mean(): -17.948055267333984 
model_pd.lagr.mean(): -17.820337295532227 
model_pd.lambdas: dict_items([('pout', tensor([1.1408], device='cuda:0')), ('power', tensor([0.8088], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8076], device='cuda:0')), ('power', tensor([-23.2967], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.12771821022033691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4526, 4.2007, 4.6469],
        [3.4526, 3.7246, 3.7363],
        [3.4526, 3.4612, 3.4538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.12761789560317993 
model_pd.l_d.mean(): -17.920541763305664 
model_pd.lagr.mean(): -17.792922973632812 
model_pd.lambdas: dict_items([('pout', tensor([1.1416], device='cuda:0')), ('power', tensor([0.8076], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8067], device='cuda:0')), ('power', tensor([-23.2959], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.12761789560317993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4551, 3.4637, 3.4563],
        [3.4551, 3.7269, 3.7384],
        [3.4551, 4.2031, 4.6489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.1275155395269394 
model_pd.l_d.mean(): -17.893030166625977 
model_pd.lagr.mean(): -17.765514373779297 
model_pd.lambdas: dict_items([('pout', tensor([1.1424], device='cuda:0')), ('power', tensor([0.8064], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8059], device='cuda:0')), ('power', tensor([-23.2950], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.1275155395269394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4578, 3.7291, 3.7404],
        [3.4578, 4.2056, 4.6510],
        [3.4578, 3.4663, 3.4590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.12741205096244812 
model_pd.l_d.mean(): -17.865520477294922 
model_pd.lagr.mean(): -17.738107681274414 
model_pd.lambdas: dict_items([('pout', tensor([1.1432], device='cuda:0')), ('power', tensor([0.8053], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8050], device='cuda:0')), ('power', tensor([-23.2941], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.12741205096244812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4604, 4.2081, 4.6531],
        [3.4604, 3.7314, 3.7425],
        [3.4604, 3.4689, 3.4616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.12730857729911804 
model_pd.l_d.mean(): -17.838014602661133 
model_pd.lagr.mean(): -17.71070671081543 
model_pd.lambdas: dict_items([('pout', tensor([1.1440], device='cuda:0')), ('power', tensor([0.8041], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8042], device='cuda:0')), ('power', tensor([-23.2932], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.12730857729911804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4630, 3.7337, 3.7446],
        [3.4630, 3.4715, 3.4642],
        [3.4630, 4.2105, 4.6551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.12720613181591034 
model_pd.l_d.mean(): -17.81051254272461 
model_pd.lagr.mean(): -17.683305740356445 
model_pd.lambdas: dict_items([('pout', tensor([1.1448], device='cuda:0')), ('power', tensor([0.8029], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8033], device='cuda:0')), ('power', tensor([-23.2924], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.12720613181591034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4656, 3.4740, 3.4667],
        [3.4656, 3.7359, 3.7466],
        [3.4656, 4.2129, 4.6571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.12710565328598022 
model_pd.l_d.mean(): -17.783016204833984 
model_pd.lagr.mean(): -17.65591049194336 
model_pd.lambdas: dict_items([('pout', tensor([1.1456], device='cuda:0')), ('power', tensor([0.8018], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8025], device='cuda:0')), ('power', tensor([-23.2915], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.12710565328598022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4681, 4.2152, 4.6590],
        [3.4681, 3.7380, 3.7485],
        [3.4681, 3.4765, 3.4693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.12700752913951874 
model_pd.l_d.mean(): -17.755521774291992 
model_pd.lagr.mean(): -17.62851333618164 
model_pd.lambdas: dict_items([('pout', tensor([1.1464], device='cuda:0')), ('power', tensor([0.8006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8016], device='cuda:0')), ('power', tensor([-23.2907], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.12700752913951874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4706, 4.2175, 4.6609],
        [3.4706, 3.7401, 3.7504],
        [3.4706, 3.4790, 3.4718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.12691164016723633 
model_pd.l_d.mean(): -17.728036880493164 
model_pd.lagr.mean(): -17.601125717163086 
model_pd.lambdas: dict_items([('pout', tensor([1.1472], device='cuda:0')), ('power', tensor([0.7994], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8008], device='cuda:0')), ('power', tensor([-23.2899], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.12691164016723633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4730, 3.4814, 3.4742],
        [3.4730, 3.7422, 3.7523],
        [3.4730, 4.2197, 4.6627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.1268174946308136 
model_pd.l_d.mean(): -17.70055389404297 
model_pd.lagr.mean(): -17.5737361907959 
model_pd.lambdas: dict_items([('pout', tensor([1.1480], device='cuda:0')), ('power', tensor([0.7983], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8000], device='cuda:0')), ('power', tensor([-23.2891], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.1268174946308136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4755, 3.7442, 3.7541],
        [3.4755, 4.2220, 4.6644],
        [3.4755, 3.4838, 3.4766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.1267242729663849 
model_pd.l_d.mean(): -17.673076629638672 
model_pd.lagr.mean(): -17.54635238647461 
model_pd.lambdas: dict_items([('pout', tensor([1.1488], device='cuda:0')), ('power', tensor([0.7971], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7992], device='cuda:0')), ('power', tensor([-23.2883], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.1267242729663849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4779, 3.4862, 3.4791],
        [3.4779, 4.2242, 4.6662],
        [3.4779, 3.7463, 3.7559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.12663112580776215 
model_pd.l_d.mean(): -17.645599365234375 
model_pd.lagr.mean(): -17.51896858215332 
model_pd.lambdas: dict_items([('pout', tensor([1.1496], device='cuda:0')), ('power', tensor([0.7960], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7985], device='cuda:0')), ('power', tensor([-23.2875], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.12663112580776215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4804, 4.2264, 4.6680],
        [3.4804, 3.4887, 3.4815],
        [3.4804, 3.7483, 3.7578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.12653735280036926 
model_pd.l_d.mean(): -17.618125915527344 
model_pd.lagr.mean(): -17.491588592529297 
model_pd.lambdas: dict_items([('pout', tensor([1.1504], device='cuda:0')), ('power', tensor([0.7948], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7977], device='cuda:0')), ('power', tensor([-23.2867], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.12653735280036926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4829, 4.2287, 4.6698],
        [3.4829, 3.4911, 3.4840],
        [3.4829, 3.7504, 3.7597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.12644259631633759 
model_pd.l_d.mean(): -17.590656280517578 
model_pd.lagr.mean(): -17.464214324951172 
model_pd.lambdas: dict_items([('pout', tensor([1.1512], device='cuda:0')), ('power', tensor([0.7936], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7969], device='cuda:0')), ('power', tensor([-23.2859], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.12644259631633759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4854, 3.4936, 3.4865],
        [3.4854, 3.7526, 3.7616],
        [3.4854, 4.2310, 4.6717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.12634679675102234 
model_pd.l_d.mean(): -17.56318473815918 
model_pd.lagr.mean(): -17.436838150024414 
model_pd.lambdas: dict_items([('pout', tensor([1.1520], device='cuda:0')), ('power', tensor([0.7925], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7960], device='cuda:0')), ('power', tensor([-23.2851], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.12634679675102234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4880, 3.7547, 3.7636],
        [3.4880, 3.4961, 3.4891],
        [3.4880, 4.2333, 4.6735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.1262502521276474 
model_pd.l_d.mean(): -17.535720825195312 
model_pd.lagr.mean(): -17.40947151184082 
model_pd.lambdas: dict_items([('pout', tensor([1.1528], device='cuda:0')), ('power', tensor([0.7913], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7952], device='cuda:0')), ('power', tensor([-23.2842], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.1262502521276474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4905, 3.7569, 3.7655],
        [3.4905, 3.4986, 3.4916],
        [3.4905, 4.2357, 4.6755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.12615343928337097 
model_pd.l_d.mean(): -17.508258819580078 
model_pd.lagr.mean(): -17.382104873657227 
model_pd.lambdas: dict_items([('pout', tensor([1.1536], device='cuda:0')), ('power', tensor([0.7901], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7944], device='cuda:0')), ('power', tensor([-23.2834], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.12615343928337097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4931, 4.2380, 4.6774],
        [3.4931, 3.5012, 3.4942],
        [3.4931, 3.7591, 3.7675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.1260569989681244 
model_pd.l_d.mean(): -17.480798721313477 
model_pd.lagr.mean(): -17.3547420501709 
model_pd.lambdas: dict_items([('pout', tensor([1.1544], device='cuda:0')), ('power', tensor([0.7890], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7936], device='cuda:0')), ('power', tensor([-23.2825], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.1260569989681244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4956, 3.7612, 3.7694],
        [3.4956, 3.5037, 3.4967],
        [3.4956, 4.2404, 4.6792]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.1259612739086151 
model_pd.l_d.mean(): -17.45334243774414 
model_pd.lagr.mean(): -17.327381134033203 
model_pd.lambdas: dict_items([('pout', tensor([1.1552], device='cuda:0')), ('power', tensor([0.7878], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7927], device='cuda:0')), ('power', tensor([-23.2817], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.1259612739086151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4982, 3.7633, 3.7713],
        [3.4982, 4.2427, 4.6811],
        [3.4982, 3.5062, 3.4992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.12586656212806702 
model_pd.l_d.mean(): -17.425888061523438 
model_pd.lagr.mean(): -17.30002212524414 
model_pd.lambdas: dict_items([('pout', tensor([1.1560], device='cuda:0')), ('power', tensor([0.7866], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7919], device='cuda:0')), ('power', tensor([-23.2808], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.12586656212806702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5007, 4.2449, 4.6829],
        [3.5007, 3.7654, 3.7732],
        [3.5007, 3.5087, 3.5018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.12577274441719055 
model_pd.l_d.mean(): -17.398441314697266 
model_pd.lagr.mean(): -17.272668838500977 
model_pd.lambdas: dict_items([('pout', tensor([1.1568], device='cuda:0')), ('power', tensor([0.7855], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7911], device='cuda:0')), ('power', tensor([-23.2800], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.12577274441719055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5032, 3.5111, 3.5043],
        [3.5032, 4.2472, 4.6847],
        [3.5032, 3.7675, 3.7751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.12567958235740662 
model_pd.l_d.mean(): -17.37099838256836 
model_pd.lagr.mean(): -17.245319366455078 
model_pd.lambdas: dict_items([('pout', tensor([1.1575], device='cuda:0')), ('power', tensor([0.7843], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7903], device='cuda:0')), ('power', tensor([-23.2792], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.12567958235740662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5057, 3.7696, 3.7770],
        [3.5057, 3.5136, 3.5068],
        [3.5057, 4.2495, 4.6865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.12558652460575104 
model_pd.l_d.mean(): -17.343557357788086 
model_pd.lagr.mean(): -17.21796989440918 
model_pd.lambdas: dict_items([('pout', tensor([1.1583], device='cuda:0')), ('power', tensor([0.7831], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7895], device='cuda:0')), ('power', tensor([-23.2783], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.12558652460575104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5082, 4.2518, 4.6883],
        [3.5082, 3.5161, 3.5093],
        [3.5082, 3.7718, 3.7789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.12549303472042084 
model_pd.l_d.mean(): -17.316120147705078 
model_pd.lagr.mean(): -17.190628051757812 
model_pd.lambdas: dict_items([('pout', tensor([1.1591], device='cuda:0')), ('power', tensor([0.7820], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7887], device='cuda:0')), ('power', tensor([-23.2775], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.12549303472042084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5108, 3.5186, 3.5119],
        [3.5108, 4.2541, 4.6902],
        [3.5108, 3.7739, 3.7808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.1253984272480011 
model_pd.l_d.mean(): -17.288684844970703 
model_pd.lagr.mean(): -17.163286209106445 
model_pd.lambdas: dict_items([('pout', tensor([1.1599], device='cuda:0')), ('power', tensor([0.7808], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7879], device='cuda:0')), ('power', tensor([-23.2766], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.1253984272480011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5134, 3.7761, 3.7827],
        [3.5134, 4.2565, 4.6921],
        [3.5134, 3.5212, 3.5144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.12530235946178436 
model_pd.l_d.mean(): -17.261249542236328 
model_pd.lagr.mean(): -17.13594627380371 
model_pd.lambdas: dict_items([('pout', tensor([1.1607], device='cuda:0')), ('power', tensor([0.7797], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7871], device='cuda:0')), ('power', tensor([-23.2757], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.12530235946178436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5160, 4.2589, 4.6941],
        [3.5160, 3.5238, 3.5171],
        [3.5160, 3.7783, 3.7848]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.12520486116409302 
model_pd.l_d.mean(): -17.233816146850586 
model_pd.lagr.mean(): -17.108612060546875 
model_pd.lambdas: dict_items([('pout', tensor([1.1615], device='cuda:0')), ('power', tensor([0.7785], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7862], device='cuda:0')), ('power', tensor([-23.2748], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.12520486116409302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5187, 3.7806, 3.7868],
        [3.5187, 3.5264, 3.5198],
        [3.5187, 4.2614, 4.6961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.12510626018047333 
model_pd.l_d.mean(): -17.20638656616211 
model_pd.lagr.mean(): -17.081279754638672 
model_pd.lambdas: dict_items([('pout', tensor([1.1623], device='cuda:0')), ('power', tensor([0.7773], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7854], device='cuda:0')), ('power', tensor([-23.2739], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.12510626018047333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5214, 4.2639, 4.6981],
        [3.5214, 3.7829, 3.7889],
        [3.5214, 3.5291, 3.5224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.1250072419643402 
model_pd.l_d.mean(): -17.178958892822266 
model_pd.lagr.mean(): -17.053951263427734 
model_pd.lambdas: dict_items([('pout', tensor([1.1631], device='cuda:0')), ('power', tensor([0.7762], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7845], device='cuda:0')), ('power', tensor([-23.2730], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.1250072419643402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5241, 3.5318, 3.5251],
        [3.5241, 4.2664, 4.7001],
        [3.5241, 3.7852, 3.7909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.12490832805633545 
model_pd.l_d.mean(): -17.15153694152832 
model_pd.lagr.mean(): -17.026628494262695 
model_pd.lambdas: dict_items([('pout', tensor([1.1638], device='cuda:0')), ('power', tensor([0.7750], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7836], device='cuda:0')), ('power', tensor([-23.2720], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.12490832805633545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5268, 3.7874, 3.7930],
        [3.5268, 3.5344, 3.5278],
        [3.5268, 4.2689, 4.7022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.12481008470058441 
model_pd.l_d.mean(): -17.124116897583008 
model_pd.lagr.mean(): -16.99930763244629 
model_pd.lambdas: dict_items([('pout', tensor([1.1646], device='cuda:0')), ('power', tensor([0.7738], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7828], device='cuda:0')), ('power', tensor([-23.2711], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.12481008470058441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5295, 3.5371, 3.5305],
        [3.5295, 4.2714, 4.7042],
        [3.5295, 3.7897, 3.7950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.12471278756856918 
model_pd.l_d.mean(): -17.09670066833496 
model_pd.lagr.mean(): -16.971988677978516 
model_pd.lambdas: dict_items([('pout', tensor([1.1654], device='cuda:0')), ('power', tensor([0.7727], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7819], device='cuda:0')), ('power', tensor([-23.2701], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.12471278756856918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5321, 4.2738, 4.7061],
        [3.5321, 3.5397, 3.5331],
        [3.5321, 3.7919, 3.7970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.12461632490158081 
model_pd.l_d.mean(): -17.069292068481445 
model_pd.lagr.mean(): -16.94467544555664 
model_pd.lambdas: dict_items([('pout', tensor([1.1662], device='cuda:0')), ('power', tensor([0.7715], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7811], device='cuda:0')), ('power', tensor([-23.2692], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.12461632490158081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5348, 3.5423, 3.5358],
        [3.5348, 4.2762, 4.7081],
        [3.5348, 3.7942, 3.7990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.12452038377523422 
model_pd.l_d.mean(): -17.041881561279297 
model_pd.lagr.mean(): -16.917360305786133 
model_pd.lambdas: dict_items([('pout', tensor([1.1670], device='cuda:0')), ('power', tensor([0.7703], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7802], device='cuda:0')), ('power', tensor([-23.2683], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.12452038377523422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5374, 3.5449, 3.5384],
        [3.5374, 4.2787, 4.7100],
        [3.5374, 3.7964, 3.8010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.12442447245121002 
model_pd.l_d.mean(): -17.01447868347168 
model_pd.lagr.mean(): -16.89005470275879 
model_pd.lambdas: dict_items([('pout', tensor([1.1677], device='cuda:0')), ('power', tensor([0.7692], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7794], device='cuda:0')), ('power', tensor([-23.2674], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.12442447245121002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5401, 4.2811, 4.7120],
        [3.5401, 3.5476, 3.5411],
        [3.5401, 3.7987, 3.8030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.1243281215429306 
model_pd.l_d.mean(): -16.987077713012695 
model_pd.lagr.mean(): -16.862749099731445 
model_pd.lambdas: dict_items([('pout', tensor([1.1685], device='cuda:0')), ('power', tensor([0.7680], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7785], device='cuda:0')), ('power', tensor([-23.2664], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.1243281215429306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5428, 3.8009, 3.8051],
        [3.5428, 3.5503, 3.5438],
        [3.5428, 4.2836, 4.7140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.1242310106754303 
model_pd.l_d.mean(): -16.959680557250977 
model_pd.lagr.mean(): -16.83544921875 
model_pd.lambdas: dict_items([('pout', tensor([1.1693], device='cuda:0')), ('power', tensor([0.7669], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7776], device='cuda:0')), ('power', tensor([-23.2655], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.1242310106754303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5456, 3.5529, 3.5466],
        [3.5456, 4.2861, 4.7160],
        [3.5456, 3.8032, 3.8071]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.12413295358419418 
model_pd.l_d.mean(): -16.93228530883789 
model_pd.lagr.mean(): -16.80815315246582 
model_pd.lambdas: dict_items([('pout', tensor([1.1701], device='cuda:0')), ('power', tensor([0.7657], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7768], device='cuda:0')), ('power', tensor([-23.2645], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.12413295358419418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5483, 3.8056, 3.8092],
        [3.5483, 4.2887, 4.7181],
        [3.5483, 3.5557, 3.5493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.1240341067314148 
model_pd.l_d.mean(): -16.904891967773438 
model_pd.lagr.mean(): -16.78085708618164 
model_pd.lambdas: dict_items([('pout', tensor([1.1709], device='cuda:0')), ('power', tensor([0.7645], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7759], device='cuda:0')), ('power', tensor([-23.2635], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.1240341067314148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5511, 4.2913, 4.7202],
        [3.5511, 3.8079, 3.8114],
        [3.5511, 3.5584, 3.5521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.12393471598625183 
model_pd.l_d.mean(): -16.877500534057617 
model_pd.lagr.mean(): -16.75356674194336 
model_pd.lambdas: dict_items([('pout', tensor([1.1716], device='cuda:0')), ('power', tensor([0.7634], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7750], device='cuda:0')), ('power', tensor([-23.2625], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.12393471598625183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5539, 3.8103, 3.8135],
        [3.5539, 3.5612, 3.5549],
        [3.5539, 4.2939, 4.7223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.12383526563644409 
model_pd.l_d.mean(): -16.850114822387695 
model_pd.lagr.mean(): -16.726280212402344 
model_pd.lambdas: dict_items([('pout', tensor([1.1724], device='cuda:0')), ('power', tensor([0.7622], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7741], device='cuda:0')), ('power', tensor([-23.2615], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.12383526563644409
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5567, 3.8127, 3.8156],
        [3.5567, 4.2965, 4.7244],
        [3.5567, 3.5639, 3.5577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.12373604625463486 
model_pd.l_d.mean(): -16.822731018066406 
model_pd.lagr.mean(): -16.69899559020996 
model_pd.lambdas: dict_items([('pout', tensor([1.1732], device='cuda:0')), ('power', tensor([0.7610], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7732], device='cuda:0')), ('power', tensor([-23.2605], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.12373604625463486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5595, 3.5667, 3.5604],
        [3.5595, 3.8150, 3.8177],
        [3.5595, 4.2990, 4.7265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.12363731861114502 
model_pd.l_d.mean(): -16.79535484313965 
model_pd.lagr.mean(): -16.671716690063477 
model_pd.lambdas: dict_items([('pout', tensor([1.1739], device='cuda:0')), ('power', tensor([0.7599], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7723], device='cuda:0')), ('power', tensor([-23.2595], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.12363731861114502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5623, 3.5694, 3.5632],
        [3.5623, 3.8174, 3.8198],
        [3.5623, 4.3016, 4.7286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.12353916466236115 
model_pd.l_d.mean(): -16.767980575561523 
model_pd.lagr.mean(): -16.644441604614258 
model_pd.lambdas: dict_items([('pout', tensor([1.1747], device='cuda:0')), ('power', tensor([0.7587], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7715], device='cuda:0')), ('power', tensor([-23.2585], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.12353916466236115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5651, 3.8197, 3.8219],
        [3.5651, 3.5722, 3.5660],
        [3.5651, 4.3042, 4.7307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.12344144284725189 
model_pd.l_d.mean(): -16.7406063079834 
model_pd.lagr.mean(): -16.617164611816406 
model_pd.lambdas: dict_items([('pout', tensor([1.1755], device='cuda:0')), ('power', tensor([0.7576], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7706], device='cuda:0')), ('power', tensor([-23.2575], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.12344144284725189
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5679, 3.5749, 3.5688],
        [3.5679, 4.3067, 4.7327],
        [3.5679, 3.8220, 3.8240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.123343825340271 
model_pd.l_d.mean(): -16.713239669799805 
model_pd.lagr.mean(): -16.589895248413086 
model_pd.lambdas: dict_items([('pout', tensor([1.1763], device='cuda:0')), ('power', tensor([0.7564], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7697], device='cuda:0')), ('power', tensor([-23.2565], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.123343825340271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5707, 4.3093, 4.7348],
        [3.5707, 3.5777, 3.5716],
        [3.5707, 3.8244, 3.8261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.12324607372283936 
model_pd.l_d.mean(): -16.685874938964844 
model_pd.lagr.mean(): -16.56262969970703 
model_pd.lambdas: dict_items([('pout', tensor([1.1770], device='cuda:0')), ('power', tensor([0.7552], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7688], device='cuda:0')), ('power', tensor([-23.2554], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:0.12324607372283936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5735, 4.3119, 4.7369],
        [3.5735, 3.5805, 3.5744],
        [3.5735, 3.8268, 3.8282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.12314793467521667 
model_pd.l_d.mean(): -16.65851402282715 
model_pd.lagr.mean(): -16.53536605834961 
model_pd.lambdas: dict_items([('pout', tensor([1.1778], device='cuda:0')), ('power', tensor([0.7541], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7679], device='cuda:0')), ('power', tensor([-23.2544], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.12314793467521667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5763, 3.5833, 3.5772],
        [3.5763, 4.3146, 4.7390],
        [3.5763, 3.8291, 3.8304]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.12304919958114624 
model_pd.l_d.mean(): -16.631155014038086 
model_pd.lagr.mean(): -16.508106231689453 
model_pd.lambdas: dict_items([('pout', tensor([1.1786], device='cuda:0')), ('power', tensor([0.7529], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7670], device='cuda:0')), ('power', tensor([-23.2534], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.12304919958114624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5792, 4.3172, 4.7412],
        [3.5792, 3.5861, 3.5801],
        [3.5792, 3.8316, 3.8325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.1229500100016594 
model_pd.l_d.mean(): -16.603801727294922 
model_pd.lagr.mean(): -16.480852127075195 
model_pd.lambdas: dict_items([('pout', tensor([1.1793], device='cuda:0')), ('power', tensor([0.7517], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7661], device='cuda:0')), ('power', tensor([-23.2523], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.1229500100016594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5820, 3.5889, 3.5829],
        [3.5820, 3.8340, 3.8347],
        [3.5820, 4.3199, 4.7434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.12285052984952927 
model_pd.l_d.mean(): -16.576448440551758 
model_pd.lagr.mean(): -16.453598022460938 
model_pd.lambdas: dict_items([('pout', tensor([1.1801], device='cuda:0')), ('power', tensor([0.7506], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7652], device='cuda:0')), ('power', tensor([-23.2512], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.12285052984952927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5849, 3.8364, 3.8369],
        [3.5849, 3.5918, 3.5858],
        [3.5849, 4.3225, 4.7455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.12275098264217377 
model_pd.l_d.mean(): -16.549102783203125 
model_pd.lagr.mean(): -16.42635154724121 
model_pd.lambdas: dict_items([('pout', tensor([1.1809], device='cuda:0')), ('power', tensor([0.7494], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7643], device='cuda:0')), ('power', tensor([-23.2502], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.12275098264217377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5878, 4.3252, 4.7477],
        [3.5878, 3.5946, 3.5887],
        [3.5878, 3.8388, 3.8391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.1226515918970108 
model_pd.l_d.mean(): -16.52175521850586 
model_pd.lagr.mean(): -16.39910316467285 
model_pd.lambdas: dict_items([('pout', tensor([1.1816], device='cuda:0')), ('power', tensor([0.7483], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7634], device='cuda:0')), ('power', tensor([-23.2491], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.1226515918970108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5907, 3.5975, 3.5916],
        [3.5907, 4.3279, 4.7499],
        [3.5907, 3.8413, 3.8412]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.12255251407623291 
model_pd.l_d.mean(): -16.494417190551758 
model_pd.lagr.mean(): -16.371864318847656 
model_pd.lambdas: dict_items([('pout', tensor([1.1824], device='cuda:0')), ('power', tensor([0.7471], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7625], device='cuda:0')), ('power', tensor([-23.2480], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.12255251407623291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5936, 4.3306, 4.7520],
        [3.5936, 3.8437, 3.8434],
        [3.5936, 3.6003, 3.5944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.12245379388332367 
model_pd.l_d.mean(): -16.467079162597656 
model_pd.lagr.mean(): -16.34462547302246 
model_pd.lambdas: dict_items([('pout', tensor([1.1831], device='cuda:0')), ('power', tensor([0.7459], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7616], device='cuda:0')), ('power', tensor([-23.2469], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.12245379388332367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5965, 3.6031, 3.5973],
        [3.5965, 3.8461, 3.8456],
        [3.5965, 4.3332, 4.7542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.12235530465841293 
model_pd.l_d.mean(): -16.439743041992188 
model_pd.lagr.mean(): -16.3173885345459 
model_pd.lambdas: dict_items([('pout', tensor([1.1839], device='cuda:0')), ('power', tensor([0.7448], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7607], device='cuda:0')), ('power', tensor([-23.2459], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.12235530465841293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5993, 4.3359, 4.7563],
        [3.5993, 3.8485, 3.8477],
        [3.5993, 3.6060, 3.6002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.12225782871246338 
model_pd.l_d.mean(): -16.41241455078125 
model_pd.lagr.mean(): -16.290157318115234 
model_pd.lambdas: dict_items([('pout', tensor([1.1847], device='cuda:0')), ('power', tensor([0.7436], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7598], device='cuda:0')), ('power', tensor([-23.2448], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.12225782871246338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6022, 3.8509, 3.8498],
        [3.6022, 4.3385, 4.7584],
        [3.6022, 3.6088, 3.6030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.12216229736804962 
model_pd.l_d.mean(): -16.38509178161621 
model_pd.lagr.mean(): -16.262929916381836 
model_pd.lambdas: dict_items([('pout', tensor([1.1854], device='cuda:0')), ('power', tensor([0.7424], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7589], device='cuda:0')), ('power', tensor([-23.2437], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.12216229736804962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6049, 4.3410, 4.7604],
        [3.6049, 3.6115, 3.6058],
        [3.6049, 3.8532, 3.8519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.12206868827342987 
model_pd.l_d.mean(): -16.357770919799805 
model_pd.lagr.mean(): -16.235702514648438 
model_pd.lambdas: dict_items([('pout', tensor([1.1862], device='cuda:0')), ('power', tensor([0.7413], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7580], device='cuda:0')), ('power', tensor([-23.2427], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.12206868827342987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6077, 3.8555, 3.8539],
        [3.6077, 3.6142, 3.6085],
        [3.6077, 4.3435, 4.7623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.12197613716125488 
model_pd.l_d.mean(): -16.330453872680664 
model_pd.lagr.mean(): -16.208477020263672 
model_pd.lambdas: dict_items([('pout', tensor([1.1869], device='cuda:0')), ('power', tensor([0.7401], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7572], device='cuda:0')), ('power', tensor([-23.2417], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.12197613716125488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6105, 3.6170, 3.6113],
        [3.6105, 4.3460, 4.7643],
        [3.6105, 3.8578, 3.8560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.12188325822353363 
model_pd.l_d.mean(): -16.303138732910156 
model_pd.lagr.mean(): -16.181255340576172 
model_pd.lambdas: dict_items([('pout', tensor([1.1877], device='cuda:0')), ('power', tensor([0.7390], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7563], device='cuda:0')), ('power', tensor([-23.2407], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.12188325822353363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6133, 3.6198, 3.6142],
        [3.6133, 3.8602, 3.8581],
        [3.6133, 4.3486, 4.7664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12178817391395569 
model_pd.l_d.mean(): -16.275827407836914 
model_pd.lagr.mean(): -16.15403938293457 
model_pd.lambdas: dict_items([('pout', tensor([1.1885], device='cuda:0')), ('power', tensor([0.7378], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7554], device='cuda:0')), ('power', tensor([-23.2396], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.12178817391395569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6163, 3.8627, 3.8603],
        [3.6163, 3.6227, 3.6171],
        [3.6163, 4.3514, 4.7686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.12168961763381958 
model_pd.l_d.mean(): -16.24851417541504 
model_pd.lagr.mean(): -16.1268253326416 
model_pd.lambdas: dict_items([('pout', tensor([1.1892], device='cuda:0')), ('power', tensor([0.7366], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7545], device='cuda:0')), ('power', tensor([-23.2385], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.12168961763381958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6192, 4.3542, 4.7709],
        [3.6192, 3.8652, 3.8626],
        [3.6192, 3.6256, 3.6201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.12158985435962677 
model_pd.l_d.mean(): -16.22119903564453 
model_pd.lagr.mean(): -16.099609375 
model_pd.lambdas: dict_items([('pout', tensor([1.1900], device='cuda:0')), ('power', tensor([0.7355], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7536], device='cuda:0')), ('power', tensor([-23.2373], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.12158985435962677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6222, 4.3569, 4.7731],
        [3.6222, 3.6285, 3.6230],
        [3.6222, 3.8677, 3.8649]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.12149076163768768 
model_pd.l_d.mean(): -16.193891525268555 
model_pd.lagr.mean(): -16.07240104675293 
model_pd.lambdas: dict_items([('pout', tensor([1.1907], device='cuda:0')), ('power', tensor([0.7343], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7526], device='cuda:0')), ('power', tensor([-23.2362], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.12149076163768768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6251, 3.6314, 3.6259],
        [3.6251, 3.8702, 3.8671],
        [3.6251, 4.3596, 4.7753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.12139366567134857 
model_pd.l_d.mean(): -16.166582107543945 
model_pd.lagr.mean(): -16.045188903808594 
model_pd.lambdas: dict_items([('pout', tensor([1.1915], device='cuda:0')), ('power', tensor([0.7331], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7517], device='cuda:0')), ('power', tensor([-23.2350], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.12139366567134857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6280, 3.6343, 3.6288],
        [3.6280, 3.8726, 3.8692],
        [3.6280, 4.3623, 4.7774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.1212988942861557 
model_pd.l_d.mean(): -16.1392822265625 
model_pd.lagr.mean(): -16.017982482910156 
model_pd.lambdas: dict_items([('pout', tensor([1.1922], device='cuda:0')), ('power', tensor([0.7320], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7508], device='cuda:0')), ('power', tensor([-23.2339], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.1212988942861557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6309, 4.3649, 4.7795],
        [3.6309, 3.6371, 3.6316],
        [3.6309, 3.8750, 3.8714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.12120522558689117 
model_pd.l_d.mean(): -16.11198616027832 
model_pd.lagr.mean(): -15.9907808303833 
model_pd.lambdas: dict_items([('pout', tensor([1.1930], device='cuda:0')), ('power', tensor([0.7308], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7499], device='cuda:0')), ('power', tensor([-23.2329], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.12120522558689117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6337, 3.6398, 3.6344],
        [3.6337, 4.3674, 4.7815],
        [3.6337, 3.8773, 3.8735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.12111341208219528 
model_pd.l_d.mean(): -16.08469009399414 
model_pd.lagr.mean(): -15.963576316833496 
model_pd.lambdas: dict_items([('pout', tensor([1.1937], device='cuda:0')), ('power', tensor([0.7297], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7491], device='cuda:0')), ('power', tensor([-23.2318], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.12111341208219528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6364, 3.8796, 3.8755],
        [3.6364, 3.6425, 3.6372],
        [3.6364, 4.3699, 4.7835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.12102393805980682 
model_pd.l_d.mean(): -16.057397842407227 
model_pd.lagr.mean(): -15.936373710632324 
model_pd.lambdas: dict_items([('pout', tensor([1.1945], device='cuda:0')), ('power', tensor([0.7285], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7482], device='cuda:0')), ('power', tensor([-23.2308], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.12102393805980682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6391, 3.6452, 3.6399],
        [3.6391, 3.8819, 3.8776],
        [3.6391, 4.3724, 4.7854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.12093567103147507 
model_pd.l_d.mean(): -16.030105590820312 
model_pd.lagr.mean(): -15.909170150756836 
model_pd.lambdas: dict_items([('pout', tensor([1.1952], device='cuda:0')), ('power', tensor([0.7273], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7474], device='cuda:0')), ('power', tensor([-23.2297], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.12093567103147507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6419, 3.8842, 3.8796],
        [3.6419, 3.6479, 3.6426],
        [3.6419, 4.3749, 4.7874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.1208464503288269 
model_pd.l_d.mean(): -16.002811431884766 
model_pd.lagr.mean(): -15.881964683532715 
model_pd.lambdas: dict_items([('pout', tensor([1.1960], device='cuda:0')), ('power', tensor([0.7262], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7465], device='cuda:0')), ('power', tensor([-23.2287], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.1208464503288269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6447, 3.8867, 3.8818],
        [3.6447, 4.3776, 4.7895],
        [3.6447, 3.6507, 3.6455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.12075339257717133 
model_pd.l_d.mean(): -15.975517272949219 
model_pd.lagr.mean(): -15.854763984680176 
model_pd.lambdas: dict_items([('pout', tensor([1.1967], device='cuda:0')), ('power', tensor([0.7250], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7456], device='cuda:0')), ('power', tensor([-23.2275], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.12075339257717133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6477, 4.3804, 4.7919],
        [3.6477, 3.6537, 3.6484],
        [3.6477, 3.8893, 3.8842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.12065483629703522 
model_pd.l_d.mean(): -15.948219299316406 
model_pd.lagr.mean(): -15.827564239501953 
model_pd.lambdas: dict_items([('pout', tensor([1.1974], device='cuda:0')), ('power', tensor([0.7239], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7447], device='cuda:0')), ('power', tensor([-23.2263], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.12065483629703522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6509, 4.3835, 4.7945],
        [3.6509, 3.6568, 3.6516],
        [3.6509, 3.8920, 3.8867]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.1205509826540947 
model_pd.l_d.mean(): -15.920921325683594 
model_pd.lagr.mean(): -15.800370216369629 
model_pd.lambdas: dict_items([('pout', tensor([1.1982], device='cuda:0')), ('power', tensor([0.7227], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7437], device='cuda:0')), ('power', tensor([-23.2250], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.1205509826540947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6541, 3.8949, 3.8893],
        [3.6541, 4.3867, 4.7973],
        [3.6541, 3.6600, 3.6548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.12044380605220795 
model_pd.l_d.mean(): -15.893617630004883 
model_pd.lagr.mean(): -15.773174285888672 
model_pd.lambdas: dict_items([('pout', tensor([1.1989], device='cuda:0')), ('power', tensor([0.7215], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7427], device='cuda:0')), ('power', tensor([-23.2235], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.12044380605220795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6574, 3.8978, 3.8920],
        [3.6574, 3.6633, 3.6581],
        [3.6574, 4.3899, 4.8001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.12033599615097046 
model_pd.l_d.mean(): -15.866323471069336 
model_pd.lagr.mean(): -15.745987892150879 
model_pd.lambdas: dict_items([('pout', tensor([1.1997], device='cuda:0')), ('power', tensor([0.7204], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7416], device='cuda:0')), ('power', tensor([-23.2221], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.12033599615097046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6606, 4.3931, 4.8029],
        [3.6606, 3.6665, 3.6613],
        [3.6606, 3.9006, 3.8946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.12022971361875534 
model_pd.l_d.mean(): -15.83903694152832 
model_pd.lagr.mean(): -15.718807220458984 
model_pd.lambdas: dict_items([('pout', tensor([1.2004], device='cuda:0')), ('power', tensor([0.7192], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7406], device='cuda:0')), ('power', tensor([-23.2207], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.12022971361875534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6638, 4.3962, 4.8055],
        [3.6638, 3.9034, 3.8971],
        [3.6638, 3.6696, 3.6645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.12012597918510437 
model_pd.l_d.mean(): -15.81175422668457 
model_pd.lagr.mean(): -15.691628456115723 
model_pd.lambdas: dict_items([('pout', tensor([1.2012], device='cuda:0')), ('power', tensor([0.7180], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7396], device='cuda:0')), ('power', tensor([-23.2194], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.12012597918510437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6669, 4.3992, 4.8081],
        [3.6669, 3.6727, 3.6676],
        [3.6669, 3.9061, 3.8996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.12002477049827576 
model_pd.l_d.mean(): -15.784478187561035 
model_pd.lagr.mean(): -15.664453506469727 
model_pd.lambdas: dict_items([('pout', tensor([1.2019], device='cuda:0')), ('power', tensor([0.7169], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7386], device='cuda:0')), ('power', tensor([-23.2180], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.12002477049827576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6700, 3.6758, 3.6707],
        [3.6700, 4.4021, 4.8106],
        [3.6700, 3.9088, 3.9020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.11992492526769638 
model_pd.l_d.mean(): -15.757207870483398 
model_pd.lagr.mean(): -15.637283325195312 
model_pd.lambdas: dict_items([('pout', tensor([1.2026], device='cuda:0')), ('power', tensor([0.7157], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7376], device='cuda:0')), ('power', tensor([-23.2167], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.11992492526769638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6732, 4.4051, 4.8132],
        [3.6732, 3.6788, 3.6738],
        [3.6732, 3.9115, 3.9045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.11982458829879761 
model_pd.l_d.mean(): -15.729942321777344 
model_pd.lagr.mean(): -15.61011791229248 
model_pd.lambdas: dict_items([('pout', tensor([1.2034], device='cuda:0')), ('power', tensor([0.7146], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7366], device='cuda:0')), ('power', tensor([-23.2154], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.11982458829879761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6763, 4.4082, 4.8158],
        [3.6763, 3.9143, 3.9070],
        [3.6763, 3.6820, 3.6770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.11972220987081528 
model_pd.l_d.mean(): -15.702679634094238 
model_pd.lagr.mean(): -15.58295726776123 
model_pd.lambdas: dict_items([('pout', tensor([1.2041], device='cuda:0')), ('power', tensor([0.7134], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7356], device='cuda:0')), ('power', tensor([-23.2140], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.11972220987081528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6796, 3.6852, 3.6803],
        [3.6796, 3.9171, 3.9096],
        [3.6796, 4.4114, 4.8186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.1196165531873703 
model_pd.l_d.mean(): -15.675416946411133 
model_pd.lagr.mean(): -15.555800437927246 
model_pd.lambdas: dict_items([('pout', tensor([1.2048], device='cuda:0')), ('power', tensor([0.7122], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7346], device='cuda:0')), ('power', tensor([-23.2126], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.1196165531873703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6830, 3.9201, 3.9123],
        [3.6830, 3.6886, 3.6837],
        [3.6830, 4.4148, 4.8215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.11950767040252686 
model_pd.l_d.mean(): -15.648158073425293 
model_pd.lagr.mean(): -15.528650283813477 
model_pd.lambdas: dict_items([('pout', tensor([1.2056], device='cuda:0')), ('power', tensor([0.7111], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7335], device='cuda:0')), ('power', tensor([-23.2111], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.11950767040252686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6865, 3.6920, 3.6871],
        [3.6865, 3.9232, 3.9151],
        [3.6865, 4.4182, 4.8246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.11939670145511627 
model_pd.l_d.mean(): -15.620898246765137 
model_pd.lagr.mean(): -15.501501083374023 
model_pd.lambdas: dict_items([('pout', tensor([1.2063], device='cuda:0')), ('power', tensor([0.7099], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7324], device='cuda:0')), ('power', tensor([-23.2095], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.11939670145511627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6899, 4.4216, 4.8276],
        [3.6899, 3.9262, 3.9180],
        [3.6899, 3.6954, 3.6906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.11928542703390121 
model_pd.l_d.mean(): -15.593645095825195 
model_pd.lagr.mean(): -15.474359512329102 
model_pd.lambdas: dict_items([('pout', tensor([1.2070], device='cuda:0')), ('power', tensor([0.7088], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7313], device='cuda:0')), ('power', tensor([-23.2080], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.11928542703390121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6934, 4.4250, 4.8306],
        [3.6934, 3.6988, 3.6940],
        [3.6934, 3.9293, 3.9207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.11917553842067719 
model_pd.l_d.mean(): -15.566398620605469 
model_pd.lagr.mean(): -15.447222709655762 
model_pd.lambdas: dict_items([('pout', tensor([1.2078], device='cuda:0')), ('power', tensor([0.7076], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7302], device='cuda:0')), ('power', tensor([-23.2064], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.11917553842067719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6968, 3.9323, 3.9235],
        [3.6968, 4.4284, 4.8335],
        [3.6968, 3.7022, 3.6974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.1190679669380188 
model_pd.l_d.mean(): -15.539156913757324 
model_pd.lagr.mean(): -15.420088768005371 
model_pd.lambdas: dict_items([('pout', tensor([1.2085], device='cuda:0')), ('power', tensor([0.7064], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7292], device='cuda:0')), ('power', tensor([-23.2049], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.1190679669380188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7001, 3.7055, 3.7008],
        [3.7001, 4.4316, 4.8364],
        [3.7001, 3.9352, 3.9261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.11896264553070068 
model_pd.l_d.mean(): -15.51192569732666 
model_pd.lagr.mean(): -15.392963409423828 
model_pd.lambdas: dict_items([('pout', tensor([1.2092], device='cuda:0')), ('power', tensor([0.7053], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7281], device='cuda:0')), ('power', tensor([-23.2034], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.11896264553070068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7034, 3.9381, 3.9287],
        [3.7034, 3.7088, 3.7041],
        [3.7034, 4.4348, 4.8392]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.11885850131511688 
model_pd.l_d.mean(): -15.484696388244629 
model_pd.lagr.mean(): -15.365838050842285 
model_pd.lambdas: dict_items([('pout', tensor([1.2100], device='cuda:0')), ('power', tensor([0.7041], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7271], device='cuda:0')), ('power', tensor([-23.2020], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.11885850131511688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7068, 3.9410, 3.9314],
        [3.7068, 3.7121, 3.7074],
        [3.7068, 4.4381, 4.8420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.11875402927398682 
model_pd.l_d.mean(): -15.457476615905762 
model_pd.lagr.mean(): -15.338722229003906 
model_pd.lambdas: dict_items([('pout', tensor([1.2107], device='cuda:0')), ('power', tensor([0.7030], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7260], device='cuda:0')), ('power', tensor([-23.2005], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.11875402927398682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7102, 3.7154, 3.7108],
        [3.7102, 4.4414, 4.8449],
        [3.7102, 3.9439, 3.9341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.11864801496267319 
model_pd.l_d.mean(): -15.430255889892578 
model_pd.lagr.mean(): -15.31160831451416 
model_pd.lambdas: dict_items([('pout', tensor([1.2114], device='cuda:0')), ('power', tensor([0.7018], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7250], device='cuda:0')), ('power', tensor([-23.1990], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.11864801496267319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7136, 3.9470, 3.9369],
        [3.7136, 3.7189, 3.7142],
        [3.7136, 4.4448, 4.8478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.11853992193937302 
model_pd.l_d.mean(): -15.403035163879395 
model_pd.lagr.mean(): -15.28449535369873 
model_pd.lambdas: dict_items([('pout', tensor([1.2121], device='cuda:0')), ('power', tensor([0.7006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7239], device='cuda:0')), ('power', tensor([-23.1974], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.11853992193937302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7172, 3.9501, 3.9397],
        [3.7172, 3.7223, 3.7178],
        [3.7172, 4.4483, 4.8509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.11843003332614899 
model_pd.l_d.mean(): -15.375823974609375 
model_pd.lagr.mean(): -15.257393836975098 
model_pd.lambdas: dict_items([('pout', tensor([1.2129], device='cuda:0')), ('power', tensor([0.6995], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7228], device='cuda:0')), ('power', tensor([-23.1958], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.11843003332614899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7207, 3.9532, 3.9426],
        [3.7207, 3.7259, 3.7213],
        [3.7207, 4.4518, 4.8540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.11831934750080109 
model_pd.l_d.mean(): -15.348611831665039 
model_pd.lagr.mean(): -15.230292320251465 
model_pd.lambdas: dict_items([('pout', tensor([1.2136], device='cuda:0')), ('power', tensor([0.6983], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7217], device='cuda:0')), ('power', tensor([-23.1941], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.11831934750080109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7243, 3.9564, 3.9454],
        [3.7243, 3.7294, 3.7249],
        [3.7243, 4.4553, 4.8572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11820903420448303 
model_pd.l_d.mean(): -15.321406364440918 
model_pd.lagr.mean(): -15.203197479248047 
model_pd.lambdas: dict_items([('pout', tensor([1.2143], device='cuda:0')), ('power', tensor([0.6972], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7205], device='cuda:0')), ('power', tensor([-23.1925], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.11820903420448303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7278, 4.4588, 4.8602],
        [3.7278, 3.9595, 3.9483],
        [3.7278, 3.7329, 3.7284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.11810006946325302 
model_pd.l_d.mean(): -15.294206619262695 
model_pd.lagr.mean(): -15.176106452941895 
model_pd.lambdas: dict_items([('pout', tensor([1.2150], device='cuda:0')), ('power', tensor([0.6960], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7194], device='cuda:0')), ('power', tensor([-23.1909], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.11810006946325302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7313, 3.9625, 3.9511],
        [3.7313, 3.7363, 3.7319],
        [3.7313, 4.4622, 4.8632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.11799278110265732 
model_pd.l_d.mean(): -15.267013549804688 
model_pd.lagr.mean(): -15.14902114868164 
model_pd.lambdas: dict_items([('pout', tensor([1.2157], device='cuda:0')), ('power', tensor([0.6948], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7183], device='cuda:0')), ('power', tensor([-23.1893], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.11799278110265732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7348, 3.7398, 3.7354],
        [3.7348, 4.4656, 4.8662],
        [3.7348, 3.9656, 3.9538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.11788667738437653 
model_pd.l_d.mean(): -15.239826202392578 
model_pd.lagr.mean(): -15.121939659118652 
model_pd.lambdas: dict_items([('pout', tensor([1.2164], device='cuda:0')), ('power', tensor([0.6937], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7173], device='cuda:0')), ('power', tensor([-23.1877], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.11788667738437653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7383, 3.7432, 3.7388],
        [3.7383, 3.9686, 3.9566],
        [3.7383, 4.4690, 4.8692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.1177808865904808 
model_pd.l_d.mean(): -15.21264362335205 
model_pd.lagr.mean(): -15.094862937927246 
model_pd.lambdas: dict_items([('pout', tensor([1.2172], device='cuda:0')), ('power', tensor([0.6925], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7162], device='cuda:0')), ('power', tensor([-23.1861], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.1177808865904808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7418, 3.7467, 3.7423],
        [3.7418, 4.4725, 4.8722],
        [3.7418, 3.9717, 3.9594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.1176743134856224 
model_pd.l_d.mean(): -15.185464859008789 
model_pd.lagr.mean(): -15.067790985107422 
model_pd.lambdas: dict_items([('pout', tensor([1.2179], device='cuda:0')), ('power', tensor([0.6914], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7151], device='cuda:0')), ('power', tensor([-23.1845], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.1176743134856224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7453, 3.9748, 3.9622],
        [3.7453, 3.7502, 3.7459],
        [3.7453, 4.4760, 4.8752]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.11756652593612671 
model_pd.l_d.mean(): -15.15829086303711 
model_pd.lagr.mean(): -15.040724754333496 
model_pd.lambdas: dict_items([('pout', tensor([1.2186], device='cuda:0')), ('power', tensor([0.6902], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7140], device='cuda:0')), ('power', tensor([-23.1829], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.11756652593612671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7489, 3.7537, 3.7495],
        [3.7489, 3.9780, 3.9651],
        [3.7489, 4.4795, 4.8784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.11745749413967133 
model_pd.l_d.mean(): -15.131119728088379 
model_pd.lagr.mean(): -15.013662338256836 
model_pd.lambdas: dict_items([('pout', tensor([1.2193], device='cuda:0')), ('power', tensor([0.6890], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7129], device='cuda:0')), ('power', tensor([-23.1812], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.11745749413967133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7526, 3.9812, 3.9681],
        [3.7526, 3.7573, 3.7531],
        [3.7526, 4.4831, 4.8815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.11734779179096222 
model_pd.l_d.mean(): -15.10395336151123 
model_pd.lagr.mean(): -14.986605644226074 
model_pd.lambdas: dict_items([('pout', tensor([1.2200], device='cuda:0')), ('power', tensor([0.6879], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7117], device='cuda:0')), ('power', tensor([-23.1795], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.11734779179096222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7562, 4.4867, 4.8847],
        [3.7562, 3.7609, 3.7567],
        [3.7562, 3.9844, 3.9710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.11723826825618744 
model_pd.l_d.mean(): -15.076791763305664 
model_pd.lagr.mean(): -14.959553718566895 
model_pd.lambdas: dict_items([('pout', tensor([1.2207], device='cuda:0')), ('power', tensor([0.6867], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7106], device='cuda:0')), ('power', tensor([-23.1778], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.11723826825618744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7599, 3.7645, 3.7604],
        [3.7599, 3.9876, 3.9739],
        [3.7599, 4.4903, 4.8879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.11712940037250519 
model_pd.l_d.mean(): -15.04963493347168 
model_pd.lagr.mean(): -14.93250560760498 
model_pd.lambdas: dict_items([('pout', tensor([1.2214], device='cuda:0')), ('power', tensor([0.6856], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7095], device='cuda:0')), ('power', tensor([-23.1761], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.11712940037250519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7635, 3.9908, 3.9768],
        [3.7635, 3.7681, 3.7640],
        [3.7635, 4.4939, 4.8910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.117021344602108 
model_pd.l_d.mean(): -15.022483825683594 
model_pd.lagr.mean(): -14.905462265014648 
model_pd.lambdas: dict_items([('pout', tensor([1.2221], device='cuda:0')), ('power', tensor([0.6844], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7083], device='cuda:0')), ('power', tensor([-23.1744], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.117021344602108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7671, 3.9939, 3.9797],
        [3.7671, 4.4974, 4.8942],
        [3.7671, 3.7717, 3.7676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.11691373586654663 
model_pd.l_d.mean(): -14.995338439941406 
model_pd.lagr.mean(): -14.878424644470215 
model_pd.lambdas: dict_items([('pout', tensor([1.2229], device='cuda:0')), ('power', tensor([0.6833], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7072], device='cuda:0')), ('power', tensor([-23.1727], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.11691373586654663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7707, 3.7753, 3.7712],
        [3.7707, 4.5010, 4.8973],
        [3.7707, 3.9971, 3.9826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.1168060451745987 
model_pd.l_d.mean(): -14.968198776245117 
model_pd.lagr.mean(): -14.85139274597168 
model_pd.lambdas: dict_items([('pout', tensor([1.2236], device='cuda:0')), ('power', tensor([0.6821], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7061], device='cuda:0')), ('power', tensor([-23.1710], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.1168060451745987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7744, 4.0003, 3.9855],
        [3.7744, 4.5046, 4.9005],
        [3.7744, 3.7789, 3.7749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.11669784784317017 
model_pd.l_d.mean(): -14.941061019897461 
model_pd.lagr.mean(): -14.824362754821777 
model_pd.lambdas: dict_items([('pout', tensor([1.2243], device='cuda:0')), ('power', tensor([0.6809], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7050], device='cuda:0')), ('power', tensor([-23.1692], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.11669784784317017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7781, 3.7825, 3.7786],
        [3.7781, 4.5083, 4.9037],
        [3.7781, 4.0036, 3.9885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.11658889055252075 
model_pd.l_d.mean(): -14.91392993927002 
model_pd.lagr.mean(): -14.797341346740723 
model_pd.lambdas: dict_items([('pout', tensor([1.2250], device='cuda:0')), ('power', tensor([0.6798], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7038], device='cuda:0')), ('power', tensor([-23.1675], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.11658889055252075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7818, 3.7862, 3.7823],
        [3.7818, 4.5120, 4.9070],
        [3.7818, 4.0069, 3.9915]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.11647932976484299 
model_pd.l_d.mean(): -14.886802673339844 
model_pd.lagr.mean(): -14.770323753356934 
model_pd.lambdas: dict_items([('pout', tensor([1.2257], device='cuda:0')), ('power', tensor([0.6786], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7027], device='cuda:0')), ('power', tensor([-23.1657], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.11647932976484299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7856, 4.5157, 4.9103],
        [3.7856, 4.0102, 3.9945],
        [3.7856, 3.7899, 3.7860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.11636953055858612 
model_pd.l_d.mean(): -14.859678268432617 
model_pd.lagr.mean(): -14.743309020996094 
model_pd.lambdas: dict_items([('pout', tensor([1.2264], device='cuda:0')), ('power', tensor([0.6775], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7015], device='cuda:0')), ('power', tensor([-23.1639], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.11636953055858612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7893, 3.7936, 3.7898],
        [3.7893, 4.0135, 3.9976],
        [3.7893, 4.5194, 4.9136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.11625986546278 
model_pd.l_d.mean(): -14.832561492919922 
model_pd.lagr.mean(): -14.716301918029785 
model_pd.lambdas: dict_items([('pout', tensor([1.2271], device='cuda:0')), ('power', tensor([0.6763], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7003], device='cuda:0')), ('power', tensor([-23.1621], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.11625986546278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7931, 4.0168, 4.0006],
        [3.7931, 3.7974, 3.7935],
        [3.7931, 4.5232, 4.9169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.11615066230297089 
model_pd.l_d.mean(): -14.805449485778809 
model_pd.lagr.mean(): -14.689298629760742 
model_pd.lambdas: dict_items([('pout', tensor([1.2278], device='cuda:0')), ('power', tensor([0.6751], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6992], device='cuda:0')), ('power', tensor([-23.1603], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.11615066230297089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7968, 4.0201, 4.0036],
        [3.7968, 4.5269, 4.9201],
        [3.7968, 3.8011, 3.7972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.11604186147451401 
model_pd.l_d.mean(): -14.778343200683594 
model_pd.lagr.mean(): -14.662301063537598 
model_pd.lambdas: dict_items([('pout', tensor([1.2285], device='cuda:0')), ('power', tensor([0.6740], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6980], device='cuda:0')), ('power', tensor([-23.1585], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.11604186147451401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8006, 4.5306, 4.9234],
        [3.8006, 4.0234, 4.0066],
        [3.8006, 3.8048, 3.8010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.11593322455883026 
model_pd.l_d.mean(): -14.751241683959961 
model_pd.lagr.mean(): -14.635308265686035 
model_pd.lambdas: dict_items([('pout', tensor([1.2292], device='cuda:0')), ('power', tensor([0.6728], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6969], device='cuda:0')), ('power', tensor([-23.1567], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.11593322455883026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8044, 3.8085, 3.8048],
        [3.8044, 4.5344, 4.9267],
        [3.8044, 4.0267, 4.0097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.11582446098327637 
model_pd.l_d.mean(): -14.72414493560791 
model_pd.lagr.mean(): -14.608320236206055 
model_pd.lambdas: dict_items([('pout', tensor([1.2299], device='cuda:0')), ('power', tensor([0.6717], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6957], device='cuda:0')), ('power', tensor([-23.1549], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.11582446098327637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8082, 4.0301, 4.0127],
        [3.8082, 3.8123, 3.8086],
        [3.8082, 4.5381, 4.9301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.11571528762578964 
model_pd.l_d.mean(): -14.697053909301758 
model_pd.lagr.mean(): -14.581338882446289 
model_pd.lambdas: dict_items([('pout', tensor([1.2306], device='cuda:0')), ('power', tensor([0.6705], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6945], device='cuda:0')), ('power', tensor([-23.1530], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.11571528762578964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8120, 4.0335, 4.0158],
        [3.8120, 4.5419, 4.9335],
        [3.8120, 3.8161, 3.8124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.11560563743114471 
model_pd.l_d.mean(): -14.669967651367188 
model_pd.lagr.mean(): -14.554362297058105 
model_pd.lambdas: dict_items([('pout', tensor([1.2312], device='cuda:0')), ('power', tensor([0.6694], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6934], device='cuda:0')), ('power', tensor([-23.1512], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.11560563743114471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8158, 4.5458, 4.9369],
        [3.8158, 4.0369, 4.0189],
        [3.8158, 3.8199, 3.8162]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.11549561470746994 
model_pd.l_d.mean(): -14.642885208129883 
model_pd.lagr.mean(): -14.527389526367188 
model_pd.lambdas: dict_items([('pout', tensor([1.2319], device='cuda:0')), ('power', tensor([0.6682], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6922], device='cuda:0')), ('power', tensor([-23.1493], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.11549561470746994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8197, 4.5496, 4.9403],
        [3.8197, 4.0403, 4.0220],
        [3.8197, 3.8237, 3.8201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.11538555473089218 
model_pd.l_d.mean(): -14.615808486938477 
model_pd.lagr.mean(): -14.500422477722168 
model_pd.lambdas: dict_items([('pout', tensor([1.2326], device='cuda:0')), ('power', tensor([0.6670], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6910], device='cuda:0')), ('power', tensor([-23.1474], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.11538555473089218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8236, 4.0437, 4.0252],
        [3.8236, 3.8275, 3.8240],
        [3.8236, 4.5535, 4.9437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.11527564376592636 
model_pd.l_d.mean(): -14.588739395141602 
model_pd.lagr.mean(): -14.473464012145996 
model_pd.lambdas: dict_items([('pout', tensor([1.2333], device='cuda:0')), ('power', tensor([0.6659], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6898], device='cuda:0')), ('power', tensor([-23.1455], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.11527564376592636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8275, 4.0471, 4.0283],
        [3.8275, 4.5574, 4.9471],
        [3.8275, 3.8314, 3.8278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.1151660680770874 
model_pd.l_d.mean(): -14.561671257019043 
model_pd.lagr.mean(): -14.446505546569824 
model_pd.lambdas: dict_items([('pout', tensor([1.2340], device='cuda:0')), ('power', tensor([0.6647], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6886], device='cuda:0')), ('power', tensor([-23.1436], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.1151660680770874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8313, 3.8352, 3.8317],
        [3.8313, 4.5612, 4.9505],
        [3.8313, 4.0506, 4.0314]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.11505675315856934 
model_pd.l_d.mean(): -14.534612655639648 
model_pd.lagr.mean(): -14.4195556640625 
model_pd.lambdas: dict_items([('pout', tensor([1.2347], device='cuda:0')), ('power', tensor([0.6636], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6874], device='cuda:0')), ('power', tensor([-23.1416], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.11505675315856934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8352, 4.5651, 4.9540],
        [3.8352, 4.0540, 4.0346],
        [3.8352, 3.8390, 3.8356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.11494748294353485 
model_pd.l_d.mean(): -14.507558822631836 
model_pd.lagr.mean(): -14.392611503601074 
model_pd.lambdas: dict_items([('pout', tensor([1.2354], device='cuda:0')), ('power', tensor([0.6624], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6862], device='cuda:0')), ('power', tensor([-23.1397], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.11494748294353485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8392, 4.5690, 4.9574],
        [3.8392, 4.0575, 4.0377],
        [3.8392, 3.8429, 3.8395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.11483801901340485 
model_pd.l_d.mean(): -14.480506896972656 
model_pd.lagr.mean(): -14.365669250488281 
model_pd.lambdas: dict_items([('pout', tensor([1.2361], device='cuda:0')), ('power', tensor([0.6613], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6850], device='cuda:0')), ('power', tensor([-23.1378], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.11483801901340485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8431, 3.8468, 3.8434],
        [3.8431, 4.5729, 4.9609],
        [3.8431, 4.0609, 4.0409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.11472821235656738 
model_pd.l_d.mean(): -14.453462600708008 
model_pd.lagr.mean(): -14.33873462677002 
model_pd.lambdas: dict_items([('pout', tensor([1.2368], device='cuda:0')), ('power', tensor([0.6601], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6838], device='cuda:0')), ('power', tensor([-23.1358], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.11472821235656738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8470, 4.0644, 4.0441],
        [3.8470, 4.5769, 4.9644],
        [3.8470, 3.8507, 3.8474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.11461814492940903 
model_pd.l_d.mean(): -14.426424026489258 
model_pd.lagr.mean(): -14.311805725097656 
model_pd.lambdas: dict_items([('pout', tensor([1.2374], device='cuda:0')), ('power', tensor([0.6589], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6826], device='cuda:0')), ('power', tensor([-23.1338], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.11461814492940903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8510, 4.0680, 4.0473],
        [3.8510, 4.5809, 4.9680],
        [3.8510, 3.8547, 3.8513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.11450789868831635 
model_pd.l_d.mean(): -14.399389266967773 
model_pd.lagr.mean(): -14.284881591796875 
model_pd.lambdas: dict_items([('pout', tensor([1.2381], device='cuda:0')), ('power', tensor([0.6578], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6814], device='cuda:0')), ('power', tensor([-23.1318], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.11450789868831635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8550, 4.0715, 4.0506],
        [3.8550, 3.8586, 3.8553],
        [3.8550, 4.5849, 4.9715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.11439769715070724 
model_pd.l_d.mean(): -14.37236213684082 
model_pd.lagr.mean(): -14.257964134216309 
model_pd.lambdas: dict_items([('pout', tensor([1.2388], device='cuda:0')), ('power', tensor([0.6566], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6802], device='cuda:0')), ('power', tensor([-23.1298], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.11439769715070724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8590, 3.8625, 3.8593],
        [3.8590, 4.5889, 4.9751],
        [3.8590, 4.0750, 4.0538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.11428767442703247 
model_pd.l_d.mean(): -14.345340728759766 
model_pd.lagr.mean(): -14.231053352355957 
model_pd.lambdas: dict_items([('pout', tensor([1.2395], device='cuda:0')), ('power', tensor([0.6555], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6790], device='cuda:0')), ('power', tensor([-23.1278], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.11428767442703247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8630, 4.5929, 4.9787],
        [3.8630, 4.0786, 4.0570],
        [3.8630, 3.8665, 3.8633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.11417780816555023 
model_pd.l_d.mean(): -14.318321228027344 
model_pd.lagr.mean(): -14.204143524169922 
model_pd.lambdas: dict_items([('pout', tensor([1.2402], device='cuda:0')), ('power', tensor([0.6543], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6777], device='cuda:0')), ('power', tensor([-23.1258], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.11417780816555023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8670, 3.8705, 3.8673],
        [3.8670, 4.5969, 4.9822],
        [3.8670, 4.0821, 4.0603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.11406801640987396 
model_pd.l_d.mean(): -14.291311264038086 
model_pd.lagr.mean(): -14.17724323272705 
model_pd.lambdas: dict_items([('pout', tensor([1.2408], device='cuda:0')), ('power', tensor([0.6532], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6765], device='cuda:0')), ('power', tensor([-23.1238], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.11406801640987396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8711, 4.6009, 4.9858],
        [3.8711, 4.0857, 4.0636],
        [3.8711, 3.8745, 3.8714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.11395807564258575 
model_pd.l_d.mean(): -14.26430606842041 
model_pd.lagr.mean(): -14.150347709655762 
model_pd.lambdas: dict_items([('pout', tensor([1.2415], device='cuda:0')), ('power', tensor([0.6520], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6753], device='cuda:0')), ('power', tensor([-23.1217], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.11395807564258575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8751, 4.0893, 4.0669],
        [3.8751, 4.6050, 4.9894],
        [3.8751, 3.8785, 3.8754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.11384788155555725 
model_pd.l_d.mean(): -14.237305641174316 
model_pd.lagr.mean(): -14.123457908630371 
model_pd.lambdas: dict_items([('pout', tensor([1.2422], device='cuda:0')), ('power', tensor([0.6508], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6740], device='cuda:0')), ('power', tensor([-23.1197], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.11384788155555725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8792, 4.0929, 4.0702],
        [3.8792, 4.6091, 4.9931],
        [3.8792, 3.8825, 3.8795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.11373738944530487 
model_pd.l_d.mean(): -14.210309028625488 
model_pd.lagr.mean(): -14.096571922302246 
model_pd.lambdas: dict_items([('pout', tensor([1.2429], device='cuda:0')), ('power', tensor([0.6497], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6728], device='cuda:0')), ('power', tensor([-23.1176], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.11373738944530487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8833, 4.6132, 4.9968],
        [3.8833, 3.8866, 3.8836],
        [3.8833, 4.0966, 4.0735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.11362671852111816 
model_pd.l_d.mean(): -14.183320999145508 
model_pd.lagr.mean(): -14.069694519042969 
model_pd.lambdas: dict_items([('pout', tensor([1.2435], device='cuda:0')), ('power', tensor([0.6485], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6716], device='cuda:0')), ('power', tensor([-23.1155], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.11362671852111816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8874, 3.8907, 3.8877],
        [3.8874, 4.1002, 4.0769],
        [3.8874, 4.6174, 5.0005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.11351599544286728 
model_pd.l_d.mean(): -14.156335830688477 
model_pd.lagr.mean(): -14.04281997680664 
model_pd.lambdas: dict_items([('pout', tensor([1.2442], device='cuda:0')), ('power', tensor([0.6474], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6703], device='cuda:0')), ('power', tensor([-23.1134], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.11351599544286728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8916, 4.6215, 5.0042],
        [3.8916, 4.1039, 4.0802],
        [3.8916, 3.8947, 3.8918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.11340530216693878 
model_pd.l_d.mean(): -14.12935733795166 
model_pd.lagr.mean(): -14.015952110290527 
model_pd.lambdas: dict_items([('pout', tensor([1.2449], device='cuda:0')), ('power', tensor([0.6462], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6691], device='cuda:0')), ('power', tensor([-23.1112], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.11340530216693878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8957, 4.6257, 5.0079],
        [3.8957, 3.8988, 3.8960],
        [3.8957, 4.1076, 4.0836]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.11329469084739685 
model_pd.l_d.mean(): -14.102385520935059 
model_pd.lagr.mean(): -13.989090919494629 
model_pd.lambdas: dict_items([('pout', tensor([1.2455], device='cuda:0')), ('power', tensor([0.6451], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6678], device='cuda:0')), ('power', tensor([-23.1091], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.11329469084739685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8999, 3.9030, 3.9001],
        [3.8999, 4.6299, 5.0117],
        [3.8999, 4.1113, 4.0870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.11318399757146835 
model_pd.l_d.mean(): -14.075420379638672 
model_pd.lagr.mean(): -13.962236404418945 
model_pd.lambdas: dict_items([('pout', tensor([1.2462], device='cuda:0')), ('power', tensor([0.6439], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6665], device='cuda:0')), ('power', tensor([-23.1070], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.11318399757146835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9041, 3.9071, 3.9043],
        [3.9041, 4.6341, 5.0154],
        [3.9041, 4.1150, 4.0904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.11307317018508911 
model_pd.l_d.mean(): -14.048458099365234 
model_pd.lagr.mean(): -13.935384750366211 
model_pd.lambdas: dict_items([('pout', tensor([1.2469], device='cuda:0')), ('power', tensor([0.6428], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6653], device='cuda:0')), ('power', tensor([-23.1048], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.11307317018508911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9083, 4.6383, 5.0192],
        [3.9083, 3.9112, 3.9085],
        [3.9083, 4.1187, 4.0938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.11296223849058151 
model_pd.l_d.mean(): -14.021501541137695 
model_pd.lagr.mean(): -13.908539772033691 
model_pd.lambdas: dict_items([('pout', tensor([1.2475], device='cuda:0')), ('power', tensor([0.6416], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6640], device='cuda:0')), ('power', tensor([-23.1026], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.11296223849058151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9125, 4.1225, 4.0972],
        [3.9125, 3.9154, 3.9127],
        [3.9125, 4.6426, 5.0230]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.11285116523504257 
model_pd.l_d.mean(): -13.994551658630371 
model_pd.lagr.mean(): -13.88170051574707 
model_pd.lambdas: dict_items([('pout', tensor([1.2482], device='cuda:0')), ('power', tensor([0.6404], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6627], device='cuda:0')), ('power', tensor([-23.1004], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.11285116523504257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9167, 4.1262, 4.1007],
        [3.9167, 4.6469, 5.0269],
        [3.9167, 3.9196, 3.9169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.1127399429678917 
model_pd.l_d.mean(): -13.967607498168945 
model_pd.lagr.mean(): -13.854867935180664 
model_pd.lambdas: dict_items([('pout', tensor([1.2489], device='cuda:0')), ('power', tensor([0.6393], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6615], device='cuda:0')), ('power', tensor([-23.0982], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.1127399429678917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9210, 4.1300, 4.1042],
        [3.9210, 4.6512, 5.0307],
        [3.9210, 3.9238, 3.9212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.11262860149145126 
model_pd.l_d.mean(): -13.940669059753418 
model_pd.lagr.mean(): -13.82804012298584 
model_pd.lambdas: dict_items([('pout', tensor([1.2495], device='cuda:0')), ('power', tensor([0.6381], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6602], device='cuda:0')), ('power', tensor([-23.0959], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.11262860149145126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9252, 4.6555, 5.0346],
        [3.9252, 3.9280, 3.9254],
        [3.9252, 4.1338, 4.1076]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.11251720786094666 
model_pd.l_d.mean(): -13.913736343383789 
model_pd.lagr.mean(): -13.80121898651123 
model_pd.lambdas: dict_items([('pout', tensor([1.2502], device='cuda:0')), ('power', tensor([0.6370], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6589], device='cuda:0')), ('power', tensor([-23.0937], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.11251720786094666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9295, 3.9323, 3.9297],
        [3.9295, 4.1377, 4.1111],
        [3.9295, 4.6598, 5.0385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.11240585148334503 
model_pd.l_d.mean(): -13.886811256408691 
model_pd.lagr.mean(): -13.774405479431152 
model_pd.lambdas: dict_items([('pout', tensor([1.2508], device='cuda:0')), ('power', tensor([0.6358], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6576], device='cuda:0')), ('power', tensor([-23.0914], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.11240585148334503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9338, 4.1415, 4.1147],
        [3.9338, 3.9365, 3.9340],
        [3.9338, 4.6642, 5.0424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.11229455471038818 
model_pd.l_d.mean(): -13.859893798828125 
model_pd.lagr.mean(): -13.747599601745605 
model_pd.lambdas: dict_items([('pout', tensor([1.2515], device='cuda:0')), ('power', tensor([0.6347], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6563], device='cuda:0')), ('power', tensor([-23.0892], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.11229455471038818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9382, 4.1454, 4.1182],
        [3.9382, 4.6685, 5.0463],
        [3.9382, 3.9408, 3.9383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.11218319833278656 
model_pd.l_d.mean(): -13.832977294921875 
model_pd.lagr.mean(): -13.720793724060059 
model_pd.lambdas: dict_items([('pout', tensor([1.2521], device='cuda:0')), ('power', tensor([0.6335], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6550], device='cuda:0')), ('power', tensor([-23.0869], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.11218319833278656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9425, 4.6729, 5.0503],
        [3.9425, 3.9451, 3.9426],
        [3.9425, 4.1492, 4.1217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.11207161843776703 
model_pd.l_d.mean(): -13.806069374084473 
model_pd.lagr.mean(): -13.693997383117676 
model_pd.lambdas: dict_items([('pout', tensor([1.2528], device='cuda:0')), ('power', tensor([0.6324], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6537], device='cuda:0')), ('power', tensor([-23.0846], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.11207161843776703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9469, 4.6774, 5.0543],
        [3.9469, 4.1531, 4.1253],
        [3.9469, 3.9494, 3.9470]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.11195981502532959 
model_pd.l_d.mean(): -13.77916431427002 
model_pd.lagr.mean(): -13.667204856872559 
model_pd.lambdas: dict_items([('pout', tensor([1.2534], device='cuda:0')), ('power', tensor([0.6312], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6524], device='cuda:0')), ('power', tensor([-23.0822], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.11195981502532959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9512, 3.9537, 3.9514],
        [3.9512, 4.1571, 4.1289],
        [3.9512, 4.6818, 5.0583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.11184778064489365 
model_pd.l_d.mean(): -13.752267837524414 
model_pd.lagr.mean(): -13.640419960021973 
model_pd.lambdas: dict_items([('pout', tensor([1.2541], device='cuda:0')), ('power', tensor([0.6301], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6511], device='cuda:0')), ('power', tensor([-23.0799], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.11184778064489365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9557, 4.1610, 4.1325],
        [3.9557, 3.9581, 3.9558],
        [3.9557, 4.6863, 5.0623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.1117355227470398 
model_pd.l_d.mean(): -13.725377082824707 
model_pd.lagr.mean(): -13.613641738891602 
model_pd.lambdas: dict_items([('pout', tensor([1.2547], device='cuda:0')), ('power', tensor([0.6289], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6497], device='cuda:0')), ('power', tensor([-23.0775], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.1117355227470398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9601, 4.6908, 5.0664],
        [3.9601, 3.9625, 3.9602],
        [3.9601, 4.1650, 4.1362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.11162331700325012 
model_pd.l_d.mean(): -13.698492050170898 
model_pd.lagr.mean(): -13.586868286132812 
model_pd.lambdas: dict_items([('pout', tensor([1.2554], device='cuda:0')), ('power', tensor([0.6278], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6484], device='cuda:0')), ('power', tensor([-23.0751], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.11162331700325012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9645, 3.9669, 3.9646],
        [3.9645, 4.6953, 5.0705],
        [3.9645, 4.1689, 4.1398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.11151117086410522 
model_pd.l_d.mean(): -13.671613693237305 
model_pd.lagr.mean(): -13.560102462768555 
model_pd.lambdas: dict_items([('pout', tensor([1.2560], device='cuda:0')), ('power', tensor([0.6266], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6471], device='cuda:0')), ('power', tensor([-23.0727], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.11151117086410522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9690, 4.1729, 4.1435],
        [3.9690, 4.6998, 5.0746],
        [3.9690, 3.9713, 3.9690]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.11139903962612152 
model_pd.l_d.mean(): -13.644739151000977 
model_pd.lagr.mean(): -13.533340454101562 
model_pd.lambdas: dict_items([('pout', tensor([1.2567], device='cuda:0')), ('power', tensor([0.6254], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6457], device='cuda:0')), ('power', tensor([-23.0703], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.11139903962612152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9734, 3.9757, 3.9735],
        [3.9734, 4.7044, 5.0787],
        [3.9734, 4.1769, 4.1472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.11128677427768707 
model_pd.l_d.mean(): -13.617874145507812 
model_pd.lagr.mean(): -13.506587028503418 
model_pd.lambdas: dict_items([('pout', tensor([1.2573], device='cuda:0')), ('power', tensor([0.6243], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6444], device='cuda:0')), ('power', tensor([-23.0679], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.11128677427768707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9779, 3.9802, 3.9780],
        [3.9779, 4.7090, 5.0829],
        [3.9779, 4.1810, 4.1509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.11117427051067352 
model_pd.l_d.mean(): -13.591014862060547 
model_pd.lagr.mean(): -13.479840278625488 
model_pd.lambdas: dict_items([('pout', tensor([1.2580], device='cuda:0')), ('power', tensor([0.6231], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6431], device='cuda:0')), ('power', tensor([-23.0654], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.11117427051067352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9825, 4.1850, 4.1546],
        [3.9825, 4.7136, 5.0871],
        [3.9825, 3.9846, 3.9825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.11106157302856445 
model_pd.l_d.mean(): -13.564157485961914 
model_pd.lagr.mean(): -13.453096389770508 
model_pd.lambdas: dict_items([('pout', tensor([1.2586], device='cuda:0')), ('power', tensor([0.6220], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6417], device='cuda:0')), ('power', tensor([-23.0630], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.11106157302856445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9870, 4.1891, 4.1584],
        [3.9870, 4.7183, 5.0913],
        [3.9870, 3.9891, 3.9871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.11094868183135986 
model_pd.l_d.mean(): -13.537310600280762 
model_pd.lagr.mean(): -13.426362037658691 
model_pd.lambdas: dict_items([('pout', tensor([1.2593], device='cuda:0')), ('power', tensor([0.6208], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6403], device='cuda:0')), ('power', tensor([-23.0605], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.11094868183135986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9916, 3.9936, 3.9916],
        [3.9916, 4.7229, 5.0955],
        [3.9916, 4.1932, 4.1622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.11083575338125229 
model_pd.l_d.mean(): -13.510470390319824 
model_pd.lagr.mean(): -13.39963436126709 
model_pd.lambdas: dict_items([('pout', tensor([1.2599], device='cuda:0')), ('power', tensor([0.6197], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6390], device='cuda:0')), ('power', tensor([-23.0580], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.11083575338125229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9962, 3.9982, 3.9962],
        [3.9962, 4.7276, 5.0998],
        [3.9962, 4.1973, 4.1660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.11072278022766113 
model_pd.l_d.mean(): -13.483635902404785 
model_pd.lagr.mean(): -13.372913360595703 
model_pd.lambdas: dict_items([('pout', tensor([1.2605], device='cuda:0')), ('power', tensor([0.6185], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6376], device='cuda:0')), ('power', tensor([-23.0554], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.11072278022766113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0007, 4.0027, 4.0008],
        [4.0007, 4.7323, 5.1041],
        [4.0007, 4.2015, 4.1698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.11060981452465057 
model_pd.l_d.mean(): -13.456808090209961 
model_pd.lagr.mean(): -13.346198081970215 
model_pd.lambdas: dict_items([('pout', tensor([1.2612], device='cuda:0')), ('power', tensor([0.6174], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6362], device='cuda:0')), ('power', tensor([-23.0529], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.11060981452465057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0054, 4.7371, 5.1084],
        [4.0054, 4.2056, 4.1736],
        [4.0054, 4.0073, 4.0054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.11049675941467285 
model_pd.l_d.mean(): -13.429985046386719 
model_pd.lagr.mean(): -13.319488525390625 
model_pd.lambdas: dict_items([('pout', tensor([1.2618], device='cuda:0')), ('power', tensor([0.6162], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6349], device='cuda:0')), ('power', tensor([-23.0503], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.11049675941467285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0100, 4.7418, 5.1127],
        [4.0100, 4.2098, 4.1774],
        [4.0100, 4.0119, 4.0100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.11038359999656677 
model_pd.l_d.mean(): -13.40317153930664 
model_pd.lagr.mean(): -13.292787551879883 
model_pd.lambdas: dict_items([('pout', tensor([1.2624], device='cuda:0')), ('power', tensor([0.6151], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6335], device='cuda:0')), ('power', tensor([-23.0478], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.11038359999656677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0147, 4.0165, 4.0147],
        [4.0147, 4.2140, 4.1813],
        [4.0147, 4.7466, 5.1171]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.11027023196220398 
model_pd.l_d.mean(): -13.376360893249512 
model_pd.lagr.mean(): -13.266090393066406 
model_pd.lambdas: dict_items([('pout', tensor([1.2631], device='cuda:0')), ('power', tensor([0.6139], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6321], device='cuda:0')), ('power', tensor([-23.0452], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.11027023196220398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0194, 4.7514, 5.1215],
        [4.0194, 4.2182, 4.1852],
        [4.0194, 4.0211, 4.0193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.11015678942203522 
model_pd.l_d.mean(): -13.34955883026123 
model_pd.lagr.mean(): -13.239401817321777 
model_pd.lambdas: dict_items([('pout', tensor([1.2637], device='cuda:0')), ('power', tensor([0.6128], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6307], device='cuda:0')), ('power', tensor([-23.0425], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.11015678942203522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0241, 4.2225, 4.1891],
        [4.0241, 4.7563, 5.1260],
        [4.0241, 4.0258, 4.0240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.11004313081502914 
model_pd.l_d.mean(): -13.32276439666748 
model_pd.lagr.mean(): -13.21272087097168 
model_pd.lambdas: dict_items([('pout', tensor([1.2643], device='cuda:0')), ('power', tensor([0.6116], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6293], device='cuda:0')), ('power', tensor([-23.0399], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.11004313081502914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0288, 4.0305, 4.0288],
        [4.0288, 4.7612, 5.1304],
        [4.0288, 4.2268, 4.1931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.1099294051527977 
model_pd.l_d.mean(): -13.295975685119629 
model_pd.lagr.mean(): -13.186046600341797 
model_pd.lambdas: dict_items([('pout', tensor([1.2650], device='cuda:0')), ('power', tensor([0.6105], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6279], device='cuda:0')), ('power', tensor([-23.0372], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.1099294051527977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0335, 4.0352, 4.0335],
        [4.0335, 4.7661, 5.1349],
        [4.0335, 4.2311, 4.1970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.10981561988592148 
model_pd.l_d.mean(): -13.26919174194336 
model_pd.lagr.mean(): -13.15937614440918 
model_pd.lambdas: dict_items([('pout', tensor([1.2656], device='cuda:0')), ('power', tensor([0.6093], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6265], device='cuda:0')), ('power', tensor([-23.0346], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.10981561988592148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0383, 4.7710, 5.1394],
        [4.0383, 4.2354, 4.2010],
        [4.0383, 4.0399, 4.0383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.10970177501440048 
model_pd.l_d.mean(): -13.242417335510254 
model_pd.lagr.mean(): -13.132715225219727 
model_pd.lambdas: dict_items([('pout', tensor([1.2662], device='cuda:0')), ('power', tensor([0.6082], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6251], device='cuda:0')), ('power', tensor([-23.0319], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.10970177501440048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0431, 4.0446, 4.0430],
        [4.0431, 4.2397, 4.2050],
        [4.0431, 4.7759, 5.1439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.10958784818649292 
model_pd.l_d.mean(): -13.21564769744873 
model_pd.lagr.mean(): -13.106060028076172 
model_pd.lambdas: dict_items([('pout', tensor([1.2668], device='cuda:0')), ('power', tensor([0.6070], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6237], device='cuda:0')), ('power', tensor([-23.0291], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.10958784818649292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0479, 4.7809, 5.1485],
        [4.0479, 4.2441, 4.2090],
        [4.0479, 4.0494, 4.0478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.10947384685277939 
model_pd.l_d.mean(): -13.188884735107422 
model_pd.lagr.mean(): -13.079410552978516 
model_pd.lambdas: dict_items([('pout', tensor([1.2675], device='cuda:0')), ('power', tensor([0.6059], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6222], device='cuda:0')), ('power', tensor([-23.0264], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.10947384685277939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0527, 4.0542, 4.0527],
        [4.0527, 4.2485, 4.2131],
        [4.0527, 4.7859, 5.1531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.10935968160629272 
model_pd.l_d.mean(): -13.162131309509277 
model_pd.lagr.mean(): -13.05277156829834 
model_pd.lambdas: dict_items([('pout', tensor([1.2681], device='cuda:0')), ('power', tensor([0.6047], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6208], device='cuda:0')), ('power', tensor([-23.0236], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.10935968160629272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0576, 4.2529, 4.2171],
        [4.0576, 4.7909, 5.1578],
        [4.0576, 4.0590, 4.0575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.10924530029296875 
model_pd.l_d.mean(): -13.135381698608398 
model_pd.lagr.mean(): -13.02613639831543 
model_pd.lambdas: dict_items([('pout', tensor([1.2687], device='cuda:0')), ('power', tensor([0.6036], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6194], device='cuda:0')), ('power', tensor([-23.0209], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.10924530029296875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0625, 4.7960, 5.1624],
        [4.0625, 4.2573, 4.2212],
        [4.0625, 4.0638, 4.0624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.10913076996803284 
model_pd.l_d.mean(): -13.108640670776367 
model_pd.lagr.mean(): -12.999509811401367 
model_pd.lambdas: dict_items([('pout', tensor([1.2693], device='cuda:0')), ('power', tensor([0.6024], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6179], device='cuda:0')), ('power', tensor([-23.0181], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.10913076996803284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0674, 4.8011, 5.1671],
        [4.0674, 4.2618, 4.2254],
        [4.0674, 4.0687, 4.0673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.10901617258787155 
model_pd.l_d.mean(): -13.081904411315918 
model_pd.lagr.mean(): -12.972887992858887 
model_pd.lambdas: dict_items([('pout', tensor([1.2699], device='cuda:0')), ('power', tensor([0.6013], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6165], device='cuda:0')), ('power', tensor([-23.0152], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.10901617258787155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0723, 4.8062, 5.1718],
        [4.0723, 4.0735, 4.0722],
        [4.0723, 4.2662, 4.2295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.10890158265829086 
model_pd.l_d.mean(): -13.055176734924316 
model_pd.lagr.mean(): -12.946274757385254 
model_pd.lambdas: dict_items([('pout', tensor([1.2705], device='cuda:0')), ('power', tensor([0.6001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6150], device='cuda:0')), ('power', tensor([-23.0124], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.10890158265829086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0773, 4.2707, 4.2336],
        [4.0773, 4.8114, 5.1766],
        [4.0773, 4.0784, 4.0771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.10878690332174301 
model_pd.l_d.mean(): -13.028456687927246 
model_pd.lagr.mean(): -12.919670104980469 
model_pd.lambdas: dict_items([('pout', tensor([1.2712], device='cuda:0')), ('power', tensor([0.5989], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6136], device='cuda:0')), ('power', tensor([-23.0095], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.10878690332174301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0822, 4.0834, 4.0821],
        [4.0822, 4.8165, 5.1814],
        [4.0822, 4.2753, 4.2378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.10867206752300262 
model_pd.l_d.mean(): -13.00174331665039 
model_pd.lagr.mean(): -12.893071174621582 
model_pd.lambdas: dict_items([('pout', tensor([1.2718], device='cuda:0')), ('power', tensor([0.5978], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6121], device='cuda:0')), ('power', tensor([-23.0067], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.10867206752300262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0872, 4.2798, 4.2420],
        [4.0872, 4.0883, 4.0871],
        [4.0872, 4.8217, 5.1862]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.10855704545974731 
model_pd.l_d.mean(): -12.975035667419434 
model_pd.lagr.mean(): -12.86647891998291 
model_pd.lambdas: dict_items([('pout', tensor([1.2724], device='cuda:0')), ('power', tensor([0.5966], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6106], device='cuda:0')), ('power', tensor([-23.0037], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.10855704545974731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0922, 4.2844, 4.2463],
        [4.0922, 4.0933, 4.0921],
        [4.0922, 4.8270, 5.1911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.10844185948371887 
model_pd.l_d.mean(): -12.94833755493164 
model_pd.lagr.mean(): -12.839895248413086 
model_pd.lambdas: dict_items([('pout', tensor([1.2730], device='cuda:0')), ('power', tensor([0.5955], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6091], device='cuda:0')), ('power', tensor([-23.0008], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.10844185948371887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0973, 4.8322, 5.1960],
        [4.0973, 4.0983, 4.0971],
        [4.0973, 4.2890, 4.2505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.10832658410072327 
model_pd.l_d.mean(): -12.921645164489746 
model_pd.lagr.mean(): -12.813318252563477 
model_pd.lambdas: dict_items([('pout', tensor([1.2736], device='cuda:0')), ('power', tensor([0.5943], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6077], device='cuda:0')), ('power', tensor([-22.9979], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.10832658410072327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1024, 4.2936, 4.2548],
        [4.1024, 4.1033, 4.1022],
        [4.1024, 4.8375, 5.2009]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.10821129381656647 
model_pd.l_d.mean(): -12.894959449768066 
model_pd.lagr.mean(): -12.786747932434082 
model_pd.lambdas: dict_items([('pout', tensor([1.2742], device='cuda:0')), ('power', tensor([0.5932], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6062], device='cuda:0')), ('power', tensor([-22.9949], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.10821129381656647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1074, 4.2983, 4.2591],
        [4.1074, 4.1083, 4.1073],
        [4.1074, 4.8429, 5.2058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.1080959364771843 
model_pd.l_d.mean(): -12.86828327178955 
model_pd.lagr.mean(): -12.760187149047852 
model_pd.lambdas: dict_items([('pout', tensor([1.2748], device='cuda:0')), ('power', tensor([0.5920], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6047], device='cuda:0')), ('power', tensor([-22.9919], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.1080959364771843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1125, 4.3029, 4.2635],
        [4.1125, 4.8482, 5.2108],
        [4.1125, 4.1134, 4.1124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.10798059403896332 
model_pd.l_d.mean(): -12.841612815856934 
model_pd.lagr.mean(): -12.73363208770752 
model_pd.lambdas: dict_items([('pout', tensor([1.2754], device='cuda:0')), ('power', tensor([0.5909], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6032], device='cuda:0')), ('power', tensor([-22.9889], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.10798059403896332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1177, 4.8536, 5.2158],
        [4.1177, 4.1185, 4.1175],
        [4.1177, 4.3076, 4.2678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.10786505788564682 
model_pd.l_d.mean(): -12.814949035644531 
model_pd.lagr.mean(): -12.707083702087402 
model_pd.lambdas: dict_items([('pout', tensor([1.2760], device='cuda:0')), ('power', tensor([0.5898], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6017], device='cuda:0')), ('power', tensor([-22.9858], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.10786505788564682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1228, 4.1236, 4.1226],
        [4.1228, 4.3124, 4.2722],
        [4.1228, 4.8590, 5.2209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.10774936527013779 
model_pd.l_d.mean(): -12.788294792175293 
model_pd.lagr.mean(): -12.680545806884766 
model_pd.lambdas: dict_items([('pout', tensor([1.2766], device='cuda:0')), ('power', tensor([0.5886], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6002], device='cuda:0')), ('power', tensor([-22.9828], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.10774936527013779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1280, 4.1287, 4.1278],
        [4.1280, 4.8644, 5.2260],
        [4.1280, 4.3171, 4.2766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.10763353109359741 
model_pd.l_d.mean(): -12.761646270751953 
model_pd.lagr.mean(): -12.654012680053711 
model_pd.lambdas: dict_items([('pout', tensor([1.2772], device='cuda:0')), ('power', tensor([0.5875], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5986], device='cuda:0')), ('power', tensor([-22.9797], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.10763353109359741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1332, 4.8699, 5.2311],
        [4.1332, 4.3219, 4.2810],
        [4.1332, 4.1339, 4.1330]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.10751762241125107 
model_pd.l_d.mean(): -12.735005378723145 
model_pd.lagr.mean(): -12.627488136291504 
model_pd.lambdas: dict_items([('pout', tensor([1.2778], device='cuda:0')), ('power', tensor([0.5863], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5971], device='cuda:0')), ('power', tensor([-22.9766], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.10751762241125107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1385, 4.3267, 4.2855],
        [4.1385, 4.8754, 5.2362],
        [4.1385, 4.1390, 4.1382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.10740160942077637 
model_pd.l_d.mean(): -12.708372116088867 
model_pd.lagr.mean(): -12.600970268249512 
model_pd.lambdas: dict_items([('pout', tensor([1.2784], device='cuda:0')), ('power', tensor([0.5852], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5956], device='cuda:0')), ('power', tensor([-22.9734], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.10740160942077637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1437, 4.1443, 4.1435],
        [4.1437, 4.8810, 5.2414],
        [4.1437, 4.3315, 4.2900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.10728561878204346 
model_pd.l_d.mean(): -12.681746482849121 
model_pd.lagr.mean(): -12.574460983276367 
model_pd.lambdas: dict_items([('pout', tensor([1.2790], device='cuda:0')), ('power', tensor([0.5840], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5940], device='cuda:0')), ('power', tensor([-22.9703], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.10728561878204346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1490, 4.1495, 4.1487],
        [4.1490, 4.8865, 5.2466],
        [4.1490, 4.3364, 4.2945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.10716953128576279 
model_pd.l_d.mean(): -12.655128479003906 
model_pd.lagr.mean(): -12.547959327697754 
model_pd.lambdas: dict_items([('pout', tensor([1.2796], device='cuda:0')), ('power', tensor([0.5829], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5925], device='cuda:0')), ('power', tensor([-22.9671], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.10716953128576279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1543, 4.1547, 4.1540],
        [4.1543, 4.3412, 4.2990],
        [4.1543, 4.8921, 5.2519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.10705333203077316 
model_pd.l_d.mean(): -12.628517150878906 
model_pd.lagr.mean(): -12.521463394165039 
model_pd.lambdas: dict_items([('pout', tensor([1.2802], device='cuda:0')), ('power', tensor([0.5817], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5910], device='cuda:0')), ('power', tensor([-22.9639], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.10705333203077316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1596, 4.1600, 4.1594],
        [4.1596, 4.3462, 4.3036],
        [4.1596, 4.8977, 5.2572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.10693693161010742 
model_pd.l_d.mean(): -12.60191535949707 
model_pd.lagr.mean(): -12.494977951049805 
model_pd.lambdas: dict_items([('pout', tensor([1.2808], device='cuda:0')), ('power', tensor([0.5806], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5894], device='cuda:0')), ('power', tensor([-22.9607], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.10693693161010742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1650, 4.1653, 4.1647],
        [4.1650, 4.9034, 5.2625],
        [4.1650, 4.3511, 4.3082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.10682040452957153 
model_pd.l_d.mean(): -12.57532024383545 
model_pd.lagr.mean(): -12.468500137329102 
model_pd.lambdas: dict_items([('pout', tensor([1.2814], device='cuda:0')), ('power', tensor([0.5794], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5878], device='cuda:0')), ('power', tensor([-22.9574], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.10682040452957153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1704, 4.1707, 4.1701],
        [4.1704, 4.3561, 4.3128],
        [4.1704, 4.9091, 5.2679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.10670384764671326 
model_pd.l_d.mean(): -12.54873275756836 
model_pd.lagr.mean(): -12.442028999328613 
model_pd.lambdas: dict_items([('pout', tensor([1.2819], device='cuda:0')), ('power', tensor([0.5783], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5863], device='cuda:0')), ('power', tensor([-22.9541], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.10670384764671326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1758, 4.3611, 4.3174],
        [4.1758, 4.9148, 5.2733],
        [4.1758, 4.1760, 4.1755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.106587253510952 
model_pd.l_d.mean(): -12.52215576171875 
model_pd.lagr.mean(): -12.415568351745605 
model_pd.lambdas: dict_items([('pout', tensor([1.2825], device='cuda:0')), ('power', tensor([0.5771], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5847], device='cuda:0')), ('power', tensor([-22.9508], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.106587253510952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1812, 4.3661, 4.3221],
        [4.1812, 4.9206, 5.2788],
        [4.1812, 4.1814, 4.1809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.10647054761648178 
model_pd.l_d.mean(): -12.495584487915039 
model_pd.lagr.mean(): -12.389114379882812 
model_pd.lambdas: dict_items([('pout', tensor([1.2831], device='cuda:0')), ('power', tensor([0.5760], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5831], device='cuda:0')), ('power', tensor([-22.9475], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.10647054761648178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1867, 4.1868, 4.1863],
        [4.1867, 4.3711, 4.3268],
        [4.1867, 4.9264, 5.2842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.106353759765625 
model_pd.l_d.mean(): -12.469019889831543 
model_pd.lagr.mean(): -12.362666130065918 
model_pd.lambdas: dict_items([('pout', tensor([1.2837], device='cuda:0')), ('power', tensor([0.5748], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5815], device='cuda:0')), ('power', tensor([-22.9442], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.106353759765625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1921, 4.3762, 4.3315],
        [4.1921, 4.1922, 4.1918],
        [4.1921, 4.9322, 5.2898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.10623686015605927 
model_pd.l_d.mean(): -12.442464828491211 
model_pd.lagr.mean(): -12.336228370666504 
model_pd.lambdas: dict_items([('pout', tensor([1.2843], device='cuda:0')), ('power', tensor([0.5737], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5799], device='cuda:0')), ('power', tensor([-22.9408], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.10623686015605927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1976, 4.9381, 5.2953],
        [4.1976, 4.1977, 4.1973],
        [4.1976, 4.3813, 4.3363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.10611977428197861 
model_pd.l_d.mean(): -12.415918350219727 
model_pd.lagr.mean(): -12.309798240661621 
model_pd.lambdas: dict_items([('pout', tensor([1.2849], device='cuda:0')), ('power', tensor([0.5725], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5783], device='cuda:0')), ('power', tensor([-22.9374], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.10611977428197861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2032, 4.2032, 4.2028],
        [4.2032, 4.3864, 4.3411],
        [4.2032, 4.9440, 5.3009]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.10600262880325317 
model_pd.l_d.mean(): -12.389379501342773 
model_pd.lagr.mean(): -12.283376693725586 
model_pd.lambdas: dict_items([('pout', tensor([1.2854], device='cuda:0')), ('power', tensor([0.5714], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5767], device='cuda:0')), ('power', tensor([-22.9340], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.10600262880325317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2087, 4.3916, 4.3459],
        [4.2087, 4.9499, 5.3066],
        [4.2087, 4.2087, 4.2084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.10588544607162476 
model_pd.l_d.mean(): -12.362850189208984 
model_pd.lagr.mean(): -12.256964683532715 
model_pd.lambdas: dict_items([('pout', tensor([1.2860], device='cuda:0')), ('power', tensor([0.5702], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5751], device='cuda:0')), ('power', tensor([-22.9305], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.10588544607162476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2143, 4.9559, 5.3122],
        [4.2143, 4.3968, 4.3507],
        [4.2143, 4.2142, 4.2140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.10576821863651276 
model_pd.l_d.mean(): -12.336328506469727 
model_pd.lagr.mean(): -12.230560302734375 
model_pd.lambdas: dict_items([('pout', tensor([1.2866], device='cuda:0')), ('power', tensor([0.5691], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5735], device='cuda:0')), ('power', tensor([-22.9270], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.10576821863651276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2199, 4.4020, 4.3556],
        [4.2199, 4.9619, 5.3180],
        [4.2199, 4.2198, 4.2196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.1056508868932724 
model_pd.l_d.mean(): -12.309813499450684 
model_pd.lagr.mean(): -12.20416259765625 
model_pd.lambdas: dict_items([('pout', tensor([1.2872], device='cuda:0')), ('power', tensor([0.5679], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5719], device='cuda:0')), ('power', tensor([-22.9235], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.1056508868932724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2256, 4.9679, 5.3237],
        [4.2256, 4.2254, 4.2252],
        [4.2256, 4.4073, 4.3605]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.10553345084190369 
model_pd.l_d.mean(): -12.283308029174805 
model_pd.lagr.mean(): -12.177774429321289 
model_pd.lambdas: dict_items([('pout', tensor([1.2877], device='cuda:0')), ('power', tensor([0.5668], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5702], device='cuda:0')), ('power', tensor([-22.9200], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.10553345084190369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2312, 4.2310, 4.2309],
        [4.2312, 4.4125, 4.3655],
        [4.2312, 4.9740, 5.3295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.10541587322950363 
model_pd.l_d.mean(): -12.256810188293457 
model_pd.lagr.mean(): -12.15139389038086 
model_pd.lambdas: dict_items([('pout', tensor([1.2883], device='cuda:0')), ('power', tensor([0.5657], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5686], device='cuda:0')), ('power', tensor([-22.9164], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.10541587322950363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2369, 4.2366, 4.2365],
        [4.2369, 4.9801, 5.3354],
        [4.2369, 4.4179, 4.3704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.10529816150665283 
model_pd.l_d.mean(): -12.230320930480957 
model_pd.lagr.mean(): -12.125022888183594 
model_pd.lambdas: dict_items([('pout', tensor([1.2889], device='cuda:0')), ('power', tensor([0.5645], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5669], device='cuda:0')), ('power', tensor([-22.9128], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.10529816150665283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2427, 4.4232, 4.3754],
        [4.2427, 4.2423, 4.2423],
        [4.2427, 4.9863, 5.3413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.10518037527799606 
model_pd.l_d.mean(): -12.203838348388672 
model_pd.lagr.mean(): -12.098657608032227 
model_pd.lambdas: dict_items([('pout', tensor([1.2894], device='cuda:0')), ('power', tensor([0.5634], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5653], device='cuda:0')), ('power', tensor([-22.9092], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.10518037527799606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2484, 4.4286, 4.3804],
        [4.2484, 4.2480, 4.2480],
        [4.2484, 4.9925, 5.3472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.10506263375282288 
model_pd.l_d.mean(): -12.177366256713867 
model_pd.lagr.mean(): -12.072303771972656 
model_pd.lambdas: dict_items([('pout', tensor([1.2900], device='cuda:0')), ('power', tensor([0.5622], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5636], device='cuda:0')), ('power', tensor([-22.9056], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.10506263375282288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2542, 4.4340, 4.3855],
        [4.2542, 4.9987, 5.3531],
        [4.2542, 4.2537, 4.2538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.10494478791952133 
model_pd.l_d.mean(): -12.150903701782227 
model_pd.lagr.mean(): -12.045958518981934 
model_pd.lambdas: dict_items([('pout', tensor([1.2905], device='cuda:0')), ('power', tensor([0.5611], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5620], device='cuda:0')), ('power', tensor([-22.9019], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.10494478791952133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2600, 4.2595, 4.2595],
        [4.2600, 5.0049, 5.3592],
        [4.2600, 4.4394, 4.3906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.10482686012983322 
model_pd.l_d.mean(): -12.124448776245117 
model_pd.lagr.mean(): -12.019621849060059 
model_pd.lambdas: dict_items([('pout', tensor([1.2911], device='cuda:0')), ('power', tensor([0.5599], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5603], device='cuda:0')), ('power', tensor([-22.8982], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.10482686012983322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2658, 4.2653, 4.2654],
        [4.2658, 4.4449, 4.3957],
        [4.2658, 5.0112, 5.3652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.10470878332853317 
model_pd.l_d.mean(): -12.098003387451172 
model_pd.lagr.mean(): -11.993294715881348 
model_pd.lambdas: dict_items([('pout', tensor([1.2917], device='cuda:0')), ('power', tensor([0.5588], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5586], device='cuda:0')), ('power', tensor([-22.8945], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.10470878332853317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2717, 4.4504, 4.4008],
        [4.2717, 4.2711, 4.2712],
        [4.2717, 5.0176, 5.3713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.10459061712026596 
model_pd.l_d.mean(): -12.071566581726074 
model_pd.lagr.mean(): -11.966976165771484 
model_pd.lambdas: dict_items([('pout', tensor([1.2922], device='cuda:0')), ('power', tensor([0.5576], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5569], device='cuda:0')), ('power', tensor([-22.8908], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.10459061712026596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2776, 5.0239, 5.3775],
        [4.2776, 4.2769, 4.2771],
        [4.2776, 4.4559, 4.4060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.1044723317027092 
model_pd.l_d.mean(): -12.045137405395508 
model_pd.lagr.mean(): -11.940665245056152 
model_pd.lambdas: dict_items([('pout', tensor([1.2928], device='cuda:0')), ('power', tensor([0.5565], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5552], device='cuda:0')), ('power', tensor([-22.8870], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.1044723317027092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2835, 5.0303, 5.3836],
        [4.2835, 4.2828, 4.2830],
        [4.2835, 4.4615, 4.4112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.10435402393341064 
model_pd.l_d.mean(): -12.018719673156738 
model_pd.lagr.mean(): -11.914365768432617 
model_pd.lambdas: dict_items([('pout', tensor([1.2933], device='cuda:0')), ('power', tensor([0.5553], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5535], device='cuda:0')), ('power', tensor([-22.8832], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.10435402393341064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2894, 5.0368, 5.3899],
        [4.2894, 4.4670, 4.4165],
        [4.2894, 4.2887, 4.2889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.1042356938123703 
model_pd.l_d.mean(): -11.992307662963867 
model_pd.lagr.mean(): -11.88807201385498 
model_pd.lambdas: dict_items([('pout', tensor([1.2939], device='cuda:0')), ('power', tensor([0.5542], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5518], device='cuda:0')), ('power', tensor([-22.8793], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.1042356938123703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2954, 5.0433, 5.3961],
        [4.2954, 4.2946, 4.2949],
        [4.2954, 4.4727, 4.4218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.10411730408668518 
model_pd.l_d.mean(): -11.965906143188477 
model_pd.lagr.mean(): -11.861788749694824 
model_pd.lambdas: dict_items([('pout', tensor([1.2944], device='cuda:0')), ('power', tensor([0.5531], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5501], device='cuda:0')), ('power', tensor([-22.8755], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.10411730408668518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3014, 4.3005, 4.3009],
        [4.3014, 5.0498, 5.4025],
        [4.3014, 4.4783, 4.4271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.10399878025054932 
model_pd.l_d.mean(): -11.939516067504883 
model_pd.lagr.mean(): -11.835516929626465 
model_pd.lambdas: dict_items([('pout', tensor([1.2950], device='cuda:0')), ('power', tensor([0.5519], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5484], device='cuda:0')), ('power', tensor([-22.8716], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.10399878025054932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3074, 4.4840, 4.4324],
        [4.3074, 4.3065, 4.3069],
        [4.3074, 5.0563, 5.4088]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.10388018190860748 
model_pd.l_d.mean(): -11.913131713867188 
model_pd.lagr.mean(): -11.80925178527832 
model_pd.lambdas: dict_items([('pout', tensor([1.2955], device='cuda:0')), ('power', tensor([0.5508], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5467], device='cuda:0')), ('power', tensor([-22.8676], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.10388018190860748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3135, 4.3125, 4.3129],
        [4.3135, 4.4897, 4.4378],
        [4.3135, 5.0630, 5.4152]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.1037614569067955 
model_pd.l_d.mean(): -11.886758804321289 
model_pd.lagr.mean(): -11.782997131347656 
model_pd.lambdas: dict_items([('pout', tensor([1.2961], device='cuda:0')), ('power', tensor([0.5496], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5449], device='cuda:0')), ('power', tensor([-22.8637], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.1037614569067955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3195, 5.0696, 5.4217],
        [4.3195, 4.3186, 4.3190],
        [4.3195, 4.4955, 4.4432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.10364265739917755 
model_pd.l_d.mean(): -11.860395431518555 
model_pd.lagr.mean(): -11.756752967834473 
model_pd.lambdas: dict_items([('pout', tensor([1.2966], device='cuda:0')), ('power', tensor([0.5485], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5432], device='cuda:0')), ('power', tensor([-22.8597], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.10364265739917755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3257, 4.3246, 4.3251],
        [4.3257, 5.0763, 5.4282],
        [4.3257, 4.5013, 4.4487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.1035238653421402 
model_pd.l_d.mean(): -11.834040641784668 
model_pd.lagr.mean(): -11.73051643371582 
model_pd.lambdas: dict_items([('pout', tensor([1.2972], device='cuda:0')), ('power', tensor([0.5473], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5414], device='cuda:0')), ('power', tensor([-22.8557], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.1035238653421402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3318, 4.3307, 4.3313],
        [4.3318, 4.5071, 4.4541],
        [4.3318, 5.0830, 5.4348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.10340499132871628 
model_pd.l_d.mean(): -11.807693481445312 
model_pd.lagr.mean(): -11.704288482666016 
model_pd.lambdas: dict_items([('pout', tensor([1.2977], device='cuda:0')), ('power', tensor([0.5462], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5397], device='cuda:0')), ('power', tensor([-22.8517], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.10340499132871628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3380, 4.3369, 4.3374],
        [4.3380, 5.0898, 5.4414],
        [4.3380, 4.5130, 4.4597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.10328608751296997 
model_pd.l_d.mean(): -11.78135871887207 
model_pd.lagr.mean(): -11.678072929382324 
model_pd.lambdas: dict_items([('pout', tensor([1.2982], device='cuda:0')), ('power', tensor([0.5451], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5379], device='cuda:0')), ('power', tensor([-22.8476], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.10328608751296997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3442, 5.0966, 5.4480],
        [4.3442, 4.3430, 4.3436],
        [4.3442, 4.5189, 4.4652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.10316701978445053 
model_pd.l_d.mean(): -11.755033493041992 
model_pd.lagr.mean(): -11.651866912841797 
model_pd.lambdas: dict_items([('pout', tensor([1.2988], device='cuda:0')), ('power', tensor([0.5439], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5361], device='cuda:0')), ('power', tensor([-22.8435], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.10316701978445053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3504, 5.1034, 5.4547],
        [4.3504, 4.5248, 4.4708],
        [4.3504, 4.3492, 4.3498]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.10304784774780273 
model_pd.l_d.mean(): -11.728716850280762 
model_pd.lagr.mean(): -11.625669479370117 
model_pd.lambdas: dict_items([('pout', tensor([1.2993], device='cuda:0')), ('power', tensor([0.5428], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5344], device='cuda:0')), ('power', tensor([-22.8393], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.10304784774780273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3567, 4.3554, 4.3561],
        [4.3567, 4.5307, 4.4764],
        [4.3567, 5.1103, 5.4615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.10292866080999374 
model_pd.l_d.mean(): -11.702409744262695 
model_pd.lagr.mean(): -11.599480628967285 
model_pd.lambdas: dict_items([('pout', tensor([1.2998], device='cuda:0')), ('power', tensor([0.5416], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5326], device='cuda:0')), ('power', tensor([-22.8352], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.10292866080999374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3630, 4.5367, 4.4821],
        [4.3630, 5.1172, 5.4683],
        [4.3630, 4.3617, 4.3624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.10280945152044296 
model_pd.l_d.mean(): -11.676109313964844 
model_pd.lagr.mean(): -11.573299407958984 
model_pd.lambdas: dict_items([('pout', tensor([1.3004], device='cuda:0')), ('power', tensor([0.5405], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5308], device='cuda:0')), ('power', tensor([-22.8310], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.10280945152044296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3693, 5.1242, 5.4751],
        [4.3693, 4.3679, 4.3687],
        [4.3693, 4.5428, 4.4878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.10269014537334442 
model_pd.l_d.mean(): -11.649822235107422 
model_pd.lagr.mean(): -11.54713249206543 
model_pd.lambdas: dict_items([('pout', tensor([1.3009], device='cuda:0')), ('power', tensor([0.5394], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5290], device='cuda:0')), ('power', tensor([-22.8268], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.10269014537334442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3756, 5.1312, 5.4820],
        [4.3756, 4.3743, 4.3750],
        [4.3756, 4.5488, 4.4935]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.10257075726985931 
model_pd.l_d.mean(): -11.62354564666748 
model_pd.lagr.mean(): -11.520975112915039 
model_pd.lambdas: dict_items([('pout', tensor([1.3014], device='cuda:0')), ('power', tensor([0.5382], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5272], device='cuda:0')), ('power', tensor([-22.8225], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.10257075726985931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3820, 4.3806, 4.3814],
        [4.3820, 5.1383, 5.4890],
        [4.3820, 4.5550, 4.4993]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.10245134681463242 
model_pd.l_d.mean(): -11.597278594970703 
model_pd.lagr.mean(): -11.494827270507812 
model_pd.lambdas: dict_items([('pout', tensor([1.3020], device='cuda:0')), ('power', tensor([0.5371], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5254], device='cuda:0')), ('power', tensor([-22.8182], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.10245134681463242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3884, 4.5611, 4.5051],
        [4.3884, 4.3870, 4.3878],
        [4.3884, 5.1454, 5.4960]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.10233180224895477 
model_pd.l_d.mean(): -11.57102108001709 
model_pd.lagr.mean(): -11.46868896484375 
model_pd.lambdas: dict_items([('pout', tensor([1.3025], device='cuda:0')), ('power', tensor([0.5359], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5235], device='cuda:0')), ('power', tensor([-22.8139], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.10233180224895477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3949, 4.5673, 4.5109],
        [4.3949, 4.3934, 4.3943],
        [4.3949, 5.1525, 5.5030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.10221221297979355 
model_pd.l_d.mean(): -11.544771194458008 
model_pd.lagr.mean(): -11.442559242248535 
model_pd.lambdas: dict_items([('pout', tensor([1.3030], device='cuda:0')), ('power', tensor([0.5348], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5217], device='cuda:0')), ('power', tensor([-22.8095], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.10221221297979355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4014, 4.5735, 4.5168],
        [4.4014, 4.3998, 4.4007],
        [4.4014, 5.1597, 5.5101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.10209259390830994 
model_pd.l_d.mean(): -11.518534660339355 
model_pd.lagr.mean(): -11.416441917419434 
model_pd.lambdas: dict_items([('pout', tensor([1.3035], device='cuda:0')), ('power', tensor([0.5336], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5199], device='cuda:0')), ('power', tensor([-22.8051], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.10209259390830994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4079, 4.5797, 4.5227],
        [4.4079, 5.1669, 5.5173],
        [4.4079, 4.4063, 4.4072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.10197290778160095 
model_pd.l_d.mean(): -11.49230670928955 
model_pd.lagr.mean(): -11.390334129333496 
model_pd.lambdas: dict_items([('pout', tensor([1.3040], device='cuda:0')), ('power', tensor([0.5325], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5180], device='cuda:0')), ('power', tensor([-22.8007], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.10197290778160095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4144, 5.1742, 5.5245],
        [4.4144, 4.5860, 4.5287],
        [4.4144, 4.4128, 4.4138]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.10185317695140839 
model_pd.l_d.mean(): -11.466089248657227 
model_pd.lagr.mean(): -11.364235877990723 
model_pd.lambdas: dict_items([('pout', tensor([1.3046], device='cuda:0')), ('power', tensor([0.5314], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5162], device='cuda:0')), ('power', tensor([-22.7962], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.10185317695140839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4210, 5.1815, 5.5318],
        [4.4210, 4.5924, 4.5347],
        [4.4210, 4.4193, 4.4203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.10173340886831284 
model_pd.l_d.mean(): -11.439882278442383 
model_pd.lagr.mean(): -11.338149070739746 
model_pd.lambdas: dict_items([('pout', tensor([1.3051], device='cuda:0')), ('power', tensor([0.5302], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5143], device='cuda:0')), ('power', tensor([-22.7918], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.10173340886831284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4276, 5.1889, 5.5391],
        [4.4276, 4.4259, 4.4269],
        [4.4276, 4.5987, 4.5407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.10161353647708893 
model_pd.l_d.mean(): -11.413687705993652 
model_pd.lagr.mean(): -11.312073707580566 
model_pd.lambdas: dict_items([('pout', tensor([1.3056], device='cuda:0')), ('power', tensor([0.5291], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5124], device='cuda:0')), ('power', tensor([-22.7872], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.10161353647708893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4342, 4.6051, 4.5468],
        [4.4342, 4.4324, 4.4336],
        [4.4342, 5.1963, 5.5465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.10149361938238144 
model_pd.l_d.mean(): -11.38750171661377 
model_pd.lagr.mean(): -11.28600788116455 
model_pd.lambdas: dict_items([('pout', tensor([1.3061], device='cuda:0')), ('power', tensor([0.5280], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5106], device='cuda:0')), ('power', tensor([-22.7827], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.10149361938238144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4409, 5.2038, 5.5539],
        [4.4409, 4.4391, 4.4402],
        [4.4409, 4.6116, 4.5529]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.10137365758419037 
model_pd.l_d.mean(): -11.361328125 
model_pd.lagr.mean(): -11.259954452514648 
model_pd.lambdas: dict_items([('pout', tensor([1.3066], device='cuda:0')), ('power', tensor([0.5268], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5087], device='cuda:0')), ('power', tensor([-22.7781], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.10137365758419037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4476, 4.4457, 4.4469],
        [4.4476, 4.6180, 4.5591],
        [4.4476, 5.2113, 5.5614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.10125364363193512 
model_pd.l_d.mean(): -11.335161209106445 
model_pd.lagr.mean(): -11.233907699584961 
model_pd.lambdas: dict_items([('pout', tensor([1.3071], device='cuda:0')), ('power', tensor([0.5257], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5068], device='cuda:0')), ('power', tensor([-22.7735], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.10125364363193512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4543, 4.6246, 4.5653],
        [4.4543, 4.4524, 4.4536],
        [4.4543, 5.2188, 5.5690]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.10113358497619629 
model_pd.l_d.mean(): -11.30900764465332 
model_pd.lagr.mean(): -11.207874298095703 
model_pd.lambdas: dict_items([('pout', tensor([1.3076], device='cuda:0')), ('power', tensor([0.5245], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5049], device='cuda:0')), ('power', tensor([-22.7688], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.10113358497619629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4611, 4.4591, 4.4604],
        [4.4611, 5.2264, 5.5766],
        [4.4611, 4.6311, 4.5715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.10101352632045746 
model_pd.l_d.mean(): -11.282864570617676 
model_pd.lagr.mean(): -11.181851387023926 
model_pd.lambdas: dict_items([('pout', tensor([1.3081], device='cuda:0')), ('power', tensor([0.5234], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5030], device='cuda:0')), ('power', tensor([-22.7641], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.10101352632045746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4679, 4.4659, 4.4672],
        [4.4679, 5.2341, 5.5842],
        [4.4679, 4.6377, 4.5778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.10089336335659027 
model_pd.l_d.mean(): -11.256733894348145 
model_pd.lagr.mean(): -11.155840873718262 
model_pd.lambdas: dict_items([('pout', tensor([1.3086], device='cuda:0')), ('power', tensor([0.5223], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5011], device='cuda:0')), ('power', tensor([-22.7594], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.10089336335659027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4747, 5.2418, 5.5919],
        [4.4747, 4.6444, 4.5841],
        [4.4747, 4.4727, 4.4740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.10077314078807831 
model_pd.l_d.mean(): -11.230611801147461 
model_pd.lagr.mean(): -11.129838943481445 
model_pd.lambdas: dict_items([('pout', tensor([1.3091], device='cuda:0')), ('power', tensor([0.5211], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4991], device='cuda:0')), ('power', tensor([-22.7546], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.10077314078807831
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4816, 4.4795, 4.4809],
        [4.4816, 5.2495, 5.5997],
        [4.4816, 4.6510, 4.5904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.10065286606550217 
model_pd.l_d.mean(): -11.204503059387207 
model_pd.lagr.mean(): -11.103850364685059 
model_pd.lambdas: dict_items([('pout', tensor([1.3096], device='cuda:0')), ('power', tensor([0.5200], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4972], device='cuda:0')), ('power', tensor([-22.7498], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.10065286606550217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4885, 5.2573, 5.6075],
        [4.4885, 4.6578, 4.5968],
        [4.4885, 4.4864, 4.4877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.10053259134292603 
model_pd.l_d.mean(): -11.1784029006958 
model_pd.lagr.mean(): -11.07787036895752 
model_pd.lambdas: dict_items([('pout', tensor([1.3101], device='cuda:0')), ('power', tensor([0.5188], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4953], device='cuda:0')), ('power', tensor([-22.7450], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.10053259134292603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4954, 4.6645, 4.6033],
        [4.4954, 4.4933, 4.4947],
        [4.4954, 5.2651, 5.6154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.10041230916976929 
model_pd.l_d.mean(): -11.152315139770508 
model_pd.lagr.mean(): -11.051902770996094 
model_pd.lambdas: dict_items([('pout', tensor([1.3106], device='cuda:0')), ('power', tensor([0.5177], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4933], device='cuda:0')), ('power', tensor([-22.7401], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.10041230916976929
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5024, 5.2730, 5.6234],
        [4.5024, 4.6713, 4.6098],
        [4.5024, 4.5002, 4.5016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.10029196739196777 
model_pd.l_d.mean(): -11.126238822937012 
model_pd.lagr.mean(): -11.025946617126465 
model_pd.lambdas: dict_items([('pout', tensor([1.3111], device='cuda:0')), ('power', tensor([0.5166], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4914], device='cuda:0')), ('power', tensor([-22.7352], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.10029196739196777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5094, 4.5071, 4.5086],
        [4.5094, 5.2810, 5.6314],
        [4.5094, 4.6782, 4.6163]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.10017156600952148 
model_pd.l_d.mean(): -11.100174903869629 
model_pd.lagr.mean(): -11.000003814697266 
model_pd.lambdas: dict_items([('pout', tensor([1.3116], device='cuda:0')), ('power', tensor([0.5154], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4894], device='cuda:0')), ('power', tensor([-22.7303], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.10017156600952148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5164, 4.6850, 4.6229],
        [4.5164, 5.2889, 5.6395],
        [4.5164, 4.5141, 4.5156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.10005110502243042 
model_pd.l_d.mean(): -11.074119567871094 
model_pd.lagr.mean(): -10.974068641662598 
model_pd.lambdas: dict_items([('pout', tensor([1.3121], device='cuda:0')), ('power', tensor([0.5143], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4874], device='cuda:0')), ('power', tensor([-22.7253], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:0.10005110502243042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5235, 4.6920, 4.6295],
        [4.5235, 4.5212, 4.5227],
        [4.5235, 5.2970, 5.6476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.09993058443069458 
model_pd.l_d.mean(): -11.048077583312988 
model_pd.lagr.mean(): -10.94814682006836 
model_pd.lambdas: dict_items([('pout', tensor([1.3126], device='cuda:0')), ('power', tensor([0.5132], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4854], device='cuda:0')), ('power', tensor([-22.7203], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.09993058443069458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5306, 4.5282, 4.5298],
        [4.5306, 5.3051, 5.6558],
        [4.5306, 4.6989, 4.6361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.09981006383895874 
model_pd.l_d.mean(): -11.022047996520996 
model_pd.lagr.mean(): -10.92223834991455 
model_pd.lambdas: dict_items([('pout', tensor([1.3130], device='cuda:0')), ('power', tensor([0.5120], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4835], device='cuda:0')), ('power', tensor([-22.7152], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.09981006383895874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5377, 4.5353, 4.5369],
        [4.5377, 5.3132, 5.6641],
        [4.5377, 4.7059, 4.6428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.09968957304954529 
model_pd.l_d.mean(): -10.9960298538208 
model_pd.lagr.mean(): -10.896340370178223 
model_pd.lambdas: dict_items([('pout', tensor([1.3135], device='cuda:0')), ('power', tensor([0.5109], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4815], device='cuda:0')), ('power', tensor([-22.7101], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.09968957304954529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5449, 4.7130, 4.6496],
        [4.5449, 5.3214, 5.6724],
        [4.5449, 4.5425, 4.5441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.09956908226013184 
model_pd.l_d.mean(): -10.970024108886719 
model_pd.lagr.mean(): -10.870454788208008 
model_pd.lambdas: dict_items([('pout', tensor([1.3140], device='cuda:0')), ('power', tensor([0.5098], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4795], device='cuda:0')), ('power', tensor([-22.7050], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.09956908226013184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5521, 4.5496, 4.5513],
        [4.5521, 4.7201, 4.6564],
        [4.5521, 5.3296, 5.6808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.09944846481084824 
model_pd.l_d.mean(): -10.944028854370117 
model_pd.lagr.mean(): -10.84458065032959 
model_pd.lambdas: dict_items([('pout', tensor([1.3145], device='cuda:0')), ('power', tensor([0.5086], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-22.6999], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.09944846481084824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5593, 4.5568, 4.5585],
        [4.5593, 4.7272, 4.6632],
        [4.5593, 5.3379, 5.6892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.09932781755924225 
model_pd.l_d.mean(): -10.918046951293945 
model_pd.lagr.mean(): -10.818718910217285 
model_pd.lambdas: dict_items([('pout', tensor([1.3149], device='cuda:0')), ('power', tensor([0.5075], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4754], device='cuda:0')), ('power', tensor([-22.6947], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.09932781755924225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5666, 4.7344, 4.6701],
        [4.5666, 5.3463, 5.6977],
        [4.5666, 4.5641, 4.5658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.09920713305473328 
model_pd.l_d.mean(): -10.89207649230957 
model_pd.lagr.mean(): -10.792869567871094 
model_pd.lambdas: dict_items([('pout', tensor([1.3154], device='cuda:0')), ('power', tensor([0.5064], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-22.6894], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.09920713305473328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5739, 4.5713, 4.5731],
        [4.5739, 5.3546, 5.7063],
        [4.5739, 4.7416, 4.6770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.0990864560008049 
model_pd.l_d.mean(): -10.866117477416992 
model_pd.lagr.mean(): -10.767030715942383 
model_pd.lambdas: dict_items([('pout', tensor([1.3159], device='cuda:0')), ('power', tensor([0.5052], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4714], device='cuda:0')), ('power', tensor([-22.6841], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.0990864560008049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5812, 5.3631, 5.7149],
        [4.5812, 4.5787, 4.5804],
        [4.5812, 4.7489, 4.6840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.09896575659513474 
model_pd.l_d.mean(): -10.840169906616211 
model_pd.lagr.mean(): -10.741204261779785 
model_pd.lambdas: dict_items([('pout', tensor([1.3164], device='cuda:0')), ('power', tensor([0.5041], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4693], device='cuda:0')), ('power', tensor([-22.6788], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.09896575659513474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5886, 4.5860, 4.5878],
        [4.5886, 4.7562, 4.6910],
        [4.5886, 5.3716, 5.7236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.09884510934352875 
model_pd.l_d.mean(): -10.814238548278809 
model_pd.lagr.mean(): -10.71539306640625 
model_pd.lambdas: dict_items([('pout', tensor([1.3168], device='cuda:0')), ('power', tensor([0.5030], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4673], device='cuda:0')), ('power', tensor([-22.6735], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.09884510934352875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5960, 4.7636, 4.6980],
        [4.5960, 5.3801, 5.7324],
        [4.5960, 4.5934, 4.5952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.09872439503669739 
model_pd.l_d.mean(): -10.788317680358887 
model_pd.lagr.mean(): -10.689593315124512 
model_pd.lambdas: dict_items([('pout', tensor([1.3173], device='cuda:0')), ('power', tensor([0.5018], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4652], device='cuda:0')), ('power', tensor([-22.6681], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.09872439503669739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6035, 5.3887, 5.7412],
        [4.6035, 4.7710, 4.7051],
        [4.6035, 4.6008, 4.6027]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.09860359877347946 
model_pd.l_d.mean(): -10.762408256530762 
model_pd.lagr.mean(): -10.66380500793457 
model_pd.lambdas: dict_items([('pout', tensor([1.3178], device='cuda:0')), ('power', tensor([0.5007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4631], device='cuda:0')), ('power', tensor([-22.6626], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.09860359877347946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6110, 5.3974, 5.7501],
        [4.6110, 4.6083, 4.6101],
        [4.6110, 4.7784, 4.7123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.09848272800445557 
model_pd.l_d.mean(): -10.73651123046875 
model_pd.lagr.mean(): -10.638028144836426 
model_pd.lambdas: dict_items([('pout', tensor([1.3182], device='cuda:0')), ('power', tensor([0.4996], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4611], device='cuda:0')), ('power', tensor([-22.6572], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.09848272800445557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6185, 4.6158, 4.6177],
        [4.6185, 5.4061, 5.7591],
        [4.6185, 4.7859, 4.7195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.09836190938949585 
model_pd.l_d.mean(): -10.710628509521484 
model_pd.lagr.mean(): -10.612266540527344 
model_pd.lambdas: dict_items([('pout', tensor([1.3187], device='cuda:0')), ('power', tensor([0.4984], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4590], device='cuda:0')), ('power', tensor([-22.6516], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.09836190938949585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6261, 4.6233, 4.6252],
        [4.6261, 4.7935, 4.7268],
        [4.6261, 5.4149, 5.7681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.09824113547801971 
model_pd.l_d.mean(): -10.684757232666016 
model_pd.lagr.mean(): -10.586516380310059 
model_pd.lambdas: dict_items([('pout', tensor([1.3191], device='cuda:0')), ('power', tensor([0.4973], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4569], device='cuda:0')), ('power', tensor([-22.6461], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.09824113547801971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6337, 5.4237, 5.7772],
        [4.6337, 4.6309, 4.6328],
        [4.6337, 4.8010, 4.7341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.09812037646770477 
model_pd.l_d.mean(): -10.65889835357666 
model_pd.lagr.mean(): -10.56077766418457 
model_pd.lambdas: dict_items([('pout', tensor([1.3196], device='cuda:0')), ('power', tensor([0.4962], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4548], device='cuda:0')), ('power', tensor([-22.6405], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.09812037646770477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6413, 4.8087, 4.7414],
        [4.6413, 5.4326, 5.7864],
        [4.6413, 4.6385, 4.6404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.09799953550100327 
model_pd.l_d.mean(): -10.633055686950684 
model_pd.lagr.mean(): -10.535056114196777 
model_pd.lambdas: dict_items([('pout', tensor([1.3200], device='cuda:0')), ('power', tensor([0.4950], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4527], device='cuda:0')), ('power', tensor([-22.6349], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.09799953550100327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6490, 4.8164, 4.7488],
        [4.6490, 5.4415, 5.7957],
        [4.6490, 4.6461, 4.6481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.09787865728139877 
model_pd.l_d.mean(): -10.607223510742188 
model_pd.lagr.mean(): -10.509345054626465 
model_pd.lambdas: dict_items([('pout', tensor([1.3205], device='cuda:0')), ('power', tensor([0.4939], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-22.6292], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.09787865728139877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6567, 5.4505, 5.8050],
        [4.6567, 4.8241, 4.7563],
        [4.6567, 4.6538, 4.6558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.09775775671005249 
model_pd.l_d.mean(): -10.581404685974121 
model_pd.lagr.mean(): -10.483647346496582 
model_pd.lambdas: dict_items([('pout', tensor([1.3209], device='cuda:0')), ('power', tensor([0.4928], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4484], device='cuda:0')), ('power', tensor([-22.6235], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.09775775671005249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6645, 4.6615, 4.6636],
        [4.6645, 4.8319, 4.7638],
        [4.6645, 5.4596, 5.8144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.09763684868812561 
model_pd.l_d.mean(): -10.555599212646484 
model_pd.lagr.mean(): -10.457962036132812 
model_pd.lambdas: dict_items([('pout', tensor([1.3214], device='cuda:0')), ('power', tensor([0.4916], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4463], device='cuda:0')), ('power', tensor([-22.6177], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.09763684868812561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6723, 4.6693, 4.6713],
        [4.6723, 4.8397, 4.7713],
        [4.6723, 5.4687, 5.8238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.0975160226225853 
model_pd.l_d.mean(): -10.529807090759277 
model_pd.lagr.mean(): -10.432291030883789 
model_pd.lambdas: dict_items([('pout', tensor([1.3218], device='cuda:0')), ('power', tensor([0.4905], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4441], device='cuda:0')), ('power', tensor([-22.6119], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.0975160226225853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6801, 4.8476, 4.7789],
        [4.6801, 5.4778, 5.8334],
        [4.6801, 4.6771, 4.6792]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.09739520400762558 
model_pd.l_d.mean(): -10.504029273986816 
model_pd.lagr.mean(): -10.406634330749512 
model_pd.lambdas: dict_items([('pout', tensor([1.3223], device='cuda:0')), ('power', tensor([0.4894], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4420], device='cuda:0')), ('power', tensor([-22.6061], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.09739520400762558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6879, 4.8555, 4.7866],
        [4.6879, 4.6849, 4.6870],
        [4.6879, 5.4870, 5.8430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.09727435559034348 
model_pd.l_d.mean(): -10.478264808654785 
model_pd.lagr.mean(): -10.380990028381348 
model_pd.lambdas: dict_items([('pout', tensor([1.3227], device='cuda:0')), ('power', tensor([0.4882], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4398], device='cuda:0')), ('power', tensor([-22.6002], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.09727435559034348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6958, 4.6928, 4.6949],
        [4.6958, 5.4963, 5.8527],
        [4.6958, 4.8635, 4.7943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.0971534252166748 
model_pd.l_d.mean(): -10.45251178741455 
model_pd.lagr.mean(): -10.355358123779297 
model_pd.lambdas: dict_items([('pout', tensor([1.3232], device='cuda:0')), ('power', tensor([0.4871], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4376], device='cuda:0')), ('power', tensor([-22.5943], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.0971534252166748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7038, 4.7007, 4.7029],
        [4.7038, 4.8715, 4.8020],
        [4.7038, 5.5057, 5.8624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.09703250974416733 
model_pd.l_d.mean(): -10.426774024963379 
model_pd.lagr.mean(): -10.329741477966309 
model_pd.lambdas: dict_items([('pout', tensor([1.3236], device='cuda:0')), ('power', tensor([0.4860], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4354], device='cuda:0')), ('power', tensor([-22.5883], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.09703250974416733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7118, 4.8795, 4.8098],
        [4.7118, 4.7086, 4.7108],
        [4.7118, 5.5151, 5.8722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.09691166132688522 
model_pd.l_d.mean(): -10.40105152130127 
model_pd.lagr.mean(): -10.304140090942383 
model_pd.lambdas: dict_items([('pout', tensor([1.3240], device='cuda:0')), ('power', tensor([0.4848], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4333], device='cuda:0')), ('power', tensor([-22.5823], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:0.09691166132688522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7198, 4.8877, 4.8177],
        [4.7198, 4.7166, 4.7189],
        [4.7198, 5.5245, 5.8821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.09679082036018372 
model_pd.l_d.mean(): -10.37533950805664 
model_pd.lagr.mean(): -10.278548240661621 
model_pd.lambdas: dict_items([('pout', tensor([1.3245], device='cuda:0')), ('power', tensor([0.4837], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4311], device='cuda:0')), ('power', tensor([-22.5762], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.09679082036018372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7278, 5.5340, 5.8921],
        [4.7278, 4.7247, 4.7269],
        [4.7278, 4.8958, 4.8256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.09667003154754639 
model_pd.l_d.mean(): -10.349645614624023 
model_pd.lagr.mean(): -10.252975463867188 
model_pd.lambdas: dict_items([('pout', tensor([1.3249], device='cuda:0')), ('power', tensor([0.4826], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4289], device='cuda:0')), ('power', tensor([-22.5701], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.09667003154754639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7360, 4.7327, 4.7350],
        [4.7360, 4.9040, 4.8336],
        [4.7360, 5.5436, 5.9021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.0965491384267807 
model_pd.l_d.mean(): -10.32396411895752 
model_pd.lagr.mean(): -10.227415084838867 
model_pd.lambdas: dict_items([('pout', tensor([1.3253], device='cuda:0')), ('power', tensor([0.4815], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4266], device='cuda:0')), ('power', tensor([-22.5640], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.0965491384267807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7441, 4.7409, 4.7431],
        [4.7441, 5.5532, 5.9123],
        [4.7441, 4.9123, 4.8416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.0964282974600792 
model_pd.l_d.mean(): -10.298297882080078 
model_pd.lagr.mean(): -10.20186996459961 
model_pd.lambdas: dict_items([('pout', tensor([1.3257], device='cuda:0')), ('power', tensor([0.4803], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4244], device='cuda:0')), ('power', tensor([-22.5578], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.0964282974600792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7523, 4.9206, 4.8496],
        [4.7523, 5.5629, 5.9225],
        [4.7523, 4.7490, 4.7513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.09630744159221649 
model_pd.l_d.mean(): -10.272643089294434 
model_pd.lagr.mean(): -10.176335334777832 
model_pd.lambdas: dict_items([('pout', tensor([1.3262], device='cuda:0')), ('power', tensor([0.4792], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4222], device='cuda:0')), ('power', tensor([-22.5516], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.09630744159221649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7605, 4.9290, 4.8578],
        [4.7605, 4.7572, 4.7595],
        [4.7605, 5.5727, 5.9327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.09618668258190155 
model_pd.l_d.mean(): -10.247005462646484 
model_pd.lagr.mean(): -10.150818824768066 
model_pd.lambdas: dict_items([('pout', tensor([1.3266], device='cuda:0')), ('power', tensor([0.4781], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4199], device='cuda:0')), ('power', tensor([-22.5453], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.09618668258190155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7687, 4.7654, 4.7678],
        [4.7687, 5.5825, 5.9431],
        [4.7687, 4.9374, 4.8659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.09606589376926422 
model_pd.l_d.mean(): -10.221381187438965 
model_pd.lagr.mean(): -10.12531566619873 
model_pd.lambdas: dict_items([('pout', tensor([1.3270], device='cuda:0')), ('power', tensor([0.4770], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4177], device='cuda:0')), ('power', tensor([-22.5390], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.09606589376926422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7770, 4.7737, 4.7761],
        [4.7770, 5.5924, 5.9535],
        [4.7770, 4.9459, 4.8742]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.09594511240720749 
model_pd.l_d.mean(): -10.195772171020508 
model_pd.lagr.mean(): -10.09982681274414 
model_pd.lambdas: dict_items([('pout', tensor([1.3274], device='cuda:0')), ('power', tensor([0.4758], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4154], device='cuda:0')), ('power', tensor([-22.5326], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.09594511240720749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7854, 5.6023, 5.9641],
        [4.7854, 4.7820, 4.7844],
        [4.7854, 4.9544, 4.8825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.09582433104515076 
model_pd.l_d.mean(): -10.170178413391113 
model_pd.lagr.mean(): -10.07435417175293 
model_pd.lambdas: dict_items([('pout', tensor([1.3278], device='cuda:0')), ('power', tensor([0.4747], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4131], device='cuda:0')), ('power', tensor([-22.5262], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.09582433104515076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7938, 4.7904, 4.7928],
        [4.7938, 4.9630, 4.8908],
        [4.7938, 5.6123, 5.9746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.09570356458425522 
model_pd.l_d.mean(): -10.144598960876465 
model_pd.lagr.mean(): -10.048895835876465 
model_pd.lambdas: dict_items([('pout', tensor([1.3282], device='cuda:0')), ('power', tensor([0.4736], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4109], device='cuda:0')), ('power', tensor([-22.5197], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.09570356458425522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8022, 4.7988, 4.8012],
        [4.8022, 5.6224, 5.9853],
        [4.8022, 4.9716, 4.8992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.09558287262916565 
model_pd.l_d.mean(): -10.119033813476562 
model_pd.lagr.mean(): -10.02345085144043 
model_pd.lambdas: dict_items([('pout', tensor([1.3286], device='cuda:0')), ('power', tensor([0.4724], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4086], device='cuda:0')), ('power', tensor([-22.5133], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.09558287262916565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8107, 4.8073, 4.8097],
        [4.8107, 5.6325, 5.9961],
        [4.8107, 4.9803, 4.9077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.09546217322349548 
model_pd.l_d.mean(): -10.093483924865723 
model_pd.lagr.mean(): -9.998022079467773 
model_pd.lambdas: dict_items([('pout', tensor([1.3290], device='cuda:0')), ('power', tensor([0.4713], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4063], device='cuda:0')), ('power', tensor([-22.5067], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.09546217322349548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8192, 4.9891, 4.9162],
        [4.8192, 5.6427, 6.0069],
        [4.8192, 4.8157, 4.8182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.0953415259718895 
model_pd.l_d.mean(): -10.067951202392578 
model_pd.lagr.mean(): -9.972609519958496 
model_pd.lambdas: dict_items([('pout', tensor([1.3294], device='cuda:0')), ('power', tensor([0.4702], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4040], device='cuda:0')), ('power', tensor([-22.5001], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.0953415259718895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8277, 4.9978, 4.9247],
        [4.8277, 4.8243, 4.8267],
        [4.8277, 5.6530, 6.0178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.0952208861708641 
model_pd.l_d.mean(): -10.042431831359863 
model_pd.lagr.mean(): -9.947211265563965 
model_pd.lambdas: dict_items([('pout', tensor([1.3299], device='cuda:0')), ('power', tensor([0.4691], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4017], device='cuda:0')), ('power', tensor([-22.4935], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.0952208861708641
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8363, 4.8328, 4.8353],
        [4.8363, 5.6633, 6.0288],
        [4.8363, 5.0067, 4.9334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.09510021656751633 
model_pd.l_d.mean(): -10.016926765441895 
model_pd.lagr.mean(): -9.921826362609863 
model_pd.lambdas: dict_items([('pout', tensor([1.3303], device='cuda:0')), ('power', tensor([0.4680], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3993], device='cuda:0')), ('power', tensor([-22.4868], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.09510021656751633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8449, 5.6737, 6.0399],
        [4.8449, 4.8415, 4.8439],
        [4.8449, 5.0156, 4.9421]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.09497955441474915 
model_pd.l_d.mean(): -9.991438865661621 
model_pd.lagr.mean(): -9.896459579467773 
model_pd.lambdas: dict_items([('pout', tensor([1.3306], device='cuda:0')), ('power', tensor([0.4668], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3970], device='cuda:0')), ('power', tensor([-22.4801], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.09497955441474915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8536, 4.8501, 4.8526],
        [4.8536, 5.0246, 4.9508],
        [4.8536, 5.6842, 6.0511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.09485884755849838 
model_pd.l_d.mean(): -9.965967178344727 
model_pd.lagr.mean(): -9.871108055114746 
model_pd.lambdas: dict_items([('pout', tensor([1.3310], device='cuda:0')), ('power', tensor([0.4657], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3947], device='cuda:0')), ('power', tensor([-22.4733], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.09485884755849838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8624, 5.6947, 6.0624],
        [4.8624, 4.8589, 4.8614],
        [4.8624, 5.0336, 4.9596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.09473803639411926 
model_pd.l_d.mean(): -9.940509796142578 
model_pd.lagr.mean(): -9.845771789550781 
model_pd.lambdas: dict_items([('pout', tensor([1.3314], device='cuda:0')), ('power', tensor([0.4646], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3923], device='cuda:0')), ('power', tensor([-22.4664], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.09473803639411926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8712, 5.0427, 4.9685],
        [4.8712, 5.7053, 6.0737],
        [4.8712, 4.8676, 4.8701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.09461717307567596 
model_pd.l_d.mean(): -9.915069580078125 
model_pd.lagr.mean(): -9.820452690124512 
model_pd.lambdas: dict_items([('pout', tensor([1.3318], device='cuda:0')), ('power', tensor([0.4635], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3899], device='cuda:0')), ('power', tensor([-22.4595], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.09461717307567596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8800, 5.7160, 6.0852],
        [4.8800, 4.8764, 4.8790],
        [4.8800, 5.0519, 4.9775]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.09449639916419983 
model_pd.l_d.mean(): -9.889644622802734 
model_pd.lagr.mean(): -9.795147895812988 
model_pd.lambdas: dict_items([('pout', tensor([1.3322], device='cuda:0')), ('power', tensor([0.4623], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3876], device='cuda:0')), ('power', tensor([-22.4526], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.09449639916419983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8888, 4.8853, 4.8878],
        [4.8888, 5.0611, 4.9865],
        [4.8888, 5.7268, 6.0967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.09437575936317444 
model_pd.l_d.mean(): -9.864235877990723 
model_pd.lagr.mean(): -9.76986026763916 
model_pd.lambdas: dict_items([('pout', tensor([1.3326], device='cuda:0')), ('power', tensor([0.4612], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3852], device='cuda:0')), ('power', tensor([-22.4456], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.09437575936317444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8977, 5.0703, 4.9955],
        [4.8977, 4.8942, 4.8967],
        [4.8977, 5.7376, 6.1083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.09425529837608337 
model_pd.l_d.mean(): -9.83884334564209 
model_pd.lagr.mean(): -9.744587898254395 
model_pd.lambdas: dict_items([('pout', tensor([1.3330], device='cuda:0')), ('power', tensor([0.4601], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3828], device='cuda:0')), ('power', tensor([-22.4386], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.09425529837608337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9067, 4.9031, 4.9057],
        [4.9067, 5.0796, 5.0046],
        [4.9067, 5.7484, 6.1200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.0941348522901535 
model_pd.l_d.mean(): -9.813467025756836 
model_pd.lagr.mean(): -9.719331741333008 
model_pd.lambdas: dict_items([('pout', tensor([1.3334], device='cuda:0')), ('power', tensor([0.4590], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3804], device='cuda:0')), ('power', tensor([-22.4315], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.0941348522901535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9157, 4.9121, 4.9147],
        [4.9157, 5.7594, 6.1318],
        [4.9157, 5.0890, 5.0138]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.09401436150074005 
model_pd.l_d.mean(): -9.788108825683594 
model_pd.lagr.mean(): -9.69409465789795 
model_pd.lambdas: dict_items([('pout', tensor([1.3337], device='cuda:0')), ('power', tensor([0.4578], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3780], device='cuda:0')), ('power', tensor([-22.4244], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.09401436150074005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9247, 5.0984, 5.0231],
        [4.9247, 5.7704, 6.1437],
        [4.9247, 4.9211, 4.9237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.09389384090900421 
model_pd.l_d.mean(): -9.762763977050781 
model_pd.lagr.mean(): -9.668869972229004 
model_pd.lambdas: dict_items([('pout', tensor([1.3341], device='cuda:0')), ('power', tensor([0.4567], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3755], device='cuda:0')), ('power', tensor([-22.4172], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.09389384090900421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9338, 5.7815, 6.1557],
        [4.9338, 4.9302, 4.9328],
        [4.9338, 5.1079, 5.0324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.09377336502075195 
model_pd.l_d.mean(): -9.73743724822998 
model_pd.lagr.mean(): -9.64366340637207 
model_pd.lambdas: dict_items([('pout', tensor([1.3345], device='cuda:0')), ('power', tensor([0.4556], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3731], device='cuda:0')), ('power', tensor([-22.4100], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.09377336502075195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9429, 5.1175, 5.0417],
        [4.9429, 5.7926, 6.1677],
        [4.9429, 4.9393, 4.9419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.09365305304527283 
model_pd.l_d.mean(): -9.712127685546875 
model_pd.lagr.mean(): -9.618474960327148 
model_pd.lambdas: dict_items([('pout', tensor([1.3349], device='cuda:0')), ('power', tensor([0.4545], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3707], device='cuda:0')), ('power', tensor([-22.4027], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.09365305304527283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9521, 5.1271, 5.0512],
        [4.9521, 5.8038, 6.1799],
        [4.9521, 4.9485, 4.9511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.09353286027908325 
model_pd.l_d.mean(): -9.686835289001465 
model_pd.lagr.mean(): -9.593302726745605 
model_pd.lambdas: dict_items([('pout', tensor([1.3352], device='cuda:0')), ('power', tensor([0.4534], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3682], device='cuda:0')), ('power', tensor([-22.3954], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.09353286027908325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9613, 5.8151, 6.1921],
        [4.9613, 5.1367, 5.0606],
        [4.9613, 4.9577, 4.9603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.09341275691986084 
model_pd.l_d.mean(): -9.661559104919434 
model_pd.lagr.mean(): -9.568146705627441 
model_pd.lambdas: dict_items([('pout', tensor([1.3356], device='cuda:0')), ('power', tensor([0.4522], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3658], device='cuda:0')), ('power', tensor([-22.3880], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.09341275691986084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9706, 5.8265, 6.2044],
        [4.9706, 5.1464, 5.0702],
        [4.9706, 4.9669, 4.9695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.09329260885715485 
model_pd.l_d.mean(): -9.636301040649414 
model_pd.lagr.mean(): -9.543008804321289 
model_pd.lambdas: dict_items([('pout', tensor([1.3360], device='cuda:0')), ('power', tensor([0.4511], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3633], device='cuda:0')), ('power', tensor([-22.3806], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.09329260885715485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9799, 5.8379, 6.2168],
        [4.9799, 5.1562, 5.0798],
        [4.9799, 4.9763, 4.9788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.09317239373922348 
model_pd.l_d.mean(): -9.611059188842773 
model_pd.lagr.mean(): -9.517887115478516 
model_pd.lambdas: dict_items([('pout', tensor([1.3363], device='cuda:0')), ('power', tensor([0.4500], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3608], device='cuda:0')), ('power', tensor([-22.3731], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.09317239373922348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9893, 4.9856, 4.9882],
        [4.9893, 5.8494, 6.2293],
        [4.9893, 5.1661, 5.0895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.09305226802825928 
model_pd.l_d.mean(): -9.585834503173828 
model_pd.lagr.mean(): -9.492782592773438 
model_pd.lambdas: dict_items([('pout', tensor([1.3367], device='cuda:0')), ('power', tensor([0.4489], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3583], device='cuda:0')), ('power', tensor([-22.3656], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.09305226802825928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9987, 5.1760, 5.0992],
        [4.9987, 5.8610, 6.2419],
        [4.9987, 4.9950, 4.9976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.09293226897716522 
model_pd.l_d.mean(): -9.560629844665527 
model_pd.lagr.mean(): -9.467697143554688 
model_pd.lambdas: dict_items([('pout', tensor([1.3370], device='cuda:0')), ('power', tensor([0.4478], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3558], device='cuda:0')), ('power', tensor([-22.3580], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.09293226897716522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0081, 5.0044, 5.0070],
        [5.0081, 5.8727, 6.2546],
        [5.0081, 5.1859, 5.1090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.0928124189376831 
model_pd.l_d.mean(): -9.535440444946289 
model_pd.lagr.mean(): -9.442627906799316 
model_pd.lambdas: dict_items([('pout', tensor([1.3374], device='cuda:0')), ('power', tensor([0.4467], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3533], device='cuda:0')), ('power', tensor([-22.3503], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.0928124189376831
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0176, 5.1959, 5.1189],
        [5.0176, 5.8844, 6.2674],
        [5.0176, 5.0139, 5.0165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.09269269555807114 
model_pd.l_d.mean(): -9.510270118713379 
model_pd.lagr.mean(): -9.417577743530273 
model_pd.lambdas: dict_items([('pout', tensor([1.3377], device='cuda:0')), ('power', tensor([0.4455], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3508], device='cuda:0')), ('power', tensor([-22.3427], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.09269269555807114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0271, 5.2060, 5.1288],
        [5.0271, 5.8962, 6.2803],
        [5.0271, 5.0235, 5.0261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.09257295727729797 
model_pd.l_d.mean(): -9.485116958618164 
model_pd.lagr.mean(): -9.39254379272461 
model_pd.lambdas: dict_items([('pout', tensor([1.3381], device='cuda:0')), ('power', tensor([0.4444], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3483], device='cuda:0')), ('power', tensor([-22.3349], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.09257295727729797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0367, 5.0330, 5.0356],
        [5.0367, 5.2162, 5.1388],
        [5.0367, 5.9080, 6.2932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.09245322644710541 
model_pd.l_d.mean(): -9.459980964660645 
model_pd.lagr.mean(): -9.367527961730957 
model_pd.lambdas: dict_items([('pout', tensor([1.3384], device='cuda:0')), ('power', tensor([0.4433], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3458], device='cuda:0')), ('power', tensor([-22.3272], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.09245322644710541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0463, 5.2264, 5.1489],
        [5.0463, 5.0427, 5.0453],
        [5.0463, 5.9200, 6.3063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.09233354032039642 
model_pd.l_d.mean(): -9.43486499786377 
model_pd.lagr.mean(): -9.342531204223633 
model_pd.lambdas: dict_items([('pout', tensor([1.3388], device='cuda:0')), ('power', tensor([0.4422], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3432], device='cuda:0')), ('power', tensor([-22.3193], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.09233354032039642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0560, 5.2367, 5.1591],
        [5.0560, 5.0524, 5.0550],
        [5.0560, 5.9320, 6.3195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.09221397340297699 
model_pd.l_d.mean(): -9.409764289855957 
model_pd.lagr.mean(): -9.317550659179688 
model_pd.lambdas: dict_items([('pout', tensor([1.3391], device='cuda:0')), ('power', tensor([0.4411], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3407], device='cuda:0')), ('power', tensor([-22.3114], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.09221397340297699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0658, 5.9441, 6.3327],
        [5.0658, 5.0621, 5.0647],
        [5.0658, 5.2470, 5.1693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.0920945256948471 
model_pd.l_d.mean(): -9.384683609008789 
model_pd.lagr.mean(): -9.29258918762207 
model_pd.lambdas: dict_items([('pout', tensor([1.3394], device='cuda:0')), ('power', tensor([0.4400], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3381], device='cuda:0')), ('power', tensor([-22.3035], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.0920945256948471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0755, 5.2574, 5.1795],
        [5.0755, 5.0719, 5.0745],
        [5.0755, 5.9563, 6.3461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.09197511523962021 
model_pd.l_d.mean(): -9.35962200164795 
model_pd.lagr.mean(): -9.267646789550781 
model_pd.lambdas: dict_items([('pout', tensor([1.3398], device='cuda:0')), ('power', tensor([0.4388], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3355], device='cuda:0')), ('power', tensor([-22.2955], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.09197511523962021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0854, 5.9685, 6.3595],
        [5.0854, 5.2679, 5.1899],
        [5.0854, 5.0817, 5.0843]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.09185568988323212 
model_pd.l_d.mean(): -9.334577560424805 
model_pd.lagr.mean(): -9.242721557617188 
model_pd.lambdas: dict_items([('pout', tensor([1.3401], device='cuda:0')), ('power', tensor([0.4377], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3330], device='cuda:0')), ('power', tensor([-22.2874], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.09185568988323212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0952, 5.2784, 5.2003],
        [5.0952, 5.9808, 6.3731],
        [5.0952, 5.0916, 5.0942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.0917363166809082 
model_pd.l_d.mean(): -9.309553146362305 
model_pd.lagr.mean(): -9.217817306518555 
model_pd.lambdas: dict_items([('pout', tensor([1.3404], device='cuda:0')), ('power', tensor([0.4366], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3304], device='cuda:0')), ('power', tensor([-22.2793], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.0917363166809082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1052, 5.2890, 5.2108],
        [5.1052, 5.1015, 5.1041],
        [5.1052, 5.9932, 6.3868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.09161706268787384 
model_pd.l_d.mean(): -9.284546852111816 
model_pd.lagr.mean(): -9.192930221557617 
model_pd.lambdas: dict_items([('pout', tensor([1.3408], device='cuda:0')), ('power', tensor([0.4355], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3278], device='cuda:0')), ('power', tensor([-22.2711], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.09161706268787384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
