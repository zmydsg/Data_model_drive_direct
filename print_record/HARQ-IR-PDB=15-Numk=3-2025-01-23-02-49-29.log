
bounds:tensor([-2.])	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.]), 'power': tensor([1.])},
vars:{'pout': tensor([0.]), 'power': tensor([0.])}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[2.3753, 2.7571, 2.9162],
        [2.3753, 2.6743, 2.7534],
        [2.3753, 2.3910, 2.3785],
        [2.3753, 2.5079, 2.4824]], grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.179398313164711 
model_pd.l_d.mean(): -23.422386169433594 
model_pd.lagr.mean(): -23.24298858642578 
model_pd.lambdas: dict_items([('pout', tensor([1.0011])), ('power', tensor([0.9988]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0813])), ('power', tensor([-24.5037]))])
epoch：0	 i:0 	 global-step:0	 l-p:0.179398313164711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[2.4887, 2.4890, 2.4887],
        [2.4887, 2.4974, 2.4899],
        [2.4887, 3.1458, 3.6241],
        [2.4887, 3.1025, 3.5211]], grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.15463048219680786 
model_pd.l_d.mean(): -23.4073429107666 
model_pd.lagr.mean(): -23.25271224975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0021])), ('power', tensor([0.9976]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0199])), ('power', tensor([-24.4583]))])
epoch：1	 i:0 	 global-step:20	 l-p:0.15463048219680786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[2.6033, 3.1886, 3.5452],
        [2.6033, 3.3008, 3.8070],
        [2.6033, 2.6581, 2.6267],
        [2.6033, 2.6827, 2.6465]], grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.1423894166946411 
model_pd.l_d.mean(): -23.38405418395996 
model_pd.lagr.mean(): -23.24166488647461 
model_pd.lambdas: dict_items([('pout', tensor([1.0031])), ('power', tensor([0.9963]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9607])), ('power', tensor([-24.4065]))])
epoch：2	 i:0 	 global-step:40	 l-p:0.1423894166946411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[2.7121, 2.7121, 2.7121],
        [2.7121, 2.8384, 2.8014],
        [2.7121, 3.1609, 3.3429],
        [2.7121, 3.1271, 3.2753]], grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.1339978277683258 
model_pd.l_d.mean(): -23.353519439697266 
model_pd.lagr.mean(): -23.21952247619629 
model_pd.lambdas: dict_items([('pout', tensor([1.0040])), ('power', tensor([0.9951]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9068])), ('power', tensor([-24.3524]))])
epoch：3	 i:0 	 global-step:60	 l-p:0.1339978277683258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[2.8091, 3.0556, 3.0609],
        [2.8091, 2.8094, 2.8091],
        [2.8091, 2.8260, 2.8122],
        [2.8091, 2.8091, 2.8091]], grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.12782026827335358 
model_pd.l_d.mean(): -23.31800079345703 
model_pd.lagr.mean(): -23.1901798248291 
model_pd.lambdas: dict_items([('pout', tensor([1.0048])), ('power', tensor([0.9939]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8606])), ('power', tensor([-24.3008]))])
epoch：4	 i:0 	 global-step:80	 l-p:0.12782026827335358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[2.8880, 2.9213, 2.8975],
        [2.8880, 2.8918, 2.8883],
        [2.8880, 2.9208, 2.8973],
        [2.8880, 2.8880, 2.8880]], grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.12332990020513535 
model_pd.l_d.mean(): -23.280363082885742 
model_pd.lagr.mean(): -23.157033920288086 
model_pd.lambdas: dict_items([('pout', tensor([1.0057])), ('power', tensor([0.9927]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8242])), ('power', tensor([-24.2565]))])
epoch：5	 i:0 	 global-step:100	 l-p:0.12332990020513535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[2.9451, 2.9791, 2.9548],
        [2.9451, 2.9451, 2.9451],
        [2.9451, 2.9451, 2.9451],
        [2.9451, 3.1160, 3.0817]], grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.1202881932258606 
model_pd.l_d.mean(): -23.243297576904297 
model_pd.lagr.mean(): -23.123008728027344 
model_pd.lambdas: dict_items([('pout', tensor([1.0065])), ('power', tensor([0.9915]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7985])), ('power', tensor([-24.2235]))])
epoch：6	 i:0 	 global-step:120	 l-p:0.1202881932258606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[2.9794, 3.3485, 3.4273],
        [2.9794, 3.2599, 3.2748],
        [2.9794, 2.9823, 2.9796],
        [2.9794, 3.2264, 3.2221]], grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.11851724982261658 
model_pd.l_d.mean(): -23.208580017089844 
model_pd.lagr.mean(): -23.090063095092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0072])), ('power', tensor([0.9903]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7834])), ('power', tensor([-24.2033]))])
epoch：7	 i:0 	 global-step:140	 l-p:0.11851724982261658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[2.9929, 3.7701, 4.2930],
        [2.9929, 3.2409, 3.2365],
        [2.9929, 3.1158, 3.0722],
        [2.9929, 3.8261, 4.4248]], grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.1178351566195488 
model_pd.l_d.mean(): -23.176815032958984 
model_pd.lagr.mean(): -23.058979034423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0080])), ('power', tensor([0.9891]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7775])), ('power', tensor([-24.1955]))])
epoch：8	 i:0 	 global-step:160	 l-p:0.1178351566195488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[2.9889, 3.1114, 3.0679],
        [2.9889, 3.0231, 2.9986],
        [2.9889, 3.0228, 2.9984],
        [2.9889, 2.9892, 2.9889]], grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.11805221438407898 
model_pd.l_d.mean(): -23.147768020629883 
model_pd.lagr.mean(): -23.02971649169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0088])), ('power', tensor([0.9878]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7793])), ('power', tensor([-24.1982]))])
epoch：9	 i:0 	 global-step:180	 l-p:0.11805221438407898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[2.9709, 3.4325, 3.5951],
        [2.9709, 3.2670, 3.2930],
        [2.9709, 3.0459, 3.0065],
        [2.9709, 2.9713, 2.9709]], grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.11898504942655563 
model_pd.l_d.mean(): -23.12075424194336 
model_pd.lagr.mean(): -23.00177001953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0096])), ('power', tensor([0.9866]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7873])), ('power', tensor([-24.2093]))])
epoch：10	 i:0 	 global-step:200	 l-p:0.11898504942655563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[2.9425, 3.0331, 2.9915],
        [2.9425, 3.2010, 3.2058],
        [2.9425, 2.9601, 2.9458],
        [2.9425, 2.9475, 2.9429]], grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.12046939134597778 
model_pd.l_d.mean(): -23.094938278198242 
model_pd.lagr.mean(): -22.974468231201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0104])), ('power', tensor([0.9854]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8000])), ('power', tensor([-24.2264]))])
epoch：11	 i:0 	 global-step:220	 l-p:0.12046939134597778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[2.9070, 3.5486, 3.9180],
        [2.9070, 2.9119, 2.9075],
        [2.9070, 3.1614, 3.1662],
        [2.9070, 2.9073, 2.9070]], grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12236243486404419 
model_pd.l_d.mean(): -23.069520950317383 
model_pd.lagr.mean(): -22.947158813476562 
model_pd.lambdas: dict_items([('pout', tensor([1.0112])), ('power', tensor([0.9842]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8160])), ('power', tensor([-24.2475]))])
epoch：12	 i:0 	 global-step:240	 l-p:0.12236243486404419
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[2.8676, 2.8676, 2.8676],
        [2.8676, 3.0796, 3.0646],
        [2.8676, 3.0006, 2.9611],
        [2.8676, 3.1332, 3.1474]], grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.12453456968069077 
model_pd.l_d.mean(): -23.04383087158203 
model_pd.lagr.mean(): -22.919296264648438 
model_pd.lambdas: dict_items([('pout', tensor([1.0120])), ('power', tensor([0.9830]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8341])), ('power', tensor([-24.2705]))])
epoch：13	 i:0 	 global-step:260	 l-p:0.12453456968069077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[2.8270, 2.8585, 2.8359],
        [2.8270, 3.2945, 3.4824],
        [2.8270, 2.9131, 2.8736],
        [2.8270, 2.8590, 2.8361]], grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.1268552988767624 
model_pd.l_d.mean(): -23.01739501953125 
model_pd.lagr.mean(): -22.890539169311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0129])), ('power', tensor([0.9818]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8529])), ('power', tensor([-24.2936]))])
epoch：14	 i:0 	 global-step:280	 l-p:0.1268552988767624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[2.7884, 2.7884, 2.7884],
        [2.7884, 3.2503, 3.4380],
        [2.7884, 2.9926, 2.9783],
        [2.7884, 2.8196, 2.7972]], grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.12917961180210114 
model_pd.l_d.mean(): -22.989967346191406 
model_pd.lagr.mean(): -22.860788345336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0138])), ('power', tensor([0.9806]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8711])), ('power', tensor([-24.3152]))])
epoch：15	 i:0 	 global-step:300	 l-p:0.12917961180210114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[2.7542, 2.8802, 2.8429],
        [2.7542, 2.7577, 2.7544],
        [2.7542, 2.7543, 2.7542],
        [2.7542, 2.7542, 2.7542]], grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.1313415765762329 
model_pd.l_d.mean(): -22.961532592773438 
model_pd.lagr.mean(): -22.830190658569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0146])), ('power', tensor([0.9794]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8874])), ('power', tensor([-24.3340]))])
epoch：16	 i:0 	 global-step:320	 l-p:0.1313415765762329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[2.7268, 2.7293, 2.7269],
        [2.7268, 3.4101, 3.8712],
        [2.7268, 2.7268, 2.7268],
        [2.7268, 2.7938, 2.7586]], grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.1331622451543808 
model_pd.l_d.mean(): -22.932260513305664 
model_pd.lagr.mean(): -22.79909896850586 
model_pd.lambdas: dict_items([('pout', tensor([1.0155])), ('power', tensor([0.9781]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9006])), ('power', tensor([-24.3489]))])
epoch：17	 i:0 	 global-step:340	 l-p:0.1331622451543808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[2.7079, 2.9534, 2.9667],
        [2.7079, 3.0310, 3.1002],
        [2.7079, 3.0536, 3.1417],
        [2.7079, 2.7889, 2.7517]], grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.13447558879852295 
model_pd.l_d.mean(): -22.902442932128906 
model_pd.lagr.mean(): -22.767967224121094 
model_pd.lambdas: dict_items([('pout', tensor([1.0165])), ('power', tensor([0.9769]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9099])), ('power', tensor([-24.3591]))])
epoch：18	 i:0 	 global-step:360	 l-p:0.13447558879852295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[2.6983, 2.7933, 2.7554],
        [2.6983, 3.1387, 3.3177],
        [2.6983, 2.8048, 2.7671],
        [2.6983, 2.9278, 2.9323]], grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.1351660192012787 
model_pd.l_d.mean(): -22.872350692749023 
model_pd.lagr.mean(): -22.737184524536133 
model_pd.lambdas: dict_items([('pout', tensor([1.0174])), ('power', tensor([0.9757]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9146])), ('power', tensor([-24.3645]))])
epoch：19	 i:0 	 global-step:380	 l-p:0.1351660192012787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[2.6981, 2.7737, 2.7375],
        [2.6981, 3.1382, 3.3172],
        [2.6981, 3.1348, 3.3103],
        [2.6981, 2.6981, 2.6981]], grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.13519889116287231 
model_pd.l_d.mean(): -22.8421573638916 
model_pd.lagr.mean(): -22.706958770751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0183])), ('power', tensor([0.9745]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9148])), ('power', tensor([-24.3650]))])
epoch：20	 i:0 	 global-step:400	 l-p:0.13519889116287231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[2.7065, 2.7065, 2.7065],
        [2.7065, 2.7869, 2.7499],
        [2.7065, 2.7359, 2.7148],
        [2.7065, 2.9429, 2.9514]], grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.13462719321250916 
model_pd.l_d.mean(): -22.811906814575195 
model_pd.lagr.mean(): -22.67728042602539 
model_pd.lambdas: dict_items([('pout', tensor([1.0192])), ('power', tensor([0.9733]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9108])), ('power', tensor([-24.3611]))])
epoch：21	 i:0 	 global-step:420	 l-p:0.13462719321250916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[2.7220, 2.7776, 2.7456],
        [2.7220, 3.1655, 3.3455],
        [2.7220, 2.7220, 2.7220],
        [2.7220, 2.7245, 2.7222]], grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.13357089459896088 
model_pd.l_d.mean(): -22.781545639038086 
model_pd.lagr.mean(): -22.647974014282227 
model_pd.lambdas: dict_items([('pout', tensor([1.0201])), ('power', tensor([0.9720]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9034])), ('power', tensor([-24.3535]))])
epoch：22	 i:0 	 global-step:440	 l-p:0.13357089459896088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[2.7430, 2.7432, 2.7430],
        [2.7430, 3.3301, 3.6677],
        [2.7430, 2.9898, 3.0027],
        [2.7430, 2.7430, 2.7430]], grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.13218332827091217 
model_pd.l_d.mean(): -22.75096893310547 
model_pd.lagr.mean(): -22.618785858154297 
model_pd.lambdas: dict_items([('pout', tensor([1.0210])), ('power', tensor([0.9708]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8934])), ('power', tensor([-24.3429]))])
epoch：23	 i:0 	 global-step:460	 l-p:0.13218332827091217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[2.7674, 2.7674, 2.7674],
        [2.7674, 2.7674, 2.7674],
        [2.7674, 3.0323, 3.0552],
        [2.7674, 2.8918, 2.8547]], grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.13062117993831635 
model_pd.l_d.mean(): -22.720102310180664 
model_pd.lagr.mean(): -22.589481353759766 
model_pd.lambdas: dict_items([('pout', tensor([1.0219])), ('power', tensor([0.9696]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8818])), ('power', tensor([-24.3303]))])
epoch：24	 i:0 	 global-step:480	 l-p:0.13062117993831635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[2.7933, 3.4912, 3.9596],
        [2.7933, 3.2494, 3.4335],
        [2.7933, 2.8759, 2.8378],
        [2.7933, 3.0146, 3.0101]], grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.12902598083019257 
model_pd.l_d.mean(): -22.688941955566406 
model_pd.lagr.mean(): -22.55991554260254 
model_pd.lambdas: dict_items([('pout', tensor([1.0227])), ('power', tensor([0.9684]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8697])), ('power', tensor([-24.3167]))])
epoch：25	 i:0 	 global-step:500	 l-p:0.12902598083019257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[2.8187, 2.9289, 2.8896],
        [2.8187, 2.8196, 2.8187],
        [2.8187, 2.8187, 2.8187],
        [2.8187, 3.2406, 3.3882]], grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.12751545011997223 
model_pd.l_d.mean(): -22.657567977905273 
model_pd.lagr.mean(): -22.530052185058594 
model_pd.lambdas: dict_items([('pout', tensor([1.0236])), ('power', tensor([0.9672]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8579])), ('power', tensor([-24.3031]))])
epoch：26	 i:0 	 global-step:520	 l-p:0.12751545011997223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[2.8418, 2.8418, 2.8418],
        [2.8418, 2.9924, 2.9586],
        [2.8418, 3.0668, 3.0620],
        [2.8418, 2.8420, 2.8418]], grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.12618160247802734 
model_pd.l_d.mean(): -22.626123428344727 
model_pd.lagr.mean(): -22.499942779541016 
model_pd.lambdas: dict_items([('pout', tensor([1.0244])), ('power', tensor([0.9660]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8472])), ('power', tensor([-24.2906]))])
epoch：27	 i:0 	 global-step:540	 l-p:0.12618160247802734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[2.8612, 2.8708, 2.8625],
        [2.8612, 2.8612, 2.8612],
        [2.8612, 2.8621, 2.8612],
        [2.8612, 2.8614, 2.8612]], grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.12509116530418396 
model_pd.l_d.mean(): -22.59478759765625 
model_pd.lagr.mean(): -22.469696044921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0253])), ('power', tensor([0.9647]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8384])), ('power', tensor([-24.2801]))])
epoch：28	 i:0 	 global-step:560	 l-p:0.12509116530418396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[2.8759, 3.2612, 3.3695],
        [2.8759, 2.8794, 2.8761],
        [2.8759, 3.0280, 2.9937],
        [2.8759, 2.8759, 2.8759]], grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.12428703159093857 
model_pd.l_d.mean(): -22.563749313354492 
model_pd.lagr.mean(): -22.439462661743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0261])), ('power', tensor([0.9635]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8317])), ('power', tensor([-24.2721]))])
epoch：29	 i:0 	 global-step:580	 l-p:0.12428703159093857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228]])
 pt:tensor([[2.8851, 2.9974, 2.9572],
        [2.8851, 2.9544, 2.9178],
        [2.8851, 3.0914, 3.0757],
        [2.8851, 3.0519, 3.0213]], grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.12378941476345062 
model_pd.l_d.mean(): -22.53313446044922 
model_pd.lagr.mean(): -22.409345626831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0269])), ('power', tensor([0.9623]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8276])), ('power', tensor([-24.2672]))])
epoch：30	 i:0 	 global-step:600	 l-p:0.12378941476345062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[2.8889, 2.8891, 2.8889],
        [2.8889, 3.2750, 3.3831],
        [2.8889, 3.3601, 3.5488],
        [2.8889, 2.9736, 2.9344]], grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.12359762191772461 
model_pd.l_d.mean(): -22.50299644470215 
model_pd.lagr.mean(): -22.379398345947266 
model_pd.lambdas: dict_items([('pout', tensor([1.0278])), ('power', tensor([0.9611]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8260])), ('power', tensor([-24.2655]))])
epoch：31	 i:0 	 global-step:620	 l-p:0.12359762191772461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[2.8875, 2.8900, 2.8876],
        [2.8875, 3.3575, 3.5456],
        [2.8875, 2.9182, 2.8961],
        [2.8875, 3.0536, 3.0230]], grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.12369213253259659 
model_pd.l_d.mean(): -22.473325729370117 
model_pd.lagr.mean(): -22.349634170532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0286])), ('power', tensor([0.9599]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8268])), ('power', tensor([-24.2668]))])
epoch：32	 i:0 	 global-step:640	 l-p:0.12369213253259659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[2.8815, 2.8816, 2.8815],
        [2.8815, 2.8824, 2.8815],
        [2.8815, 2.8815, 2.8815],
        [2.8815, 2.8840, 2.8816]], grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.12403678894042969 
model_pd.l_d.mean(): -22.44402503967285 
model_pd.lagr.mean(): -22.319988250732422 
model_pd.lambdas: dict_items([('pout', tensor([1.0294])), ('power', tensor([0.9587]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8296])), ('power', tensor([-24.2707]))])
epoch：33	 i:0 	 global-step:660	 l-p:0.12403678894042969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[2.8719, 2.8719, 2.8719],
        [2.8719, 2.9026, 2.8806],
        [2.8719, 3.0361, 3.0058],
        [2.8719, 3.1271, 3.1389]], grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.12458188831806183 
model_pd.l_d.mean(): -22.41497039794922 
model_pd.lagr.mean(): -22.290388107299805 
model_pd.lambdas: dict_items([('pout', tensor([1.0303])), ('power', tensor([0.9575]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8341])), ('power', tensor([-24.2766]))])
epoch：34	 i:0 	 global-step:680	 l-p:0.12458188831806183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[2.8600, 2.8609, 2.8600],
        [2.8600, 2.8780, 2.8636],
        [2.8600, 3.0827, 3.0772],
        [2.8600, 2.8906, 2.8686]], grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.1252666413784027 
model_pd.l_d.mean(): -22.386016845703125 
model_pd.lagr.mean(): -22.26074981689453 
model_pd.lambdas: dict_items([('pout', tensor([1.0311])), ('power', tensor([0.9563]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8397])), ('power', tensor([-24.2839]))])
epoch：35	 i:0 	 global-step:700	 l-p:0.1252666413784027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[2.8470, 3.0086, 2.9787],
        [2.8470, 3.4396, 3.7699],
        [2.8470, 3.4511, 3.7949],
        [2.8470, 3.2222, 3.3268]], grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.12602229416370392 
model_pd.l_d.mean(): -22.35703468322754 
model_pd.lagr.mean(): -22.23101234436035 
model_pd.lambdas: dict_items([('pout', tensor([1.0319])), ('power', tensor([0.9550]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8458])), ('power', tensor([-24.2918]))])
epoch：36	 i:0 	 global-step:720	 l-p:0.12602229416370392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[2.8342, 3.2505, 3.3943],
        [2.8342, 2.8367, 2.8344],
        [2.8342, 2.8344, 2.8342],
        [2.8342, 3.2066, 3.3102]], grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.126775860786438 
model_pd.l_d.mean(): -22.327926635742188 
model_pd.lagr.mean(): -22.20115089416504 
model_pd.lambdas: dict_items([('pout', tensor([1.0328])), ('power', tensor([0.9538]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8518])), ('power', tensor([-24.2994]))])
epoch：37	 i:0 	 global-step:740	 l-p:0.126775860786438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[2.8230, 3.0627, 3.0693],
        [2.8230, 3.1498, 3.2169],
        [2.8230, 2.8784, 2.8463],
        [2.8230, 2.8230, 2.8230]], grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.12745466828346252 
model_pd.l_d.mean(): -22.29863739013672 
model_pd.lagr.mean(): -22.17118263244629 
model_pd.lambdas: dict_items([('pout', tensor([1.0337])), ('power', tensor([0.9526]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8572])), ('power', tensor([-24.3062]))])
epoch：38	 i:0 	 global-step:760	 l-p:0.12745466828346252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[2.8142, 2.8185, 2.8145],
        [2.8142, 3.2593, 3.4345],
        [2.8142, 2.8143, 2.8142],
        [2.8142, 3.1390, 3.2055]], grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.12799516320228577 
model_pd.l_d.mean(): -22.26915740966797 
model_pd.lagr.mean(): -22.141162872314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0345])), ('power', tensor([0.9514]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8614])), ('power', tensor([-24.3117]))])
epoch：39	 i:0 	 global-step:780	 l-p:0.12799516320228577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[2.8086, 2.8086, 2.8086],
        [2.8086, 3.2174, 3.3582],
        [2.8086, 2.8110, 2.8087],
        [2.8086, 2.8375, 2.8167]], grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.1283482015132904 
model_pd.l_d.mean(): -22.23948860168457 
model_pd.lagr.mean(): -22.111141204833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0354])), ('power', tensor([0.9502]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8641])), ('power', tensor([-24.3153]))])
epoch：40	 i:0 	 global-step:800	 l-p:0.1283482015132904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[2.8066, 3.0355, 3.0377],
        [2.8066, 2.8219, 2.8095],
        [2.8066, 2.9003, 2.8623],
        [2.8066, 2.8066, 2.8067]], grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.1284840703010559 
model_pd.l_d.mean(): -22.209646224975586 
model_pd.lagr.mean(): -22.081161499023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0362])), ('power', tensor([0.9490]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8652])), ('power', tensor([-24.3169]))])
epoch：41	 i:0 	 global-step:820	 l-p:0.1284840703010559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[2.8084, 2.8084, 2.8084],
        [2.8084, 3.0367, 3.0388],
        [2.8084, 2.8084, 2.8084],
        [2.8084, 3.0514, 3.0619]], grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.12839727103710175 
model_pd.l_d.mean(): -22.179656982421875 
model_pd.lagr.mean(): -22.051259994506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0371])), ('power', tensor([0.9477]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8645])), ('power', tensor([-24.3165]))])
epoch：42	 i:0 	 global-step:840	 l-p:0.12839727103710175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[2.8136, 2.9694, 2.9402],
        [2.8136, 2.8144, 2.8136],
        [2.8136, 3.0722, 3.0923],
        [2.8136, 2.8136, 2.8136]], grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.12810593843460083 
model_pd.l_d.mean(): -22.149539947509766 
model_pd.lagr.mean(): -22.021434783935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0380])), ('power', tensor([0.9465]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8622])), ('power', tensor([-24.3142]))])
epoch：43	 i:0 	 global-step:860	 l-p:0.12810593843460083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[2.8216, 2.8958, 2.8597],
        [2.8216, 3.1433, 3.2084],
        [2.8216, 2.9149, 2.8769],
        [2.8216, 2.8497, 2.8293]], grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.1276477575302124 
model_pd.l_d.mean(): -22.11929702758789 
model_pd.lagr.mean(): -21.991649627685547 
model_pd.lambdas: dict_items([('pout', tensor([1.0388])), ('power', tensor([0.9453]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8585])), ('power', tensor([-24.3103]))])
epoch：44	 i:0 	 global-step:880	 l-p:0.1276477575302124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[2.8317, 2.8325, 2.8317],
        [2.8317, 3.0909, 3.1107],
        [2.8317, 3.4104, 3.7310],
        [2.8317, 2.8359, 2.8320]], grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.12707369029521942 
model_pd.l_d.mean(): -22.088956832885742 
model_pd.lagr.mean(): -21.961883544921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0397])), ('power', tensor([0.9441]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8540])), ('power', tensor([-24.3052]))])
epoch：45	 i:0 	 global-step:900	 l-p:0.12707369029521942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[2.8429, 3.1660, 3.2309],
        [2.8429, 2.8453, 2.8430],
        [2.8429, 2.8432, 2.8429],
        [2.8429, 3.2900, 3.4663]], grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.1264406442642212 
model_pd.l_d.mean(): -22.05853271484375 
model_pd.lagr.mean(): -21.932092666625977 
model_pd.lambdas: dict_items([('pout', tensor([1.0405])), ('power', tensor([0.9429]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8489])), ('power', tensor([-24.2995]))])
epoch：46	 i:0 	 global-step:920	 l-p:0.1264406442642212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[2.8543, 2.8543, 2.8543],
        [2.8543, 2.9751, 2.9380],
        [2.8543, 3.4750, 3.8422],
        [2.8543, 3.0109, 2.9811]], grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.12580543756484985 
model_pd.l_d.mean(): -22.028076171875 
model_pd.lagr.mean(): -21.902271270751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0414])), ('power', tensor([0.9417]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8438])), ('power', tensor([-24.2936]))])
epoch：47	 i:0 	 global-step:940	 l-p:0.12580543756484985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[2.8650, 2.8802, 2.8678],
        [2.8650, 2.9296, 2.8952],
        [2.8650, 3.0145, 2.9826],
        [2.8650, 2.9859, 2.9487]], grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.1252192258834839 
model_pd.l_d.mean(): -21.997636795043945 
model_pd.lagr.mean(): -21.872417449951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0422])), ('power', tensor([0.9405]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8390])), ('power', tensor([-24.2880]))])
epoch：48	 i:0 	 global-step:960	 l-p:0.1252192258834839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[2.8741, 2.9023, 2.8818],
        [2.8741, 3.1117, 3.1168],
        [2.8741, 2.8774, 2.8744],
        [2.8741, 2.8893, 2.8770]], grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.12472376972436905 
model_pd.l_d.mean(): -21.96727180480957 
model_pd.lagr.mean(): -21.842548370361328 
model_pd.lambdas: dict_items([('pout', tensor([1.0430])), ('power', tensor([0.9392]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8349])), ('power', tensor([-24.2834]))])
epoch：49	 i:0 	 global-step:980	 l-p:0.12472376972436905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[2.8812, 2.9093, 2.8889],
        [2.8812, 2.8820, 2.8812],
        [2.8812, 2.8814, 2.8812],
        [2.8812, 3.5061, 3.8747]], grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.12434875220060349 
model_pd.l_d.mean(): -21.93703842163086 
model_pd.lagr.mean(): -21.81268882751465 
model_pd.lambdas: dict_items([('pout', tensor([1.0439])), ('power', tensor([0.9380]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8318])), ('power', tensor([-24.2798]))])
epoch：50	 i:0 	 global-step:1000	 l-p:0.12434875220060349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[2.8859, 3.0289, 2.9953],
        [2.8859, 3.1009, 3.0938],
        [2.8859, 3.0067, 2.9693],
        [2.8859, 2.8860, 2.8859]], grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.12411046773195267 
model_pd.l_d.mean(): -21.906965255737305 
model_pd.lagr.mean(): -21.782854080200195 
model_pd.lambdas: dict_items([('pout', tensor([1.0447])), ('power', tensor([0.9368]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8298])), ('power', tensor([-24.2777]))])
epoch：51	 i:0 	 global-step:1020	 l-p:0.12411046773195267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[2.8880, 2.9420, 2.9105],
        [2.8880, 2.9169, 2.8961],
        [2.8880, 3.1026, 3.0954],
        [2.8880, 2.8880, 2.8880]], grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.12401144206523895 
model_pd.l_d.mean(): -21.87705421447754 
model_pd.lagr.mean(): -21.753042221069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0455])), ('power', tensor([0.9356]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8290])), ('power', tensor([-24.2771]))])
epoch：52	 i:0 	 global-step:1040	 l-p:0.12401144206523895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[2.8878, 2.9028, 2.8906],
        [2.8878, 3.0363, 3.0043],
        [2.8878, 3.1167, 3.1171],
        [2.8878, 3.4725, 3.7935]], grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.12404132634401321 
model_pd.l_d.mean(): -21.847309112548828 
model_pd.lagr.mean(): -21.723268508911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0464])), ('power', tensor([0.9344]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8292])), ('power', tensor([-24.2777]))])
epoch：53	 i:0 	 global-step:1060	 l-p:0.12404132634401321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[2.8857, 3.6342, 4.1604],
        [2.8857, 3.5071, 3.8724],
        [2.8857, 3.0272, 2.9937],
        [2.8857, 3.0782, 3.0614]], grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.12417829781770706 
model_pd.l_d.mean(): -21.817676544189453 
model_pd.lagr.mean(): -21.693498611450195 
model_pd.lambdas: dict_items([('pout', tensor([1.0472])), ('power', tensor([0.9332]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8303])), ('power', tensor([-24.2795]))])
epoch：54	 i:0 	 global-step:1080	 l-p:0.12417829781770706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[2.8821, 2.9741, 2.9362],
        [2.8821, 3.2032, 3.2659],
        [2.8821, 2.8824, 2.8821],
        [2.8821, 2.8821, 2.8821]], grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.12439146637916565 
model_pd.l_d.mean(): -21.788122177124023 
model_pd.lagr.mean(): -21.66373062133789 
model_pd.lambdas: dict_items([('pout', tensor([1.0480])), ('power', tensor([0.9320]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8320])), ('power', tensor([-24.2821]))])
epoch：55	 i:0 	 global-step:1100	 l-p:0.12439146637916565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[2.8779, 2.8945, 2.8812],
        [2.8779, 3.2840, 3.4201],
        [2.8779, 3.1185, 3.1268],
        [2.8779, 2.9552, 2.9188]], grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.12464406341314316 
model_pd.l_d.mean(): -21.758588790893555 
model_pd.lagr.mean(): -21.63394546508789 
model_pd.lambdas: dict_items([('pout', tensor([1.0489])), ('power', tensor([0.9307]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8341])), ('power', tensor([-24.2851]))])
epoch：56	 i:0 	 global-step:1120	 l-p:0.12464406341314316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[2.8737, 3.3128, 3.4811],
        [2.8737, 2.8737, 2.8737],
        [2.8737, 3.4615, 3.7901],
        [2.8737, 3.1918, 3.2536]], grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.12489696592092514 
model_pd.l_d.mean(): -21.72903060913086 
model_pd.lagr.mean(): -21.60413360595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0497])), ('power', tensor([0.9295]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8361])), ('power', tensor([-24.2881]))])
epoch：57	 i:0 	 global-step:1140	 l-p:0.12489696592092514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[2.8702, 3.3106, 3.4811],
        [2.8702, 2.9225, 2.8920],
        [2.8702, 3.1242, 3.1415],
        [2.8702, 3.3111, 3.4821]], grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.12511295080184937 
model_pd.l_d.mean(): -21.699411392211914 
model_pd.lagr.mean(): -21.574298858642578 
model_pd.lambdas: dict_items([('pout', tensor([1.0505])), ('power', tensor([0.9283]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8379])), ('power', tensor([-24.2907]))])
epoch：58	 i:0 	 global-step:1160	 l-p:0.12511295080184937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[2.8679, 2.9299, 2.8967],
        [2.8679, 2.8680, 2.8679],
        [2.8679, 2.8843, 2.8712],
        [2.8679, 2.9395, 2.9043]], grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.12526050209999084 
model_pd.l_d.mean(): -21.669710159301758 
model_pd.lagr.mean(): -21.544448852539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0514])), ('power', tensor([0.9271]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8391])), ('power', tensor([-24.2926]))])
epoch：59	 i:0 	 global-step:1180	 l-p:0.12526050209999084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[2.8673, 2.8673, 2.8673],
        [2.8673, 3.2241, 3.3189],
        [2.8673, 2.9833, 2.9469],
        [2.8673, 3.0109, 2.9794]], grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.1253175586462021 
model_pd.l_d.mean(): -21.639925003051758 
model_pd.lagr.mean(): -21.51460838317871 
model_pd.lambdas: dict_items([('pout', tensor([1.0522])), ('power', tensor([0.9259]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8395])), ('power', tensor([-24.2936]))])
epoch：60	 i:0 	 global-step:1200	 l-p:0.1253175586462021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[2.8684, 3.2245, 3.3188],
        [2.8684, 3.1043, 3.1117],
        [2.8684, 3.2053, 3.2836],
        [2.8684, 2.9840, 2.9477]], grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.12527385354042053 
model_pd.l_d.mean(): -21.610036849975586 
model_pd.lagr.mean(): -21.48476219177246 
model_pd.lambdas: dict_items([('pout', tensor([1.0531])), ('power', tensor([0.9247]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8391])), ('power', tensor([-24.2935]))])
epoch：61	 i:0 	 global-step:1220	 l-p:0.12527385354042053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[2.8712, 3.0993, 3.1024],
        [2.8712, 3.5531, 3.9988],
        [2.8712, 2.8856, 2.8739],
        [2.8712, 3.0777, 3.0695]], grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.12513166666030884 
model_pd.l_d.mean(): -21.580068588256836 
model_pd.lagr.mean(): -21.454936981201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0539])), ('power', tensor([0.9235]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8379])), ('power', tensor([-24.2924]))])
epoch：62	 i:0 	 global-step:1240	 l-p:0.12513166666030884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[2.8756, 2.9507, 2.9151],
        [2.8756, 3.2311, 3.3248],
        [2.8756, 2.8756, 2.8756],
        [2.8756, 2.9646, 2.9276]], grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.12490493059158325 
model_pd.l_d.mean(): -21.550020217895508 
model_pd.lagr.mean(): -21.42511558532715 
model_pd.lambdas: dict_items([('pout', tensor([1.0547])), ('power', tensor([0.9222]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8361])), ('power', tensor([-24.2904]))])
epoch：63	 i:0 	 global-step:1260	 l-p:0.12490493059158325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[2.8811, 2.8811, 2.8811],
        [2.8811, 3.2800, 3.4117],
        [2.8811, 2.9085, 2.8887],
        [2.8811, 2.9700, 2.9330]], grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.12461685389280319 
model_pd.l_d.mean(): -21.51991844177246 
model_pd.lagr.mean(): -21.395301818847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0556])), ('power', tensor([0.9210]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8337])), ('power', tensor([-24.2878]))])
epoch：64	 i:0 	 global-step:1280	 l-p:0.12461685389280319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[2.8872, 2.9032, 2.8904],
        [2.8872, 2.8872, 2.8872],
        [2.8872, 2.8880, 2.8872],
        [2.8872, 2.8872, 2.8872]], grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.12429642677307129 
model_pd.l_d.mean(): -21.489784240722656 
model_pd.lagr.mean(): -21.365488052368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0564])), ('power', tensor([0.9198]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8310])), ('power', tensor([-24.2848]))])
epoch：65	 i:0 	 global-step:1300	 l-p:0.12429642677307129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[2.8934, 3.5777, 4.0233],
        [2.8934, 3.0358, 3.0040],
        [2.8934, 2.8936, 2.8934],
        [2.8934, 2.8937, 2.8934]], grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.12397423386573792 
model_pd.l_d.mean(): -21.459640502929688 
model_pd.lagr.mean(): -21.33566665649414 
model_pd.lambdas: dict_items([('pout', tensor([1.0572])), ('power', tensor([0.9186]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8283])), ('power', tensor([-24.2818]))])
epoch：66	 i:0 	 global-step:1320	 l-p:0.12397423386573792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[2.8991, 2.9133, 2.9017],
        [2.8991, 3.3367, 3.5041],
        [2.8991, 3.2354, 3.3124],
        [2.8991, 2.8991, 2.8991]], grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.1236790120601654 
model_pd.l_d.mean(): -21.429519653320312 
model_pd.lagr.mean(): -21.30584144592285 
model_pd.lambdas: dict_items([('pout', tensor([1.0581])), ('power', tensor([0.9174]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8259])), ('power', tensor([-24.2790]))])
epoch：67	 i:0 	 global-step:1340	 l-p:0.1236790120601654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[2.9039, 2.9039, 2.9039],
        [2.9039, 3.1543, 3.1698],
        [2.9039, 3.2170, 3.2756],
        [2.9039, 3.6420, 4.1552]], grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.12343408912420273 
model_pd.l_d.mean(): -21.399457931518555 
model_pd.lagr.mean(): -21.276023864746094 
model_pd.lambdas: dict_items([('pout', tensor([1.0589])), ('power', tensor([0.9162]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8238])), ('power', tensor([-24.2767]))])
epoch：68	 i:0 	 global-step:1360	 l-p:0.12343408912420273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[2.9075, 3.2435, 3.3198],
        [2.9075, 2.9097, 2.9077],
        [2.9075, 3.1420, 3.1481],
        [2.9075, 2.9343, 2.9149]], grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.12325499206781387 
model_pd.l_d.mean(): -21.36946678161621 
model_pd.lagr.mean(): -21.246212005615234 
model_pd.lambdas: dict_items([('pout', tensor([1.0597])), ('power', tensor([0.9150]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8223])), ('power', tensor([-24.2751]))])
epoch：69	 i:0 	 global-step:1380	 l-p:0.12325499206781387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[2.9098, 2.9120, 2.9100],
        [2.9098, 2.9098, 2.9098],
        [2.9098, 2.9098, 2.9098],
        [2.9098, 2.9101, 2.9098]], grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.12314807623624802 
model_pd.l_d.mean(): -21.339550018310547 
model_pd.lagr.mean(): -21.216402053833008 
model_pd.lambdas: dict_items([('pout', tensor([1.0605])), ('power', tensor([0.9137]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8214])), ('power', tensor([-24.2743]))])
epoch：70	 i:0 	 global-step:1400	 l-p:0.12314807623624802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[2.9109, 3.6478, 4.1590],
        [2.9109, 3.3085, 3.4382],
        [2.9109, 2.9249, 2.9134],
        [2.9109, 2.9111, 2.9109]], grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.12311001121997833 
model_pd.l_d.mean(): -21.30971908569336 
model_pd.lagr.mean(): -21.186609268188477 
model_pd.lambdas: dict_items([('pout', tensor([1.0613])), ('power', tensor([0.9125]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8210])), ('power', tensor([-24.2742]))])
epoch：71	 i:0 	 global-step:1420	 l-p:0.12311001121997833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[2.9109, 3.6466, 4.1566],
        [2.9109, 2.9377, 2.9183],
        [2.9109, 2.9110, 2.9109],
        [2.9109, 2.9190, 2.9119]], grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.12312886118888855 
model_pd.l_d.mean(): -21.27994155883789 
model_pd.lagr.mean(): -21.15681266784668 
model_pd.lambdas: dict_items([('pout', tensor([1.0622])), ('power', tensor([0.9113]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8211])), ('power', tensor([-24.2748]))])
epoch：72	 i:0 	 global-step:1440	 l-p:0.12312886118888855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[2.9101, 3.1276, 3.1253],
        [2.9101, 3.1131, 3.1036],
        [2.9101, 2.9602, 2.9307],
        [2.9101, 2.9101, 2.9101]], grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.12318582832813263 
model_pd.l_d.mean(): -21.25020408630371 
model_pd.lagr.mean(): -21.127017974853516 
model_pd.lambdas: dict_items([('pout', tensor([1.0630])), ('power', tensor([0.9101]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8216])), ('power', tensor([-24.2757]))])
epoch：73	 i:0 	 global-step:1460	 l-p:0.12318582832813263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[2.9091, 2.9958, 2.9593],
        [2.9091, 2.9822, 2.9472],
        [2.9091, 3.3382, 3.4985],
        [2.9091, 3.3418, 3.5056]], grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.12325801700353622 
model_pd.l_d.mean(): -21.22047996520996 
model_pd.lagr.mean(): -21.09722137451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0638])), ('power', tensor([0.9089]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8222])), ('power', tensor([-24.2769]))])
epoch：74	 i:0 	 global-step:1480	 l-p:0.12325801700353622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[2.9083, 2.9347, 2.9156],
        [2.9083, 3.4853, 3.8022],
        [2.9083, 3.0412, 3.0081],
        [2.9083, 2.9346, 2.9155]], grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.12332155555486679 
model_pd.l_d.mean(): -21.19074058532715 
model_pd.lagr.mean(): -21.067419052124023 
model_pd.lambdas: dict_items([('pout', tensor([1.0646])), ('power', tensor([0.9077]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8227])), ('power', tensor([-24.2779]))])
epoch：75	 i:0 	 global-step:1500	 l-p:0.12332155555486679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[2.9080, 2.9342, 2.9152],
        [2.9080, 2.9101, 2.9081],
        [2.9080, 3.3349, 3.4938],
        [2.9080, 3.5108, 3.8579]], grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.1233549565076828 
model_pd.l_d.mean(): -21.160966873168945 
model_pd.lagr.mean(): -21.03761100769043 
model_pd.lambdas: dict_items([('pout', tensor([1.0654])), ('power', tensor([0.9065]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8229])), ('power', tensor([-24.2786]))])
epoch：76	 i:0 	 global-step:1520	 l-p:0.1233549565076828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[2.9086, 2.9087, 2.9086],
        [2.9086, 3.3382, 3.5000],
        [2.9086, 2.9093, 2.9086],
        [2.9086, 2.9346, 2.9157]], grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.12334230542182922 
model_pd.l_d.mean(): -21.131147384643555 
model_pd.lagr.mean(): -21.00780487060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0663])), ('power', tensor([0.9052]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8228])), ('power', tensor([-24.2788]))])
epoch：77	 i:0 	 global-step:1540	 l-p:0.12334230542182922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[2.9101, 3.4728, 3.7742],
        [2.9101, 2.9592, 2.9302],
        [2.9101, 3.1243, 3.1213],
        [2.9101, 3.0477, 3.0160]], grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.12327511608600616 
model_pd.l_d.mean(): -21.101272583007812 
model_pd.lagr.mean(): -20.977996826171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0671])), ('power', tensor([0.9040]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8222])), ('power', tensor([-24.2785]))])
epoch：78	 i:0 	 global-step:1560	 l-p:0.12327511608600616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[2.9127, 2.9156, 2.9129],
        [2.9127, 3.3031, 3.4285],
        [2.9127, 2.9127, 2.9127],
        [2.9127, 2.9127, 2.9127]], grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.12315342575311661 
model_pd.l_d.mean(): -21.071353912353516 
model_pd.lagr.mean(): -20.948200225830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0679])), ('power', tensor([0.9028]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8212])), ('power', tensor([-24.2775]))])
epoch：79	 i:0 	 global-step:1580	 l-p:0.12315342575311661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[2.9162, 3.3407, 3.4977],
        [2.9162, 2.9834, 2.9497],
        [2.9162, 2.9162, 2.9162],
        [2.9162, 3.0600, 3.0299]], grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.12298562377691269 
model_pd.l_d.mean(): -21.041391372680664 
model_pd.lagr.mean(): -20.918405532836914 
model_pd.lambdas: dict_items([('pout', tensor([1.0687])), ('power', tensor([0.9016]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8198])), ('power', tensor([-24.2760]))])
epoch：80	 i:0 	 global-step:1600	 l-p:0.12298562377691269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[2.9202, 2.9239, 2.9205],
        [2.9202, 2.9202, 2.9202],
        [2.9202, 2.9202, 2.9202],
        [2.9202, 3.2476, 3.3195]], grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.12278667092323303 
model_pd.l_d.mean(): -21.011398315429688 
model_pd.lagr.mean(): -20.88861083984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0696])), ('power', tensor([0.9004]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8181])), ('power', tensor([-24.2741]))])
epoch：81	 i:0 	 global-step:1620	 l-p:0.12278667092323303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[2.9245, 3.1516, 3.1555],
        [2.9245, 2.9499, 2.9314],
        [2.9245, 3.1443, 3.1443],
        [2.9245, 2.9245, 2.9245]], grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.12257538735866547 
model_pd.l_d.mean(): -20.981395721435547 
model_pd.lagr.mean(): -20.85881996154785 
model_pd.lambdas: dict_items([('pout', tensor([1.0704])), ('power', tensor([0.8992]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8163])), ('power', tensor([-24.2721]))])
epoch：82	 i:0 	 global-step:1640	 l-p:0.12257538735866547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[2.9286, 3.5293, 3.8729],
        [2.9286, 2.9542, 2.9356],
        [2.9286, 3.0589, 3.0259],
        [2.9286, 2.9323, 2.9289]], grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.12237129360437393 
model_pd.l_d.mean(): -20.95140266418457 
model_pd.lagr.mean(): -20.829030990600586 
model_pd.lambdas: dict_items([('pout', tensor([1.0712])), ('power', tensor([0.8980]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8145])), ('power', tensor([-24.2702]))])
epoch：83	 i:0 	 global-step:1660	 l-p:0.12237129360437393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[2.9323, 2.9401, 2.9333],
        [2.9323, 3.5330, 3.8762],
        [2.9323, 2.9352, 2.9325],
        [2.9323, 3.4943, 3.7933]], grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.12219160050153732 
model_pd.l_d.mean(): -20.92144012451172 
model_pd.lagr.mean(): -20.79924774169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0720])), ('power', tensor([0.8968]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8130])), ('power', tensor([-24.2685]))])
epoch：84	 i:0 	 global-step:1680	 l-p:0.12219160050153732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[2.9354, 2.9353, 2.9354],
        [2.9354, 2.9503, 2.9383],
        [2.9354, 3.0193, 2.9835],
        [2.9354, 2.9354, 2.9354]], grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.12204847484827042 
model_pd.l_d.mean(): -20.891511917114258 
model_pd.lagr.mean(): -20.76946258544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0728])), ('power', tensor([0.8955]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8117])), ('power', tensor([-24.2673]))])
epoch：85	 i:0 	 global-step:1700	 l-p:0.12204847484827042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[2.9376, 2.9377, 2.9376],
        [2.9376, 2.9376, 2.9376],
        [2.9376, 3.1786, 3.1905],
        [2.9376, 2.9383, 2.9376]], grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.12194753438234329 
model_pd.l_d.mean(): -20.86163330078125 
model_pd.lagr.mean(): -20.73968505859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0736])), ('power', tensor([0.8943]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8109])), ('power', tensor([-24.2665]))])
epoch：86	 i:0 	 global-step:1720	 l-p:0.12194753438234329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[2.9391, 2.9391, 2.9391],
        [2.9391, 2.9411, 2.9392],
        [2.9391, 3.0681, 3.0351],
        [2.9391, 2.9393, 2.9391]], grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.1218869760632515 
model_pd.l_d.mean(): -20.831796646118164 
model_pd.lagr.mean(): -20.709909439086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0744])), ('power', tensor([0.8931]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8103])), ('power', tensor([-24.2661]))])
epoch：87	 i:0 	 global-step:1740	 l-p:0.1218869760632515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[2.9399, 2.9652, 2.9468],
        [2.9399, 3.6137, 4.0442],
        [2.9399, 2.9401, 2.9399],
        [2.9399, 3.0099, 2.9760]], grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.12185853719711304 
model_pd.l_d.mean(): -20.801998138427734 
model_pd.lagr.mean(): -20.680139541625977 
model_pd.lambdas: dict_items([('pout', tensor([1.0752])), ('power', tensor([0.8919]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8101])), ('power', tensor([-24.2661]))])
epoch：88	 i:0 	 global-step:1760	 l-p:0.12185853719711304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[2.9405, 3.6132, 4.0427],
        [2.9405, 3.0233, 2.9878],
        [2.9405, 2.9407, 2.9405],
        [2.9405, 2.9480, 2.9414]], grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.12184888124465942 
model_pd.l_d.mean(): -20.772218704223633 
model_pd.lagr.mean(): -20.65036964416504 
model_pd.lambdas: dict_items([('pout', tensor([1.0761])), ('power', tensor([0.8907]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8099])), ('power', tensor([-24.2664]))])
epoch：89	 i:0 	 global-step:1780	 l-p:0.12184888124465942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[2.9409, 2.9411, 2.9409],
        [2.9409, 2.9410, 2.9409],
        [2.9409, 3.0813, 3.0509],
        [2.9409, 3.0339, 2.9980]], grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.12184207141399384 
model_pd.l_d.mean(): -20.74243927001953 
model_pd.lagr.mean(): -20.62059783935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0769])), ('power', tensor([0.8895]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8099])), ('power', tensor([-24.2666]))])
epoch：90	 i:0 	 global-step:1800	 l-p:0.12184207141399384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[2.9416, 3.2827, 3.3663],
        [2.9416, 2.9886, 2.9606],
        [2.9416, 3.0238, 2.9885],
        [2.9416, 2.9491, 2.9426]], grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.12182256579399109 
model_pd.l_d.mean(): -20.712644577026367 
model_pd.lagr.mean(): -20.590822219848633 
model_pd.lambdas: dict_items([('pout', tensor([1.0777])), ('power', tensor([0.8883]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8097])), ('power', tensor([-24.2667]))])
epoch：91	 i:0 	 global-step:1820	 l-p:0.12182256579399109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[2.9428, 3.0117, 2.9781],
        [2.9428, 3.0075, 2.9747],
        [2.9428, 3.4987, 3.7920],
        [2.9428, 2.9428, 2.9428]], grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.12177804112434387 
model_pd.l_d.mean(): -20.682828903198242 
model_pd.lagr.mean(): -20.561050415039062 
model_pd.lambdas: dict_items([('pout', tensor([1.0785])), ('power', tensor([0.8870]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8093])), ('power', tensor([-24.2666]))])
epoch：92	 i:0 	 global-step:1840	 l-p:0.12177804112434387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[2.9446, 2.9912, 2.9634],
        [2.9446, 2.9693, 2.9513],
        [2.9446, 3.0091, 2.9763],
        [2.9446, 3.6674, 4.1598]], grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.12170127034187317 
model_pd.l_d.mean(): -20.652978897094727 
model_pd.lagr.mean(): -20.53127670288086 
model_pd.lambdas: dict_items([('pout', tensor([1.0793])), ('power', tensor([0.8858]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8086])), ('power', tensor([-24.2660]))])
epoch：93	 i:0 	 global-step:1860	 l-p:0.12170127034187317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[2.9469, 2.9469, 2.9469],
        [2.9469, 2.9505, 2.9472],
        [2.9469, 2.9707, 2.9532],
        [2.9469, 3.1832, 3.1934]], grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.12159501016139984 
model_pd.l_d.mean(): -20.62310028076172 
model_pd.lagr.mean(): -20.50150489807129 
model_pd.lambdas: dict_items([('pout', tensor([1.0801])), ('power', tensor([0.8846]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8077])), ('power', tensor([-24.2651]))])
epoch：94	 i:0 	 global-step:1880	 l-p:0.12159501016139984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[2.9497, 3.0138, 2.9812],
        [2.9497, 2.9497, 2.9497],
        [2.9497, 3.0180, 2.9846],
        [2.9497, 3.0814, 3.0495]], grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.12146816402673721 
model_pd.l_d.mean(): -20.593202590942383 
model_pd.lagr.mean(): -20.47173500061035 
model_pd.lambdas: dict_items([('pout', tensor([1.0809])), ('power', tensor([0.8834]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8066])), ('power', tensor([-24.2640]))])
epoch：95	 i:0 	 global-step:1900	 l-p:0.12146816402673721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[2.9526, 3.6213, 4.0455],
        [2.9526, 3.1729, 3.1744],
        [2.9526, 3.5457, 3.8804],
        [2.9526, 2.9528, 2.9526]], grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.12133249640464783 
model_pd.l_d.mean(): -20.563297271728516 
model_pd.lagr.mean(): -20.441965103149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0817])), ('power', tensor([0.8822]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8054])), ('power', tensor([-24.2628]))])
epoch：96	 i:0 	 global-step:1920	 l-p:0.12133249640464783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[2.9555, 3.6774, 4.1677],
        [2.9555, 3.0465, 3.0109],
        [2.9555, 3.3747, 3.5273],
        [2.9555, 2.9555, 2.9555]], grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.12119971960783005 
model_pd.l_d.mean(): -20.533395767211914 
model_pd.lagr.mean(): -20.412195205688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0825])), ('power', tensor([0.8810]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8042])), ('power', tensor([-24.2615]))])
epoch：97	 i:0 	 global-step:1940	 l-p:0.12119971960783005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[2.9581, 3.0489, 3.0134],
        [2.9581, 3.0217, 2.9892],
        [2.9581, 3.2956, 3.3767],
        [2.9581, 2.9581, 2.9581]], grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.1210799515247345 
model_pd.l_d.mean(): -20.503509521484375 
model_pd.lagr.mean(): -20.382429122924805 
model_pd.lambdas: dict_items([('pout', tensor([1.0833])), ('power', tensor([0.8798]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8032])), ('power', tensor([-24.2605]))])
epoch：98	 i:0 	 global-step:1960	 l-p:0.1210799515247345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[2.9603, 3.6281, 4.0506],
        [2.9603, 3.3750, 3.5233],
        [2.9603, 3.0908, 3.0589],
        [2.9603, 3.3786, 3.5303]], grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.12098183482885361 
model_pd.l_d.mean(): -20.473642349243164 
model_pd.lagr.mean(): -20.3526611328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0841])), ('power', tensor([0.8786]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8023])), ('power', tensor([-24.2596]))])
epoch：99	 i:0 	 global-step:1980	 l-p:0.12098183482885361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[2.9620, 2.9760, 2.9647],
        [2.9620, 3.5262, 3.8278],
        [2.9620, 3.0293, 2.9962],
        [2.9620, 2.9853, 2.9681]], grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.12090866267681122 
model_pd.l_d.mean(): -20.44379997253418 
model_pd.lagr.mean(): -20.322891235351562 
model_pd.lambdas: dict_items([('pout', tensor([1.0849])), ('power', tensor([0.8773]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8017])), ('power', tensor([-24.2591]))])
epoch：100	 i:0 	 global-step:2000	 l-p:0.12090866267681122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[2.9633, 2.9633, 2.9633],
        [2.9633, 2.9633, 2.9633],
        [2.9633, 3.6836, 4.1710],
        [2.9633, 2.9652, 2.9634]], grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.12085648626089096 
model_pd.l_d.mean(): -20.413976669311523 
model_pd.lagr.mean(): -20.293119430541992 
model_pd.lambdas: dict_items([('pout', tensor([1.0857])), ('power', tensor([0.8761]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8012])), ('power', tensor([-24.2588]))])
epoch：101	 i:0 	 global-step:2020	 l-p:0.12085648626089096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[2.9644, 3.0186, 2.9886],
        [2.9644, 2.9645, 2.9644],
        [2.9644, 3.2806, 3.3453],
        [2.9644, 3.3772, 3.5240]], grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.12081632018089294 
model_pd.l_d.mean(): -20.384164810180664 
model_pd.lagr.mean(): -20.263347625732422 
model_pd.lambdas: dict_items([('pout', tensor([1.0865])), ('power', tensor([0.8749]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8008])), ('power', tensor([-24.2586]))])
epoch：102	 i:0 	 global-step:2040	 l-p:0.12081632018089294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[2.9655, 3.0280, 2.9960],
        [2.9655, 3.0689, 3.0338],
        [2.9655, 3.1686, 3.1620],
        [2.9655, 2.9655, 2.9655]], grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.1207762360572815 
model_pd.l_d.mean(): -20.354351043701172 
model_pd.lagr.mean(): -20.23357391357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0873])), ('power', tensor([0.8737]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8004])), ('power', tensor([-24.2585]))])
epoch：103	 i:0 	 global-step:2060	 l-p:0.1207762360572815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[2.9668, 2.9669, 2.9668],
        [2.9668, 2.9670, 2.9668],
        [2.9668, 2.9902, 2.9730],
        [2.9668, 2.9668, 2.9668]], grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.12072454392910004 
model_pd.l_d.mean(): -20.32452392578125 
model_pd.lagr.mean(): -20.203800201416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0881])), ('power', tensor([0.8725]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8000])), ('power', tensor([-24.2581]))])
epoch：104	 i:0 	 global-step:2080	 l-p:0.12072454392910004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[2.9685, 2.9822, 2.9712],
        [2.9685, 2.9687, 2.9685],
        [2.9685, 3.1774, 3.1738],
        [2.9685, 2.9685, 2.9685]], grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.1206524595618248 
model_pd.l_d.mean(): -20.294679641723633 
model_pd.lagr.mean(): -20.174026489257812 
model_pd.lambdas: dict_items([('pout', tensor([1.0889])), ('power', tensor([0.8713]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7993])), ('power', tensor([-24.2576]))])
epoch：105	 i:0 	 global-step:2100	 l-p:0.1206524595618248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[2.9707, 3.3847, 3.5329],
        [2.9707, 2.9708, 2.9707],
        [2.9707, 2.9707, 2.9707],
        [2.9707, 2.9844, 2.9733]], grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.12055604159832001 
model_pd.l_d.mean(): -20.264808654785156 
model_pd.lagr.mean(): -20.14425277709961 
model_pd.lambdas: dict_items([('pout', tensor([1.0897])), ('power', tensor([0.8701]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7985])), ('power', tensor([-24.2567]))])
epoch：106	 i:0 	 global-step:2120	 l-p:0.12055604159832001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[2.9733, 3.2035, 3.2113],
        [2.9733, 2.9751, 2.9734],
        [2.9733, 2.9733, 2.9733],
        [2.9733, 2.9733, 2.9733]], grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.12043723464012146 
model_pd.l_d.mean(): -20.234922409057617 
model_pd.lagr.mean(): -20.114484786987305 
model_pd.lambdas: dict_items([('pout', tensor([1.0905])), ('power', tensor([0.8688]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7974])), ('power', tensor([-24.2556]))])
epoch：107	 i:0 	 global-step:2140	 l-p:0.12043723464012146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[2.9762, 2.9832, 2.9771],
        [2.9762, 2.9762, 2.9762],
        [2.9762, 2.9795, 2.9765],
        [2.9762, 3.1633, 3.1498]], grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.12030291557312012 
model_pd.l_d.mean(): -20.20502281188965 
model_pd.lagr.mean(): -20.084720611572266 
model_pd.lambdas: dict_items([('pout', tensor([1.0913])), ('power', tensor([0.8676]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7963])), ('power', tensor([-24.2543]))])
epoch：108	 i:0 	 global-step:2160	 l-p:0.12030291557312012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[2.9792, 3.0447, 3.0122],
        [2.9792, 2.9792, 2.9792],
        [2.9792, 2.9912, 2.9814],
        [2.9792, 3.0810, 3.0461]], grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.12016314268112183 
model_pd.l_d.mean(): -20.17511749267578 
model_pd.lagr.mean(): -20.054954528808594 
model_pd.lambdas: dict_items([('pout', tensor([1.0921])), ('power', tensor([0.8664]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7950])), ('power', tensor([-24.2529]))])
epoch：109	 i:0 	 global-step:2180	 l-p:0.12016314268112183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[2.9822, 2.9822, 2.9822],
        [2.9822, 3.1498, 3.1286],
        [2.9822, 3.6994, 4.1814],
        [2.9822, 2.9891, 2.9831]], grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.12002849578857422 
model_pd.l_d.mean(): -20.145219802856445 
model_pd.lagr.mean(): -20.025192260742188 
model_pd.lambdas: dict_items([('pout', tensor([1.0929])), ('power', tensor([0.8652]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7938])), ('power', tensor([-24.2516]))])
epoch：110	 i:0 	 global-step:2200	 l-p:0.12002849578857422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[2.9848, 3.0077, 2.9909],
        [2.9848, 2.9849, 2.9848],
        [2.9848, 2.9850, 2.9848],
        [2.9848, 3.1521, 3.1309]], grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.11990735679864883 
model_pd.l_d.mean(): -20.115341186523438 
model_pd.lagr.mean(): -19.995433807373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0937])), ('power', tensor([0.8640]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7928])), ('power', tensor([-24.2504]))])
epoch：111	 i:0 	 global-step:2220	 l-p:0.11990735679864883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[2.9872, 3.1541, 3.1328],
        [2.9872, 3.0101, 2.9933],
        [2.9872, 2.9872, 2.9872],
        [2.9872, 2.9872, 2.9872]], grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.11980406939983368 
model_pd.l_d.mean(): -20.085481643676758 
model_pd.lagr.mean(): -19.96567726135254 
model_pd.lambdas: dict_items([('pout', tensor([1.0945])), ('power', tensor([0.8628]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7918])), ('power', tensor([-24.2495]))])
epoch：112	 i:0 	 global-step:2240	 l-p:0.11980406939983368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[2.9892, 2.9892, 2.9892],
        [2.9892, 3.1952, 3.1904],
        [2.9892, 2.9924, 2.9894],
        [2.9892, 2.9910, 2.9893]], grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.11971809715032578 
model_pd.l_d.mean(): -20.05564308166504 
model_pd.lagr.mean(): -19.935924530029297 
model_pd.lambdas: dict_items([('pout', tensor([1.0953])), ('power', tensor([0.8616]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7911])), ('power', tensor([-24.2487]))])
epoch：113	 i:0 	 global-step:2260	 l-p:0.11971809715032578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[2.9909, 3.0138, 2.9970],
        [2.9909, 2.9911, 2.9909],
        [2.9909, 2.9909, 2.9909],
        [2.9909, 3.7071, 4.1869]], grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.11964456737041473 
model_pd.l_d.mean(): -20.025821685791016 
model_pd.lagr.mean(): -19.906177520751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0961])), ('power', tensor([0.8604]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7904])), ('power', tensor([-24.2481]))])
epoch：114	 i:0 	 global-step:2280	 l-p:0.11964456737041473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[2.9926, 3.1977, 3.1926],
        [2.9926, 3.0445, 3.0154],
        [2.9926, 3.5776, 3.9016],
        [2.9926, 2.9926, 2.9926]], grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.1195753961801529 
model_pd.l_d.mean(): -19.99599838256836 
model_pd.lagr.mean(): -19.876422882080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0969])), ('power', tensor([0.8591]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7898])), ('power', tensor([-24.2476]))])
epoch：115	 i:0 	 global-step:2300	 l-p:0.1195753961801529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[2.9943, 3.0162, 3.0000],
        [2.9943, 3.3660, 3.4764],
        [2.9943, 3.1258, 3.0949],
        [2.9943, 3.0706, 3.0367]], grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.11950189620256424 
model_pd.l_d.mean(): -19.966176986694336 
model_pd.lagr.mean(): -19.846675872802734 
model_pd.lambdas: dict_items([('pout', tensor([1.0976])), ('power', tensor([0.8579]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7891])), ('power', tensor([-24.2469]))])
epoch：116	 i:0 	 global-step:2320	 l-p:0.11950189620256424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[2.9962, 3.5802, 3.9032],
        [2.9962, 3.0185, 3.0020],
        [2.9962, 3.0029, 2.9970],
        [2.9962, 2.9962, 2.9962]], grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.11942217499017715 
model_pd.l_d.mean(): -19.936344146728516 
model_pd.lagr.mean(): -19.81692123413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0984])), ('power', tensor([0.8567]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7884])), ('power', tensor([-24.2462]))])
epoch：117	 i:0 	 global-step:2340	 l-p:0.11942217499017715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[2.9979, 3.0004, 2.9981],
        [2.9979, 3.0738, 3.0400],
        [2.9979, 3.1625, 3.1408],
        [2.9979, 3.0616, 3.0297]], grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.11934985220432281 
model_pd.l_d.mean(): -19.906517028808594 
model_pd.lagr.mean(): -19.787166595458984 
model_pd.lambdas: dict_items([('pout', tensor([1.0992])), ('power', tensor([0.8555]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7878])), ('power', tensor([-24.2456]))])
epoch：118	 i:0 	 global-step:2360	 l-p:0.11934985220432281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[2.9994, 3.0026, 2.9997],
        [2.9994, 3.4041, 3.5438],
        [2.9994, 2.9994, 2.9994],
        [2.9994, 3.3265, 3.4005]], grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.11928854882717133 
model_pd.l_d.mean(): -19.87670135498047 
model_pd.lagr.mean(): -19.75741195678711 
model_pd.lambdas: dict_items([('pout', tensor([1.1000])), ('power', tensor([0.8543]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7872])), ('power', tensor([-24.2451]))])
epoch：119	 i:0 	 global-step:2380	 l-p:0.11928854882717133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[3.0008, 3.3703, 3.4791],
        [3.0008, 3.0123, 3.0028],
        [3.0008, 3.0008, 3.0008],
        [3.0008, 3.0009, 3.0008]], grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.11923455446958542 
model_pd.l_d.mean(): -19.846891403198242 
model_pd.lagr.mean(): -19.727657318115234 
model_pd.lambdas: dict_items([('pout', tensor([1.1008])), ('power', tensor([0.8531]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7867])), ('power', tensor([-24.2447]))])
epoch：120	 i:0 	 global-step:2400	 l-p:0.11923455446958542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[3.0021, 3.1256, 3.0934],
        [3.0021, 3.0053, 3.0024],
        [3.0021, 3.0447, 3.0188],
        [3.0021, 3.0774, 3.0437]], grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.11917968839406967 
model_pd.l_d.mean(): -19.81708335876465 
model_pd.lagr.mean(): -19.697904586791992 
model_pd.lambdas: dict_items([('pout', tensor([1.1016])), ('power', tensor([0.8519]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7862])), ('power', tensor([-24.2443]))])
epoch：121	 i:0 	 global-step:2420	 l-p:0.11917968839406967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.0037, 3.0166, 3.0062],
        [3.0037, 3.0038, 3.0037],
        [3.0037, 3.0787, 3.0451],
        [3.0037, 3.0260, 3.0096]], grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.11911480128765106 
model_pd.l_d.mean(): -19.78726577758789 
model_pd.lagr.mean(): -19.66815185546875 
model_pd.lambdas: dict_items([('pout', tensor([1.1024])), ('power', tensor([0.8507]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7856])), ('power', tensor([-24.2438]))])
epoch：122	 i:0 	 global-step:2440	 l-p:0.11911480128765106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.0055, 3.0073, 3.0056],
        [3.0055, 3.3306, 3.4032],
        [3.0055, 3.5865, 3.9059],
        [3.0055, 3.0277, 3.0114]], grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.11903836578130722 
model_pd.l_d.mean(): -19.757431030273438 
model_pd.lagr.mean(): -19.63839340209961 
model_pd.lambdas: dict_items([('pout', tensor([1.1032])), ('power', tensor([0.8495]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7849])), ('power', tensor([-24.2431]))])
epoch：123	 i:0 	 global-step:2460	 l-p:0.11903836578130722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[3.0075, 3.0293, 3.0132],
        [3.0075, 3.2159, 3.2130],
        [3.0075, 3.1886, 3.1733],
        [3.0075, 3.0919, 3.0575]], grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.1189541444182396 
model_pd.l_d.mean(): -19.727582931518555 
model_pd.lagr.mean(): -19.60862922668457 
model_pd.lambdas: dict_items([('pout', tensor([1.1039])), ('power', tensor([0.8482]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7842])), ('power', tensor([-24.2423]))])
epoch：124	 i:0 	 global-step:2480	 l-p:0.1189541444182396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.0095, 3.0720, 3.0405],
        [3.0095, 3.2040, 3.1945],
        [3.0095, 3.0096, 3.0095],
        [3.0095, 3.3150, 3.3727]], grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.11886376142501831 
model_pd.l_d.mean(): -19.697725296020508 
model_pd.lagr.mean(): -19.578861236572266 
model_pd.lambdas: dict_items([('pout', tensor([1.1047])), ('power', tensor([0.8470]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7833])), ('power', tensor([-24.2414]))])
epoch：125	 i:0 	 global-step:2500	 l-p:0.11886376142501831
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[3.0117, 3.0329, 3.0172],
        [3.0117, 3.2196, 3.2164],
        [3.0117, 3.4132, 3.5501],
        [3.0117, 3.2343, 3.2388]], grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.11876803636550903 
model_pd.l_d.mean(): -19.667865753173828 
model_pd.lagr.mean(): -19.549097061157227 
model_pd.lambdas: dict_items([('pout', tensor([1.1055])), ('power', tensor([0.8458]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7825])), ('power', tensor([-24.2404]))])
epoch：126	 i:0 	 global-step:2520	 l-p:0.11876803636550903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.0140, 3.0140, 3.0140],
        [3.0140, 3.7255, 4.1975],
        [3.0140, 3.0141, 3.0140],
        [3.0140, 3.0642, 3.0358]], grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.11866750568151474 
model_pd.l_d.mean(): -19.637996673583984 
model_pd.lagr.mean(): -19.519329071044922 
model_pd.lambdas: dict_items([('pout', tensor([1.1063])), ('power', tensor([0.8446]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7816])), ('power', tensor([-24.2394]))])
epoch：127	 i:0 	 global-step:2540	 l-p:0.11866750568151474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[3.0163, 3.2984, 3.3395],
        [3.0163, 3.1134, 3.0790],
        [3.0163, 3.0784, 3.0470],
        [3.0163, 3.4209, 3.5604]], grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.11856263130903244 
model_pd.l_d.mean(): -19.608135223388672 
model_pd.lagr.mean(): -19.489572525024414 
model_pd.lambdas: dict_items([('pout', tensor([1.1071])), ('power', tensor([0.8434]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7806])), ('power', tensor([-24.2383]))])
epoch：128	 i:0 	 global-step:2560	 l-p:0.11856263130903244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[3.0188, 3.5707, 3.8573],
        [3.0188, 3.3419, 3.4130],
        [3.0188, 3.0205, 3.0189],
        [3.0188, 3.0926, 3.0593]], grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.11845402419567108 
model_pd.l_d.mean(): -19.57826805114746 
model_pd.lagr.mean(): -19.459814071655273 
model_pd.lambdas: dict_items([('pout', tensor([1.1078])), ('power', tensor([0.8422]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7797])), ('power', tensor([-24.2372]))])
epoch：129	 i:0 	 global-step:2580	 l-p:0.11845402419567108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.0213, 3.1180, 3.0835],
        [3.0213, 3.5612, 3.8348],
        [3.0213, 3.0277, 3.0221],
        [3.0213, 3.0711, 3.0429]], grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.11834288388490677 
model_pd.l_d.mean(): -19.548402786254883 
model_pd.lagr.mean(): -19.4300594329834 
model_pd.lambdas: dict_items([('pout', tensor([1.1086])), ('power', tensor([0.8410]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7787])), ('power', tensor([-24.2360]))])
epoch：130	 i:0 	 global-step:2600	 l-p:0.11834288388490677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[3.0238, 3.3275, 3.3839],
        [3.0238, 3.1513, 3.1201],
        [3.0238, 3.0238, 3.0238],
        [3.0238, 3.3051, 3.3455]], grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.11823060363531113 
model_pd.l_d.mean(): -19.518543243408203 
model_pd.lagr.mean(): -19.400312423706055 
model_pd.lambdas: dict_items([('pout', tensor([1.1094])), ('power', tensor([0.8398]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7776])), ('power', tensor([-24.2348]))])
epoch：131	 i:0 	 global-step:2620	 l-p:0.11823060363531113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[3.0263, 3.0263, 3.0263],
        [3.0263, 3.2473, 3.2509],
        [3.0263, 3.0388, 3.0286],
        [3.0263, 3.4263, 3.5614]], grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.11811843514442444 
model_pd.l_d.mean(): -19.48868751525879 
model_pd.lagr.mean(): -19.370569229125977 
model_pd.lambdas: dict_items([('pout', tensor([1.1102])), ('power', tensor([0.8385]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7766])), ('power', tensor([-24.2336]))])
epoch：132	 i:0 	 global-step:2640	 l-p:0.11811843514442444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[3.0288, 3.0496, 3.0341],
        [3.0288, 3.1020, 3.0689],
        [3.0288, 3.1440, 3.1109],
        [3.0288, 3.0289, 3.0288]], grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.1180071085691452 
model_pd.l_d.mean(): -19.458837509155273 
model_pd.lagr.mean(): -19.340829849243164 
model_pd.lambdas: dict_items([('pout', tensor([1.1110])), ('power', tensor([0.8373]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7756])), ('power', tensor([-24.2324]))])
epoch：133	 i:0 	 global-step:2660	 l-p:0.1180071085691452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[3.0313, 3.0314, 3.0313],
        [3.0313, 3.2518, 3.2551],
        [3.0313, 3.2301, 3.2226],
        [3.0313, 3.0313, 3.0313]], grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.11789669841527939 
model_pd.l_d.mean(): -19.42898941040039 
model_pd.lagr.mean(): -19.311092376708984 
model_pd.lambdas: dict_items([('pout', tensor([1.1117])), ('power', tensor([0.8361]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7746])), ('power', tensor([-24.2312]))])
epoch：134	 i:0 	 global-step:2680	 l-p:0.11789669841527939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.0338, 3.0368, 3.0340],
        [3.0338, 3.1163, 3.0822],
        [3.0338, 3.5848, 3.8694],
        [3.0338, 3.3364, 3.3918]], grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.11778680980205536 
model_pd.l_d.mean(): -19.399150848388672 
model_pd.lagr.mean(): -19.28136444091797 
model_pd.lambdas: dict_items([('pout', tensor([1.1125])), ('power', tensor([0.8349]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7736])), ('power', tensor([-24.2300]))])
epoch：135	 i:0 	 global-step:2700	 l-p:0.11778680980205536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.0363, 3.1563, 3.1240],
        [3.0363, 3.6149, 3.9296],
        [3.0363, 3.0393, 3.0365],
        [3.0363, 3.0363, 3.0363]], grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.11767663806676865 
model_pd.l_d.mean(): -19.369314193725586 
model_pd.lagr.mean(): -19.251638412475586 
model_pd.lambdas: dict_items([('pout', tensor([1.1133])), ('power', tensor([0.8337]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7726])), ('power', tensor([-24.2288]))])
epoch：136	 i:0 	 global-step:2720	 l-p:0.11767663806676865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.0388, 3.0418, 3.0391],
        [3.0388, 3.1649, 3.1336],
        [3.0388, 3.0878, 3.0599],
        [3.0388, 3.3409, 3.3958]], grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.11756517738103867 
model_pd.l_d.mean(): -19.3394832611084 
model_pd.lagr.mean(): -19.2219181060791 
model_pd.lambdas: dict_items([('pout', tensor([1.1140])), ('power', tensor([0.8325]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7716])), ('power', tensor([-24.2276]))])
epoch：137	 i:0 	 global-step:2740	 l-p:0.11756517738103867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[3.0414, 3.0619, 3.0466],
        [3.0414, 3.5800, 3.8510],
        [3.0414, 3.4051, 3.5085],
        [3.0414, 3.0623, 3.0468]], grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.1174519807100296 
model_pd.l_d.mean(): -19.30965232849121 
model_pd.lagr.mean(): -19.19219970703125 
model_pd.lambdas: dict_items([('pout', tensor([1.1148])), ('power', tensor([0.8313]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7706])), ('power', tensor([-24.2264]))])
epoch：138	 i:0 	 global-step:2760	 l-p:0.1174519807100296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[3.0440, 3.0441, 3.0440],
        [3.0440, 3.2484, 3.2435],
        [3.0440, 3.0470, 3.0442],
        [3.0440, 3.7000, 4.1022]], grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.11733698099851608 
model_pd.l_d.mean(): -19.279829025268555 
model_pd.lagr.mean(): -19.162492752075195 
model_pd.lambdas: dict_items([('pout', tensor([1.1156])), ('power', tensor([0.8301]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7696])), ('power', tensor([-24.2252]))])
epoch：139	 i:0 	 global-step:2780	 l-p:0.11733698099851608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[3.0466, 3.1721, 3.1406],
        [3.0466, 3.4446, 3.5773],
        [3.0466, 3.0483, 3.0467],
        [3.0466, 3.2051, 3.1819]], grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.11722051352262497 
model_pd.l_d.mean(): -19.2500057220459 
model_pd.lagr.mean(): -19.13278579711914 
model_pd.lambdas: dict_items([('pout', tensor([1.1164])), ('power', tensor([0.8289]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7685])), ('power', tensor([-24.2239]))])
epoch：140	 i:0 	 global-step:2800	 l-p:0.11722051352262497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.0493, 3.0555, 3.0501],
        [3.0493, 3.0704, 3.0548],
        [3.0493, 3.1627, 3.1296],
        [3.0493, 3.4501, 3.5853]], grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.1171031966805458 
model_pd.l_d.mean(): -19.22018814086914 
model_pd.lagr.mean(): -19.103084564208984 
model_pd.lambdas: dict_items([('pout', tensor([1.1171])), ('power', tensor([0.8276]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7675])), ('power', tensor([-24.2226]))])
epoch：141	 i:0 	 global-step:2820	 l-p:0.1171031966805458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[3.0520, 3.2555, 3.2503],
        [3.0520, 3.0627, 3.0538],
        [3.0520, 3.0521, 3.0520],
        [3.0520, 3.0542, 3.0522]], grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.11698559671640396 
model_pd.l_d.mean(): -19.190378189086914 
model_pd.lagr.mean(): -19.073392868041992 
model_pd.lambdas: dict_items([('pout', tensor([1.1179])), ('power', tensor([0.8264]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7664])), ('power', tensor([-24.2213]))])
epoch：142	 i:0 	 global-step:2840	 l-p:0.11698559671640396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[3.0547, 3.0576, 3.0549],
        [3.0547, 3.0667, 3.0569],
        [3.0547, 3.4550, 3.5896],
        [3.0547, 3.0563, 3.0548]], grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.11686815321445465 
model_pd.l_d.mean(): -19.160568237304688 
model_pd.lagr.mean(): -19.043699264526367 
model_pd.lambdas: dict_items([('pout', tensor([1.1187])), ('power', tensor([0.8252]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7653])), ('power', tensor([-24.2200]))])
epoch：143	 i:0 	 global-step:2860	 l-p:0.11686815321445465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[3.0574, 3.0576, 3.0574],
        [3.0574, 3.0574, 3.0574],
        [3.0574, 3.0574, 3.0574],
        [3.0574, 3.0579, 3.0574]], grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.1167510524392128 
model_pd.l_d.mean(): -19.130769729614258 
model_pd.lagr.mean(): -19.014019012451172 
model_pd.lambdas: dict_items([('pout', tensor([1.1194])), ('power', tensor([0.8240]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7643])), ('power', tensor([-24.2187]))])
epoch：144	 i:0 	 global-step:2880	 l-p:0.1167510524392128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[3.0600, 3.0600, 3.0600],
        [3.0600, 3.0801, 3.0651],
        [3.0600, 3.7158, 4.1162],
        [3.0600, 3.1158, 3.0861]], grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.11663422733545303 
model_pd.l_d.mean(): -19.100967407226562 
model_pd.lagr.mean(): -18.984333038330078 
model_pd.lambdas: dict_items([('pout', tensor([1.1202])), ('power', tensor([0.8228]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7632])), ('power', tensor([-24.2173]))])
epoch：145	 i:0 	 global-step:2900	 l-p:0.11663422733545303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[3.0627, 3.0656, 3.0630],
        [3.0627, 3.0688, 3.0635],
        [3.0627, 3.0733, 3.0645],
        [3.0627, 3.4243, 3.5254]], grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.1165173128247261 
model_pd.l_d.mean(): -19.071176528930664 
model_pd.lagr.mean(): -18.95465850830078 
model_pd.lambdas: dict_items([('pout', tensor([1.1209])), ('power', tensor([0.8216]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7621])), ('power', tensor([-24.2160]))])
epoch：146	 i:0 	 global-step:2920	 l-p:0.1165173128247261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[3.0654, 3.6026, 3.8707],
        [3.0654, 3.0860, 3.0707],
        [3.0654, 3.3836, 3.4501],
        [3.0654, 3.0655, 3.0654]], grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.11639989167451859 
model_pd.l_d.mean(): -19.041383743286133 
model_pd.lagr.mean(): -18.924983978271484 
model_pd.lambdas: dict_items([('pout', tensor([1.1217])), ('power', tensor([0.8204]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7611])), ('power', tensor([-24.2147]))])
epoch：147	 i:0 	 global-step:2940	 l-p:0.11639989167451859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[3.0681, 3.2245, 3.2007],
        [3.0681, 3.7238, 4.1232],
        [3.0681, 3.1077, 3.0832],
        [3.0681, 3.1273, 3.0967]], grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.1162814125418663 
model_pd.l_d.mean(): -19.011598587036133 
model_pd.lagr.mean(): -18.89531707763672 
model_pd.lambdas: dict_items([('pout', tensor([1.1225])), ('power', tensor([0.8192]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7600])), ('power', tensor([-24.2134]))])
epoch：148	 i:0 	 global-step:2960	 l-p:0.1162814125418663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[3.0709, 3.0709, 3.0709],
        [3.0709, 3.0710, 3.0709],
        [3.0709, 3.0714, 3.0709],
        [3.0709, 3.0907, 3.0759]], grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.11616151034832001 
model_pd.l_d.mean(): -18.981815338134766 
model_pd.lagr.mean(): -18.86565399169922 
model_pd.lambdas: dict_items([('pout', tensor([1.1232])), ('power', tensor([0.8180]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7589])), ('power', tensor([-24.2120]))])
epoch：149	 i:0 	 global-step:2980	 l-p:0.11616151034832001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.0737, 3.2749, 3.2686],
        [3.0737, 3.3910, 3.4567],
        [3.0737, 3.0737, 3.0737],
        [3.0737, 3.4723, 3.6047]], grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.11603999137878418 
model_pd.l_d.mean(): -18.95203399658203 
model_pd.lagr.mean(): -18.835994720458984 
model_pd.lambdas: dict_items([('pout', tensor([1.1240])), ('power', tensor([0.8167]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7578])), ('power', tensor([-24.2106]))])
epoch：150	 i:0 	 global-step:3000	 l-p:0.11603999137878418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.0765, 3.0766, 3.0765],
        [3.0765, 3.0767, 3.0765],
        [3.0765, 3.0869, 3.0783],
        [3.0765, 3.1237, 3.0965]], grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.11591704189777374 
model_pd.l_d.mean(): -18.922260284423828 
model_pd.lagr.mean(): -18.80634307861328 
model_pd.lambdas: dict_items([('pout', tensor([1.1247])), ('power', tensor([0.8155]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7567])), ('power', tensor([-24.2092]))])
epoch：151	 i:0 	 global-step:3020	 l-p:0.11591704189777374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[3.0794, 3.7909, 4.2555],
        [3.0794, 3.2801, 3.2734],
        [3.0794, 3.2665, 3.2539],
        [3.0794, 3.1717, 3.1375]], grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.1157931387424469 
model_pd.l_d.mean(): -18.89249038696289 
model_pd.lagr.mean(): -18.776697158813477 
model_pd.lambdas: dict_items([('pout', tensor([1.1255])), ('power', tensor([0.8143]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7555])), ('power', tensor([-24.2078]))])
epoch：152	 i:0 	 global-step:3040	 l-p:0.1157931387424469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[3.0823, 3.0823, 3.0823],
        [3.0823, 3.0823, 3.0823],
        [3.0823, 3.3797, 3.4308],
        [3.0823, 3.1022, 3.0873]], grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.11566884815692902 
model_pd.l_d.mean(): -18.862722396850586 
model_pd.lagr.mean(): -18.747053146362305 
model_pd.lambdas: dict_items([('pout', tensor([1.1262])), ('power', tensor([0.8131]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7544])), ('power', tensor([-24.2063]))])
epoch：153	 i:0 	 global-step:3060	 l-p:0.11566884815692902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[3.0851, 3.6214, 3.8872],
        [3.0851, 3.1396, 3.1103],
        [3.0851, 3.0851, 3.0851],
        [3.0851, 3.7406, 4.1384]], grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.11554459482431412 
model_pd.l_d.mean(): -18.83296012878418 
model_pd.lagr.mean(): -18.71741485595703 
model_pd.lambdas: dict_items([('pout', tensor([1.1270])), ('power', tensor([0.8119]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7533])), ('power', tensor([-24.2049]))])
epoch：154	 i:0 	 global-step:3080	 l-p:0.11554459482431412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.0880, 3.4859, 3.6172],
        [3.0880, 3.1347, 3.1076],
        [3.0880, 3.3025, 3.3025],
        [3.0880, 3.1081, 3.0931]], grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.11542057991027832 
model_pd.l_d.mean(): -18.80320167541504 
model_pd.lagr.mean(): -18.687780380249023 
model_pd.lambdas: dict_items([('pout', tensor([1.1278])), ('power', tensor([0.8107]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7521])), ('power', tensor([-24.2034]))])
epoch：155	 i:0 	 global-step:3100	 l-p:0.11542057991027832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[3.0909, 3.2835, 3.2731],
        [3.0909, 3.3650, 3.3999],
        [3.0909, 3.1451, 3.1158],
        [3.0909, 3.0909, 3.0909]], grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.11529684066772461 
model_pd.l_d.mean(): -18.773452758789062 
model_pd.lagr.mean(): -18.65815544128418 
model_pd.lambdas: dict_items([('pout', tensor([1.1285])), ('power', tensor([0.8095]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7510])), ('power', tensor([-24.2020]))])
epoch：156	 i:0 	 global-step:3120	 l-p:0.11529684066772461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[3.0938, 3.0938, 3.0938],
        [3.0938, 3.4875, 3.6150],
        [3.0938, 3.6296, 3.8944],
        [3.0938, 3.1478, 3.1186]], grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.11517304182052612 
model_pd.l_d.mean(): -18.74370574951172 
model_pd.lagr.mean(): -18.62853240966797 
model_pd.lambdas: dict_items([('pout', tensor([1.1293])), ('power', tensor([0.8083]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7499])), ('power', tensor([-24.2005]))])
epoch：157	 i:0 	 global-step:3140	 l-p:0.11517304182052612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.0966, 3.0967, 3.0966],
        [3.0966, 3.1351, 3.1111],
        [3.0966, 3.1429, 3.1160],
        [3.0966, 3.1166, 3.1017]], grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.11504868417978287 
model_pd.l_d.mean(): -18.713960647583008 
model_pd.lagr.mean(): -18.59891128540039 
model_pd.lambdas: dict_items([('pout', tensor([1.1300])), ('power', tensor([0.8071]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7487])), ('power', tensor([-24.1991]))])
epoch：158	 i:0 	 global-step:3160	 l-p:0.11504868417978287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.0996, 3.0998, 3.0996],
        [3.0996, 3.2530, 3.2283],
        [3.0996, 3.3728, 3.4070],
        [3.0996, 3.0996, 3.0996]], grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.11492323875427246 
model_pd.l_d.mean(): -18.684219360351562 
model_pd.lagr.mean(): -18.56929588317871 
model_pd.lambdas: dict_items([('pout', tensor([1.1307])), ('power', tensor([0.8059]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7476])), ('power', tensor([-24.1976]))])
epoch：159	 i:0 	 global-step:3180	 l-p:0.11492323875427246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[3.1025, 3.2940, 3.2831],
        [3.1025, 3.1030, 3.1025],
        [3.1025, 3.1598, 3.1298],
        [3.1025, 3.6501, 3.9267]], grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.11479633301496506 
model_pd.l_d.mean(): -18.654489517211914 
model_pd.lagr.mean(): -18.53969383239746 
model_pd.lambdas: dict_items([('pout', tensor([1.1315])), ('power', tensor([0.8046]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7464])), ('power', tensor([-24.1961]))])
epoch：160	 i:0 	 global-step:3200	 l-p:0.11479633301496506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[3.1055, 3.7610, 4.1568],
        [3.1055, 3.1514, 3.1246],
        [3.1055, 3.1055, 3.1055],
        [3.1055, 3.1056, 3.1055]], grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.1146678701043129 
model_pd.l_d.mean(): -18.624753952026367 
model_pd.lagr.mean(): -18.510086059570312 
model_pd.lambdas: dict_items([('pout', tensor([1.1322])), ('power', tensor([0.8034]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7452])), ('power', tensor([-24.1946]))])
epoch：161	 i:0 	 global-step:3220	 l-p:0.1146678701043129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[3.1085, 3.3064, 3.2982],
        [3.1085, 3.1768, 3.1446],
        [3.1085, 3.1086, 3.1085],
        [3.1085, 3.1185, 3.1102]], grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.1145380437374115 
model_pd.l_d.mean(): -18.59502410888672 
model_pd.lagr.mean(): -18.480485916137695 
model_pd.lambdas: dict_items([('pout', tensor([1.1330])), ('power', tensor([0.8022]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7440])), ('power', tensor([-24.1930]))])
epoch：162	 i:0 	 global-step:3240	 l-p:0.1145380437374115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[3.1116, 3.1121, 3.1116],
        [3.1116, 3.1131, 3.1117],
        [3.1116, 3.2254, 3.1924],
        [3.1116, 3.1215, 3.1133]], grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.11440721154212952 
model_pd.l_d.mean(): -18.56529998779297 
model_pd.lagr.mean(): -18.45089340209961 
model_pd.lambdas: dict_items([('pout', tensor([1.1337])), ('power', tensor([0.8010]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7428])), ('power', tensor([-24.1914]))])
epoch：163	 i:0 	 global-step:3260	 l-p:0.11440721154212952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[3.1146, 3.1334, 3.1193],
        [3.1146, 3.2343, 3.2021],
        [3.1146, 3.2045, 3.1704],
        [3.1146, 3.2850, 3.2657]], grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.11427611112594604 
model_pd.l_d.mean(): -18.535579681396484 
model_pd.lagr.mean(): -18.421302795410156 
model_pd.lambdas: dict_items([('pout', tensor([1.1345])), ('power', tensor([0.7998]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7416])), ('power', tensor([-24.1898]))])
epoch：164	 i:0 	 global-step:3280	 l-p:0.11427611112594604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[3.1177, 3.7734, 4.1681],
        [3.1177, 3.5132, 3.6416],
        [3.1177, 3.1178, 3.1177],
        [3.1177, 3.8302, 4.2916]], grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.1141451820731163 
model_pd.l_d.mean(): -18.5058650970459 
model_pd.lagr.mean(): -18.391719818115234 
model_pd.lambdas: dict_items([('pout', tensor([1.1352])), ('power', tensor([0.7986]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7404])), ('power', tensor([-24.1882]))])
epoch：165	 i:0 	 global-step:3300	 l-p:0.1141451820731163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.1208, 3.5161, 3.6441],
        [3.1208, 3.1209, 3.1208],
        [3.1208, 3.1208, 3.1208],
        [3.1208, 3.1208, 3.1208]], grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.11401452869176865 
model_pd.l_d.mean(): -18.47615623474121 
model_pd.lagr.mean(): -18.36214256286621 
model_pd.lambdas: dict_items([('pout', tensor([1.1359])), ('power', tensor([0.7974]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7392])), ('power', tensor([-24.1867]))])
epoch：166	 i:0 	 global-step:3320	 l-p:0.11401452869176865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.1238, 3.1265, 3.1240],
        [3.1238, 3.1349, 3.1258],
        [3.1238, 3.6586, 3.9204],
        [3.1238, 3.1238, 3.1238]], grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.11388393491506577 
model_pd.l_d.mean(): -18.44645118713379 
model_pd.lagr.mean(): -18.33256721496582 
model_pd.lambdas: dict_items([('pout', tensor([1.1367])), ('power', tensor([0.7962]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7380])), ('power', tensor([-24.1851]))])
epoch：167	 i:0 	 global-step:3340	 l-p:0.11388393491506577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[3.1269, 3.2395, 3.2064],
        [3.1269, 3.1379, 3.1289],
        [3.1269, 3.1274, 3.1269],
        [3.1269, 3.3095, 3.2948]], grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.1137530505657196 
model_pd.l_d.mean(): -18.4167537689209 
model_pd.lagr.mean(): -18.303001403808594 
model_pd.lambdas: dict_items([('pout', tensor([1.1374])), ('power', tensor([0.7950]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7368])), ('power', tensor([-24.1835]))])
epoch：168	 i:0 	 global-step:3360	 l-p:0.1137530505657196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[3.1300, 3.1490, 3.1347],
        [3.1300, 3.4230, 3.4700],
        [3.1300, 3.1300, 3.1300],
        [3.1300, 3.1484, 3.1345]], grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.11362151056528091 
model_pd.l_d.mean(): -18.38705825805664 
model_pd.lagr.mean(): -18.2734375 
model_pd.lambdas: dict_items([('pout', tensor([1.1382])), ('power', tensor([0.7938]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7356])), ('power', tensor([-24.1818]))])
epoch：169	 i:0 	 global-step:3380	 l-p:0.11362151056528091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[3.1331, 3.2090, 3.1758],
        [3.1331, 3.1427, 3.1347],
        [3.1331, 3.1522, 3.1378],
        [3.1331, 3.4885, 3.5826]], grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.11348900198936462 
model_pd.l_d.mean(): -18.357370376586914 
model_pd.lagr.mean(): -18.243881225585938 
model_pd.lambdas: dict_items([('pout', tensor([1.1389])), ('power', tensor([0.7925]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7344])), ('power', tensor([-24.1802]))])
epoch：170	 i:0 	 global-step:3400	 l-p:0.11348900198936462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[3.1362, 3.2542, 3.2217],
        [3.1362, 3.2029, 3.1710],
        [3.1362, 3.2481, 3.2149],
        [3.1362, 3.1382, 3.1364]], grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.11335540562868118 
model_pd.l_d.mean(): -18.327682495117188 
model_pd.lagr.mean(): -18.214326858520508 
model_pd.lambdas: dict_items([('pout', tensor([1.1396])), ('power', tensor([0.7913]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7332])), ('power', tensor([-24.1786]))])
epoch：171	 i:0 	 global-step:3420	 l-p:0.11335540562868118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[3.1394, 3.1582, 3.1440],
        [3.1394, 3.1394, 3.1394],
        [3.1394, 3.2457, 3.2120],
        [3.1394, 3.4507, 3.5105]], grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.11322090774774551 
model_pd.l_d.mean(): -18.298002243041992 
model_pd.lagr.mean(): -18.184782028198242 
model_pd.lambdas: dict_items([('pout', tensor([1.1404])), ('power', tensor([0.7901]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7319])), ('power', tensor([-24.1769]))])
epoch：172	 i:0 	 global-step:3440	 l-p:0.11322090774774551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[3.1426, 3.1427, 3.1426],
        [3.1426, 3.1614, 3.1472],
        [3.1426, 3.1445, 3.1427],
        [3.1426, 3.1451, 3.1428]], grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.11308570951223373 
model_pd.l_d.mean(): -18.268325805664062 
model_pd.lagr.mean(): -18.155241012573242 
model_pd.lambdas: dict_items([('pout', tensor([1.1411])), ('power', tensor([0.7889]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7307])), ('power', tensor([-24.1752]))])
epoch：173	 i:0 	 global-step:3460	 l-p:0.11308570951223373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[3.1458, 3.1462, 3.1458],
        [3.1458, 3.2630, 3.2304],
        [3.1458, 3.8019, 4.1943],
        [3.1458, 3.8593, 4.3188]], grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.1129501461982727 
model_pd.l_d.mean(): -18.2386531829834 
model_pd.lagr.mean(): -18.125703811645508 
model_pd.lambdas: dict_items([('pout', tensor([1.1418])), ('power', tensor([0.7877]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7295])), ('power', tensor([-24.1735]))])
epoch：174	 i:0 	 global-step:3480	 l-p:0.1129501461982727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[3.1490, 3.3296, 3.3140],
        [3.1490, 3.1490, 3.1490],
        [3.1490, 3.1677, 3.1536],
        [3.1490, 3.2599, 3.2266]], grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.11281439661979675 
model_pd.l_d.mean(): -18.208988189697266 
model_pd.lagr.mean(): -18.096174240112305 
model_pd.lambdas: dict_items([('pout', tensor([1.1425])), ('power', tensor([0.7865]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7282])), ('power', tensor([-24.1718]))])
epoch：175	 i:0 	 global-step:3500	 l-p:0.11281439661979675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[3.1522, 3.1526, 3.1522],
        [3.1522, 3.4432, 3.4884],
        [3.1522, 3.5448, 3.6693],
        [3.1522, 3.1522, 3.1522]], grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.11267869919538498 
model_pd.l_d.mean(): -18.17932891845703 
model_pd.lagr.mean(): -18.066650390625 
model_pd.lambdas: dict_items([('pout', tensor([1.1433])), ('power', tensor([0.7853]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7270])), ('power', tensor([-24.1701]))])
epoch：176	 i:0 	 global-step:3520	 l-p:0.11267869919538498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[3.1554, 3.2425, 3.2084],
        [3.1554, 3.5091, 3.6011],
        [3.1554, 3.3420, 3.3287],
        [3.1554, 3.1554, 3.1554]], grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.11254287511110306 
model_pd.l_d.mean(): -18.14967155456543 
model_pd.lagr.mean(): -18.037128448486328 
model_pd.lambdas: dict_items([('pout', tensor([1.1440])), ('power', tensor([0.7841]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7257])), ('power', tensor([-24.1684]))])
epoch：177	 i:0 	 global-step:3540	 l-p:0.11254287511110306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[3.1586, 3.1770, 3.1631],
        [3.1586, 3.3664, 3.3625],
        [3.1586, 3.1605, 3.1587],
        [3.1586, 3.3519, 3.3413]], grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.11240669339895248 
model_pd.l_d.mean(): -18.12002182006836 
model_pd.lagr.mean(): -18.00761604309082 
model_pd.lambdas: dict_items([('pout', tensor([1.1447])), ('power', tensor([0.7829]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7245])), ('power', tensor([-24.1667]))])
epoch：178	 i:0 	 global-step:3560	 l-p:0.11240669339895248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[3.1618, 3.3694, 3.3653],
        [3.1618, 3.8761, 4.3344],
        [3.1618, 3.1618, 3.1618],
        [3.1618, 3.1618, 3.1618]], grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.11226984113454819 
model_pd.l_d.mean(): -18.090375900268555 
model_pd.lagr.mean(): -17.978105545043945 
model_pd.lambdas: dict_items([('pout', tensor([1.1454])), ('power', tensor([0.7817]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7232])), ('power', tensor([-24.1649]))])
epoch：179	 i:0 	 global-step:3580	 l-p:0.11226984113454819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.1651, 3.2391, 3.2061],
        [3.1651, 3.2515, 3.2175],
        [3.1651, 3.1676, 3.1653],
        [3.1651, 3.1651, 3.1651]], grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.11213211715221405 
model_pd.l_d.mean(): -18.06073570251465 
model_pd.lagr.mean(): -17.9486026763916 
model_pd.lambdas: dict_items([('pout', tensor([1.1462])), ('power', tensor([0.7805]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7219])), ('power', tensor([-24.1632]))])
epoch：180	 i:0 	 global-step:3600	 l-p:0.11213211715221405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.1684, 3.2332, 3.2017],
        [3.1684, 3.3160, 3.2892],
        [3.1684, 3.1776, 3.1699],
        [3.1684, 3.4581, 3.5020]], grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.11199333518743515 
model_pd.l_d.mean(): -18.031099319458008 
model_pd.lagr.mean(): -17.919105529785156 
model_pd.lambdas: dict_items([('pout', tensor([1.1469])), ('power', tensor([0.7793]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7207])), ('power', tensor([-24.1614]))])
epoch：181	 i:0 	 global-step:3620	 l-p:0.11199333518743515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[3.1717, 3.1808, 3.1732],
        [3.1717, 3.2147, 3.1890],
        [3.1717, 3.2073, 3.1845],
        [3.1717, 3.3190, 3.2922]], grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.11185364425182343 
model_pd.l_d.mean(): -18.001468658447266 
model_pd.lagr.mean(): -17.88961410522461 
model_pd.lambdas: dict_items([('pout', tensor([1.1476])), ('power', tensor([0.7780]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7194])), ('power', tensor([-24.1596]))])
epoch：182	 i:0 	 global-step:3640	 l-p:0.11185364425182343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[3.1750, 3.1775, 3.1752],
        [3.1750, 3.1750, 3.1750],
        [3.1750, 3.1750, 3.1750],
        [3.1750, 3.2251, 3.1971]], grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.11171320080757141 
model_pd.l_d.mean(): -17.971843719482422 
model_pd.lagr.mean(): -17.860130310058594 
model_pd.lambdas: dict_items([('pout', tensor([1.1483])), ('power', tensor([0.7768]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7181])), ('power', tensor([-24.1578]))])
epoch：183	 i:0 	 global-step:3660	 l-p:0.11171320080757141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[3.1783, 3.3251, 3.2981],
        [3.1783, 3.3699, 3.3583],
        [3.1783, 3.1963, 3.1827],
        [3.1783, 3.1802, 3.1785]], grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.1115722581744194 
model_pd.l_d.mean(): -17.94222068786621 
model_pd.lagr.mean(): -17.83064842224121 
model_pd.lambdas: dict_items([('pout', tensor([1.1490])), ('power', tensor([0.7756]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7168])), ('power', tensor([-24.1560]))])
epoch：184	 i:0 	 global-step:3680	 l-p:0.1115722581744194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[3.1817, 3.1818, 3.1817],
        [3.1817, 3.7151, 3.9714],
        [3.1817, 3.2547, 3.2218],
        [3.1817, 3.2962, 3.2632]], grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.11143097281455994 
model_pd.l_d.mean(): -17.91260528564453 
model_pd.lagr.mean(): -17.80117416381836 
model_pd.lambdas: dict_items([('pout', tensor([1.1498])), ('power', tensor([0.7744]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7155])), ('power', tensor([-24.1542]))])
epoch：185	 i:0 	 global-step:3700	 l-p:0.11143097281455994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[3.1851, 3.4927, 3.5486],
        [3.1851, 3.2030, 3.1894],
        [3.1851, 3.2881, 3.2541],
        [3.1851, 3.2933, 3.2596]], grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.11128917336463928 
model_pd.l_d.mean(): -17.88299560546875 
model_pd.lagr.mean(): -17.771705627441406 
model_pd.lambdas: dict_items([('pout', tensor([1.1505])), ('power', tensor([0.7732]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7142])), ('power', tensor([-24.1524]))])
epoch：186	 i:0 	 global-step:3720	 l-p:0.11128917336463928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[3.1884, 3.1885, 3.1884],
        [3.1884, 3.5756, 3.6939],
        [3.1884, 3.3344, 3.3071],
        [3.1884, 3.3657, 3.3483]], grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.11114688217639923 
model_pd.l_d.mean(): -17.853391647338867 
model_pd.lagr.mean(): -17.742244720458984 
model_pd.lambdas: dict_items([('pout', tensor([1.1512])), ('power', tensor([0.7720]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7129])), ('power', tensor([-24.1505]))])
epoch：187	 i:0 	 global-step:3740	 l-p:0.11114688217639923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[3.1918, 3.1918, 3.1918],
        [3.1918, 3.4797, 3.5217],
        [3.1918, 3.2096, 3.1961],
        [3.1918, 3.3968, 3.3910]], grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.11100387573242188 
model_pd.l_d.mean(): -17.823789596557617 
model_pd.lagr.mean(): -17.712785720825195 
model_pd.lambdas: dict_items([('pout', tensor([1.1519])), ('power', tensor([0.7708]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7116])), ('power', tensor([-24.1487]))])
epoch：188	 i:0 	 global-step:3760	 l-p:0.11100387573242188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[3.1952, 3.8527, 4.2413],
        [3.1952, 3.1953, 3.1952],
        [3.1952, 3.2129, 3.1995],
        [3.1952, 3.2041, 3.1967]], grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.11085999757051468 
model_pd.l_d.mean(): -17.7941951751709 
model_pd.lagr.mean(): -17.683334350585938 
model_pd.lambdas: dict_items([('pout', tensor([1.1526])), ('power', tensor([0.7696]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7103])), ('power', tensor([-24.1468]))])
epoch：189	 i:0 	 global-step:3780	 l-p:0.11085999757051468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.1987, 3.4632, 3.4899],
        [3.1987, 3.2087, 3.2004],
        [3.1987, 3.5494, 3.6374],
        [3.1987, 3.5885, 3.7085]], grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.11071518063545227 
model_pd.l_d.mean(): -17.764604568481445 
model_pd.lagr.mean(): -17.653888702392578 
model_pd.lambdas: dict_items([('pout', tensor([1.1533])), ('power', tensor([0.7684]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7090])), ('power', tensor([-24.1449]))])
epoch：190	 i:0 	 global-step:3800	 l-p:0.11071518063545227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[3.2021, 3.3152, 3.2818],
        [3.2021, 3.2021, 3.2021],
        [3.2021, 3.2039, 3.2022],
        [3.2021, 3.3848, 3.3693]], grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.11056928336620331 
model_pd.l_d.mean(): -17.73501968383789 
model_pd.lagr.mean(): -17.62445068359375 
model_pd.lambdas: dict_items([('pout', tensor([1.1540])), ('power', tensor([0.7672]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7076])), ('power', tensor([-24.1430]))])
epoch：191	 i:0 	 global-step:3820	 l-p:0.11056928336620331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[3.2056, 3.2772, 3.2445],
        [3.2056, 3.2056, 3.2056],
        [3.2056, 3.2057, 3.2056],
        [3.2056, 3.2230, 3.2097]], grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.11042243987321854 
model_pd.l_d.mean(): -17.7054386138916 
model_pd.lagr.mean(): -17.595016479492188 
model_pd.lambdas: dict_items([('pout', tensor([1.1547])), ('power', tensor([0.7660]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7063])), ('power', tensor([-24.1411]))])
epoch：192	 i:0 	 global-step:3840	 l-p:0.11042243987321854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.2091, 3.2092, 3.2091],
        [3.2091, 3.2263, 3.2132],
        [3.2091, 3.2718, 3.2406],
        [3.2091, 3.2091, 3.2091]], grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.11027474701404572 
model_pd.l_d.mean(): -17.675867080688477 
model_pd.lagr.mean(): -17.56559181213379 
model_pd.lambdas: dict_items([('pout', tensor([1.1554])), ('power', tensor([0.7648]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7050])), ('power', tensor([-24.1391]))])
epoch：193	 i:0 	 global-step:3860	 l-p:0.11027474701404572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[3.2126, 3.3747, 3.3517],
        [3.2126, 3.2128, 3.2126],
        [3.2126, 3.2539, 3.2289],
        [3.2126, 3.4159, 3.4090]], grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.11012613028287888 
model_pd.l_d.mean(): -17.646297454833984 
model_pd.lagr.mean(): -17.536170959472656 
model_pd.lambdas: dict_items([('pout', tensor([1.1561])), ('power', tensor([0.7636]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7036])), ('power', tensor([-24.1372]))])
epoch：194	 i:0 	 global-step:3880	 l-p:0.11012613028287888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[3.2162, 3.7492, 4.0024],
        [3.2162, 3.2501, 3.2281],
        [3.2162, 3.2162, 3.2162],
        [3.2162, 3.2163, 3.2162]], grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.10997674614191055 
model_pd.l_d.mean(): -17.616731643676758 
model_pd.lagr.mean(): -17.506755828857422 
model_pd.lambdas: dict_items([('pout', tensor([1.1568])), ('power', tensor([0.7624]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7023])), ('power', tensor([-24.1352]))])
epoch：195	 i:0 	 global-step:3900	 l-p:0.10997674614191055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[3.2197, 3.2366, 3.2237],
        [3.2197, 3.2197, 3.2197],
        [3.2197, 3.7950, 4.0916],
        [3.2197, 3.3812, 3.3580]], grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.10982643812894821 
model_pd.l_d.mean(): -17.587173461914062 
model_pd.lagr.mean(): -17.477346420288086 
model_pd.lambdas: dict_items([('pout', tensor([1.1575])), ('power', tensor([0.7611]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7009])), ('power', tensor([-24.1332]))])
epoch：196	 i:0 	 global-step:3920	 l-p:0.10982643812894821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.2233, 3.2403, 3.2273],
        [3.2233, 3.8818, 4.2686],
        [3.2233, 3.7986, 4.0949],
        [3.2233, 3.6119, 3.7297]], grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.10967504233121872 
model_pd.l_d.mean(): -17.5576171875 
model_pd.lagr.mean(): -17.44794273376465 
model_pd.lambdas: dict_items([('pout', tensor([1.1582])), ('power', tensor([0.7599]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6995])), ('power', tensor([-24.1312]))])
epoch：197	 i:0 	 global-step:3940	 l-p:0.10967504233121872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[3.2269, 3.8856, 4.2721],
        [3.2269, 3.3382, 3.3046],
        [3.2269, 3.7599, 4.0122],
        [3.2269, 3.6120, 3.7268]], grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.10952230542898178 
model_pd.l_d.mean(): -17.528074264526367 
model_pd.lagr.mean(): -17.41855239868164 
model_pd.lambdas: dict_items([('pout', tensor([1.1589])), ('power', tensor([0.7587]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6981])), ('power', tensor([-24.1292]))])
epoch：198	 i:0 	 global-step:3960	 l-p:0.10952230542898178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.2306, 3.6193, 3.7369],
        [3.2306, 3.2475, 3.2345],
        [3.2306, 3.2306, 3.2306],
        [3.2306, 3.6188, 3.7359]], grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.10936814546585083 
model_pd.l_d.mean(): -17.4985294342041 
model_pd.lagr.mean(): -17.389162063598633 
model_pd.lambdas: dict_items([('pout', tensor([1.1596])), ('power', tensor([0.7575]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6968])), ('power', tensor([-24.1271]))])
epoch：199	 i:0 	 global-step:3980	 l-p:0.10936814546585083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[3.2342, 3.9528, 4.4069],
        [3.2342, 3.6190, 3.7331],
        [3.2342, 3.6223, 3.7391],
        [3.2342, 3.2747, 3.2499]], grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.10921235382556915 
model_pd.l_d.mean(): -17.468990325927734 
model_pd.lagr.mean(): -17.359777450561523 
model_pd.lambdas: dict_items([('pout', tensor([1.1603])), ('power', tensor([0.7563]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6953])), ('power', tensor([-24.1250]))])
epoch：200	 i:0 	 global-step:4000	 l-p:0.10921235382556915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[3.2379, 3.2545, 3.2418],
        [3.2379, 3.2886, 3.2603],
        [3.2379, 3.2379, 3.2379],
        [3.2379, 3.3374, 3.3030]], grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.10905510187149048 
model_pd.l_d.mean(): -17.439456939697266 
model_pd.lagr.mean(): -17.330402374267578 
model_pd.lambdas: dict_items([('pout', tensor([1.1610])), ('power', tensor([0.7551]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6939])), ('power', tensor([-24.1229]))])
epoch：201	 i:0 	 global-step:4020	 l-p:0.10905510187149048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[3.2417, 3.4428, 3.4343],
        [3.2417, 3.2417, 3.2417],
        [3.2417, 3.2439, 3.2418],
        [3.2417, 3.2511, 3.2433]], grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.10889632999897003 
model_pd.l_d.mean(): -17.409927368164062 
model_pd.lagr.mean(): -17.3010311126709 
model_pd.lambdas: dict_items([('pout', tensor([1.1617])), ('power', tensor([0.7539]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6925])), ('power', tensor([-24.1208]))])
epoch：202	 i:0 	 global-step:4040	 l-p:0.10889632999897003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[3.2454, 3.2615, 3.2491],
        [3.2454, 3.2456, 3.2454],
        [3.2454, 3.2537, 3.2467],
        [3.2454, 3.5935, 3.6775]], grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.10873617231845856 
model_pd.l_d.mean(): -17.380401611328125 
model_pd.lagr.mean(): -17.271665573120117 
model_pd.lambdas: dict_items([('pout', tensor([1.1624])), ('power', tensor([0.7527]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6911])), ('power', tensor([-24.1186]))])
epoch：203	 i:0 	 global-step:4060	 l-p:0.10873617231845856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.2492, 3.6372, 3.7532],
        [3.2492, 3.3531, 3.3188],
        [3.2492, 3.2514, 3.2494],
        [3.2492, 3.2492, 3.2492]], grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.10857465863227844 
model_pd.l_d.mean(): -17.35088539123535 
model_pd.lagr.mean(): -17.242311477661133 
model_pd.lambdas: dict_items([('pout', tensor([1.1631])), ('power', tensor([0.7515]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6896])), ('power', tensor([-24.1165]))])
epoch：204	 i:0 	 global-step:4080	 l-p:0.10857465863227844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[3.2530, 3.5367, 3.5740],
        [3.2530, 3.3030, 3.2750],
        [3.2530, 3.6404, 3.7556],
        [3.2530, 3.3516, 3.3170]], grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.10841172188520432 
model_pd.l_d.mean(): -17.32137107849121 
model_pd.lagr.mean(): -17.21295928955078 
model_pd.lambdas: dict_items([('pout', tensor([1.1638])), ('power', tensor([0.7503]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6882])), ('power', tensor([-24.1143]))])
epoch：205	 i:0 	 global-step:4100	 l-p:0.10841172188520432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[3.2569, 3.2569, 3.2569],
        [3.2569, 3.2569, 3.2569],
        [3.2569, 3.4290, 3.4087],
        [3.2569, 3.3377, 3.3037]], grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.10824724286794662 
model_pd.l_d.mean(): -17.29186248779297 
model_pd.lagr.mean(): -17.18361473083496 
model_pd.lambdas: dict_items([('pout', tensor([1.1645])), ('power', tensor([0.7491]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6867])), ('power', tensor([-24.1121]))])
epoch：206	 i:0 	 global-step:4120	 l-p:0.10824724286794662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[3.2608, 3.2608, 3.2608],
        [3.2608, 3.6446, 3.7564],
        [3.2608, 3.8068, 4.0694],
        [3.2608, 3.4391, 3.4210]], grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.10808097571134567 
model_pd.l_d.mean(): -17.262359619140625 
model_pd.lagr.mean(): -17.154277801513672 
model_pd.lambdas: dict_items([('pout', tensor([1.1652])), ('power', tensor([0.7479]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6852])), ('power', tensor([-24.1098]))])
epoch：207	 i:0 	 global-step:4140	 l-p:0.10808097571134567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[3.2647, 3.2659, 3.2648],
        [3.2647, 3.3451, 3.3111],
        [3.2647, 3.2651, 3.2647],
        [3.2647, 3.5672, 3.6168]], grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.1079128161072731 
model_pd.l_d.mean(): -17.232858657836914 
model_pd.lagr.mean(): -17.12494659423828 
model_pd.lambdas: dict_items([('pout', tensor([1.1658])), ('power', tensor([0.7467]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6837])), ('power', tensor([-24.1075]))])
epoch：208	 i:0 	 global-step:4160	 l-p:0.1079128161072731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[3.2687, 3.2687, 3.2687],
        [3.2687, 3.3284, 3.2977],
        [3.2687, 3.5284, 3.5502],
        [3.2687, 3.2687, 3.2687]], grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.10774251818656921 
model_pd.l_d.mean(): -17.203365325927734 
model_pd.lagr.mean(): -17.095623016357422 
model_pd.lambdas: dict_items([('pout', tensor([1.1665])), ('power', tensor([0.7455]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6822])), ('power', tensor([-24.1052]))])
epoch：209	 i:0 	 global-step:4180	 l-p:0.10774251818656921
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[3.2727, 3.2772, 3.2732],
        [3.2727, 3.2727, 3.2727],
        [3.2727, 3.2748, 3.2728],
        [3.2727, 3.8190, 4.0808]], grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.10757000744342804 
model_pd.l_d.mean(): -17.17387580871582 
model_pd.lagr.mean(): -17.06630516052246 
model_pd.lambdas: dict_items([('pout', tensor([1.1672])), ('power', tensor([0.7443]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6807])), ('power', tensor([-24.1029]))])
epoch：210	 i:0 	 global-step:4200	 l-p:0.10757000744342804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.2767, 3.4342, 3.4090],
        [3.2767, 3.2771, 3.2767],
        [3.2767, 3.2767, 3.2767],
        [3.2767, 3.2767, 3.2767]], grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.1073952466249466 
model_pd.l_d.mean(): -17.144392013549805 
model_pd.lagr.mean(): -17.036996841430664 
model_pd.lambdas: dict_items([('pout', tensor([1.1679])), ('power', tensor([0.7431]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6792])), ('power', tensor([-24.1005]))])
epoch：211	 i:0 	 global-step:4220	 l-p:0.1073952466249466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.2808, 3.8274, 4.0886],
        [3.2808, 3.2887, 3.2820],
        [3.2808, 3.2808, 3.2808],
        [3.2808, 3.3196, 3.2954]], grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.1072181984782219 
model_pd.l_d.mean(): -17.114912033081055 
model_pd.lagr.mean(): -17.007694244384766 
model_pd.lambdas: dict_items([('pout', tensor([1.1686])), ('power', tensor([0.7419]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6776])), ('power', tensor([-24.0981]))])
epoch：212	 i:0 	 global-step:4240	 l-p:0.1072181984782219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.2849, 3.3006, 3.2885],
        [3.2849, 3.6681, 3.7781],
        [3.2849, 3.2865, 3.2850],
        [3.2849, 3.2849, 3.2849]], grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.10703882575035095 
model_pd.l_d.mean(): -17.085439682006836 
model_pd.lagr.mean(): -16.97840118408203 
model_pd.lambdas: dict_items([('pout', tensor([1.1692])), ('power', tensor([0.7406]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6760])), ('power', tensor([-24.0957]))])
epoch：213	 i:0 	 global-step:4260	 l-p:0.10703882575035095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[3.2891, 3.4281, 3.3979],
        [3.2891, 3.3047, 3.2926],
        [3.2891, 3.8359, 4.0967],
        [3.2891, 3.4657, 3.4464]], grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.10685709118843079 
model_pd.l_d.mean(): -17.05596923828125 
model_pd.lagr.mean(): -16.949111938476562 
model_pd.lambdas: dict_items([('pout', tensor([1.1699])), ('power', tensor([0.7394]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6745])), ('power', tensor([-24.0932]))])
epoch：214	 i:0 	 global-step:4280	 l-p:0.10685709118843079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[3.2933, 3.5518, 3.5720],
        [3.2933, 3.2933, 3.2933],
        [3.2933, 3.6796, 3.7918],
        [3.2933, 3.3090, 3.2969]], grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.10667280852794647 
model_pd.l_d.mean(): -17.026504516601562 
model_pd.lagr.mean(): -16.919832229614258 
model_pd.lambdas: dict_items([('pout', tensor([1.1706])), ('power', tensor([0.7382]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6729])), ('power', tensor([-24.0907]))])
epoch：215	 i:0 	 global-step:4300	 l-p:0.10667280852794647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[3.2976, 3.2992, 3.2977],
        [3.2976, 3.2977, 3.2976],
        [3.2976, 3.2976, 3.2976],
        [3.2976, 3.4361, 3.4057]], grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.10648582130670547 
model_pd.l_d.mean(): -16.99704360961914 
model_pd.lagr.mean(): -16.89055824279785 
model_pd.lambdas: dict_items([('pout', tensor([1.1713])), ('power', tensor([0.7370]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6713])), ('power', tensor([-24.0881]))])
epoch：216	 i:0 	 global-step:4320	 l-p:0.10648582130670547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[3.3019, 3.3020, 3.3019],
        [3.3019, 3.6848, 3.7936],
        [3.3019, 3.3107, 3.3034],
        [3.3019, 3.6028, 3.6498]], grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.10629577934741974 
model_pd.l_d.mean(): -16.967586517333984 
model_pd.lagr.mean(): -16.861289978027344 
model_pd.lambdas: dict_items([('pout', tensor([1.1719])), ('power', tensor([0.7358]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6696])), ('power', tensor([-24.0855]))])
epoch：217	 i:0 	 global-step:4340	 l-p:0.10629577934741974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.3063, 3.5642, 3.5837],
        [3.3063, 3.3063, 3.3063],
        [3.3063, 3.3107, 3.3068],
        [3.3063, 3.5874, 3.6211]], grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.10610244423151016 
model_pd.l_d.mean(): -16.938135147094727 
model_pd.lagr.mean(): -16.832033157348633 
model_pd.lambdas: dict_items([('pout', tensor([1.1726])), ('power', tensor([0.7346]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6680])), ('power', tensor([-24.0829]))])
epoch：218	 i:0 	 global-step:4360	 l-p:0.10610244423151016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.3108, 3.3108, 3.3108],
        [3.3108, 3.4862, 3.4660],
        [3.3108, 3.3108, 3.3108],
        [3.3108, 3.5917, 3.6251]], grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.10590555518865585 
model_pd.l_d.mean(): -16.90869140625 
model_pd.lagr.mean(): -16.802785873413086 
model_pd.lambdas: dict_items([('pout', tensor([1.1733])), ('power', tensor([0.7334]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6663])), ('power', tensor([-24.0802]))])
epoch：219	 i:0 	 global-step:4380	 l-p:0.10590555518865585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.3153, 3.4840, 3.4614],
        [3.3153, 3.4528, 3.4220],
        [3.3153, 3.4105, 3.3753],
        [3.3153, 3.3153, 3.3153]], grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.10570478439331055 
model_pd.l_d.mean(): -16.87924575805664 
model_pd.lagr.mean(): -16.773540496826172 
model_pd.lambdas: dict_items([('pout', tensor([1.1739])), ('power', tensor([0.7322]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6646])), ('power', tensor([-24.0775]))])
epoch：220	 i:0 	 global-step:4400	 l-p:0.10570478439331055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.3198, 3.8681, 4.1274],
        [3.3198, 3.4149, 3.3797],
        [3.3198, 3.3352, 3.3232],
        [3.3198, 3.3574, 3.3336]], grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.1055000051856041 
model_pd.l_d.mean(): -16.849807739257812 
model_pd.lagr.mean(): -16.744308471679688 
model_pd.lambdas: dict_items([('pout', tensor([1.1746])), ('power', tensor([0.7310]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6628])), ('power', tensor([-24.0747]))])
epoch：221	 i:0 	 global-step:4420	 l-p:0.1055000051856041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[3.3245, 3.6051, 3.6377],
        [3.3245, 3.3903, 3.3580],
        [3.3245, 3.7072, 3.8146],
        [3.3245, 3.7111, 3.8217]], grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.10529091209173203 
model_pd.l_d.mean(): -16.82037353515625 
model_pd.lagr.mean(): -16.7150821685791 
model_pd.lambdas: dict_items([('pout', tensor([1.1753])), ('power', tensor([0.7298]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6611])), ('power', tensor([-24.0719]))])
epoch：222	 i:0 	 global-step:4440	 l-p:0.10529091209173203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[3.3292, 3.3598, 3.3392],
        [3.3292, 3.3292, 3.3292],
        [3.3292, 3.3312, 3.3294],
        [3.3292, 3.9089, 4.1993]], grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.10507750511169434 
model_pd.l_d.mean(): -16.79094123840332 
model_pd.lagr.mean(): -16.685863494873047 
model_pd.lambdas: dict_items([('pout', tensor([1.1759])), ('power', tensor([0.7286]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6593])), ('power', tensor([-24.0690]))])
epoch：223	 i:0 	 global-step:4460	 l-p:0.10507750511169434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[3.3339, 3.3489, 3.3372],
        [3.3339, 3.3414, 3.3351],
        [3.3339, 3.3490, 3.3372],
        [3.3339, 4.0008, 4.3834]], grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.10486268252134323 
model_pd.l_d.mean(): -16.76150894165039 
model_pd.lagr.mean(): -16.656646728515625 
model_pd.lambdas: dict_items([('pout', tensor([1.1766])), ('power', tensor([0.7274]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6575])), ('power', tensor([-24.0660]))])
epoch：224	 i:0 	 global-step:4480	 l-p:0.10486268252134323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[3.3387, 3.3471, 3.3400],
        [3.3387, 3.4752, 3.4438],
        [3.3387, 3.7215, 3.8281],
        [3.3387, 3.4439, 3.4086]], grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.10464729368686676 
model_pd.l_d.mean(): -16.732093811035156 
model_pd.lagr.mean(): -16.6274471282959 
model_pd.lambdas: dict_items([('pout', tensor([1.1772])), ('power', tensor([0.7262]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6557])), ('power', tensor([-24.0631]))])
epoch：225	 i:0 	 global-step:4500	 l-p:0.10464729368686676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[3.3434, 3.3869, 3.3608],
        [3.3434, 3.3803, 3.3568],
        [3.3434, 3.3517, 3.3447],
        [3.3434, 3.4203, 3.3861]], grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.10443177074193954 
model_pd.l_d.mean(): -16.702678680419922 
model_pd.lagr.mean(): -16.598247528076172 
model_pd.lambdas: dict_items([('pout', tensor([1.1779])), ('power', tensor([0.7250]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6540])), ('power', tensor([-24.0602]))])
epoch：226	 i:0 	 global-step:4520	 l-p:0.10443177074193954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[3.3481, 3.3500, 3.3482],
        [3.3481, 3.7310, 3.8371],
        [3.3481, 3.3522, 3.3485],
        [3.3481, 3.5289, 3.5098]], grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.10421493649482727 
model_pd.l_d.mean(): -16.67327117919922 
model_pd.lagr.mean(): -16.569055557250977 
model_pd.lambdas: dict_items([('pout', tensor([1.1785])), ('power', tensor([0.7238]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6522])), ('power', tensor([-24.0573]))])
epoch：227	 i:0 	 global-step:4540	 l-p:0.10421493649482727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[3.3529, 3.3612, 3.3542],
        [3.3529, 3.3570, 3.3533],
        [3.3529, 4.0219, 4.4044],
        [3.3529, 3.3602, 3.3539]], grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.10399267077445984 
model_pd.l_d.mean(): -16.64386749267578 
model_pd.lagr.mean(): -16.539875030517578 
model_pd.lambdas: dict_items([('pout', tensor([1.1792])), ('power', tensor([0.7226]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6504])), ('power', tensor([-24.0543]))])
epoch：228	 i:0 	 global-step:4560	 l-p:0.10399267077445984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.3579, 3.3726, 3.3610],
        [3.3579, 3.3579, 3.3579],
        [3.3579, 3.3725, 3.3610],
        [3.3579, 3.7443, 3.8527]], grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.1037597581744194 
model_pd.l_d.mean(): -16.614458084106445 
model_pd.lagr.mean(): -16.510698318481445 
model_pd.lambdas: dict_items([('pout', tensor([1.1798])), ('power', tensor([0.7214]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6485])), ('power', tensor([-24.0512]))])
epoch：229	 i:0 	 global-step:4580	 l-p:0.1037597581744194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[3.3631, 3.7498, 3.8579],
        [3.3631, 3.6431, 3.6737],
        [3.3631, 3.4989, 3.4668],
        [3.3631, 3.3634, 3.3631]], grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.1035117655992508 
model_pd.l_d.mean(): -16.585044860839844 
model_pd.lagr.mean(): -16.48153305053711 
model_pd.lambdas: dict_items([('pout', tensor([1.1805])), ('power', tensor([0.7202]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6466])), ('power', tensor([-24.0478]))])
epoch：230	 i:0 	 global-step:4600	 l-p:0.1035117655992508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[3.3686, 3.3985, 3.3781],
        [3.3686, 3.7561, 3.8644],
        [3.3686, 3.5491, 3.5293],
        [3.3686, 3.3832, 3.3717]], grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.10324615240097046 
model_pd.l_d.mean(): -16.555622100830078 
model_pd.lagr.mean(): -16.452375411987305 
model_pd.lambdas: dict_items([('pout', tensor([1.1811])), ('power', tensor([0.7190]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6445])), ('power', tensor([-24.0443]))])
epoch：231	 i:0 	 global-step:4620	 l-p:0.10324615240097046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[3.3745, 3.3745, 3.3745],
        [3.3745, 3.3745, 3.3745],
        [3.3745, 3.4507, 3.4162],
        [3.3745, 3.7208, 3.7965]], grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.10296255350112915 
model_pd.l_d.mean(): -16.526193618774414 
model_pd.lagr.mean(): -16.42323112487793 
model_pd.lambdas: dict_items([('pout', tensor([1.1818])), ('power', tensor([0.7178]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6422])), ('power', tensor([-24.0405]))])
epoch：232	 i:0 	 global-step:4640	 l-p:0.10296255350112915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.3806, 3.7648, 3.8697],
        [3.3806, 3.4235, 3.3974],
        [3.3806, 3.4367, 3.4063],
        [3.3806, 3.3806, 3.3806]], grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.10266178846359253 
model_pd.l_d.mean(): -16.49675941467285 
model_pd.lagr.mean(): -16.39409828186035 
model_pd.lambdas: dict_items([('pout', tensor([1.1824])), ('power', tensor([0.7166]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6399])), ('power', tensor([-24.0365]))])
epoch：233	 i:0 	 global-step:4660	 l-p:0.10266178846359253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[3.3869, 3.4329, 3.4057],
        [3.3869, 3.4010, 3.3899],
        [3.3869, 3.4166, 3.3963],
        [3.3869, 3.3870, 3.3869]], grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.10234656184911728 
model_pd.l_d.mean(): -16.467317581176758 
model_pd.lagr.mean(): -16.364971160888672 
model_pd.lambdas: dict_items([('pout', tensor([1.1830])), ('power', tensor([0.7154]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6375])), ('power', tensor([-24.0323]))])
epoch：234	 i:0 	 global-step:4680	 l-p:0.10234656184911728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[3.3933, 3.4865, 3.4505],
        [3.3933, 3.4393, 3.4120],
        [3.3933, 3.6943, 3.7369],
        [3.3933, 3.3933, 3.3933]], grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.10202520340681076 
model_pd.l_d.mean(): -16.437877655029297 
model_pd.lagr.mean(): -16.335851669311523 
model_pd.lambdas: dict_items([('pout', tensor([1.1837])), ('power', tensor([0.7142]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6351])), ('power', tensor([-24.0281]))])
epoch：235	 i:0 	 global-step:4700	 l-p:0.10202520340681076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[3.3998, 3.4457, 3.4184],
        [3.3998, 3.3998, 3.3998],
        [3.3998, 3.4069, 3.4008],
        [3.3998, 3.4293, 3.4091]], grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.10169679671525955 
model_pd.l_d.mean(): -16.408443450927734 
model_pd.lagr.mean(): -16.306747436523438 
model_pd.lambdas: dict_items([('pout', tensor([1.1843])), ('power', tensor([0.7130]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6327])), ('power', tensor([-24.0239]))])
epoch：236	 i:0 	 global-step:4720	 l-p:0.10169679671525955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[3.4064, 3.4425, 3.4191],
        [3.4064, 3.6878, 3.7172],
        [3.4064, 4.0841, 4.4689],
        [3.4064, 3.4104, 3.4068]], grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.10135423392057419 
model_pd.l_d.mean(): -16.37900161743164 
model_pd.lagr.mean(): -16.277647018432617 
model_pd.lambdas: dict_items([('pout', tensor([1.1849])), ('power', tensor([0.7118]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6302])), ('power', tensor([-24.0195]))])
epoch：237	 i:0 	 global-step:4740	 l-p:0.10135423392057419
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[3.4134, 4.0924, 4.4778],
        [3.4134, 3.4134, 3.4134],
        [3.4134, 3.4277, 3.4164],
        [3.4134, 3.4148, 3.4135]], grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.10098543763160706 
model_pd.l_d.mean(): -16.349552154541016 
model_pd.lagr.mean(): -16.248567581176758 
model_pd.lambdas: dict_items([('pout', tensor([1.1856])), ('power', tensor([0.7106]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6275])), ('power', tensor([-24.0148]))])
epoch：238	 i:0 	 global-step:4760	 l-p:0.10098543763160706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[3.4210, 3.7033, 3.7324],
        [3.4210, 3.5751, 3.5457],
        [3.4210, 3.4213, 3.4210],
        [3.4210, 3.4349, 3.4238]], grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.10057586431503296 
model_pd.l_d.mean(): -16.320070266723633 
model_pd.lagr.mean(): -16.219493865966797 
model_pd.lambdas: dict_items([('pout', tensor([1.1862])), ('power', tensor([0.7094]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6247])), ('power', tensor([-24.0096]))])
epoch：239	 i:0 	 global-step:4780	 l-p:0.10057586431503296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[3.4294, 3.4434, 3.4323],
        [3.4294, 3.4294, 3.4294],
        [3.4294, 3.4294, 3.4294],
        [3.4294, 3.5056, 3.4706]], grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.10010838508605957 
model_pd.l_d.mean(): -16.29054832458496 
model_pd.lagr.mean(): -16.190439224243164 
model_pd.lambdas: dict_items([('pout', tensor([1.1868])), ('power', tensor([0.7082]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6214])), ('power', tensor([-24.0038]))])
epoch：240	 i:0 	 global-step:4800	 l-p:0.10010838508605957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.4390, 4.0335, 4.3262],
        [3.4390, 3.5153, 3.4803],
        [3.4390, 3.7903, 3.8655],
        [3.4390, 3.4390, 3.4390]], grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.09956230968236923 
model_pd.l_d.mean(): -16.260971069335938 
model_pd.lagr.mean(): -16.161409378051758 
model_pd.lambdas: dict_items([('pout', tensor([1.1874])), ('power', tensor([0.7070]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6178])), ('power', tensor([-23.9972]))])
epoch：241	 i:0 	 global-step:4820	 l-p:0.09956230968236923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[3.4501, 3.8028, 3.8782],
        [3.4501, 3.5492, 3.5123],
        [3.4501, 3.7350, 3.7641],
        [3.4501, 3.7555, 3.7978]], grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.09890859574079514 
model_pd.l_d.mean(): -16.231304168701172 
model_pd.lagr.mean(): -16.132394790649414 
model_pd.lambdas: dict_items([('pout', tensor([1.1880])), ('power', tensor([0.7058]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6136])), ('power', tensor([-23.9894]))])
epoch：242	 i:0 	 global-step:4840	 l-p:0.09890859574079514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[3.4633, 3.7254, 3.7402],
        [3.4633, 3.8606, 3.9695],
        [3.4633, 3.4713, 3.4645],
        [3.4633, 3.5198, 3.4888]], grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.09809844940900803 
model_pd.l_d.mean(): -16.201513290405273 
model_pd.lagr.mean(): -16.10341453552246 
model_pd.lambdas: dict_items([('pout', tensor([1.1887])), ('power', tensor([0.7046]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6086])), ('power', tensor([-23.9801]))])
epoch：243	 i:0 	 global-step:4860	 l-p:0.09809844940900803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[3.4795, 3.8790, 3.9883],
        [3.4795, 3.5365, 3.5052],
        [3.4795, 3.4835, 3.4799],
        [3.4795, 3.8754, 3.9819]], grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.09703812748193741 
model_pd.l_d.mean(): -16.17151641845703 
model_pd.lagr.mean(): -16.074478149414062 
model_pd.lambdas: dict_items([('pout', tensor([1.1893])), ('power', tensor([0.7034]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6024])), ('power', tensor([-23.9684]))])
epoch：244	 i:0 	 global-step:4880	 l-p:0.09703812748193741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.5009, 3.5009, 3.5009],
        [3.5009, 4.0775, 4.3446],
        [3.5009, 3.5050, 3.5014],
        [3.5009, 3.5009, 3.5009]], grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.09552019089460373 
model_pd.l_d.mean(): -16.14114761352539 
model_pd.lagr.mean(): -16.04562759399414 
model_pd.lambdas: dict_items([('pout', tensor([1.1898])), ('power', tensor([0.7022]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5943])), ('power', tensor([-23.9528]))])
epoch：245	 i:0 	 global-step:4900	 l-p:0.09552019089460373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[3.5323, 3.5703, 3.5457],
        [3.5323, 3.5323, 3.5323],
        [3.5323, 3.9418, 4.0545],
        [3.5323, 3.7081, 3.6815]], grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.09298138320446014 
model_pd.l_d.mean(): -16.109987258911133 
model_pd.lagr.mean(): -16.017005920410156 
model_pd.lambdas: dict_items([('pout', tensor([1.1904])), ('power', tensor([0.7010]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5824])), ('power', tensor([-23.9297]))])
epoch：246	 i:0 	 global-step:4920	 l-p:0.09298138320446014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[3.5865, 3.8655, 3.8829],
        [3.5865, 3.6566, 3.6214],
        [3.5865, 4.3165, 4.7314],
        [3.5865, 4.0079, 4.1253]], grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.08717993646860123 
model_pd.l_d.mean(): -16.076757431030273 
model_pd.lagr.mean(): -15.989577293395996 
model_pd.lambdas: dict_items([('pout', tensor([1.1910])), ('power', tensor([0.6998]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5621])), ('power', tensor([-23.8891]))])
epoch：247	 i:0 	 global-step:4940	 l-p:0.08717993646860123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[3.7037, 4.0009, 4.0218],
        [3.7037, 3.7049, 3.7037],
        [3.7037, 4.5447, 5.0652],
        [3.7037, 3.7390, 3.7152]], grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.048880867660045624 
model_pd.l_d.mean(): -16.036466598510742 
model_pd.lagr.mean(): -15.98758602142334 
model_pd.lambdas: dict_items([('pout', tensor([1.1915])), ('power', tensor([0.6986]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5191])), ('power', tensor([-23.7995]))])
epoch：248	 i:0 	 global-step:4960	 l-p:0.048880867660045624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[3.9234, 3.9236, 3.9234],
        [3.9234, 3.9255, 3.9236],
        [3.9234, 3.9235, 3.9234],
        [3.9234, 3.9864, 3.9507]], grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.12316706031560898 
model_pd.l_d.mean(): -15.979192733764648 
model_pd.lagr.mean(): -15.856025695800781 
model_pd.lambdas: dict_items([('pout', tensor([1.1920])), ('power', tensor([0.6974]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4420])), ('power', tensor([-23.6270]))])
epoch：249	 i:0 	 global-step:4980	 l-p:0.12316706031560898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.1381, 4.1387, 4.1382],
        [4.1381, 4.6250, 4.7440],
        [4.1381, 4.1381, 4.1381],
        [4.1381, 5.1387, 5.7706]], grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.10623077303171158 
model_pd.l_d.mean(): -15.915034294128418 
model_pd.lagr.mean(): -15.80880355834961 
model_pd.lambdas: dict_items([('pout', tensor([1.1923])), ('power', tensor([0.6962]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3708])), ('power', tensor([-23.4537]))])
epoch：250	 i:0 	 global-step:5000	 l-p:0.10623077303171158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.1757, 4.1759, 4.1757],
        [4.1757, 4.1763, 4.1757],
        [4.1757, 4.9469, 5.3215],
        [4.1757, 4.1757, 4.1757]], grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.10477828234434128 
model_pd.l_d.mean(): -15.88040542602539 
model_pd.lagr.mean(): -15.775627136230469 
model_pd.lambdas: dict_items([('pout', tensor([1.1927])), ('power', tensor([0.6951]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3588])), ('power', tensor([-23.4231]))])
epoch：251	 i:0 	 global-step:5020	 l-p:0.10477828234434128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.0914, 4.1868, 4.1422],
        [4.0914, 4.1135, 4.0964],
        [4.0914, 4.1142, 4.0967],
        [4.0914, 4.4806, 4.5330]], grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.1083771139383316 
model_pd.l_d.mean(): -15.868295669555664 
model_pd.lagr.mean(): -15.759918212890625 
model_pd.lambdas: dict_items([('pout', tensor([1.1931])), ('power', tensor([0.6939]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3861])), ('power', tensor([-23.4921]))])
epoch：252	 i:0 	 global-step:5040	 l-p:0.1083771139383316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[3.9308, 4.7764, 5.2661],
        [3.9308, 3.9308, 3.9308],
        [3.9308, 3.9322, 3.9308],
        [3.9308, 4.0331, 3.9895]], grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.12213453650474548 
model_pd.l_d.mean(): -15.866578102111816 
model_pd.lagr.mean(): -15.744443893432617 
model_pd.lambdas: dict_items([('pout', tensor([1.1935])), ('power', tensor([0.6927]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396])), ('power', tensor([-23.6217]))])
epoch：253	 i:0 	 global-step:5060	 l-p:0.12213453650474548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.7912, 3.7964, 3.7918],
        [3.7912, 3.8486, 3.8155],
        [3.7912, 4.0278, 4.0134],
        [3.7912, 3.7912, 3.7912]], grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.2689417004585266 
model_pd.l_d.mean(): -15.857293128967285 
model_pd.lagr.mean(): -15.588351249694824 
model_pd.lambdas: dict_items([('pout', tensor([1.1940])), ('power', tensor([0.6915]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4880])), ('power', tensor([-23.7322]))])
epoch：254	 i:0 	 global-step:5080	 l-p:0.2689417004585266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[3.7738, 4.5651, 5.0191],
        [3.7738, 3.7921, 3.7779],
        [3.7738, 3.9746, 3.9475],
        [3.7738, 3.9905, 3.9691]], grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 1.013122320175171 
model_pd.l_d.mean(): -15.830992698669434 
model_pd.lagr.mean(): -14.817870140075684 
model_pd.lambdas: dict_items([('pout', tensor([1.1945])), ('power', tensor([0.6903]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4942])), ('power', tensor([-23.7459]))])
epoch：255	 i:0 	 global-step:5100	 l-p:1.013122320175171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.8509, 4.3233, 4.4578],
        [3.8509, 3.8509, 3.8509],
        [3.8509, 3.8529, 3.8511],
        [3.8509, 3.8509, 3.8509]], grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.14561203122138977 
model_pd.l_d.mean(): -15.793069839477539 
model_pd.lagr.mean(): -15.64745807647705 
model_pd.lambdas: dict_items([('pout', tensor([1.1950])), ('power', tensor([0.6892]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671])), ('power', tensor([-23.6853]))])
epoch：256	 i:0 	 global-step:5120	 l-p:0.14561203122138977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.0032, 4.1393, 4.0938],
        [4.0032, 4.5089, 4.6561],
        [4.0032, 4.5132, 4.6640],
        [4.0032, 4.0139, 4.0049]], grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.1140497624874115 
model_pd.l_d.mean(): -15.743046760559082 
model_pd.lagr.mean(): -15.628996849060059 
model_pd.lambdas: dict_items([('pout', tensor([1.1954])), ('power', tensor([0.6880]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4153])), ('power', tensor([-23.5639]))])
epoch：257	 i:0 	 global-step:5140	 l-p:0.1140497624874115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.0966, 4.0983, 4.0967],
        [4.0966, 4.0997, 4.0969],
        [4.0966, 4.9981, 5.5241],
        [4.0966, 4.0966, 4.0966]], grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.10815000534057617 
model_pd.l_d.mean(): -15.699925422668457 
model_pd.lagr.mean(): -15.591775894165039 
model_pd.lambdas: dict_items([('pout', tensor([1.1958])), ('power', tensor([0.6868]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3845])), ('power', tensor([-23.4883]))])
epoch：258	 i:0 	 global-step:5160	 l-p:0.10815000534057617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[4.1106, 4.1328, 4.1156],
        [4.1106, 4.6441, 4.8039],
        [4.1106, 4.1106, 4.1106],
        [4.1106, 4.1236, 4.1128]], grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.10748641937971115 
model_pd.l_d.mean(): -15.669833183288574 
model_pd.lagr.mean(): -15.562346458435059 
model_pd.lambdas: dict_items([('pout', tensor([1.1961])), ('power', tensor([0.6856]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3800])), ('power', tensor([-23.4770]))])
epoch：259	 i:0 	 global-step:5180	 l-p:0.10748641937971115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.0553, 4.0558, 4.0553],
        [4.0553, 4.0553, 4.0553],
        [4.0553, 4.2696, 4.2368],
        [4.0553, 4.1182, 4.0818]], grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.11040262132883072 
model_pd.l_d.mean(): -15.6513090133667 
model_pd.lagr.mean(): -15.540905952453613 
model_pd.lambdas: dict_items([('pout', tensor([1.1965])), ('power', tensor([0.6845]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3980])), ('power', tensor([-23.5219]))])
epoch：260	 i:0 	 global-step:5200	 l-p:0.11040262132883072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[3.9531, 3.9740, 3.9579],
        [3.9531, 4.4524, 4.5993],
        [3.9531, 3.9531, 3.9531],
        [3.9531, 3.9732, 3.9576]], grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.11913255602121353 
model_pd.l_d.mean(): -15.639016151428223 
model_pd.lagr.mean(): -15.519883155822754 
model_pd.lambdas: dict_items([('pout', tensor([1.1970])), ('power', tensor([0.6833]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4321])), ('power', tensor([-23.6042]))])
epoch：261	 i:0 	 global-step:5220	 l-p:0.11913255602121353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[3.8514, 3.9108, 3.8767],
        [3.8514, 3.9068, 3.8741],
        [3.8514, 4.2226, 4.2807],
        [3.8514, 3.9339, 3.8940]], grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.14557196199893951 
model_pd.l_d.mean(): -15.624536514282227 
model_pd.lagr.mean(): -15.478964805603027 
model_pd.lambdas: dict_items([('pout', tensor([1.1974])), ('power', tensor([0.6821]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4670])), ('power', tensor([-23.6851]))])
epoch：262	 i:0 	 global-step:5240	 l-p:0.14557196199893951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[3.8392, 3.8396, 3.8392],
        [3.8392, 3.9355, 3.8937],
        [3.8392, 3.8392, 3.8392],
        [3.8392, 3.8580, 3.8433]], grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.15369324386119843 
model_pd.l_d.mean(): -15.597771644592285 
model_pd.lagr.mean(): -15.44407844543457 
model_pd.lambdas: dict_items([('pout', tensor([1.1979])), ('power', tensor([0.6809]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4713])), ('power', tensor([-23.6948]))])
epoch：263	 i:0 	 global-step:5260	 l-p:0.15369324386119843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[3.9013, 4.5900, 4.9180],
        [3.9013, 3.9213, 3.9058],
        [3.9013, 4.3842, 4.5225],
        [3.9013, 3.9013, 3.9013]], grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.1278192400932312 
model_pd.l_d.mean(): -15.561687469482422 
model_pd.lagr.mean(): -15.433868408203125 
model_pd.lambdas: dict_items([('pout', tensor([1.1984])), ('power', tensor([0.6797]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4498])), ('power', tensor([-23.6456]))])
epoch：264	 i:0 	 global-step:5280	 l-p:0.1278192400932312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.0027, 4.1463, 4.1013],
        [4.0027, 4.3754, 4.4235],
        [4.0027, 4.0134, 4.0044],
        [4.0027, 4.0454, 4.0172]], grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.11412426829338074 
model_pd.l_d.mean(): -15.51948356628418 
model_pd.lagr.mean(): -15.405359268188477 
model_pd.lambdas: dict_items([('pout', tensor([1.1988])), ('power', tensor([0.6785]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4155])), ('power', tensor([-23.5645]))])
epoch：265	 i:0 	 global-step:5300	 l-p:0.11412426829338074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[4.0629, 4.0844, 4.0678],
        [4.0629, 4.4720, 4.5403],
        [4.0629, 4.2957, 4.2686],
        [4.0629, 4.0851, 4.0680]], grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.10996738076210022 
model_pd.l_d.mean(): -15.482460021972656 
model_pd.lagr.mean(): -15.372492790222168 
model_pd.lambdas: dict_items([('pout', tensor([1.1992])), ('power', tensor([0.6774]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3956])), ('power', tensor([-23.5159]))])
epoch：266	 i:0 	 global-step:5320	 l-p:0.10996738076210022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.0699, 4.0700, 4.0699],
        [4.0699, 4.2855, 4.2526],
        [4.0699, 4.0699, 4.0699],
        [4.0699, 4.1144, 4.0852]], grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.10956773161888123 
model_pd.l_d.mean(): -15.453563690185547 
model_pd.lagr.mean(): -15.343996047973633 
model_pd.lambdas: dict_items([('pout', tensor([1.1996])), ('power', tensor([0.6762]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3933])), ('power', tensor([-23.5102]))])
epoch：267	 i:0 	 global-step:5340	 l-p:0.10956773161888123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.0295, 4.0295, 4.0295],
        [4.0295, 4.5452, 4.6984],
        [4.0295, 4.9869, 5.5879],
        [4.0295, 4.0296, 4.0295]], grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.11209314316511154 
model_pd.l_d.mean(): -15.431917190551758 
model_pd.lagr.mean(): -15.31982421875 
model_pd.lambdas: dict_items([('pout', tensor([1.2000])), ('power', tensor([0.6750]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4066])), ('power', tensor([-23.5430]))])
epoch：268	 i:0 	 global-step:5360	 l-p:0.11209314316511154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[3.9571, 4.7008, 5.0775],
        [3.9571, 3.9985, 3.9711],
        [3.9571, 4.3218, 4.3680],
        [3.9571, 4.0894, 4.0447]], grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.11868657171726227 
model_pd.l_d.mean(): -15.414277076721191 
model_pd.lagr.mean(): -15.2955904006958 
model_pd.lambdas: dict_items([('pout', tensor([1.2004])), ('power', tensor([0.6738]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4308])), ('power', tensor([-23.6012]))])
epoch：269	 i:0 	 global-step:5380	 l-p:0.11868657171726227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[3.8850, 3.9586, 3.9203],
        [3.8850, 3.9691, 3.9286],
        [3.8850, 4.7892, 5.3529],
        [3.8850, 3.8850, 3.8850]], grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.13207069039344788 
model_pd.l_d.mean(): -15.395462036132812 
model_pd.lagr.mean(): -15.263391494750977 
model_pd.lambdas: dict_items([('pout', tensor([1.2009])), ('power', tensor([0.6727]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4554])), ('power', tensor([-23.6587]))])
epoch：270	 i:0 	 global-step:5400	 l-p:0.13207069039344788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.8893, 3.9005, 3.8912],
        [3.8893, 3.8893, 3.8893],
        [3.8893, 3.9089, 3.8937],
        [3.8893, 3.8893, 3.8893]], grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.1308632344007492 
model_pd.l_d.mean(): -15.366754531860352 
model_pd.lagr.mean(): -15.235891342163086 
model_pd.lambdas: dict_items([('pout', tensor([1.2013])), ('power', tensor([0.6715]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4539])), ('power', tensor([-23.6552]))])
epoch：271	 i:0 	 global-step:5420	 l-p:0.1308632344007492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[3.9585, 4.3485, 4.4117],
        [3.9585, 4.4535, 4.5964],
        [3.9585, 3.9585, 3.9585],
        [3.9585, 4.7025, 5.0793]], grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.1185271292924881 
model_pd.l_d.mean(): -15.329851150512695 
model_pd.lagr.mean(): -15.211323738098145 
model_pd.lambdas: dict_items([('pout', tensor([1.2017])), ('power', tensor([0.6703]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4304])), ('power', tensor([-23.6001]))])
epoch：272	 i:0 	 global-step:5440	 l-p:0.1185271292924881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.0151, 4.2793, 4.2667],
        [4.0151, 4.0151, 4.0151],
        [4.0151, 4.0366, 4.0201],
        [4.0151, 4.0581, 4.0298]], grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.11315559595823288 
model_pd.l_d.mean(): -15.294135093688965 
model_pd.lagr.mean(): -15.18097972869873 
model_pd.lambdas: dict_items([('pout', tensor([1.2021])), ('power', tensor([0.6691]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4114])), ('power', tensor([-23.5546]))])
epoch：273	 i:0 	 global-step:5460	 l-p:0.11315559595823288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[4.0381, 4.3857, 4.4157],
        [4.0381, 4.0599, 4.0431],
        [4.0381, 4.0382, 4.0381],
        [4.0381, 4.2764, 4.2524]], grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.1115243211388588 
model_pd.l_d.mean(): -15.262994766235352 
model_pd.lagr.mean(): -15.151470184326172 
model_pd.lambdas: dict_items([('pout', tensor([1.2026])), ('power', tensor([0.6679]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4038])), ('power', tensor([-23.5362]))])
epoch：274	 i:0 	 global-step:5480	 l-p:0.1115243211388588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.0253, 4.0254, 4.0253],
        [4.0253, 4.2535, 4.2263],
        [4.0253, 4.1051, 4.0642],
        [4.0253, 4.4013, 4.4501]], grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.11240840703248978 
model_pd.l_d.mean(): -15.236946105957031 
model_pd.lagr.mean(): -15.124537467956543 
model_pd.lambdas: dict_items([('pout', tensor([1.2030])), ('power', tensor([0.6668]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4080])), ('power', tensor([-23.5465]))])
epoch：275	 i:0 	 global-step:5500	 l-p:0.11240840703248978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[3.9844, 3.9845, 3.9844],
        [3.9844, 3.9963, 3.9864],
        [3.9844, 4.7359, 5.1171],
        [3.9844, 3.9865, 3.9845]], grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.11579155921936035 
model_pd.l_d.mean(): -15.21461296081543 
model_pd.lagr.mean(): -15.098821640014648 
model_pd.lambdas: dict_items([('pout', tensor([1.2034])), ('power', tensor([0.6656]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217])), ('power', tensor([-23.5795]))])
epoch：276	 i:0 	 global-step:5520	 l-p:0.11579155921936035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[3.9318, 3.9318, 3.9318],
        [3.9318, 3.9376, 3.9325],
        [3.9318, 4.4206, 4.5610],
        [3.9318, 4.1325, 4.1000]], grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.12219051271677017 
model_pd.l_d.mean(): -15.193305969238281 
model_pd.lagr.mean(): -15.071115493774414 
model_pd.lambdas: dict_items([('pout', tensor([1.2038])), ('power', tensor([0.6644]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4395])), ('power', tensor([-23.6216]))])
epoch：277	 i:0 	 global-step:5540	 l-p:0.12219051271677017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[3.8991, 3.9475, 3.9172],
        [3.8991, 3.8991, 3.8991],
        [3.8991, 4.1129, 4.0857],
        [3.8991, 4.6244, 4.9903]], grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.1284736543893814 
model_pd.l_d.mean(): -15.169096946716309 
model_pd.lagr.mean(): -15.040623664855957 
model_pd.lambdas: dict_items([('pout', tensor([1.2043])), ('power', tensor([0.6632]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4506])), ('power', tensor([-23.6477]))])
epoch：278	 i:0 	 global-step:5560	 l-p:0.1284736543893814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[3.9214, 4.0982, 4.0600],
        [3.9214, 4.0443, 4.0000],
        [3.9214, 4.4126, 4.5560],
        [3.9214, 3.9705, 3.9399]], grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.12393596768379211 
model_pd.l_d.mean(): -15.138350486755371 
model_pd.lagr.mean(): -15.01441478729248 
model_pd.lambdas: dict_items([('pout', tensor([1.2047])), ('power', tensor([0.6620]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4430])), ('power', tensor([-23.6300]))])
epoch：279	 i:0 	 global-step:5580	 l-p:0.12393596768379211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[3.9661, 4.7116, 5.0892],
        [3.9661, 3.9663, 3.9661],
        [3.9661, 4.6559, 4.9768],
        [3.9661, 4.6725, 5.0101]], grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.1177055835723877 
model_pd.l_d.mean(): -15.104848861694336 
model_pd.lagr.mean(): -14.987143516540527 
model_pd.lambdas: dict_items([('pout', tensor([1.2051])), ('power', tensor([0.6609]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4279])), ('power', tensor([-23.5943]))])
epoch：280	 i:0 	 global-step:5600	 l-p:0.1177055835723877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.0007, 4.0213, 4.0053],
        [4.0007, 4.0068, 4.0014],
        [4.0007, 4.5092, 4.6593],
        [4.0007, 4.3419, 4.3706]], grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.11434728652238846 
model_pd.l_d.mean(): -15.07245922088623 
model_pd.lagr.mean(): -14.958111763000488 
model_pd.lambdas: dict_items([('pout', tensor([1.2056])), ('power', tensor([0.6597]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4163])), ('power', tensor([-23.5665]))])
epoch：281	 i:0 	 global-step:5620	 l-p:0.11434728652238846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[4.0130, 4.0130, 4.0130],
        [4.0130, 4.1488, 4.1032],
        [4.0130, 4.5233, 4.6738],
        [4.0130, 4.4710, 4.5795]], grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.11335974931716919 
model_pd.l_d.mean(): -15.042948722839355 
model_pd.lagr.mean(): -14.92958927154541 
model_pd.lambdas: dict_items([('pout', tensor([1.2060])), ('power', tensor([0.6585]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4122])), ('power', tensor([-23.5567]))])
epoch：282	 i:0 	 global-step:5640	 l-p:0.11335974931716919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.0017, 4.3428, 4.3714],
        [4.0017, 4.5101, 4.6601],
        [4.0017, 4.0045, 4.0019],
        [4.0017, 4.0227, 4.0064]], grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.1142798438668251 
model_pd.l_d.mean(): -15.016495704650879 
model_pd.lagr.mean(): -14.902215957641602 
model_pd.lambdas: dict_items([('pout', tensor([1.2064])), ('power', tensor([0.6573]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4160])), ('power', tensor([-23.5658]))])
epoch：283	 i:0 	 global-step:5660	 l-p:0.1142798438668251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[3.9732, 4.0369, 4.0007],
        [3.9732, 4.0994, 4.0543],
        [3.9732, 4.7205, 5.0989],
        [3.9732, 3.9732, 3.9732]], grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.11696401238441467 
model_pd.l_d.mean(): -14.992104530334473 
model_pd.lagr.mean(): -14.875140190124512 
model_pd.lambdas: dict_items([('pout', tensor([1.2068])), ('power', tensor([0.6561]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4255])), ('power', tensor([-23.5888]))])
epoch：284	 i:0 	 global-step:5680	 l-p:0.11696401238441467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[3.9405, 4.0644, 4.0198],
        [3.9405, 4.3262, 4.3879],
        [3.9405, 3.9609, 3.9452],
        [3.9405, 4.4303, 4.5710]], grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.1209719106554985 
model_pd.l_d.mean(): -14.967964172363281 
model_pd.lagr.mean(): -14.846992492675781 
model_pd.lambdas: dict_items([('pout', tensor([1.2072])), ('power', tensor([0.6550]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366])), ('power', tensor([-23.6150]))])
epoch：285	 i:0 	 global-step:5700	 l-p:0.1209719106554985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[3.9230, 4.8397, 5.4114],
        [3.9230, 4.6162, 4.9461],
        [3.9230, 4.1389, 4.1116],
        [3.9230, 3.9256, 3.9232]], grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.12374717742204666 
model_pd.l_d.mean(): -14.941875457763672 
model_pd.lagr.mean(): -14.81812858581543 
model_pd.lambdas: dict_items([('pout', tensor([1.2077])), ('power', tensor([0.6538]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4425])), ('power', tensor([-23.6290]))])
epoch：286	 i:0 	 global-step:5720	 l-p:0.12374717742204666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.9341, 3.9962, 3.9608],
        [3.9341, 4.1682, 4.1470],
        [3.9341, 3.9341, 3.9341],
        [3.9341, 3.9544, 3.9387]], grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.1219460740685463 
model_pd.l_d.mean(): -14.912551879882812 
model_pd.lagr.mean(): -14.790605545043945 
model_pd.lambdas: dict_items([('pout', tensor([1.2081])), ('power', tensor([0.6526]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4388])), ('power', tensor([-23.6202]))])
epoch：287	 i:0 	 global-step:5740	 l-p:0.1219460740685463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[3.9619, 4.0119, 3.9807],
        [3.9619, 3.9620, 3.9619],
        [3.9619, 3.9639, 3.9620],
        [3.9619, 4.8145, 5.3078]], grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.11825238913297653 
model_pd.l_d.mean(): -14.88134479522705 
model_pd.lagr.mean(): -14.763092041015625 
model_pd.lambdas: dict_items([('pout', tensor([1.2086])), ('power', tensor([0.6514]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4294])), ('power', tensor([-23.5980]))])
epoch：288	 i:0 	 global-step:5760	 l-p:0.11825238913297653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[3.9874, 4.2277, 4.2066],
        [3.9874, 3.9879, 3.9874],
        [3.9874, 4.0760, 4.0337],
        [3.9874, 4.2187, 4.1943]], grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.11559226363897324 
model_pd.l_d.mean(): -14.850363731384277 
model_pd.lagr.mean(): -14.734771728515625 
model_pd.lambdas: dict_items([('pout', tensor([1.2090])), ('power', tensor([0.6502]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4208])), ('power', tensor([-23.5776]))])
epoch：289	 i:0 	 global-step:5780	 l-p:0.11559226363897324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.0000, 4.0027, 4.0002],
        [4.0000, 4.8653, 5.3668],
        [4.0000, 4.1343, 4.0889],
        [4.0000, 4.3701, 4.4172]], grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.11447051912546158 
model_pd.l_d.mean(): -14.82091999053955 
model_pd.lagr.mean(): -14.706449508666992 
model_pd.lambdas: dict_items([('pout', tensor([1.2094])), ('power', tensor([0.6491]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4166])), ('power', tensor([-23.5676]))])
epoch：290	 i:0 	 global-step:5800	 l-p:0.11447051912546158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[3.9968, 4.9399, 5.5299],
        [3.9968, 4.0747, 4.0345],
        [3.9968, 4.1014, 4.0569],
        [3.9968, 4.0028, 3.9975]], grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.11474981904029846 
model_pd.l_d.mean(): -14.793363571166992 
model_pd.lagr.mean(): -14.678613662719727 
model_pd.lambdas: dict_items([('pout', tensor([1.2098])), ('power', tensor([0.6479]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4177])), ('power', tensor([-23.5701]))])
epoch：291	 i:0 	 global-step:5820	 l-p:0.11474981904029846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[3.9811, 3.9914, 3.9827],
        [3.9811, 4.4788, 4.6222],
        [3.9811, 4.1857, 4.1527],
        [3.9811, 3.9816, 3.9811]], grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.1162274107336998 
model_pd.l_d.mean(): -14.767251014709473 
model_pd.lagr.mean(): -14.651023864746094 
model_pd.lambdas: dict_items([('pout', tensor([1.2102])), ('power', tensor([0.6467]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4230])), ('power', tensor([-23.5829]))])
epoch：292	 i:0 	 global-step:5840	 l-p:0.1162274107336998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.9604, 3.9806, 3.9649],
        [3.9604, 3.9605, 3.9604],
        [3.9604, 3.9801, 3.9648],
        [3.9604, 4.3234, 4.3687]], grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.11847932636737823 
model_pd.l_d.mean(): -14.741589546203613 
model_pd.lagr.mean(): -14.623109817504883 
model_pd.lambdas: dict_items([('pout', tensor([1.2107])), ('power', tensor([0.6455]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4300])), ('power', tensor([-23.5995]))])
epoch：293	 i:0 	 global-step:5860	 l-p:0.11847932636737823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[3.9456, 4.1234, 4.0848],
        [3.9456, 4.0832, 4.0390],
        [3.9456, 3.9456, 3.9456],
        [3.9456, 4.3310, 4.3923]], grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.12038172036409378 
model_pd.l_d.mean(): -14.715169906616211 
model_pd.lagr.mean(): -14.594788551330566 
model_pd.lambdas: dict_items([('pout', tensor([1.2111])), ('power', tensor([0.6443]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4350])), ('power', tensor([-23.6114]))])
epoch：294	 i:0 	 global-step:5880	 l-p:0.12038172036409378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[3.9455, 3.9655, 3.9500],
        [3.9455, 3.9460, 3.9455],
        [3.9455, 3.9455, 3.9455],
        [3.9455, 4.0076, 3.9722]], grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.12040047347545624 
model_pd.l_d.mean(): -14.687129974365234 
model_pd.lagr.mean(): -14.566729545593262 
model_pd.lambdas: dict_items([('pout', tensor([1.2115])), ('power', tensor([0.6432]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4350])), ('power', tensor([-23.6115]))])
epoch：295	 i:0 	 global-step:5900	 l-p:0.12040047347545624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[3.9594, 4.0837, 4.0389],
        [3.9594, 4.4560, 4.6007],
        [3.9594, 3.9799, 3.9640],
        [3.9594, 3.9595, 3.9594]], grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.11863241344690323 
model_pd.l_d.mean(): -14.657625198364258 
model_pd.lagr.mean(): -14.538992881774902 
model_pd.lambdas: dict_items([('pout', tensor([1.2120])), ('power', tensor([0.6420]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4304])), ('power', tensor([-23.6005]))])
epoch：296	 i:0 	 global-step:5920	 l-p:0.11863241344690323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.9779, 4.4269, 4.5319],
        [3.9779, 3.9779, 3.9779],
        [3.9779, 4.6854, 5.0229],
        [3.9779, 4.4785, 4.6246]], grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.11659588664770126 
model_pd.l_d.mean(): -14.62763500213623 
model_pd.lagr.mean(): -14.511038780212402 
model_pd.lambdas: dict_items([('pout', tensor([1.2124])), ('power', tensor([0.6408]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4241])), ('power', tensor([-23.5857]))])
epoch：297	 i:0 	 global-step:5940	 l-p:0.11659588664770126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[3.9919, 3.9919, 3.9919],
        [3.9919, 3.9919, 3.9919],
        [3.9919, 4.0514, 4.0166],
        [3.9919, 3.9919, 3.9919]], grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.11524534970521927 
model_pd.l_d.mean(): -14.598137855529785 
model_pd.lagr.mean(): -14.482892036437988 
model_pd.lambdas: dict_items([('pout', tensor([1.2128])), ('power', tensor([0.6396]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4194])), ('power', tensor([-23.5745]))])
epoch：298	 i:0 	 global-step:5960	 l-p:0.11524534970521927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[3.9968, 3.9969, 3.9968],
        [3.9968, 3.9968, 3.9968],
        [3.9968, 3.9968, 3.9968],
        [3.9968, 4.0027, 3.9975]], grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.11481735110282898 
model_pd.l_d.mean(): -14.56966495513916 
model_pd.lagr.mean(): -14.45484733581543 
model_pd.lambdas: dict_items([('pout', tensor([1.2132])), ('power', tensor([0.6384]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4178])), ('power', tensor([-23.5706]))])
epoch：299	 i:0 	 global-step:5980	 l-p:0.11481735110282898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[3.9921, 4.7426, 5.1223],
        [3.9921, 4.0957, 4.0514],
        [3.9921, 3.9979, 3.9927],
        [3.9921, 4.0802, 4.0380]], grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.11525262892246246 
model_pd.l_d.mean(): -14.542243003845215 
model_pd.lagr.mean(): -14.426990509033203 
model_pd.lambdas: dict_items([('pout', tensor([1.2136])), ('power', tensor([0.6373]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4194])), ('power', tensor([-23.5745]))])
epoch：300	 i:0 	 global-step:6000	 l-p:0.11525262892246246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[3.9809, 4.0398, 4.0052],
        [3.9809, 4.4815, 4.6275],
        [3.9809, 4.1130, 4.0679],
        [3.9809, 4.0440, 4.0081]], grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.11633223295211792 
model_pd.l_d.mean(): -14.51547908782959 
model_pd.lagr.mean(): -14.399147033691406 
model_pd.lambdas: dict_items([('pout', tensor([1.2141])), ('power', tensor([0.6361]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232])), ('power', tensor([-23.5835]))])
epoch：301	 i:0 	 global-step:6020	 l-p:0.11633223295211792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.9689, 4.0448, 4.0054],
        [3.9689, 3.9894, 3.9735],
        [3.9689, 3.9709, 3.9690],
        [3.9689, 4.0186, 3.9875]], grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.11760572344064713 
model_pd.l_d.mean(): -14.488740921020508 
model_pd.lagr.mean(): -14.371134757995605 
model_pd.lambdas: dict_items([('pout', tensor([1.2145])), ('power', tensor([0.6349]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4272])), ('power', tensor([-23.5932]))])
epoch：302	 i:0 	 global-step:6040	 l-p:0.11760572344064713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[3.9623, 4.2946, 4.3208],
        [3.9623, 4.1891, 4.1642],
        [3.9623, 3.9680, 3.9629],
        [3.9623, 3.9823, 3.9668]], grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.11837618052959442 
model_pd.l_d.mean(): -14.461408615112305 
model_pd.lagr.mean(): -14.343032836914062 
model_pd.lambdas: dict_items([('pout', tensor([1.2149])), ('power', tensor([0.6337]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4295])), ('power', tensor([-23.5986]))])
epoch：303	 i:0 	 global-step:6060	 l-p:0.11837618052959442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.9645, 4.0268, 3.9912],
        [3.9645, 4.1914, 4.1665],
        [3.9645, 3.9645, 3.9645],
        [3.9645, 3.9645, 3.9645]], grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.11812981963157654 
model_pd.l_d.mean(): -14.433183670043945 
model_pd.lagr.mean(): -14.315053939819336 
model_pd.lambdas: dict_items([('pout', tensor([1.2154])), ('power', tensor([0.6326]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4288])), ('power', tensor([-23.5969]))])
epoch：304	 i:0 	 global-step:6080	 l-p:0.11812981963157654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[3.9741, 3.9944, 3.9786],
        [3.9741, 4.1936, 4.1657],
        [3.9741, 3.9741, 3.9741],
        [3.9741, 4.9064, 5.4880]], grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.11708175390958786 
model_pd.l_d.mean(): -14.404239654541016 
model_pd.lagr.mean(): -14.287158012390137 
model_pd.lambdas: dict_items([('pout', tensor([1.2158])), ('power', tensor([0.6314]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4256])), ('power', tensor([-23.5893]))])
epoch：305	 i:0 	 global-step:6100	 l-p:0.11708175390958786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[3.9860, 4.0271, 3.9998],
        [3.9860, 3.9860, 3.9860],
        [3.9860, 3.9860, 3.9860],
        [3.9860, 4.0734, 4.0314]], grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.11588431149721146 
model_pd.l_d.mean(): -14.375079154968262 
model_pd.lagr.mean(): -14.259194374084473 
model_pd.lambdas: dict_items([('pout', tensor([1.2162])), ('power', tensor([0.6302]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4216])), ('power', tensor([-23.5798]))])
epoch：306	 i:0 	 global-step:6120	 l-p:0.11588431149721146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.9953, 4.0721, 4.0322],
        [3.9953, 4.1278, 4.0826],
        [3.9953, 3.9954, 3.9953],
        [3.9953, 4.0160, 4.0000]], grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.11502688378095627 
model_pd.l_d.mean(): -14.346196174621582 
model_pd.lagr.mean(): -14.231169700622559 
model_pd.lambdas: dict_items([('pout', tensor([1.2166])), ('power', tensor([0.6290]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4185])), ('power', tensor([-23.5724]))])
epoch：307	 i:0 	 global-step:6140	 l-p:0.11502688378095627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.9991, 4.3364, 4.3635],
        [3.9991, 3.9991, 3.9991],
        [3.9991, 3.9991, 3.9991],
        [3.9991, 3.9991, 3.9991]], grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.11469927430152893 
model_pd.l_d.mean(): -14.31788158416748 
model_pd.lagr.mean(): -14.203182220458984 
model_pd.lambdas: dict_items([('pout', tensor([1.2170])), ('power', tensor([0.6278]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4172])), ('power', tensor([-23.5694]))])
epoch：308	 i:0 	 global-step:6160	 l-p:0.11469927430152893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[3.9970, 4.4955, 4.6384],
        [3.9970, 3.9970, 3.9970],
        [3.9970, 3.9970, 3.9970],
        [3.9970, 4.1229, 4.0774]], grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.11489096283912659 
model_pd.l_d.mean(): -14.29015827178955 
model_pd.lagr.mean(): -14.175267219543457 
model_pd.lambdas: dict_items([('pout', tensor([1.2175])), ('power', tensor([0.6267]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4179])), ('power', tensor([-23.5711]))])
epoch：309	 i:0 	 global-step:6180	 l-p:0.11489096283912659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[3.9909, 4.0540, 4.0180],
        [3.9909, 3.9910, 3.9909],
        [3.9909, 4.6999, 5.0375],
        [3.9909, 4.3820, 4.4442]], grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.1154559925198555 
model_pd.l_d.mean(): -14.262813568115234 
model_pd.lagr.mean(): -14.147357940673828 
model_pd.lambdas: dict_items([('pout', tensor([1.2179])), ('power', tensor([0.6255]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4200])), ('power', tensor([-23.5761]))])
epoch：310	 i:0 	 global-step:6200	 l-p:0.1154559925198555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[3.9841, 4.6742, 4.9937],
        [3.9841, 3.9861, 3.9842],
        [3.9841, 4.0601, 4.0205],
        [3.9841, 4.0710, 4.0291]], grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.11612559109926224 
model_pd.l_d.mean(): -14.235519409179688 
model_pd.lagr.mean(): -14.119393348693848 
model_pd.lambdas: dict_items([('pout', tensor([1.2183])), ('power', tensor([0.6243]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4223])), ('power', tensor([-23.5817]))])
epoch：311	 i:0 	 global-step:6220	 l-p:0.11612559109926224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[3.9799, 3.9999, 3.9843],
        [3.9799, 4.2164, 4.1946],
        [3.9799, 3.9799, 3.9799],
        [3.9799, 3.9799, 3.9799]], grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.11655842512845993 
model_pd.l_d.mean(): -14.20794677734375 
model_pd.lagr.mean(): -14.091388702392578 
model_pd.lambdas: dict_items([('pout', tensor([1.2187])), ('power', tensor([0.6231]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4237])), ('power', tensor([-23.5851]))])
epoch：312	 i:0 	 global-step:6240	 l-p:0.11655842512845993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.9805, 4.8351, 5.3282],
        [3.9805, 3.9807, 3.9805],
        [3.9805, 3.9862, 3.9811],
        [3.9805, 3.9805, 3.9805]], grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.11650698632001877 
model_pd.l_d.mean(): -14.179919242858887 
model_pd.lagr.mean(): -14.0634126663208 
model_pd.lambdas: dict_items([('pout', tensor([1.2191])), ('power', tensor([0.6219]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4235])), ('power', tensor([-23.5847]))])
epoch：313	 i:0 	 global-step:6260	 l-p:0.11650698632001877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[3.9857, 4.0265, 3.9993],
        [3.9857, 4.0616, 4.0220],
        [3.9857, 4.0060, 3.9903],
        [3.9857, 3.9857, 3.9857]], grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.11599615216255188 
model_pd.l_d.mean(): -14.151473999023438 
model_pd.lagr.mean(): -14.035477638244629 
model_pd.lambdas: dict_items([('pout', tensor([1.2196])), ('power', tensor([0.6208]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4218])), ('power', tensor([-23.5805]))])
epoch：314	 i:0 	 global-step:6280	 l-p:0.11599615216255188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]])
 pt:tensor([[3.9934, 4.4424, 4.5466],
        [3.9934, 4.1966, 4.1631],
        [3.9934, 4.0696, 4.0299],
        [3.9934, 4.7415, 5.1189]], grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.1152816191315651 
model_pd.l_d.mean(): -14.122820854187012 
model_pd.lagr.mean(): -14.007538795471191 
model_pd.lambdas: dict_items([('pout', tensor([1.2200])), ('power', tensor([0.6196]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4193])), ('power', tensor([-23.5744]))])
epoch：315	 i:0 	 global-step:6300	 l-p:0.1152816191315651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.0006, 4.0770, 4.0372],
        [4.0006, 4.0880, 4.0459],
        [4.0006, 4.7112, 5.0494],
        [4.0006, 4.0064, 4.0013]], grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.11464425176382065 
model_pd.l_d.mean(): -14.094223022460938 
model_pd.lagr.mean(): -13.979578971862793 
model_pd.lambdas: dict_items([('pout', tensor([1.2204])), ('power', tensor([0.6184]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169])), ('power', tensor([-23.5687]))])
epoch：316	 i:0 	 global-step:6320	 l-p:0.11464425176382065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.0052, 4.1309, 4.0854],
        [4.0052, 4.0818, 4.0419],
        [4.0052, 4.0057, 4.0052],
        [4.0052, 4.0053, 4.0052]], grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.11425715684890747 
model_pd.l_d.mean(): -14.065877914428711 
model_pd.lagr.mean(): -13.951621055603027 
model_pd.lambdas: dict_items([('pout', tensor([1.2208])), ('power', tensor([0.6172]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4153])), ('power', tensor([-23.5651]))])
epoch：317	 i:0 	 global-step:6340	 l-p:0.11425715684890747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[4.0064, 4.1873, 4.1479],
        [4.0064, 4.0270, 4.0110],
        [4.0064, 4.0084, 4.0065],
        [4.0064, 4.5052, 4.6478]], grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.11417091637849808 
model_pd.l_d.mean(): -14.037859916687012 
model_pd.lagr.mean(): -13.923688888549805 
model_pd.lambdas: dict_items([('pout', tensor([1.2212])), ('power', tensor([0.6160]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4150])), ('power', tensor([-23.5642]))])
epoch：318	 i:0 	 global-step:6360	 l-p:0.11417091637849808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.0045, 4.2620, 4.2477],
        [4.0045, 4.0544, 4.0232],
        [4.0045, 4.0065, 4.0046],
        [4.0045, 4.0045, 4.0045]], grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.11433935910463333 
model_pd.l_d.mean(): -14.010112762451172 
model_pd.lagr.mean(): -13.895772933959961 
model_pd.lambdas: dict_items([('pout', tensor([1.2217])), ('power', tensor([0.6149]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4156])), ('power', tensor([-23.5658]))])
epoch：319	 i:0 	 global-step:6380	 l-p:0.11433935910463333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[4.0011, 4.5034, 4.6494],
        [4.0011, 4.0016, 4.0011],
        [4.0011, 4.2391, 4.2170],
        [4.0011, 4.0509, 4.0197]], grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.11464320123195648 
model_pd.l_d.mean(): -13.982492446899414 
model_pd.lagr.mean(): -13.867849349975586 
model_pd.lambdas: dict_items([('pout', tensor([1.2221])), ('power', tensor([0.6137]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4168])), ('power', tensor([-23.5686]))])
epoch：320	 i:0 	 global-step:6400	 l-p:0.11464320123195648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[3.9981, 4.4988, 4.6439],
        [3.9981, 4.1371, 4.0921],
        [3.9981, 3.9981, 3.9981],
        [3.9981, 3.9981, 3.9981]], grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.11491961777210236 
model_pd.l_d.mean(): -13.954826354980469 
model_pd.lagr.mean(): -13.839906692504883 
model_pd.lambdas: dict_items([('pout', tensor([1.2225])), ('power', tensor([0.6125]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4178])), ('power', tensor([-23.5711]))])
epoch：321	 i:0 	 global-step:6420	 l-p:0.11491961777210236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[3.9971, 3.9971, 3.9971],
        [3.9971, 4.4932, 4.6345],
        [3.9971, 3.9971, 3.9971],
        [3.9971, 4.0554, 4.0210]], grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.11501697450876236 
model_pd.l_d.mean(): -13.92697525024414 
model_pd.lagr.mean(): -13.811958312988281 
model_pd.lambdas: dict_items([('pout', tensor([1.2229])), ('power', tensor([0.6113]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4181])), ('power', tensor([-23.5720]))])
epoch：322	 i:0 	 global-step:6440	 l-p:0.11501697450876236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[3.9989, 3.9989, 3.9989],
        [3.9989, 4.0016, 3.9991],
        [3.9989, 4.0103, 4.0008],
        [3.9989, 4.1301, 4.0849]], grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.11486572027206421 
model_pd.l_d.mean(): -13.898889541625977 
model_pd.lagr.mean(): -13.784024238586426 
model_pd.lambdas: dict_items([('pout', tensor([1.2233])), ('power', tensor([0.6102]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4175])), ('power', tensor([-23.5706]))])
epoch：323	 i:0 	 global-step:6460	 l-p:0.11486572027206421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.0031, 4.2061, 4.1723],
        [4.0031, 4.0232, 4.0076],
        [4.0031, 4.2597, 4.2451],
        [4.0031, 4.0132, 4.0046]], grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.11451064050197601 
model_pd.l_d.mean(): -13.870615005493164 
model_pd.lagr.mean(): -13.756104469299316 
model_pd.lambdas: dict_items([('pout', tensor([1.2237])), ('power', tensor([0.6090]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4162])), ('power', tensor([-23.5673]))])
epoch：324	 i:0 	 global-step:6480	 l-p:0.11451064050197601
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.0084, 4.7590, 5.1372],
        [4.0084, 4.0089, 4.0084],
        [4.0084, 4.0493, 4.0220],
        [4.0084, 4.0085, 4.0084]], grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.11407279223203659 
model_pd.l_d.mean(): -13.842254638671875 
model_pd.lagr.mean(): -13.728181838989258 
model_pd.lambdas: dict_items([('pout', tensor([1.2242])), ('power', tensor([0.6078]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4144])), ('power', tensor([-23.5631]))])
epoch：325	 i:0 	 global-step:6500	 l-p:0.11407279223203659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[4.0133, 4.2171, 4.1833],
        [4.0133, 4.5164, 4.6621],
        [4.0133, 4.0133, 4.0133],
        [4.0133, 4.0134, 4.0133]], grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.11367876827716827 
model_pd.l_d.mean(): -13.813940048217773 
model_pd.lagr.mean(): -13.700261116027832 
model_pd.lambdas: dict_items([('pout', tensor([1.2246])), ('power', tensor([0.6066]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4128])), ('power', tensor([-23.5592]))])
epoch：326	 i:0 	 global-step:6520	 l-p:0.11367876827716827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.0168, 4.0168, 4.0168],
        [4.0168, 4.3839, 4.4287],
        [4.0168, 4.0173, 4.0168],
        [4.0168, 4.0755, 4.0408]], grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.1134125143289566 
model_pd.l_d.mean(): -13.785758018493652 
model_pd.lagr.mean(): -13.672345161437988 
model_pd.lambdas: dict_items([('pout', tensor([1.2250])), ('power', tensor([0.6054]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4117])), ('power', tensor([-23.5565]))])
epoch：327	 i:0 	 global-step:6540	 l-p:0.1134125143289566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.0183, 4.0240, 4.0189],
        [4.0183, 4.0184, 4.0183],
        [4.0183, 4.2482, 4.2225],
        [4.0183, 4.0284, 4.0198]], grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.11330191791057587 
model_pd.l_d.mean(): -13.75773811340332 
model_pd.lagr.mean(): -13.64443588256836 
model_pd.lambdas: dict_items([('pout', tensor([1.2254])), ('power', tensor([0.6043]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4112])), ('power', tensor([-23.5553]))])
epoch：328	 i:0 	 global-step:6560	 l-p:0.11330191791057587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[4.0181, 4.1501, 4.1045],
        [4.0181, 4.0944, 4.0545],
        [4.0181, 4.0810, 4.0450],
        [4.0181, 4.0196, 4.0182]], grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.11332444101572037 
model_pd.l_d.mean(): -13.729864120483398 
model_pd.lagr.mean(): -13.61653995513916 
model_pd.lambdas: dict_items([('pout', tensor([1.2258])), ('power', tensor([0.6031]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4113])), ('power', tensor([-23.5555]))])
epoch：329	 i:0 	 global-step:6580	 l-p:0.11332444101572037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.0170, 4.1042, 4.0620],
        [4.0170, 4.5208, 4.6669],
        [4.0170, 4.0375, 4.0216],
        [4.0170, 4.1422, 4.0965]], grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.11342024058103561 
model_pd.l_d.mean(): -13.702059745788574 
model_pd.lagr.mean(): -13.588639259338379 
model_pd.lambdas: dict_items([('pout', tensor([1.2262])), ('power', tensor([0.6019]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4116])), ('power', tensor([-23.5565]))])
epoch：330	 i:0 	 global-step:6600	 l-p:0.11342024058103561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.0160, 4.3522, 4.3780],
        [4.0160, 4.0161, 4.0160],
        [4.0160, 4.0261, 4.0175],
        [4.0160, 4.1030, 4.0609]], grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.11351136118173599 
model_pd.l_d.mean(): -13.674247741699219 
model_pd.lagr.mean(): -13.560736656188965 
model_pd.lambdas: dict_items([('pout', tensor([1.2266])), ('power', tensor([0.6007]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4120])), ('power', tensor([-23.5574]))])
epoch：331	 i:0 	 global-step:6620	 l-p:0.11351136118173599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[4.0160, 4.0160, 4.0160],
        [4.0160, 4.0567, 4.0295],
        [4.0160, 4.0355, 4.0202],
        [4.0160, 4.5184, 4.6636]], grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.11352638155221939 
model_pd.l_d.mean(): -13.646358489990234 
model_pd.lagr.mean(): -13.532832145690918 
model_pd.lambdas: dict_items([('pout', tensor([1.2270])), ('power', tensor([0.5996]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4120])), ('power', tensor([-23.5575]))])
epoch：332	 i:0 	 global-step:6640	 l-p:0.11352638155221939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[4.0173, 4.3534, 4.3791],
        [4.0173, 4.1422, 4.0966],
        [4.0173, 4.0174, 4.0173],
        [4.0173, 4.0376, 4.0219]], grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.11342637985944748 
model_pd.l_d.mean(): -13.618359565734863 
model_pd.lagr.mean(): -13.50493335723877 
model_pd.lambdas: dict_items([('pout', tensor([1.2274])), ('power', tensor([0.5984]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4116])), ('power', tensor([-23.5565]))])
epoch：333	 i:0 	 global-step:6660	 l-p:0.11342637985944748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[4.0201, 4.0202, 4.0201],
        [4.0201, 4.0397, 4.0244],
        [4.0201, 4.7724, 5.1509],
        [4.0201, 4.4709, 4.5748]], grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.11321750283241272 
model_pd.l_d.mean(): -13.590256690979004 
model_pd.lagr.mean(): -13.477039337158203 
model_pd.lambdas: dict_items([('pout', tensor([1.2279])), ('power', tensor([0.5972]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4107])), ('power', tensor([-23.5543]))])
epoch：334	 i:0 	 global-step:6680	 l-p:0.11321750283241272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[4.0238, 4.2533, 4.2274],
        [4.0238, 4.1556, 4.1099],
        [4.0238, 4.0646, 4.0374],
        [4.0238, 4.1264, 4.0819]], grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.11294278502464294 
model_pd.l_d.mean(): -13.562093734741211 
model_pd.lagr.mean(): -13.449151039123535 
model_pd.lambdas: dict_items([('pout', tensor([1.2283])), ('power', tensor([0.5960]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4095])), ('power', tensor([-23.5514]))])
epoch：335	 i:0 	 global-step:6700	 l-p:0.11294278502464294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.0277, 4.0278, 4.0277],
        [4.0277, 4.2575, 4.2316],
        [4.0277, 4.0334, 4.0284],
        [4.0277, 4.0279, 4.0277]], grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.11266012489795685 
model_pd.l_d.mean(): -13.533927917480469 
model_pd.lagr.mean(): -13.42126750946045 
model_pd.lambdas: dict_items([('pout', tensor([1.2287])), ('power', tensor([0.5948]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4082])), ('power', tensor([-23.5483]))])
epoch：336	 i:0 	 global-step:6720	 l-p:0.11266012489795685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.0312, 4.5316, 4.6738],
        [4.0312, 4.0312, 4.0312],
        [4.0312, 4.0312, 4.0312],
        [4.0312, 4.0313, 4.0312]], grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.112418994307518 
model_pd.l_d.mean(): -13.505805015563965 
model_pd.lagr.mean(): -13.393385887145996 
model_pd.lambdas: dict_items([('pout', tensor([1.2291])), ('power', tensor([0.5937]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4071])), ('power', tensor([-23.5456]))])
epoch：337	 i:0 	 global-step:6740	 l-p:0.112418994307518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.0337, 4.2145, 4.1746],
        [4.0337, 4.5388, 4.6846],
        [4.0337, 4.2379, 4.2037],
        [4.0337, 4.0337, 4.0337]], grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.11224771291017532 
model_pd.l_d.mean(): -13.477758407592773 
model_pd.lagr.mean(): -13.365510940551758 
model_pd.lambdas: dict_items([('pout', tensor([1.2295])), ('power', tensor([0.5925]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4063])), ('power', tensor([-23.5437]))])
epoch：338	 i:0 	 global-step:6760	 l-p:0.11224771291017532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.0352, 4.0352, 4.0352],
        [4.0352, 4.0353, 4.0352],
        [4.0352, 4.4032, 4.4477],
        [4.0352, 4.0352, 4.0352]], grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.1121501550078392 
model_pd.l_d.mean(): -13.44979190826416 
model_pd.lagr.mean(): -13.337641716003418 
model_pd.lambdas: dict_items([('pout', tensor([1.2299])), ('power', tensor([0.5913]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4058])), ('power', tensor([-23.5426]))])
epoch：339	 i:0 	 global-step:6780	 l-p:0.1121501550078392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.0359, 4.0359, 4.0359],
        [4.0359, 4.9834, 5.5731],
        [4.0359, 4.0856, 4.0544],
        [4.0359, 4.0416, 4.0365]], grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.11210896819829941 
model_pd.l_d.mean(): -13.421886444091797 
model_pd.lagr.mean(): -13.30977725982666 
model_pd.lambdas: dict_items([('pout', tensor([1.2303])), ('power', tensor([0.5901]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4056])), ('power', tensor([-23.5421]))])
epoch：340	 i:0 	 global-step:6800	 l-p:0.11210896819829941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.0362, 4.7518, 5.0911],
        [4.0362, 4.2577, 4.2287],
        [4.0362, 4.0364, 4.0362],
        [4.0362, 4.0362, 4.0362]], grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.11209289729595184 
model_pd.l_d.mean(): -13.394010543823242 
model_pd.lagr.mean(): -13.281917572021484 
model_pd.lambdas: dict_items([('pout', tensor([1.2307])), ('power', tensor([0.5890]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4055])), ('power', tensor([-23.5419]))])
epoch：341	 i:0 	 global-step:6820	 l-p:0.11209289729595184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[4.0367, 4.3742, 4.3997],
        [4.0367, 4.0468, 4.0382],
        [4.0367, 4.0369, 4.0367],
        [4.0367, 4.0372, 4.0367]], grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.11206704378128052 
model_pd.l_d.mean(): -13.366122245788574 
model_pd.lagr.mean(): -13.25405502319336 
model_pd.lambdas: dict_items([('pout', tensor([1.2311])), ('power', tensor([0.5878]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4054])), ('power', tensor([-23.5415]))])
epoch：342	 i:0 	 global-step:6840	 l-p:0.11206704378128052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228]])
 pt:tensor([[4.0378, 4.1629, 4.1170],
        [4.0378, 4.1139, 4.0739],
        [4.0378, 4.5427, 4.6882],
        [4.0378, 4.7364, 5.0586]], grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.11200255155563354 
model_pd.l_d.mean(): -13.338197708129883 
model_pd.lagr.mean(): -13.226195335388184 
model_pd.lambdas: dict_items([('pout', tensor([1.2315])), ('power', tensor([0.5866]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4050])), ('power', tensor([-23.5408]))])
epoch：343	 i:0 	 global-step:6860	 l-p:0.11200255155563354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[4.0396, 4.9075, 5.4072],
        [4.0396, 4.7555, 5.0947],
        [4.0396, 4.0598, 4.0441],
        [4.0396, 4.2609, 4.2318]], grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.11188527941703796 
model_pd.l_d.mean(): -13.310225486755371 
model_pd.lagr.mean(): -13.19834041595459 
model_pd.lambdas: dict_items([('pout', tensor([1.2319])), ('power', tensor([0.5854]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4044])), ('power', tensor([-23.5394]))])
epoch：344	 i:0 	 global-step:6880	 l-p:0.11188527941703796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.0421, 4.4102, 4.4544],
        [4.0421, 4.0421, 4.0421],
        [4.0421, 4.1049, 4.0687],
        [4.0421, 4.0421, 4.0421]], grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.11171850562095642 
model_pd.l_d.mean(): -13.282209396362305 
model_pd.lagr.mean(): -13.170491218566895 
model_pd.lambdas: dict_items([('pout', tensor([1.2323])), ('power', tensor([0.5843]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4036])), ('power', tensor([-23.5374]))])
epoch：345	 i:0 	 global-step:6900	 l-p:0.11171850562095642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[4.0452, 4.4394, 4.5006],
        [4.0452, 4.0949, 4.0636],
        [4.0452, 4.0653, 4.0496],
        [4.0452, 4.9146, 5.4151]], grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.11151935905218124 
model_pd.l_d.mean(): -13.254165649414062 
model_pd.lagr.mean(): -13.142645835876465 
model_pd.lambdas: dict_items([('pout', tensor([1.2327])), ('power', tensor([0.5831]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4026])), ('power', tensor([-23.5350]))])
epoch：346	 i:0 	 global-step:6920	 l-p:0.11151935905218124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.0485, 4.1247, 4.0847],
        [4.0485, 4.1883, 4.1427],
        [4.0485, 4.3868, 4.4123],
        [4.0485, 4.0681, 4.0527]], grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.11131161451339722 
model_pd.l_d.mean(): -13.226119041442871 
model_pd.lagr.mean(): -13.11480712890625 
model_pd.lambdas: dict_items([('pout', tensor([1.2331])), ('power', tensor([0.5819]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4016])), ('power', tensor([-23.5325]))])
epoch：347	 i:0 	 global-step:6940	 l-p:0.11131161451339722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[4.0516, 4.0516, 4.0516],
        [4.0516, 4.5541, 4.6965],
        [4.0516, 4.0516, 4.0516],
        [4.0516, 4.2912, 4.2682]], grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.11111731827259064 
model_pd.l_d.mean(): -13.198090553283691 
model_pd.lagr.mean(): -13.086973190307617 
model_pd.lambdas: dict_items([('pout', tensor([1.2335])), ('power', tensor([0.5807]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4006])), ('power', tensor([-23.5300]))])
epoch：348	 i:0 	 global-step:6960	 l-p:0.11111731827259064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.0543, 4.0747, 4.0588],
        [4.0543, 4.0548, 4.0543],
        [4.0543, 4.0657, 4.0561],
        [4.0543, 4.3932, 4.4187]], grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.11095129698514938 
model_pd.l_d.mean(): -13.17009449005127 
model_pd.lagr.mean(): -13.05914306640625 
model_pd.lambdas: dict_items([('pout', tensor([1.2339])), ('power', tensor([0.5795]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3997])), ('power', tensor([-23.5279]))])
epoch：349	 i:0 	 global-step:6980	 l-p:0.11095129698514938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[4.0565, 4.0566, 4.0565],
        [4.0565, 4.0566, 4.0565],
        [4.0565, 4.0565, 4.0565],
        [4.0565, 4.2379, 4.1975]], grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.11081866919994354 
model_pd.l_d.mean(): -13.14213752746582 
model_pd.lagr.mean(): -13.031318664550781 
model_pd.lambdas: dict_items([('pout', tensor([1.2343])), ('power', tensor([0.5784]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3990])), ('power', tensor([-23.5262]))])
epoch：350	 i:0 	 global-step:7000	 l-p:0.11081866919994354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.0583, 4.2983, 4.2751],
        [4.0583, 4.1907, 4.1445],
        [4.0583, 4.0787, 4.0628],
        [4.0583, 4.0584, 4.0583]], grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.11071527749300003 
model_pd.l_d.mean(): -13.114214897155762 
model_pd.lagr.mean(): -13.003499984741211 
model_pd.lambdas: dict_items([('pout', tensor([1.2347])), ('power', tensor([0.5772]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3984])), ('power', tensor([-23.5248]))])
epoch：351	 i:0 	 global-step:7020	 l-p:0.11071527749300003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[4.0598, 4.0654, 4.0604],
        [4.0598, 4.0712, 4.0616],
        [4.0598, 4.0598, 4.0598],
        [4.0598, 4.2907, 4.2642]], grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.11063016206026077 
model_pd.l_d.mean(): -13.086316108703613 
model_pd.lagr.mean(): -12.975686073303223 
model_pd.lambdas: dict_items([('pout', tensor([1.2351])), ('power', tensor([0.5760]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3980])), ('power', tensor([-23.5237]))])
epoch：352	 i:0 	 global-step:7040	 l-p:0.11063016206026077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[4.0612, 4.0612, 4.0612],
        [4.0612, 4.0612, 4.0612],
        [4.0612, 4.9343, 5.4367],
        [4.0612, 4.0726, 4.0630]], grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.11054891347885132 
model_pd.l_d.mean(): -13.058423042297363 
model_pd.lagr.mean(): -12.947874069213867 
model_pd.lambdas: dict_items([('pout', tensor([1.2355])), ('power', tensor([0.5748]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3975])), ('power', tensor([-23.5226]))])
epoch：353	 i:0 	 global-step:7060	 l-p:0.11054891347885132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[4.0628, 4.0629, 4.0628],
        [4.0628, 4.0685, 4.0634],
        [4.0628, 4.1391, 4.0990],
        [4.0628, 4.0628, 4.0628]], grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.11045791953802109 
model_pd.l_d.mean(): -13.030523300170898 
model_pd.lagr.mean(): -12.920064926147461 
model_pd.lambdas: dict_items([('pout', tensor([1.2359])), ('power', tensor([0.5737]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3970])), ('power', tensor([-23.5214]))])
epoch：354	 i:0 	 global-step:7080	 l-p:0.11045791953802109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[4.0647, 4.4347, 4.4788],
        [4.0647, 4.2049, 4.1590],
        [4.0647, 4.1276, 4.0914],
        [4.0647, 4.1971, 4.1509]], grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.11034750193357468 
model_pd.l_d.mean(): -13.002610206604004 
model_pd.lagr.mean(): -12.89226245880127 
model_pd.lambdas: dict_items([('pout', tensor([1.2363])), ('power', tensor([0.5725]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3964])), ('power', tensor([-23.5199]))])
epoch：355	 i:0 	 global-step:7100	 l-p:0.11034750193357468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[4.0671, 4.1545, 4.1119],
        [4.0671, 4.1995, 4.1533],
        [4.0671, 4.1169, 4.0855],
        [4.0671, 4.2721, 4.2373]], grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.11021414399147034 
model_pd.l_d.mean(): -12.974678039550781 
model_pd.lagr.mean(): -12.864463806152344 
model_pd.lambdas: dict_items([('pout', tensor([1.2367])), ('power', tensor([0.5713]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3956])), ('power', tensor([-23.5181]))])
epoch：356	 i:0 	 global-step:7120	 l-p:0.11021414399147034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[4.0698, 4.0699, 4.0698],
        [4.0698, 4.0699, 4.0698],
        [4.0698, 4.5787, 4.7249],
        [4.0698, 4.0712, 4.0699]], grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.11006031930446625 
model_pd.l_d.mean(): -12.946733474731445 
model_pd.lagr.mean(): -12.83667278289795 
model_pd.lambdas: dict_items([('pout', tensor([1.2371])), ('power', tensor([0.5701]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3948])), ('power', tensor([-23.5160]))])
epoch：357	 i:0 	 global-step:7140	 l-p:0.11006031930446625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[4.0728, 4.8354, 5.2183],
        [4.0728, 4.1759, 4.1309],
        [4.0728, 4.2132, 4.1671],
        [4.0728, 4.2054, 4.1591]], grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.10989312827587128 
model_pd.l_d.mean(): -12.918777465820312 
model_pd.lagr.mean(): -12.808884620666504 
model_pd.lambdas: dict_items([('pout', tensor([1.2375])), ('power', tensor([0.5690]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3938])), ('power', tensor([-23.5136]))])
epoch：358	 i:0 	 global-step:7160	 l-p:0.10989312827587128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.0759, 4.0963, 4.0804],
        [4.0759, 4.2164, 4.1703],
        [4.0759, 4.0759, 4.0759],
        [4.0759, 4.1168, 4.0894]], grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.10972175002098083 
model_pd.l_d.mean(): -12.890823364257812 
model_pd.lagr.mean(): -12.78110122680664 
model_pd.lambdas: dict_items([('pout', tensor([1.2379])), ('power', tensor([0.5678]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3928])), ('power', tensor([-23.5112]))])
epoch：359	 i:0 	 global-step:7180	 l-p:0.10972175002098083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.0789, 4.4503, 4.4944],
        [4.0789, 4.7852, 5.1106],
        [4.0789, 4.2610, 4.2203],
        [4.0789, 4.0846, 4.0796]], grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.10955479741096497 
model_pd.l_d.mean(): -12.862878799438477 
model_pd.lagr.mean(): -12.753323554992676 
model_pd.lambdas: dict_items([('pout', tensor([1.2383])), ('power', tensor([0.5666]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3918])), ('power', tensor([-23.5088]))])
epoch：360	 i:0 	 global-step:7200	 l-p:0.10955479741096497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[4.0818, 4.4795, 4.5409],
        [4.0818, 4.8060, 5.1487],
        [4.0818, 4.0818, 4.0818],
        [4.0818, 4.5925, 4.7391]], grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.10939859598875046 
model_pd.l_d.mean(): -12.834951400756836 
model_pd.lagr.mean(): -12.725552558898926 
model_pd.lambdas: dict_items([('pout', tensor([1.2387])), ('power', tensor([0.5654]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3909])), ('power', tensor([-23.5065]))])
epoch：361	 i:0 	 global-step:7220	 l-p:0.10939859598875046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.0845, 4.1880, 4.1428],
        [4.0845, 4.1256, 4.0981],
        [4.0845, 4.0847, 4.0845],
        [4.0845, 4.7920, 5.1178]], grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.10925579071044922 
model_pd.l_d.mean(): -12.80704402923584 
model_pd.lagr.mean(): -12.69778823852539 
model_pd.lambdas: dict_items([('pout', tensor([1.2391])), ('power', tensor([0.5643]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3900])), ('power', tensor([-23.5044]))])
epoch：362	 i:0 	 global-step:7240	 l-p:0.10925579071044922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.0870, 4.0871, 4.0871],
        [4.0870, 4.8528, 5.2372],
        [4.0870, 4.1074, 4.0916],
        [4.0870, 4.4284, 4.4538]], grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.10912574827671051 
model_pd.l_d.mean(): -12.779152870178223 
model_pd.lagr.mean(): -12.670026779174805 
model_pd.lambdas: dict_items([('pout', tensor([1.2395])), ('power', tensor([0.5631]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3892])), ('power', tensor([-23.5024]))])
epoch：363	 i:0 	 global-step:7260	 l-p:0.10912574827671051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.0894, 4.5966, 4.7399],
        [4.0894, 4.1098, 4.0939],
        [4.0894, 4.0914, 4.0895],
        [4.0894, 5.0507, 5.6481]], grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.109004907310009 
model_pd.l_d.mean(): -12.751280784606934 
model_pd.lagr.mean(): -12.6422758102417 
model_pd.lambdas: dict_items([('pout', tensor([1.2398])), ('power', tensor([0.5619]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3885])), ('power', tensor([-23.5006]))])
epoch：364	 i:0 	 global-step:7280	 l-p:0.109004907310009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.0917, 4.6043, 4.7518],
        [4.0917, 4.0917, 4.0917],
        [4.0917, 4.1796, 4.1367],
        [4.0917, 4.0917, 4.0917]], grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.10888812690973282 
model_pd.l_d.mean(): -12.723414421081543 
model_pd.lagr.mean(): -12.614526748657227 
model_pd.lambdas: dict_items([('pout', tensor([1.2402])), ('power', tensor([0.5607]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3878])), ('power', tensor([-23.4988]))])
epoch：365	 i:0 	 global-step:7300	 l-p:0.10888812690973282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[4.0941, 4.0961, 4.0942],
        [4.0941, 4.0967, 4.0943],
        [4.0941, 4.1976, 4.1523],
        [4.0941, 4.6069, 4.7545]], grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.10877002775669098 
model_pd.l_d.mean(): -12.69555377960205 
model_pd.lagr.mean(): -12.586783409118652 
model_pd.lambdas: dict_items([('pout', tensor([1.2406])), ('power', tensor([0.5596]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3870])), ('power', tensor([-23.4970]))])
epoch：366	 i:0 	 global-step:7320	 l-p:0.10877002775669098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.0966, 4.0967, 4.0966],
        [4.0966, 4.1167, 4.1010],
        [4.0966, 4.1734, 4.1329],
        [4.0966, 4.2231, 4.1764]], grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.10864607989788055 
model_pd.l_d.mean(): -12.66769027709961 
model_pd.lagr.mean(): -12.559043884277344 
model_pd.lambdas: dict_items([('pout', tensor([1.2410])), ('power', tensor([0.5584]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3862])), ('power', tensor([-23.4950]))])
epoch：367	 i:0 	 global-step:7340	 l-p:0.10864607989788055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[4.0992, 4.1582, 4.1231],
        [4.0992, 4.3321, 4.3051],
        [4.0992, 4.2404, 4.1940],
        [4.0992, 4.2258, 4.1791]], grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.10851350426673889 
model_pd.l_d.mean(): -12.639826774597168 
model_pd.lagr.mean(): -12.531312942504883 
model_pd.lambdas: dict_items([('pout', tensor([1.2414])), ('power', tensor([0.5572]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3854])), ('power', tensor([-23.4929]))])
epoch：368	 i:0 	 global-step:7360	 l-p:0.10851350426673889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[4.1021, 4.4447, 4.4700],
        [4.1021, 4.2434, 4.1969],
        [4.1021, 4.5619, 4.6669],
        [4.1021, 4.1522, 4.1206]], grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.10837176442146301 
model_pd.l_d.mean(): -12.611957550048828 
model_pd.lagr.mean(): -12.503585815429688 
model_pd.lambdas: dict_items([('pout', tensor([1.2418])), ('power', tensor([0.5560]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3844])), ('power', tensor([-23.4907]))])
epoch：369	 i:0 	 global-step:7380	 l-p:0.10837176442146301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.1052, 4.1109, 4.1058],
        [4.1052, 4.1822, 4.1416],
        [4.1052, 5.0707, 5.6706],
        [4.1052, 4.1254, 4.1097]], grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.10822201520204544 
model_pd.l_d.mean(): -12.584088325500488 
model_pd.lagr.mean(): -12.475866317749023 
model_pd.lambdas: dict_items([('pout', tensor([1.2422])), ('power', tensor([0.5549]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3835])), ('power', tensor([-23.4882]))])
epoch：370	 i:0 	 global-step:7400	 l-p:0.10822201520204544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[4.1084, 4.1966, 4.1535],
        [4.1084, 4.2421, 4.1953],
        [4.1084, 4.5691, 4.6742],
        [4.1084, 4.2124, 4.1669]], grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.10806703567504883 
model_pd.l_d.mean(): -12.556220054626465 
model_pd.lagr.mean(): -12.448152542114258 
model_pd.lambdas: dict_items([('pout', tensor([1.2425])), ('power', tensor([0.5537]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3824])), ('power', tensor([-23.4857]))])
epoch：371	 i:0 	 global-step:7420	 l-p:0.10806703567504883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.1118, 4.1119, 4.1118],
        [4.1118, 4.1118, 4.1118],
        [4.1118, 4.1138, 4.1119],
        [4.1118, 4.1118, 4.1118]], grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.10791002213954926 
model_pd.l_d.mean(): -12.528355598449707 
model_pd.lagr.mean(): -12.420445442199707 
model_pd.lambdas: dict_items([('pout', tensor([1.2429])), ('power', tensor([0.5525]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3814])), ('power', tensor([-23.4830]))])
epoch：372	 i:0 	 global-step:7440	 l-p:0.10791002213954926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.1151, 4.1153, 4.1151],
        [4.1151, 4.2568, 4.2102],
        [4.1151, 4.1922, 4.1515],
        [4.1151, 4.5162, 4.5779]], grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.10775413364171982 
model_pd.l_d.mean(): -12.500499725341797 
model_pd.lagr.mean(): -12.392745971679688 
model_pd.lambdas: dict_items([('pout', tensor([1.2433])), ('power', tensor([0.5513]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3803])), ('power', tensor([-23.4804]))])
epoch：373	 i:0 	 global-step:7460	 l-p:0.10775413364171982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[4.1184, 4.3022, 4.2609],
        [4.1184, 4.1186, 4.1184],
        [4.1184, 4.1241, 4.1190],
        [4.1184, 5.0057, 5.5158]], grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.10760168731212616 
model_pd.l_d.mean(): -12.47265338897705 
model_pd.lagr.mean(): -12.36505126953125 
model_pd.lambdas: dict_items([('pout', tensor([1.2437])), ('power', tensor([0.5502]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3793])), ('power', tensor([-23.4778]))])
epoch：374	 i:0 	 global-step:7480	 l-p:0.10760168731212616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.1217, 4.3560, 4.3287],
        [4.1217, 4.1243, 4.1218],
        [4.1217, 4.5235, 4.5853],
        [4.1217, 4.1318, 4.1232]], grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.10745402425527573 
model_pd.l_d.mean(): -12.444817543029785 
model_pd.lagr.mean(): -12.337363243103027 
model_pd.lambdas: dict_items([('pout', tensor([1.2441])), ('power', tensor([0.5490]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3782])), ('power', tensor([-23.4752]))])
epoch：375	 i:0 	 global-step:7500	 l-p:0.10745402425527573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.1248, 4.3089, 4.2676],
        [4.1248, 4.1248, 4.1248],
        [4.1248, 4.2292, 4.1835],
        [4.1248, 5.0960, 5.6993]], grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.10731115937232971 
model_pd.l_d.mean(): -12.416997909545898 
model_pd.lagr.mean(): -12.309686660766602 
model_pd.lambdas: dict_items([('pout', tensor([1.2444])), ('power', tensor([0.5478]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3772])), ('power', tensor([-23.4727]))])
epoch：376	 i:0 	 global-step:7520	 l-p:0.10731115937232971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[4.1280, 4.2702, 4.2234],
        [4.1280, 4.3719, 4.3479],
        [4.1280, 4.6450, 4.7933],
        [4.1280, 4.1300, 4.1281]], grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.10717228055000305 
model_pd.l_d.mean(): -12.389185905456543 
model_pd.lagr.mean(): -12.282013893127441 
model_pd.lambdas: dict_items([('pout', tensor([1.2448])), ('power', tensor([0.5466]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3762])), ('power', tensor([-23.4702]))])
epoch：377	 i:0 	 global-step:7540	 l-p:0.10717228055000305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.1311, 4.3394, 4.3037],
        [4.1311, 4.1312, 4.1311],
        [4.1311, 4.1816, 4.1497],
        [4.1311, 4.2587, 4.2115]], grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.10703584551811218 
model_pd.l_d.mean(): -12.361381530761719 
model_pd.lagr.mean(): -12.254345893859863 
model_pd.lambdas: dict_items([('pout', tensor([1.2452])), ('power', tensor([0.5455]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3752])), ('power', tensor([-23.4678]))])
epoch：378	 i:0 	 global-step:7560	 l-p:0.10703584551811218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]])
 pt:tensor([[4.1342, 5.1080, 5.7130],
        [4.1342, 4.2767, 4.2298],
        [4.1342, 4.5982, 4.7040],
        [4.1342, 4.9103, 5.2996]], grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.10689988732337952 
model_pd.l_d.mean(): -12.333585739135742 
model_pd.lagr.mean(): -12.226685523986816 
model_pd.lambdas: dict_items([('pout', tensor([1.2456])), ('power', tensor([0.5443]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3742])), ('power', tensor([-23.4653]))])
epoch：379	 i:0 	 global-step:7580	 l-p:0.10689988732337952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.1374, 4.1573, 4.1418],
        [4.1374, 4.4017, 4.3854],
        [4.1374, 4.6514, 4.7963],
        [4.1374, 4.1789, 4.1511]], grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.10676272213459015 
model_pd.l_d.mean(): -12.30579662322998 
model_pd.lagr.mean(): -12.199033737182617 
model_pd.lambdas: dict_items([('pout', tensor([1.2459])), ('power', tensor([0.5431]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3732])), ('power', tensor([-23.4627]))])
epoch：380	 i:0 	 global-step:7600	 l-p:0.10676272213459015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.1408, 4.1428, 4.1409],
        [4.1408, 4.5182, 4.5628],
        [4.1408, 4.1408, 4.1408],
        [4.1408, 4.1408, 4.1408]], grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.10662297904491425 
model_pd.l_d.mean(): -12.278009414672852 
model_pd.lagr.mean(): -12.17138671875 
model_pd.lambdas: dict_items([('pout', tensor([1.2463])), ('power', tensor([0.5419]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3722])), ('power', tensor([-23.4601]))])
epoch：381	 i:0 	 global-step:7620	 l-p:0.10662297904491425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.1442, 4.3799, 4.3524],
        [4.1442, 4.1456, 4.1443],
        [4.1442, 4.3711, 4.3407],
        [4.1442, 4.2722, 4.2249]], grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.10647986084222794 
model_pd.l_d.mean(): -12.250226974487305 
model_pd.lagr.mean(): -12.143747329711914 
model_pd.lambdas: dict_items([('pout', tensor([1.2467])), ('power', tensor([0.5408]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3711])), ('power', tensor([-23.4574]))])
epoch：382	 i:0 	 global-step:7640	 l-p:0.10647986084222794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[4.1477, 4.6684, 4.8180],
        [4.1477, 4.1497, 4.1478],
        [4.1477, 4.2527, 4.2067],
        [4.1477, 4.2828, 4.2354]], grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.10633327811956406 
model_pd.l_d.mean(): -12.222451210021973 
model_pd.lagr.mean(): -12.116117477416992 
model_pd.lambdas: dict_items([('pout', tensor([1.2470])), ('power', tensor([0.5396]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3700])), ('power', tensor([-23.4546]))])
epoch：383	 i:0 	 global-step:7660	 l-p:0.10633327811956406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[4.1514, 4.1720, 4.1559],
        [4.1514, 4.2797, 4.2322],
        [4.1514, 4.3788, 4.3483],
        [4.1514, 4.1519, 4.1514]], grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.10618370771408081 
model_pd.l_d.mean(): -12.194676399230957 
model_pd.lagr.mean(): -12.088492393493652 
model_pd.lambdas: dict_items([('pout', tensor([1.2474])), ('power', tensor([0.5384]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3688])), ('power', tensor([-23.4516]))])
epoch：384	 i:0 	 global-step:7680	 l-p:0.10618370771408081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[4.1551, 4.1654, 4.1567],
        [4.1551, 4.1552, 4.1551],
        [4.1551, 4.1551, 4.1551],
        [4.1551, 4.2331, 4.1919]], grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.10603199154138565 
model_pd.l_d.mean(): -12.166908264160156 
model_pd.lagr.mean(): -12.06087589263916 
model_pd.lambdas: dict_items([('pout', tensor([1.2478])), ('power', tensor([0.5373]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3676])), ('power', tensor([-23.4486]))])
epoch：385	 i:0 	 global-step:7700	 l-p:0.10603199154138565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[4.1590, 4.3448, 4.3030],
        [4.1590, 4.2483, 4.2046],
        [4.1590, 4.2876, 4.2400],
        [4.1590, 4.1591, 4.1590]], grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.10587911307811737 
model_pd.l_d.mean(): -12.139147758483887 
model_pd.lagr.mean(): -12.033268928527832 
model_pd.lambdas: dict_items([('pout', tensor([1.2482])), ('power', tensor([0.5361]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3664])), ('power', tensor([-23.4456]))])
epoch：386	 i:0 	 global-step:7720	 l-p:0.10587911307811737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[4.1629, 4.1629, 4.1629],
        [4.1629, 4.1634, 4.1629],
        [4.1629, 4.1629, 4.1629],
        [4.1629, 4.6308, 4.7374]], grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.10572626441717148 
model_pd.l_d.mean(): -12.111392974853516 
model_pd.lagr.mean(): -12.005666732788086 
model_pd.lambdas: dict_items([('pout', tensor([1.2485])), ('power', tensor([0.5349]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3652])), ('power', tensor([-23.4425]))])
epoch：387	 i:0 	 global-step:7740	 l-p:0.10572626441717148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.1668, 4.5157, 4.5413],
        [4.1668, 4.1668, 4.1668],
        [4.1668, 4.1670, 4.1668],
        [4.1668, 4.1668, 4.1668]], grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.10557417571544647 
model_pd.l_d.mean(): -12.083648681640625 
model_pd.lagr.mean(): -11.978074073791504 
model_pd.lambdas: dict_items([('pout', tensor([1.2489])), ('power', tensor([0.5337]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3640])), ('power', tensor([-23.4393]))])
epoch：388	 i:0 	 global-step:7760	 l-p:0.10557417571544647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[4.1707, 5.1552, 5.7668],
        [4.1707, 4.1708, 4.1707],
        [4.1707, 4.9553, 5.3488],
        [4.1707, 4.3147, 4.2672]], grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.10542349517345428 
model_pd.l_d.mean(): -12.055913925170898 
model_pd.lagr.mean(): -11.95048999786377 
model_pd.lambdas: dict_items([('pout', tensor([1.2492])), ('power', tensor([0.5326]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3627])), ('power', tensor([-23.4362]))])
epoch：389	 i:0 	 global-step:7780	 l-p:0.10542349517345428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[4.1746, 4.2805, 4.2341],
        [4.1746, 4.4418, 4.4253],
        [4.1746, 4.1746, 4.1746],
        [4.1746, 4.1863, 4.1765]], grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.10527441650629044 
model_pd.l_d.mean(): -12.028189659118652 
model_pd.lagr.mean(): -11.9229154586792 
model_pd.lambdas: dict_items([('pout', tensor([1.2496])), ('power', tensor([0.5314]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3615])), ('power', tensor([-23.4331]))])
epoch：390	 i:0 	 global-step:7800	 l-p:0.10527441650629044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[4.1786, 4.1786, 4.1786],
        [4.1786, 4.1994, 4.1832],
        [4.1786, 4.1806, 4.1787],
        [4.1786, 4.2432, 4.2058]], grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.10512683540582657 
model_pd.l_d.mean(): -12.00047492980957 
model_pd.lagr.mean(): -11.89534854888916 
model_pd.lambdas: dict_items([('pout', tensor([1.2500])), ('power', tensor([0.5302]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3602])), ('power', tensor([-23.4299]))])
epoch：391	 i:0 	 global-step:7820	 l-p:0.10512683540582657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[4.1826, 4.9104, 5.2453],
        [4.1826, 4.3190, 4.2712],
        [4.1826, 4.1831, 4.1826],
        [4.1826, 4.4121, 4.3813]], grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.10498039424419403 
model_pd.l_d.mean(): -11.972768783569336 
model_pd.lagr.mean(): -11.867788314819336 
model_pd.lambdas: dict_items([('pout', tensor([1.2503])), ('power', tensor([0.5291]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3590])), ('power', tensor([-23.4267]))])
epoch：392	 i:0 	 global-step:7840	 l-p:0.10498039424419403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[4.1866, 4.5962, 4.6591],
        [4.1866, 4.5693, 4.6145],
        [4.1866, 4.7127, 4.8635],
        [4.1866, 4.1893, 4.1868]], grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.10483463108539581 
model_pd.l_d.mean(): -11.94507122039795 
model_pd.lagr.mean(): -11.84023666381836 
model_pd.lambdas: dict_items([('pout', tensor([1.2507])), ('power', tensor([0.5279]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3577])), ('power', tensor([-23.4235]))])
epoch：393	 i:0 	 global-step:7860	 l-p:0.10483463108539581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[4.1906, 4.3205, 4.2724],
        [4.1906, 4.1906, 4.1906],
        [4.1906, 4.4391, 4.4146],
        [4.1906, 4.7129, 4.8601]], grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.10468894988298416 
model_pd.l_d.mean(): -11.917383193969727 
model_pd.lagr.mean(): -11.812694549560547 
model_pd.lambdas: dict_items([('pout', tensor([1.2510])), ('power', tensor([0.5267]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3565])), ('power', tensor([-23.4202]))])
epoch：394	 i:0 	 global-step:7880	 l-p:0.10468894988298416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.1947, 4.1947, 4.1947],
        [4.1947, 4.3826, 4.3404],
        [4.1947, 4.1947, 4.1947],
        [4.1947, 4.6053, 4.6684]], grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.10454282909631729 
model_pd.l_d.mean(): -11.889704704284668 
model_pd.lagr.mean(): -11.785161972045898 
model_pd.lambdas: dict_items([('pout', tensor([1.2514])), ('power', tensor([0.5255]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3552])), ('power', tensor([-23.4169]))])
epoch：395	 i:0 	 global-step:7900	 l-p:0.10454282909631729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.1989, 4.4480, 4.4235],
        [4.1989, 4.2504, 4.2178],
        [4.1989, 4.2198, 4.2035],
        [4.1989, 5.1919, 5.8088]], grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.10439588874578476 
model_pd.l_d.mean(): -11.862031936645508 
model_pd.lagr.mean(): -11.757636070251465 
model_pd.lambdas: dict_items([('pout', tensor([1.2517])), ('power', tensor([0.5244]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3539])), ('power', tensor([-23.4136]))])
epoch：396	 i:0 	 global-step:7920	 l-p:0.10439588874578476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.2032, 4.2032, 4.2032],
        [4.2032, 4.9535, 5.3083],
        [4.2032, 4.6769, 4.7849],
        [4.2032, 4.2032, 4.2032]], grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.1042478010058403 
model_pd.l_d.mean(): -11.834367752075195 
model_pd.lagr.mean(): -11.730119705200195 
model_pd.lambdas: dict_items([('pout', tensor([1.2521])), ('power', tensor([0.5232]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3525])), ('power', tensor([-23.4102]))])
epoch：397	 i:0 	 global-step:7940	 l-p:0.1042478010058403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.2075, 4.7378, 4.8902],
        [4.2075, 5.1189, 5.6428],
        [4.2075, 4.2286, 4.2122],
        [4.2075, 4.9410, 5.2785]], grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.10409852862358093 
model_pd.l_d.mean(): -11.806714057922363 
model_pd.lagr.mean(): -11.702615737915039 
model_pd.lambdas: dict_items([('pout', tensor([1.2525])), ('power', tensor([0.5220]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3512])), ('power', tensor([-23.4066]))])
epoch：398	 i:0 	 global-step:7960	 l-p:0.10409852862358093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[4.2120, 4.7377, 4.8860],
        [4.2120, 4.2134, 4.2120],
        [4.2120, 4.2121, 4.2120],
        [4.2120, 5.1246, 5.6493]], grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.10394822806119919 
model_pd.l_d.mean(): -11.77906322479248 
model_pd.lagr.mean(): -11.675114631652832 
model_pd.lambdas: dict_items([('pout', tensor([1.2528])), ('power', tensor([0.5209]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3498])), ('power', tensor([-23.4031]))])
epoch：399	 i:0 	 global-step:7980	 l-p:0.10394822806119919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.2165, 4.2224, 4.2171],
        [4.2165, 4.2376, 4.2212],
        [4.2165, 4.2165, 4.2165],
        [4.2165, 4.2368, 4.2209]], grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.10379710048437119 
model_pd.l_d.mean(): -11.751423835754395 
model_pd.lagr.mean(): -11.647626876831055 
model_pd.lambdas: dict_items([('pout', tensor([1.2531])), ('power', tensor([0.5197]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3484])), ('power', tensor([-23.3994]))])
epoch：400	 i:0 	 global-step:8000	 l-p:0.10379710048437119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.2211, 4.2216, 4.2211],
        [4.2211, 4.2270, 4.2217],
        [4.2211, 4.2211, 4.2211],
        [4.2211, 4.2821, 4.2458]], grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.1036454513669014 
model_pd.l_d.mean(): -11.723793983459473 
model_pd.lagr.mean(): -11.620148658752441 
model_pd.lambdas: dict_items([('pout', tensor([1.2535])), ('power', tensor([0.5185]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3469])), ('power', tensor([-23.3957]))])
epoch：401	 i:0 	 global-step:8020	 l-p:0.1036454513669014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.2257, 4.2284, 4.2259],
        [4.2257, 4.3333, 4.2862],
        [4.2257, 4.2258, 4.2257],
        [4.2257, 4.3571, 4.3085]], grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.10349362343549728 
model_pd.l_d.mean(): -11.696172714233398 
model_pd.lagr.mean(): -11.592679023742676 
model_pd.lambdas: dict_items([('pout', tensor([1.2538])), ('power', tensor([0.5174]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3455])), ('power', tensor([-23.3920]))])
epoch：402	 i:0 	 global-step:8040	 l-p:0.10349362343549728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.2304, 4.7083, 4.8173],
        [4.2304, 4.2309, 4.2304],
        [4.2304, 4.2331, 4.2306],
        [4.2304, 4.2304, 4.2304]], grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.10334188491106033 
model_pd.l_d.mean(): -11.668560981750488 
model_pd.lagr.mean(): -11.565218925476074 
model_pd.lambdas: dict_items([('pout', tensor([1.2542])), ('power', tensor([0.5162]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3440])), ('power', tensor([-23.3881]))])
epoch：403	 i:0 	 global-step:8060	 l-p:0.10334188491106033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[4.2352, 4.6511, 4.7151],
        [4.2352, 4.2352, 4.2352],
        [4.2352, 4.2564, 4.2399],
        [4.2352, 4.3740, 4.3254]], grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.10319049656391144 
model_pd.l_d.mean(): -11.640959739685059 
model_pd.lagr.mean(): -11.537769317626953 
model_pd.lambdas: dict_items([('pout', tensor([1.2545])), ('power', tensor([0.5150]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3425])), ('power', tensor([-23.3843]))])
epoch：404	 i:0 	 global-step:8080	 l-p:0.10319049656391144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.2400, 4.3015, 4.2649],
        [4.2400, 4.7704, 4.9202],
        [4.2400, 4.3059, 4.2678],
        [4.2400, 4.2400, 4.2400]], grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.10303961485624313 
model_pd.l_d.mean(): -11.613370895385742 
model_pd.lagr.mean(): -11.510331153869629 
model_pd.lambdas: dict_items([('pout', tensor([1.2549])), ('power', tensor([0.5138]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3411])), ('power', tensor([-23.3804]))])
epoch：405	 i:0 	 global-step:8100	 l-p:0.10303961485624313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[4.2448, 4.4791, 4.4477],
        [4.2448, 4.2469, 4.2449],
        [4.2448, 4.2448, 4.2448],
        [4.2448, 4.7815, 4.9358]], grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.10288933664560318 
model_pd.l_d.mean(): -11.58579158782959 
model_pd.lagr.mean(): -11.482902526855469 
model_pd.lambdas: dict_items([('pout', tensor([1.2552])), ('power', tensor([0.5127]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3395])), ('power', tensor([-23.3765]))])
epoch：406	 i:0 	 global-step:8120	 l-p:0.10288933664560318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[4.2497, 4.2497, 4.2497],
        [4.2497, 4.3159, 4.2776],
        [4.2497, 4.2498, 4.2497],
        [4.2497, 4.4843, 4.4530]], grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.1027396023273468 
model_pd.l_d.mean(): -11.558222770690918 
model_pd.lagr.mean(): -11.455483436584473 
model_pd.lambdas: dict_items([('pout', tensor([1.2555])), ('power', tensor([0.5115]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3380])), ('power', tensor([-23.3725]))])
epoch：407	 i:0 	 global-step:8140	 l-p:0.1027396023273468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228]])
 pt:tensor([[4.2546, 4.5083, 4.4835],
        [4.2546, 4.6732, 4.7377],
        [4.2546, 4.4897, 4.4583],
        [4.2546, 4.5287, 4.5120]], grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.10259035229682922 
model_pd.l_d.mean(): -11.530664443969727 
model_pd.lagr.mean(): -11.42807388305664 
model_pd.lambdas: dict_items([('pout', tensor([1.2559])), ('power', tensor([0.5103]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3365])), ('power', tensor([-23.3685]))])
epoch：408	 i:0 	 global-step:8160	 l-p:0.10259035229682922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.2596, 4.6789, 4.7435],
        [4.2596, 5.0233, 5.3848],
        [4.2596, 4.3520, 4.3068],
        [4.2596, 4.2808, 4.2643]], grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.1024414449930191 
model_pd.l_d.mean(): -11.503116607666016 
model_pd.lagr.mean(): -11.400674819946289 
model_pd.lambdas: dict_items([('pout', tensor([1.2562])), ('power', tensor([0.5092]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3350])), ('power', tensor([-23.3644]))])
epoch：409	 i:0 	 global-step:8180	 l-p:0.1024414449930191
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[4.2646, 4.3572, 4.3120],
        [4.2646, 4.3312, 4.2927],
        [4.2646, 4.6846, 4.7494],
        [4.2646, 4.3978, 4.3486]], grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.10229270905256271 
model_pd.l_d.mean(): -11.475578308105469 
model_pd.lagr.mean(): -11.373285293579102 
model_pd.lambdas: dict_items([('pout', tensor([1.2566])), ('power', tensor([0.5080]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3334])), ('power', tensor([-23.3604]))])
epoch：410	 i:0 	 global-step:8200	 l-p:0.10229270905256271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.2697, 4.3130, 4.2840],
        [4.2697, 4.2718, 4.2699],
        [4.2697, 4.8107, 4.9665],
        [4.2697, 4.4031, 4.3539]], grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.1021440327167511 
model_pd.l_d.mean(): -11.448054313659668 
model_pd.lagr.mean(): -11.34591007232666 
model_pd.lambdas: dict_items([('pout', tensor([1.2569])), ('power', tensor([0.5068]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3318])), ('power', tensor([-23.3562]))])
epoch：411	 i:0 	 global-step:8220	 l-p:0.1021440327167511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[4.2749, 4.2957, 4.2794],
        [4.2749, 5.2056, 5.7411],
        [4.2749, 4.8167, 4.9728],
        [4.2749, 4.4929, 4.4557]], grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.10199527442455292 
model_pd.l_d.mean(): -11.420539855957031 
model_pd.lagr.mean(): -11.318544387817383 
model_pd.lambdas: dict_items([('pout', tensor([1.2572])), ('power', tensor([0.5057]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3302])), ('power', tensor([-23.3520]))])
epoch：412	 i:0 	 global-step:8240	 l-p:0.10199527442455292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[4.2801, 4.2822, 4.2802],
        [4.2801, 5.2984, 5.9316],
        [4.2801, 4.5263, 4.4979],
        [4.2801, 4.4985, 4.4613]], grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.1018463596701622 
model_pd.l_d.mean(): -11.393034934997559 
model_pd.lagr.mean(): -11.29118824005127 
model_pd.lambdas: dict_items([('pout', tensor([1.2575])), ('power', tensor([0.5045]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3286])), ('power', tensor([-23.3477]))])
epoch：413	 i:0 	 global-step:8260	 l-p:0.1018463596701622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.2854, 4.8291, 4.9858],
        [4.2854, 4.2869, 4.2855],
        [4.2854, 5.2192, 5.7566],
        [4.2854, 4.2854, 4.2854]], grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.10169725120067596 
model_pd.l_d.mean(): -11.365540504455566 
model_pd.lagr.mean(): -11.263843536376953 
model_pd.lambdas: dict_items([('pout', tensor([1.2579])), ('power', tensor([0.5033]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3270])), ('power', tensor([-23.3434]))])
epoch：414	 i:0 	 global-step:8280	 l-p:0.10169725120067596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.2908, 4.7781, 4.8896],
        [4.2908, 4.2968, 4.2914],
        [4.2908, 4.3580, 4.3192],
        [4.2908, 4.3126, 4.2956]], grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.10154788941144943 
model_pd.l_d.mean(): -11.338058471679688 
model_pd.lagr.mean(): -11.236510276794434 
model_pd.lambdas: dict_items([('pout', tensor([1.2582])), ('power', tensor([0.5022]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3253])), ('power', tensor([-23.3390]))])
epoch：415	 i:0 	 global-step:8300	 l-p:0.10154788941144943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[4.2962, 4.3179, 4.3010],
        [4.2962, 4.8418, 4.9991],
        [4.2962, 4.4309, 4.3813],
        [4.2962, 4.7844, 4.8962]], grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.10139843821525574 
model_pd.l_d.mean(): -11.310587882995605 
model_pd.lagr.mean(): -11.209189414978027 
model_pd.lambdas: dict_items([('pout', tensor([1.2585])), ('power', tensor([0.5010]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3237])), ('power', tensor([-23.3346]))])
epoch：416	 i:0 	 global-step:8320	 l-p:0.10139843821525574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.3017, 4.7908, 4.9028],
        [4.3017, 4.3018, 4.3017],
        [4.3017, 4.4522, 4.4028],
        [4.3017, 4.3233, 4.3065]], grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.10124889016151428 
model_pd.l_d.mean(): -11.283130645751953 
model_pd.lagr.mean(): -11.18188190460205 
model_pd.lambdas: dict_items([('pout', tensor([1.2588])), ('power', tensor([0.4998]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3220])), ('power', tensor([-23.3301]))])
epoch：417	 i:0 	 global-step:8340	 l-p:0.10124889016151428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[4.3073, 4.5278, 4.4903],
        [4.3073, 4.8542, 5.0115],
        [4.3073, 4.4425, 4.3927],
        [4.3073, 4.3703, 4.3329]], grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.10109935700893402 
model_pd.l_d.mean(): -11.255681037902832 
model_pd.lagr.mean(): -11.154582023620605 
model_pd.lambdas: dict_items([('pout', tensor([1.2592])), ('power', tensor([0.4987]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3203])), ('power', tensor([-23.3255]))])
epoch：418	 i:0 	 global-step:8360	 l-p:0.10109935700893402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[4.3129, 4.3952, 4.3519],
        [4.3129, 4.3237, 4.3145],
        [4.3129, 4.3190, 4.3136],
        [4.3129, 4.5339, 4.4963]], grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.1009499728679657 
model_pd.l_d.mean(): -11.228245735168457 
model_pd.lagr.mean(): -11.12729549407959 
model_pd.lambdas: dict_items([('pout', tensor([1.2595])), ('power', tensor([0.4975]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3185])), ('power', tensor([-23.3209]))])
epoch：419	 i:0 	 global-step:8380	 l-p:0.1009499728679657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.3186, 4.3191, 4.3186],
        [4.3186, 4.3406, 4.3235],
        [4.3186, 4.3186, 4.3186],
        [4.3186, 4.3188, 4.3186]], grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.1008007600903511 
model_pd.l_d.mean(): -11.200818061828613 
model_pd.lagr.mean(): -11.100017547607422 
model_pd.lambdas: dict_items([('pout', tensor([1.2598])), ('power', tensor([0.4963]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3168])), ('power', tensor([-23.3162]))])
epoch：420	 i:0 	 global-step:8400	 l-p:0.1008007600903511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[4.3244, 5.3568, 5.9993],
        [4.3244, 4.3463, 4.3292],
        [4.3244, 4.3244, 4.3244],
        [4.3244, 4.5842, 4.5591]], grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.10065187513828278 
model_pd.l_d.mean(): -11.173409461975098 
model_pd.lagr.mean(): -11.072757720947266 
model_pd.lambdas: dict_items([('pout', tensor([1.2601])), ('power', tensor([0.4952]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3151])), ('power', tensor([-23.3115]))])
epoch：421	 i:0 	 global-step:8420	 l-p:0.10065187513828278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.3302, 4.3302, 4.3302],
        [4.3302, 5.0927, 5.4444],
        [4.3302, 4.3745, 4.3448],
        [4.3302, 4.3304, 4.3302]], grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.1005033329129219 
model_pd.l_d.mean(): -11.146008491516113 
model_pd.lagr.mean(): -11.04550552368164 
model_pd.lambdas: dict_items([('pout', tensor([1.2604])), ('power', tensor([0.4940]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3133])), ('power', tensor([-23.3067]))])
epoch：422	 i:0 	 global-step:8440	 l-p:0.1005033329129219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[4.3360, 4.3360, 4.3360],
        [4.3360, 4.5778, 4.5459],
        [4.3360, 4.3469, 4.3377],
        [4.3360, 4.8881, 5.0471]], grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.10035514831542969 
model_pd.l_d.mean(): -11.118622779846191 
model_pd.lagr.mean(): -11.018267631530762 
model_pd.lambdas: dict_items([('pout', tensor([1.2607])), ('power', tensor([0.4928]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3115])), ('power', tensor([-23.3019]))])
epoch：423	 i:0 	 global-step:8460	 l-p:0.10035514831542969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.3419, 4.6244, 4.6076],
        [4.3419, 5.1073, 5.4605],
        [4.3419, 4.5842, 4.5522],
        [4.3419, 4.3420, 4.3419]], grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.1002073585987091 
model_pd.l_d.mean(): -11.09124755859375 
model_pd.lagr.mean(): -10.991040229797363 
model_pd.lambdas: dict_items([('pout', tensor([1.2610])), ('power', tensor([0.4917]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3097])), ('power', tensor([-23.2971]))])
epoch：424	 i:0 	 global-step:8480	 l-p:0.1002073585987091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.3479, 4.3481, 4.3479],
        [4.3479, 4.3702, 4.3529],
        [4.3479, 4.3541, 4.3486],
        [4.3479, 5.3880, 6.0355]], grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.10005994886159897 
model_pd.l_d.mean(): -11.063886642456055 
model_pd.lagr.mean(): -10.963827133178711 
model_pd.lambdas: dict_items([('pout', tensor([1.2614])), ('power', tensor([0.4905]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3079])), ('power', tensor([-23.2921]))])
epoch：425	 i:0 	 global-step:8500	 l-p:0.10005994886159897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[4.3539, 4.4228, 4.3831],
        [4.3539, 4.3539, 4.3539],
        [4.3539, 4.5524, 4.5083],
        [4.3539, 4.9092, 5.0694]], grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.09991288930177689 
model_pd.l_d.mean(): -11.036539077758789 
model_pd.lagr.mean(): -10.936626434326172 
model_pd.lambdas: dict_items([('pout', tensor([1.2617])), ('power', tensor([0.4893]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3061])), ('power', tensor([-23.2872]))])
epoch：426	 i:0 	 global-step:8520	 l-p:0.09991288930177689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[4.3600, 4.6038, 4.5718],
        [4.3600, 4.4438, 4.3998],
        [4.3600, 4.5137, 4.4635],
        [4.3600, 4.4147, 4.3802]], grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.09976615756750107 
model_pd.l_d.mean(): -11.009203910827637 
model_pd.lagr.mean(): -10.909438133239746 
model_pd.lambdas: dict_items([('pout', tensor([1.2620])), ('power', tensor([0.4882]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3042])), ('power', tensor([-23.2822]))])
epoch：427	 i:0 	 global-step:8540	 l-p:0.09976615756750107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.3661, 4.3661, 4.3661],
        [4.3661, 4.3882, 4.3710],
        [4.3661, 4.3662, 4.3661],
        [4.3661, 4.3661, 4.3661]], grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.09961970150470734 
model_pd.l_d.mean(): -10.981881141662598 
model_pd.lagr.mean(): -10.882261276245117 
model_pd.lambdas: dict_items([('pout', tensor([1.2623])), ('power', tensor([0.4870]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3023])), ('power', tensor([-23.2771]))])
epoch：428	 i:0 	 global-step:8560	 l-p:0.09961970150470734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[4.3723, 4.7792, 4.8283],
        [4.3723, 4.6578, 4.6411],
        [4.3723, 5.4204, 6.0731],
        [4.3723, 4.6172, 4.5851]], grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.0994735062122345 
model_pd.l_d.mean(): -10.954571723937988 
model_pd.lagr.mean(): -10.855097770690918 
model_pd.lambdas: dict_items([('pout', tensor([1.2626])), ('power', tensor([0.4859]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3005])), ('power', tensor([-23.2720]))])
epoch：429	 i:0 	 global-step:8580	 l-p:0.0994735062122345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.3786, 4.3787, 4.3786],
        [4.3786, 4.3801, 4.3786],
        [4.3786, 4.3786, 4.3786],
        [4.3786, 4.3849, 4.3793]], grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.09932752698659897 
model_pd.l_d.mean(): -10.927275657653809 
model_pd.lagr.mean(): -10.827948570251465 
model_pd.lambdas: dict_items([('pout', tensor([1.2629])), ('power', tensor([0.4847]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2986])), ('power', tensor([-23.2668]))])
epoch：430	 i:0 	 global-step:8600	 l-p:0.09932752698659897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[4.3849, 4.4067, 4.3896],
        [4.3849, 4.5856, 4.5412],
        [4.3849, 4.9411, 5.0992],
        [4.3849, 4.6403, 4.6114]], grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.09918172657489777 
model_pd.l_d.mean(): -10.899994850158691 
model_pd.lagr.mean(): -10.800812721252441 
model_pd.lambdas: dict_items([('pout', tensor([1.2632])), ('power', tensor([0.4835]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2967])), ('power', tensor([-23.2616]))])
epoch：431	 i:0 	 global-step:8620	 l-p:0.09918172657489777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.3913, 4.3914, 4.3913],
        [4.3913, 4.5925, 4.5480],
        [4.3913, 4.3913, 4.3913],
        [4.3913, 5.1688, 5.5282]], grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.09903610497713089 
model_pd.l_d.mean(): -10.872725486755371 
model_pd.lagr.mean(): -10.773689270019531 
model_pd.lambdas: dict_items([('pout', tensor([1.2635])), ('power', tensor([0.4824]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2948])), ('power', tensor([-23.2563]))])
epoch：432	 i:0 	 global-step:8640	 l-p:0.09903610497713089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.3977, 4.4201, 4.4027],
        [4.3977, 4.3977, 4.3977],
        [4.3977, 4.6448, 4.6126],
        [4.3977, 4.4950, 4.4478]], grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.09889070689678192 
model_pd.l_d.mean(): -10.845471382141113 
model_pd.lagr.mean(): -10.746581077575684 
model_pd.lambdas: dict_items([('pout', tensor([1.2637])), ('power', tensor([0.4812]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2928])), ('power', tensor([-23.2510]))])
epoch：433	 i:0 	 global-step:8660	 l-p:0.09889070689678192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[4.4042, 4.6614, 4.6324],
        [4.4042, 4.6929, 4.6762],
        [4.4042, 4.4058, 4.4043],
        [4.4042, 4.6519, 4.6196]], grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.09874552488327026 
model_pd.l_d.mean(): -10.818227767944336 
model_pd.lagr.mean(): -10.719482421875 
model_pd.lambdas: dict_items([('pout', tensor([1.2640])), ('power', tensor([0.4800]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2909])), ('power', tensor([-23.2456]))])
epoch：434	 i:0 	 global-step:8680	 l-p:0.09874552488327026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.4108, 4.4337, 4.4159],
        [4.4108, 4.4113, 4.4108],
        [4.4108, 4.4328, 4.4156],
        [4.4108, 4.7891, 4.8182]], grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.09860055893659592 
model_pd.l_d.mean(): -10.791000366210938 
model_pd.lagr.mean(): -10.692399978637695 
model_pd.lambdas: dict_items([('pout', tensor([1.2643])), ('power', tensor([0.4789]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2889])), ('power', tensor([-23.2401]))])
epoch：435	 i:0 	 global-step:8700	 l-p:0.09860055893659592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[4.4174, 4.4174, 4.4174],
        [4.4174, 4.6758, 4.6468],
        [4.4174, 4.4402, 4.4225],
        [4.4174, 4.4175, 4.4174]], grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.09845590591430664 
model_pd.l_d.mean(): -10.763789176940918 
model_pd.lagr.mean(): -10.665332794189453 
model_pd.lambdas: dict_items([('pout', tensor([1.2646])), ('power', tensor([0.4777]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2869])), ('power', tensor([-23.2346]))])
epoch：436	 i:0 	 global-step:8720	 l-p:0.09845590591430664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.4241, 4.8042, 4.8335],
        [4.4241, 4.4802, 4.4449],
        [4.4241, 4.4257, 4.4241],
        [4.4241, 4.4901, 4.4511]], grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.09831149131059647 
model_pd.l_d.mean(): -10.736588478088379 
model_pd.lagr.mean(): -10.638277053833008 
model_pd.lambdas: dict_items([('pout', tensor([1.2649])), ('power', tensor([0.4766]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2849])), ('power', tensor([-23.2291]))])
epoch：437	 i:0 	 global-step:8740	 l-p:0.09831149131059647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[4.4308, 5.2823, 5.7117],
        [4.4308, 4.5018, 4.4611],
        [4.4308, 4.5799, 4.5284],
        [4.4308, 4.4314, 4.4308]], grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.09816741943359375 
model_pd.l_d.mean(): -10.709403991699219 
model_pd.lagr.mean(): -10.611236572265625 
model_pd.lambdas: dict_items([('pout', tensor([1.2652])), ('power', tensor([0.4754]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2829])), ('power', tensor([-23.2235]))])
epoch：438	 i:0 	 global-step:8760	 l-p:0.09816741943359375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[4.4376, 4.6687, 4.6302],
        [4.4376, 4.4376, 4.4376],
        [4.4376, 4.4392, 4.4377],
        [4.4376, 4.4377, 4.4376]], grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.0980236753821373 
model_pd.l_d.mean(): -10.682232856750488 
model_pd.lagr.mean(): -10.584209442138672 
model_pd.lambdas: dict_items([('pout', tensor([1.2655])), ('power', tensor([0.4742]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2809])), ('power', tensor([-23.2178]))])
epoch：439	 i:0 	 global-step:8780	 l-p:0.0980236753821373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[4.4445, 4.7054, 4.6763],
        [4.4445, 5.2354, 5.6016],
        [4.4445, 4.5311, 4.4858],
        [4.4445, 4.4461, 4.4445]], grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.0978802964091301 
model_pd.l_d.mean(): -10.655078887939453 
model_pd.lagr.mean(): -10.557198524475098 
model_pd.lambdas: dict_items([('pout', tensor([1.2657])), ('power', tensor([0.4731]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2788])), ('power', tensor([-23.2121]))])
epoch：440	 i:0 	 global-step:8800	 l-p:0.0978802964091301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.4514, 4.4629, 4.4531],
        [4.4514, 5.4352, 6.0032],
        [4.4514, 4.4745, 4.4565],
        [4.4514, 4.5508, 4.5027]], grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.09773731976747513 
model_pd.l_d.mean(): -10.627936363220215 
model_pd.lagr.mean(): -10.53019905090332 
model_pd.lambdas: dict_items([('pout', tensor([1.2660])), ('power', tensor([0.4719]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2768])), ('power', tensor([-23.2063]))])
epoch：441	 i:0 	 global-step:8820	 l-p:0.09773731976747513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[4.4583, 4.5757, 4.5249],
        [4.4583, 4.6646, 4.6194],
        [4.4583, 4.5301, 4.4890],
        [4.4583, 4.4606, 4.4585]], grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.09759476035833359 
model_pd.l_d.mean(): -10.600810050964355 
model_pd.lagr.mean(): -10.503214836120605 
model_pd.lambdas: dict_items([('pout', tensor([1.2663])), ('power', tensor([0.4708]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2747])), ('power', tensor([-23.2005]))])
epoch：442	 i:0 	 global-step:8840	 l-p:0.09759476035833359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.4653, 4.4655, 4.4653],
        [4.4653, 4.4654, 4.4653],
        [4.4653, 4.5325, 4.4929],
        [4.4653, 4.5122, 4.4810]], grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.09745261818170547 
model_pd.l_d.mean(): -10.573701858520508 
model_pd.lagr.mean(): -10.476249694824219 
model_pd.lambdas: dict_items([('pout', tensor([1.2666])), ('power', tensor([0.4696]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2726])), ('power', tensor([-23.1947]))])
epoch：443	 i:0 	 global-step:8860	 l-p:0.09745261818170547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[4.4724, 4.4724, 4.4724],
        [4.4724, 4.5904, 4.5394],
        [4.4724, 4.8935, 4.9452],
        [4.4724, 4.5600, 4.5143]], grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.0973108783364296 
model_pd.l_d.mean(): -10.546603202819824 
model_pd.lagr.mean(): -10.449292182922363 
model_pd.lambdas: dict_items([('pout', tensor([1.2668])), ('power', tensor([0.4684]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2705])), ('power', tensor([-23.1888]))])
epoch：444	 i:0 	 global-step:8880	 l-p:0.0973108783364296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[4.4795, 5.2988, 5.6889],
        [4.4795, 4.4795, 4.4795],
        [4.4795, 4.9310, 5.0025],
        [4.4795, 4.5520, 4.5105]], grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.09716950356960297 
model_pd.l_d.mean(): -10.519524574279785 
model_pd.lagr.mean(): -10.422354698181152 
model_pd.lambdas: dict_items([('pout', tensor([1.2671])), ('power', tensor([0.4673]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2684])), ('power', tensor([-23.1828]))])
epoch：445	 i:0 	 global-step:8900	 l-p:0.09716950356960297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.4867, 4.4868, 4.4867],
        [4.4867, 4.4868, 4.4867],
        [4.4867, 4.4868, 4.4867],
        [4.4867, 5.2884, 5.6601]], grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.09702853858470917 
model_pd.l_d.mean(): -10.49245834350586 
model_pd.lagr.mean(): -10.395429611206055 
model_pd.lambdas: dict_items([('pout', tensor([1.2674])), ('power', tensor([0.4661]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2663])), ('power', tensor([-23.1768]))])
epoch：446	 i:0 	 global-step:8920	 l-p:0.09702853858470917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[4.4939, 5.3169, 5.7091],
        [4.4939, 4.7029, 4.6574],
        [4.4939, 4.7920, 4.7755],
        [4.4939, 5.0764, 5.2463]], grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.09688792377710342 
model_pd.l_d.mean(): -10.465410232543945 
model_pd.lagr.mean(): -10.368522644042969 
model_pd.lambdas: dict_items([('pout', tensor([1.2676])), ('power', tensor([0.4650]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2642])), ('power', tensor([-23.1707]))])
epoch：447	 i:0 	 global-step:8940	 l-p:0.09688792377710342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[4.5012, 5.5005, 6.0782],
        [4.5012, 5.3067, 5.6803],
        [4.5012, 5.0240, 5.1457],
        [4.5012, 4.7780, 4.7526]], grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.09674764424562454 
model_pd.l_d.mean(): -10.438374519348145 
model_pd.lagr.mean(): -10.34162712097168 
model_pd.lambdas: dict_items([('pout', tensor([1.2679])), ('power', tensor([0.4638]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2620])), ('power', tensor([-23.1646]))])
epoch：448	 i:0 	 global-step:8960	 l-p:0.09674764424562454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.5086, 4.5324, 4.5139],
        [4.5086, 4.5975, 4.5512],
        [4.5086, 4.5204, 4.5104],
        [4.5086, 5.3159, 5.6905]], grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.09660767763853073 
model_pd.l_d.mean(): -10.411355972290039 
model_pd.lagr.mean(): -10.314748764038086 
model_pd.lambdas: dict_items([('pout', tensor([1.2682])), ('power', tensor([0.4626]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2599])), ('power', tensor([-23.1585]))])
epoch：449	 i:0 	 global-step:8980	 l-p:0.09660767763853073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.5160, 5.3448, 5.7399],
        [4.5160, 4.7838, 4.7545],
        [4.5160, 4.6360, 4.5843],
        [4.5160, 4.5278, 4.5178]], grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.09646803140640259 
model_pd.l_d.mean(): -10.384353637695312 
model_pd.lagr.mean(): -10.287885665893555 
model_pd.lambdas: dict_items([('pout', tensor([1.2684])), ('power', tensor([0.4615]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2577])), ('power', tensor([-23.1522]))])
epoch：450	 i:0 	 global-step:9000	 l-p:0.09646803140640259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.5234, 4.8247, 4.8083],
        [4.5234, 5.3346, 5.7111],
        [4.5234, 4.5234, 4.5234],
        [4.5234, 4.9817, 5.0548]], grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.09632869809865952 
model_pd.l_d.mean(): -10.357366561889648 
model_pd.lagr.mean(): -10.261037826538086 
model_pd.lambdas: dict_items([('pout', tensor([1.2687])), ('power', tensor([0.4603]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2555])), ('power', tensor([-23.1460]))])
epoch：451	 i:0 	 global-step:9020	 l-p:0.09632869809865952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[4.5310, 4.5310, 4.5310],
        [4.5310, 4.5310, 4.5310],
        [4.5310, 4.6336, 4.5842],
        [4.5310, 4.5550, 4.5364]], grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.09618967026472092 
model_pd.l_d.mean(): -10.330394744873047 
model_pd.lagr.mean(): -10.23420524597168 
model_pd.lambdas: dict_items([('pout', tensor([1.2689])), ('power', tensor([0.4592]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2533])), ('power', tensor([-23.1396]))])
epoch：452	 i:0 	 global-step:9040	 l-p:0.09618967026472092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[4.5385, 5.3733, 5.7716],
        [4.5385, 4.5869, 4.5547],
        [4.5385, 4.5385, 4.5385],
        [4.5385, 4.8414, 4.8251]], grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.09605094790458679 
model_pd.l_d.mean(): -10.303441047668457 
model_pd.lagr.mean(): -10.207389831542969 
model_pd.lambdas: dict_items([('pout', tensor([1.2692])), ('power', tensor([0.4580]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2511])), ('power', tensor([-23.1333]))])
epoch：453	 i:0 	 global-step:9060	 l-p:0.09605094790458679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[4.5462, 5.1387, 5.3122],
        [4.5462, 5.3829, 5.7823],
        [4.5462, 4.7593, 4.7132],
        [4.5462, 4.8170, 4.7876]], grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.09591256827116013 
model_pd.l_d.mean(): -10.276501655578613 
model_pd.lagr.mean(): -10.180588722229004 
model_pd.lambdas: dict_items([('pout', tensor([1.2694])), ('power', tensor([0.4569]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2488])), ('power', tensor([-23.1268]))])
epoch：454	 i:0 	 global-step:9080	 l-p:0.09591256827116013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.5539, 4.5539, 4.5539],
        [4.5539, 5.4392, 5.8873],
        [4.5539, 4.9869, 5.0410],
        [4.5539, 4.7024, 4.6488]], grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.09577445685863495 
model_pd.l_d.mean(): -10.249579429626465 
model_pd.lagr.mean(): -10.153804779052734 
model_pd.lambdas: dict_items([('pout', tensor([1.2697])), ('power', tensor([0.4557]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2466])), ('power', tensor([-23.1203]))])
epoch：455	 i:0 	 global-step:9100	 l-p:0.09577445685863495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.5616, 4.5640, 4.5618],
        [4.5616, 4.6105, 4.5780],
        [4.5616, 4.7276, 4.6744],
        [4.5616, 4.5617, 4.5616]], grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.09563671052455902 
model_pd.l_d.mean(): -10.222672462463379 
model_pd.lagr.mean(): -10.127036094665527 
model_pd.lambdas: dict_items([('pout', tensor([1.2699])), ('power', tensor([0.4545]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2443])), ('power', tensor([-23.1138]))])
epoch：456	 i:0 	 global-step:9120	 l-p:0.09563671052455902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.5694, 4.7844, 4.7381],
        [4.5694, 4.8425, 4.8131],
        [4.5694, 4.5719, 4.5696],
        [4.5694, 4.6185, 4.5859]], grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.09549932181835175 
model_pd.l_d.mean(): -10.195784568786621 
model_pd.lagr.mean(): -10.100285530090332 
model_pd.lambdas: dict_items([('pout', tensor([1.2702])), ('power', tensor([0.4534]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2421])), ('power', tensor([-23.1072]))])
epoch：457	 i:0 	 global-step:9140	 l-p:0.09549932181835175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[4.5773, 4.5773, 4.5773],
        [4.5773, 5.1760, 5.3516],
        [4.5773, 4.8511, 4.8217],
        [4.5773, 4.5779, 4.5773]], grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.09536229819059372 
model_pd.l_d.mean(): -10.16891098022461 
model_pd.lagr.mean(): -10.073548316955566 
model_pd.lambdas: dict_items([('pout', tensor([1.2704])), ('power', tensor([0.4522]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2398])), ('power', tensor([-23.1006]))])
epoch：458	 i:0 	 global-step:9160	 l-p:0.09536229819059372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.5852, 4.6090, 4.5905],
        [4.5852, 4.8291, 4.7897],
        [4.5852, 4.6901, 4.6398],
        [4.5852, 4.5852, 4.5852]], grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.09522564709186554 
model_pd.l_d.mean(): -10.142054557800293 
model_pd.lagr.mean(): -10.046829223632812 
model_pd.lambdas: dict_items([('pout', tensor([1.2706])), ('power', tensor([0.4511]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2375])), ('power', tensor([-23.0939]))])
epoch：459	 i:0 	 global-step:9180	 l-p:0.09522564709186554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.5932, 4.6691, 4.6259],
        [4.5932, 5.1950, 5.3718],
        [4.5932, 4.5934, 4.5932],
        [4.5932, 4.5932, 4.5932]], grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.0950893759727478 
model_pd.l_d.mean(): -10.115215301513672 
model_pd.lagr.mean(): -10.020126342773438 
model_pd.lambdas: dict_items([('pout', tensor([1.2709])), ('power', tensor([0.4499]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2352])), ('power', tensor([-23.0871]))])
epoch：460	 i:0 	 global-step:9200	 l-p:0.0950893759727478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.6012, 4.6012, 4.6012],
        [4.6012, 4.8882, 4.8627],
        [4.6012, 4.6037, 4.6013],
        [4.6012, 4.7525, 4.6981]], grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.0949534922838211 
model_pd.l_d.mean(): -10.088394165039062 
model_pd.lagr.mean(): -9.993440628051758 
model_pd.lambdas: dict_items([('pout', tensor([1.2711])), ('power', tensor([0.4488]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2329])), ('power', tensor([-23.0803]))])
epoch：461	 i:0 	 global-step:9220	 l-p:0.0949534922838211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[4.6093, 4.6093, 4.6093],
        [4.6093, 4.7019, 4.6540],
        [4.6093, 4.6093, 4.6093],
        [4.6093, 4.8863, 4.8568]], grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.09481800347566605 
model_pd.l_d.mean(): -10.061589241027832 
model_pd.lagr.mean(): -9.966771125793457 
model_pd.lambdas: dict_items([('pout', tensor([1.2713])), ('power', tensor([0.4476]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2306])), ('power', tensor([-23.0735]))])
epoch：462	 i:0 	 global-step:9240	 l-p:0.09481800347566605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[4.6174, 4.7237, 4.6728],
        [4.6174, 4.6424, 4.6231],
        [4.6174, 4.7777, 4.7233],
        [4.6174, 5.1608, 5.2888]], grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.09468289464712143 
model_pd.l_d.mean(): -10.034801483154297 
model_pd.lagr.mean(): -9.940118789672852 
model_pd.lambdas: dict_items([('pout', tensor([1.2716])), ('power', tensor([0.4465]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2282])), ('power', tensor([-23.0666]))])
epoch：463	 i:0 	 global-step:9260	 l-p:0.09468289464712143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.6256, 4.9380, 4.9220],
        [4.6256, 4.6257, 4.6256],
        [4.6256, 4.6508, 4.6313],
        [4.6256, 5.0693, 5.1256]], grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.09454812109470367 
model_pd.l_d.mean(): -10.008031845092773 
model_pd.lagr.mean(): -9.913483619689941 
model_pd.lambdas: dict_items([('pout', tensor([1.2718])), ('power', tensor([0.4453]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2259])), ('power', tensor([-23.0596]))])
epoch：464	 i:0 	 global-step:9280	 l-p:0.09454812109470367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[4.6338, 4.6591, 4.6396],
        [4.6338, 4.7274, 4.6790],
        [4.6338, 4.6480, 4.6362],
        [4.6338, 4.9472, 4.9312]], grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.09441375732421875 
model_pd.l_d.mean(): -9.981278419494629 
model_pd.lagr.mean(): -9.88686466217041 
model_pd.lambdas: dict_items([('pout', tensor([1.2720])), ('power', tensor([0.4442]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2235])), ('power', tensor([-23.0527]))])
epoch：465	 i:0 	 global-step:9300	 l-p:0.09441375732421875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.6421, 4.7495, 4.6982],
        [4.6421, 4.6666, 4.6476],
        [4.6421, 4.6671, 4.6478],
        [4.6421, 4.6421, 4.6421]], grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.09427974373102188 
model_pd.l_d.mean(): -9.954543113708496 
model_pd.lagr.mean(): -9.86026382446289 
model_pd.lambdas: dict_items([('pout', tensor([1.2722])), ('power', tensor([0.4430]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2212])), ('power', tensor([-23.0456]))])
epoch：466	 i:0 	 global-step:9320	 l-p:0.09427974373102188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[4.6504, 4.6504, 4.6504],
        [4.6504, 4.9657, 4.9498],
        [4.6504, 5.6967, 6.3040],
        [4.6504, 4.6506, 4.6504]], grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.09414607286453247 
model_pd.l_d.mean(): -9.927825927734375 
model_pd.lagr.mean(): -9.833680152893066 
model_pd.lambdas: dict_items([('pout', tensor([1.2725])), ('power', tensor([0.4419]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2188])), ('power', tensor([-23.0385]))])
epoch：467	 i:0 	 global-step:9340	 l-p:0.09414607286453247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[4.6588, 4.9095, 4.8696],
        [4.6588, 5.0711, 5.1055],
        [4.6588, 4.6661, 4.6597],
        [4.6588, 5.5257, 5.9412]], grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.09401274472475052 
model_pd.l_d.mean(): -9.901124954223633 
model_pd.lagr.mean(): -9.807111740112305 
model_pd.lambdas: dict_items([('pout', tensor([1.2727])), ('power', tensor([0.4407]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2164])), ('power', tensor([-23.0313]))])
epoch：468	 i:0 	 global-step:9360	 l-p:0.09401274472475052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.6673, 4.6674, 4.6673],
        [4.6673, 5.5161, 5.9122],
        [4.6673, 4.6707, 4.6676],
        [4.6673, 5.0808, 5.1154]], grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.09387975931167603 
model_pd.l_d.mean(): -9.874444007873535 
model_pd.lagr.mean(): -9.780564308166504 
model_pd.lambdas: dict_items([('pout', tensor([1.2729])), ('power', tensor([0.4395]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2140])), ('power', tensor([-23.0241]))])
epoch：469	 i:0 	 global-step:9380	 l-p:0.09387975931167603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[4.6758, 4.6759, 4.6758],
        [4.6758, 4.9596, 4.9301],
        [4.6758, 4.7544, 4.7099],
        [4.6758, 4.6792, 4.6761]], grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.09374707192182541 
model_pd.l_d.mean(): -9.847780227661133 
model_pd.lagr.mean(): -9.754033088684082 
model_pd.lambdas: dict_items([('pout', tensor([1.2731])), ('power', tensor([0.4384]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2116])), ('power', tensor([-23.0169]))])
epoch：470	 i:0 	 global-step:9400	 l-p:0.09374707192182541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[4.6844, 4.7799, 4.7307],
        [4.6844, 4.6917, 4.6852],
        [4.6844, 4.9586, 4.9254],
        [4.6844, 4.7633, 4.7186]], grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.09361471980810165 
model_pd.l_d.mean(): -9.821136474609375 
model_pd.lagr.mean(): -9.727521896362305 
model_pd.lambdas: dict_items([('pout', tensor([1.2733])), ('power', tensor([0.4372]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2091])), ('power', tensor([-23.0096]))])
epoch：471	 i:0 	 global-step:9420	 l-p:0.09361471980810165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[4.6930, 4.9468, 4.9067],
        [4.6930, 5.0130, 4.9974],
        [4.6930, 4.8026, 4.7504],
        [4.6930, 4.7669, 4.7238]], grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.09348270297050476 
model_pd.l_d.mean(): -9.794507026672363 
model_pd.lagr.mean(): -9.701024055480957 
model_pd.lambdas: dict_items([('pout', tensor([1.2735])), ('power', tensor([0.4361]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2067])), ('power', tensor([-23.0022]))])
epoch：472	 i:0 	 global-step:9440	 l-p:0.09348270297050476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.7017, 4.7274, 4.7075],
        [4.7017, 4.7268, 4.7073],
        [4.7017, 5.2605, 5.3932],
        [4.7017, 4.7017, 4.7017]], grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.09335099905729294 
model_pd.l_d.mean(): -9.76789665222168 
model_pd.lagr.mean(): -9.674545288085938 
model_pd.lambdas: dict_items([('pout', tensor([1.2737])), ('power', tensor([0.4349]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2042])), ('power', tensor([-22.9948]))])
epoch：473	 i:0 	 global-step:9460	 l-p:0.09335099905729294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.7104, 4.9370, 4.8893],
        [4.7104, 4.7626, 4.7281],
        [4.7104, 5.3295, 5.5097],
        [4.7104, 4.7104, 4.7104]], grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.09321962296962738 
model_pd.l_d.mean(): -9.741307258605957 
model_pd.lagr.mean(): -9.648087501525879 
model_pd.lambdas: dict_items([('pout', tensor([1.2739])), ('power', tensor([0.4338]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2018])), ('power', tensor([-22.9873]))])
epoch：474	 i:0 	 global-step:9480	 l-p:0.09321962296962738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.7192, 4.7194, 4.7192],
        [4.7192, 4.9754, 4.9352],
        [4.7192, 4.7192, 4.7192],
        [4.7192, 5.2088, 5.2894]], grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.09308856725692749 
model_pd.l_d.mean(): -9.714733123779297 
model_pd.lagr.mean(): -9.621644973754883 
model_pd.lambdas: dict_items([('pout', tensor([1.2741])), ('power', tensor([0.4326]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1993])), ('power', tensor([-22.9798]))])
epoch：475	 i:0 	 global-step:9500	 l-p:0.09308856725692749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[4.7281, 4.8870, 4.8306],
        [4.7281, 5.6625, 6.1383],
        [4.7281, 4.7315, 4.7283],
        [4.7281, 4.7919, 4.7524]], grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.09295784682035446 
model_pd.l_d.mean(): -9.688180923461914 
model_pd.lagr.mean(): -9.595223426818848 
model_pd.lambdas: dict_items([('pout', tensor([1.2743])), ('power', tensor([0.4315]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1968])), ('power', tensor([-22.9722]))])
epoch：476	 i:0 	 global-step:9520	 l-p:0.09295784682035446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.7370, 4.7519, 4.7394],
        [4.7370, 5.2295, 5.3107],
        [4.7370, 5.0620, 5.0466],
        [4.7370, 4.7370, 4.7370]], grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.0928274616599083 
model_pd.l_d.mean(): -9.66164493560791 
model_pd.lagr.mean(): -9.568817138671875 
model_pd.lambdas: dict_items([('pout', tensor([1.2745])), ('power', tensor([0.4304]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1943])), ('power', tensor([-22.9646]))])
epoch：477	 i:0 	 global-step:9540	 l-p:0.0928274616599083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[4.7459, 4.9143, 4.8580],
        [4.7459, 4.8774, 4.8219],
        [4.7459, 4.7989, 4.7639],
        [4.7459, 4.7724, 4.7520]], grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.09269741177558899 
model_pd.l_d.mean(): -9.635129928588867 
model_pd.lagr.mean(): -9.54243278503418 
model_pd.lambdas: dict_items([('pout', tensor([1.2747])), ('power', tensor([0.4292]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1918])), ('power', tensor([-22.9569]))])
epoch：478	 i:0 	 global-step:9560	 l-p:0.09269741177558899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.7549, 5.6971, 6.1772],
        [4.7549, 4.7556, 4.7550],
        [4.7549, 4.8362, 4.7904],
        [4.7549, 4.9155, 4.8587]], grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.09256768971681595 
model_pd.l_d.mean(): -9.608633041381836 
model_pd.lagr.mean(): -9.51606559753418 
model_pd.lambdas: dict_items([('pout', tensor([1.2749])), ('power', tensor([0.4281]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1893])), ('power', tensor([-22.9492]))])
epoch：479	 i:0 	 global-step:9580	 l-p:0.09256768971681595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.7640, 4.9433, 4.8872],
        [4.7640, 5.8467, 6.4772],
        [4.7640, 5.7088, 6.1903],
        [4.7640, 4.7909, 4.7702]], grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.09243831038475037 
model_pd.l_d.mean(): -9.582154273986816 
model_pd.lagr.mean(): -9.489715576171875 
model_pd.lambdas: dict_items([('pout', tensor([1.2751])), ('power', tensor([0.4269]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1868])), ('power', tensor([-22.9414]))])
epoch：480	 i:0 	 global-step:9600	 l-p:0.09243831038475037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[4.7731, 5.4103, 5.5996],
        [4.7731, 4.7991, 4.7790],
        [4.7731, 4.7731, 4.7731],
        [4.7731, 4.7731, 4.7731]], grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.09230924397706985 
model_pd.l_d.mean(): -9.555695533752441 
model_pd.lagr.mean(): -9.463386535644531 
model_pd.lambdas: dict_items([('pout', tensor([1.2753])), ('power', tensor([0.4258]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1842])), ('power', tensor([-22.9336]))])
epoch：481	 i:0 	 global-step:9620	 l-p:0.09230924397706985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.7823, 5.0885, 5.0632],
        [4.7823, 4.8817, 4.8308],
        [4.7823, 5.0149, 4.9666],
        [4.7823, 4.8084, 4.7882]], grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.09218055009841919 
model_pd.l_d.mean(): -9.529256820678711 
model_pd.lagr.mean(): -9.437076568603516 
model_pd.lambdas: dict_items([('pout', tensor([1.2754])), ('power', tensor([0.4246]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1817])), ('power', tensor([-22.9257]))])
epoch：482	 i:0 	 global-step:9640	 l-p:0.09218055009841919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.7915, 4.7915, 4.7915],
        [4.7915, 5.0250, 4.9765],
        [4.7915, 5.7442, 6.2303],
        [4.7915, 4.7918, 4.7915]], grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.092052161693573 
model_pd.l_d.mean(): -9.502836227416992 
model_pd.lagr.mean(): -9.410783767700195 
model_pd.lambdas: dict_items([('pout', tensor([1.2756])), ('power', tensor([0.4235]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1791])), ('power', tensor([-22.9177]))])
epoch：483	 i:0 	 global-step:9660	 l-p:0.092052161693573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[4.8008, 4.9352, 4.8787],
        [4.8008, 4.8011, 4.8008],
        [4.8008, 4.8008, 4.8008],
        [4.8008, 5.0868, 5.0532]], grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.09192411601543427 
model_pd.l_d.mean(): -9.476435661315918 
model_pd.lagr.mean(): -9.384511947631836 
model_pd.lambdas: dict_items([('pout', tensor([1.2758])), ('power', tensor([0.4223]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1765])), ('power', tensor([-22.9097]))])
epoch：484	 i:0 	 global-step:9680	 l-p:0.09192411601543427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[4.8102, 5.0452, 4.9966],
        [4.8102, 4.8374, 4.8164],
        [4.8102, 4.9827, 4.9254],
        [4.8102, 5.4494, 5.6369]], grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.0917963758111 
model_pd.l_d.mean(): -9.450054168701172 
model_pd.lagr.mean(): -9.358258247375488 
model_pd.lambdas: dict_items([('pout', tensor([1.2760])), ('power', tensor([0.4212]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1740])), ('power', tensor([-22.9017]))])
epoch：485	 i:0 	 global-step:9700	 l-p:0.0917963758111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[4.8196, 4.8332, 4.8217],
        [4.8196, 5.1074, 5.0738],
        [4.8196, 4.8859, 4.8450],
        [4.8196, 4.8196, 4.8196]], grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.09166897088289261 
model_pd.l_d.mean(): -9.423691749572754 
model_pd.lagr.mean(): -9.332022666931152 
model_pd.lambdas: dict_items([('pout', tensor([1.2761])), ('power', tensor([0.4200]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1714])), ('power', tensor([-22.8936]))])
epoch：486	 i:0 	 global-step:9720	 l-p:0.09166897088289261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.8290, 4.8291, 4.8290],
        [4.8290, 4.8839, 4.8478],
        [4.8290, 4.8291, 4.8290],
        [4.8290, 5.3368, 5.4218]], grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.09154187887907028 
model_pd.l_d.mean(): -9.397350311279297 
model_pd.lagr.mean(): -9.305808067321777 
model_pd.lambdas: dict_items([('pout', tensor([1.2763])), ('power', tensor([0.4189]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1688])), ('power', tensor([-22.8854]))])
epoch：487	 i:0 	 global-step:9740	 l-p:0.09154187887907028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.8385, 5.4891, 5.6833],
        [4.8385, 4.8385, 4.8385],
        [4.8385, 4.8663, 4.8449],
        [4.8385, 4.9546, 4.9000]], grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.09141507744789124 
model_pd.l_d.mean(): -9.371030807495117 
model_pd.lagr.mean(): -9.27961540222168 
model_pd.lambdas: dict_items([('pout', tensor([1.2765])), ('power', tensor([0.4177]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1661])), ('power', tensor([-22.8772]))])
epoch：488	 i:0 	 global-step:9760	 l-p:0.09141507744789124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[4.8481, 5.0864, 5.0374],
        [4.8481, 5.0231, 4.9652],
        [4.8481, 4.8560, 4.8490],
        [4.8481, 4.8481, 4.8481]], grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.09128861129283905 
model_pd.l_d.mean(): -9.3447265625 
model_pd.lagr.mean(): -9.253437995910645 
model_pd.lambdas: dict_items([('pout', tensor([1.2766])), ('power', tensor([0.4166]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1635])), ('power', tensor([-22.8689]))])
epoch：489	 i:0 	 global-step:9780	 l-p:0.09128861129283905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.8577, 4.8656, 4.8586],
        [4.8577, 5.1270, 5.0860],
        [4.8577, 4.8716, 4.8599],
        [4.8577, 4.8578, 4.8577]], grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.09116244316101074 
model_pd.l_d.mean(): -9.318445205688477 
model_pd.lagr.mean(): -9.227282524108887 
model_pd.lambdas: dict_items([('pout', tensor([1.2768])), ('power', tensor([0.4155]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1609])), ('power', tensor([-22.8606]))])
epoch：490	 i:0 	 global-step:9800	 l-p:0.09116244316101074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.8674, 4.8954, 4.8738],
        [4.8674, 5.4570, 5.5994],
        [4.8674, 5.1376, 5.0966],
        [4.8674, 4.8675, 4.8674]], grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.09103656560182571 
model_pd.l_d.mean(): -9.292182922363281 
model_pd.lagr.mean(): -9.201146125793457 
model_pd.lambdas: dict_items([('pout', tensor([1.2770])), ('power', tensor([0.4143]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1582])), ('power', tensor([-22.8522]))])
epoch：491	 i:0 	 global-step:9820	 l-p:0.09103656560182571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[4.8771, 4.8772, 4.8771],
        [4.8771, 4.9450, 4.9033],
        [4.8771, 4.8912, 4.8793],
        [4.8771, 5.1937, 5.1685]], grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.09091100841760635 
model_pd.l_d.mean(): -9.26594352722168 
model_pd.lagr.mean(): -9.175032615661621 
model_pd.lambdas: dict_items([('pout', tensor([1.2771])), ('power', tensor([0.4132]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1556])), ('power', tensor([-22.8438]))])
epoch：492	 i:0 	 global-step:9840	 l-p:0.09091100841760635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[4.8869, 4.8872, 4.8869],
        [4.8869, 5.5483, 5.7470],
        [4.8869, 5.5419, 5.7352],
        [4.8869, 5.8163, 6.2656]], grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.09078573435544968 
model_pd.l_d.mean(): -9.239721298217773 
model_pd.lagr.mean(): -9.148935317993164 
model_pd.lambdas: dict_items([('pout', tensor([1.2773])), ('power', tensor([0.4120]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1529])), ('power', tensor([-22.8353]))])
epoch：493	 i:0 	 global-step:9860	 l-p:0.09078573435544968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.8967, 5.2037, 5.1740],
        [4.8967, 4.8967, 4.8967],
        [4.8967, 5.5538, 5.7478],
        [4.8967, 4.8970, 4.8968]], grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.09066074341535568 
model_pd.l_d.mean(): -9.213522911071777 
model_pd.lagr.mean(): -9.122861862182617 
model_pd.lambdas: dict_items([('pout', tensor([1.2774])), ('power', tensor([0.4109]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1503])), ('power', tensor([-22.8268]))])
epoch：494	 i:0 	 global-step:9880	 l-p:0.09066074341535568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[4.9067, 4.9067, 4.9067],
        [4.9067, 5.0259, 4.9701],
        [4.9067, 5.0958, 5.0376],
        [4.9067, 5.0466, 4.9883]], grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.09053604304790497 
model_pd.l_d.mean(): -9.18734359741211 
model_pd.lagr.mean(): -9.096807479858398 
model_pd.lambdas: dict_items([('pout', tensor([1.2776])), ('power', tensor([0.4098]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1476])), ('power', tensor([-22.8182]))])
epoch：495	 i:0 	 global-step:9900	 l-p:0.09053604304790497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.9166, 4.9454, 4.9233],
        [4.9166, 4.9204, 4.9169],
        [4.9166, 5.2626, 5.2482],
        [4.9166, 4.9167, 4.9166]], grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.09041164070367813 
model_pd.l_d.mean(): -9.161184310913086 
model_pd.lagr.mean(): -9.070773124694824 
model_pd.lambdas: dict_items([('pout', tensor([1.2777])), ('power', tensor([0.4086]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1449])), ('power', tensor([-22.8095]))])
epoch：496	 i:0 	 global-step:9920	 l-p:0.09041164070367813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.9266, 5.2026, 5.1613],
        [4.9266, 5.4508, 5.5399],
        [4.9266, 4.9268, 4.9266],
        [4.9266, 5.0080, 4.9611]], grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.09028753638267517 
model_pd.l_d.mean(): -9.13504695892334 
model_pd.lagr.mean(): -9.044759750366211 
model_pd.lambdas: dict_items([('pout', tensor([1.2779])), ('power', tensor([0.4075]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1422])), ('power', tensor([-22.8008]))])
epoch：497	 i:0 	 global-step:9940	 l-p:0.09028753638267517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[4.9367, 5.1280, 5.0694],
        [4.9367, 5.0242, 4.9754],
        [4.9367, 4.9367, 4.9367],
        [4.9367, 5.2599, 5.2349]], grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.0901637151837349 
model_pd.l_d.mean(): -9.108929634094238 
model_pd.lagr.mean(): -9.018765449523926 
model_pd.lambdas: dict_items([('pout', tensor([1.2780])), ('power', tensor([0.4063]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1395])), ('power', tensor([-22.7920]))])
epoch：498	 i:0 	 global-step:9960	 l-p:0.0901637151837349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[4.9468, 4.9469, 4.9469],
        [4.9468, 5.0045, 4.9668],
        [4.9468, 5.1195, 5.0597],
        [4.9468, 5.0890, 5.0299]], grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.0900401771068573 
model_pd.l_d.mean(): -9.08283519744873 
model_pd.lagr.mean(): -8.99279499053955 
model_pd.lambdas: dict_items([('pout', tensor([1.2781])), ('power', tensor([0.4052]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1367])), ('power', tensor([-22.7832]))])
epoch：499	 i:0 	 global-step:9980	 l-p:0.0900401771068573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.9570, 4.9570, 4.9570],
        [4.9570, 5.2825, 5.2575],
        [4.9570, 4.9736, 4.9598],
        [4.9570, 4.9852, 4.9635]], grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.08991692215204239 
model_pd.l_d.mean(): -9.056761741638184 
model_pd.lagr.mean(): -8.96684455871582 
model_pd.lambdas: dict_items([('pout', tensor([1.2783])), ('power', tensor([0.4041]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1340])), ('power', tensor([-22.7744]))])
epoch：500	 i:0 	 global-step:10000	 l-p:0.08991692215204239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[4.9673, 5.0743, 5.0202],
        [4.9673, 6.2199, 7.0113],
        [4.9673, 5.0500, 5.0025],
        [4.9673, 5.1105, 5.0511]], grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.08979396522045135 
model_pd.l_d.mean(): -9.030707359313965 
model_pd.lagr.mean(): -8.940913200378418 
model_pd.lambdas: dict_items([('pout', tensor([1.2784])), ('power', tensor([0.4029]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1313])), ('power', tensor([-22.7654]))])
epoch：501	 i:0 	 global-step:10020	 l-p:0.08979396522045135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.9776, 4.9815, 4.9779],
        [4.9776, 4.9797, 4.9777],
        [4.9776, 4.9778, 4.9776],
        [4.9776, 5.1002, 5.0431]], grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.08967128396034241 
model_pd.l_d.mean(): -9.004676818847656 
model_pd.lagr.mean(): -8.915005683898926 
model_pd.lambdas: dict_items([('pout', tensor([1.2785])), ('power', tensor([0.4018]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1285])), ('power', tensor([-22.7564]))])
epoch：502	 i:0 	 global-step:10040	 l-p:0.08967128396034241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.9880, 5.0047, 4.9908],
        [4.9880, 4.9881, 4.9880],
        [4.9880, 5.3048, 5.2751],
        [4.9880, 5.9234, 6.3655]], grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.08954888582229614 
model_pd.l_d.mean(): -8.978667259216309 
model_pd.lagr.mean(): -8.889118194580078 
model_pd.lambdas: dict_items([('pout', tensor([1.2786])), ('power', tensor([0.4006]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1257])), ('power', tensor([-22.7474]))])
epoch：503	 i:0 	 global-step:10060	 l-p:0.08954888582229614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.9984, 5.0881, 5.0382],
        [4.9984, 5.0014, 4.9986],
        [4.9984, 4.9984, 4.9984],
        [4.9984, 5.9367, 6.3804]], grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.08942677825689316 
model_pd.l_d.mean(): -8.952676773071289 
model_pd.lagr.mean(): -8.863249778747559 
model_pd.lambdas: dict_items([('pout', tensor([1.2788])), ('power', tensor([0.3995]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1230])), ('power', tensor([-22.7383]))])
epoch：504	 i:0 	 global-step:10080	 l-p:0.08942677825689316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.0089, 5.0384, 5.0157],
        [5.0089, 6.0249, 6.5471],
        [5.0089, 5.0258, 5.0118],
        [5.0089, 5.0089, 5.0089]], grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.08930494636297226 
model_pd.l_d.mean(): -8.926712036132812 
model_pd.lagr.mean(): -8.837407112121582 
model_pd.lambdas: dict_items([('pout', tensor([1.2789])), ('power', tensor([0.3984]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1202])), ('power', tensor([-22.7291]))])
epoch：505	 i:0 	 global-step:10100	 l-p:0.08930494636297226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[5.0194, 5.1098, 5.0596],
        [5.0194, 5.4845, 5.5282],
        [5.0194, 5.7086, 5.9176],
        [5.0194, 5.0491, 5.0263]], grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.08918336033821106 
model_pd.l_d.mean(): -8.900766372680664 
model_pd.lagr.mean(): -8.811582565307617 
model_pd.lambdas: dict_items([('pout', tensor([1.2790])), ('power', tensor([0.3972]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1174])), ('power', tensor([-22.7199]))])
epoch：506	 i:0 	 global-step:10120	 l-p:0.08918336033821106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.0300, 5.0300, 5.0300],
        [5.0300, 5.0601, 5.0371],
        [5.0300, 5.3636, 5.3388],
        [5.0300, 5.0303, 5.0300]], grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.08906206488609314 
model_pd.l_d.mean(): -8.87484359741211 
model_pd.lagr.mean(): -8.785781860351562 
model_pd.lambdas: dict_items([('pout', tensor([1.2791])), ('power', tensor([0.3961]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1146])), ('power', tensor([-22.7106]))])
epoch：507	 i:0 	 global-step:10140	 l-p:0.08906206488609314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[5.0407, 5.0407, 5.0407],
        [5.0407, 5.0558, 5.0431],
        [5.0407, 5.2959, 5.2452],
        [5.0407, 5.0493, 5.0417]], grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.08894103020429611 
model_pd.l_d.mean(): -8.848942756652832 
model_pd.lagr.mean(): -8.760002136230469 
model_pd.lambdas: dict_items([('pout', tensor([1.2792])), ('power', tensor([0.3950]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1118])), ('power', tensor([-22.7013]))])
epoch：508	 i:0 	 global-step:10160	 l-p:0.08894103020429611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.0514, 5.0522, 5.0514],
        [5.0514, 5.0819, 5.0586],
        [5.0514, 5.1620, 5.1064],
        [5.0514, 5.0515, 5.0514]], grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.08882027864456177 
model_pd.l_d.mean(): -8.823064804077148 
model_pd.lagr.mean(): -8.734244346618652 
model_pd.lambdas: dict_items([('pout', tensor([1.2793])), ('power', tensor([0.3938]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1089])), ('power', tensor([-22.6919]))])
epoch：509	 i:0 	 global-step:10180	 l-p:0.08882027864456177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[5.0622, 5.7604, 5.9728],
        [5.0622, 5.0622, 5.0622],
        [5.0622, 5.0653, 5.0624],
        [5.0622, 5.0624, 5.0622]], grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.08869978040456772 
model_pd.l_d.mean(): -8.797208786010742 
model_pd.lagr.mean(): -8.70850944519043 
model_pd.lambdas: dict_items([('pout', tensor([1.2794])), ('power', tensor([0.3927]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1061])), ('power', tensor([-22.6824]))])
epoch：510	 i:0 	 global-step:10200	 l-p:0.08869978040456772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[5.0730, 5.2221, 5.1608],
        [5.0730, 5.7726, 5.9851],
        [5.0730, 5.2632, 5.2018],
        [5.0730, 5.0904, 5.0760]], grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.08857954293489456 
model_pd.l_d.mean(): -8.771374702453613 
model_pd.lagr.mean(): -8.682795524597168 
model_pd.lambdas: dict_items([('pout', tensor([1.2796])), ('power', tensor([0.3916]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1033])), ('power', tensor([-22.6729]))])
epoch：511	 i:0 	 global-step:10220	 l-p:0.08857954293489456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[5.0839, 5.7150, 5.8707],
        [5.0839, 5.0840, 5.0839],
        [5.0839, 5.1449, 5.1052],
        [5.0839, 5.4236, 5.3990]], grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.08845958858728409 
model_pd.l_d.mean(): -8.745564460754395 
model_pd.lagr.mean(): -8.6571044921875 
model_pd.lambdas: dict_items([('pout', tensor([1.2797])), ('power', tensor([0.3904]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1004])), ('power', tensor([-22.6633]))])
epoch：512	 i:0 	 global-step:10240	 l-p:0.08845958858728409
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[5.0949, 5.0952, 5.0949],
        [5.0949, 5.7991, 6.0134],
        [5.0949, 5.2772, 5.2150],
        [5.0949, 5.0981, 5.0951]], grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.08833986520767212 
model_pd.l_d.mean(): -8.719776153564453 
model_pd.lagr.mean(): -8.631436347961426 
model_pd.lambdas: dict_items([('pout', tensor([1.2797])), ('power', tensor([0.3893]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0975])), ('power', tensor([-22.6537]))])
epoch：513	 i:0 	 global-step:10260	 l-p:0.08833986520767212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[5.1059, 5.1370, 5.1132],
        [5.1059, 5.4481, 5.4235],
        [5.1059, 5.1059, 5.1059],
        [5.1059, 5.1994, 5.1477]], grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.08822042495012283 
model_pd.l_d.mean(): -8.694009780883789 
model_pd.lagr.mean(): -8.605789184570312 
model_pd.lambdas: dict_items([('pout', tensor([1.2798])), ('power', tensor([0.3882]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0947])), ('power', tensor([-22.6440]))])
epoch：514	 i:0 	 global-step:10280	 l-p:0.08822042495012283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[5.1170, 5.8259, 6.0419],
        [5.1170, 5.6737, 5.7711],
        [5.1170, 5.2047, 5.1547],
        [5.1170, 5.1482, 5.1244]], grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.08810121566057205 
model_pd.l_d.mean(): -8.668267250061035 
model_pd.lagr.mean(): -8.58016586303711 
model_pd.lambdas: dict_items([('pout', tensor([1.2799])), ('power', tensor([0.3870]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0918])), ('power', tensor([-22.6343]))])
epoch：515	 i:0 	 global-step:10300	 l-p:0.08810121566057205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[5.1282, 5.1283, 5.1282],
        [5.1282, 5.3221, 5.2599],
        [5.1282, 5.4729, 5.4484],
        [5.1282, 5.1282, 5.1282]], grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.08798226714134216 
model_pd.l_d.mean(): -8.642546653747559 
model_pd.lagr.mean(): -8.554564476013184 
model_pd.lambdas: dict_items([('pout', tensor([1.2800])), ('power', tensor([0.3859]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0889])), ('power', tensor([-22.6245]))])
epoch：516	 i:0 	 global-step:10320	 l-p:0.08798226714134216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[5.1394, 5.4728, 5.4432],
        [5.1394, 5.1394, 5.1394],
        [5.1394, 5.5121, 5.4993],
        [5.1394, 5.8539, 6.0726]], grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.08786357939243317 
model_pd.l_d.mean(): -8.616849899291992 
model_pd.lagr.mean(): -8.528985977172852 
model_pd.lambdas: dict_items([('pout', tensor([1.2801])), ('power', tensor([0.3848]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0860])), ('power', tensor([-22.6146]))])
epoch：517	 i:0 	 global-step:10340	 l-p:0.08786357939243317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[5.1507, 5.1507, 5.1507],
        [5.1507, 5.4853, 5.4557],
        [5.1507, 6.3605, 7.0727],
        [5.1507, 5.2265, 5.1804]], grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.08774513006210327 
model_pd.l_d.mean(): -8.59117603302002 
model_pd.lagr.mean(): -8.50343132019043 
model_pd.lambdas: dict_items([('pout', tensor([1.2802])), ('power', tensor([0.3836]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0831])), ('power', tensor([-22.6047]))])
epoch：518	 i:0 	 global-step:10360	 l-p:0.08774513006210327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[5.1620, 5.2512, 5.2004],
        [5.1620, 5.3487, 5.2854],
        [5.1620, 5.4281, 5.3763],
        [5.1620, 5.3583, 5.2955]], grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.08762692660093307 
model_pd.l_d.mean(): -8.565526008605957 
model_pd.lagr.mean(): -8.477899551391602 
model_pd.lambdas: dict_items([('pout', tensor([1.2803])), ('power', tensor([0.3825]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0802])), ('power', tensor([-22.5947]))])
epoch：519	 i:0 	 global-step:10380	 l-p:0.08762692660093307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[5.1734, 6.4996, 7.3419],
        [5.1734, 5.1767, 5.1736],
        [5.1734, 5.1735, 5.1734],
        [5.1734, 5.3608, 5.2974]], grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.08750896155834198 
model_pd.l_d.mean(): -8.539897918701172 
model_pd.lagr.mean(): -8.452388763427734 
model_pd.lambdas: dict_items([('pout', tensor([1.2804])), ('power', tensor([0.3814]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0772])), ('power', tensor([-22.5846]))])
epoch：520	 i:0 	 global-step:10400	 l-p:0.08750896155834198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.1849, 5.9091, 6.1314],
        [5.1849, 6.5152, 7.3604],
        [5.1849, 5.8355, 5.9975],
        [5.1849, 5.1851, 5.1849]], grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.08739122748374939 
model_pd.l_d.mean(): -8.51429557800293 
model_pd.lagr.mean(): -8.426904678344727 
model_pd.lambdas: dict_items([('pout', tensor([1.2804])), ('power', tensor([0.3802]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0743])), ('power', tensor([-22.5745]))])
epoch：521	 i:0 	 global-step:10420	 l-p:0.08739122748374939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.1964, 5.2288, 5.2041],
        [5.1964, 5.4991, 5.4565],
        [5.1964, 5.2126, 5.1990],
        [5.1964, 5.1964, 5.1964]], grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.08727376908063889 
model_pd.l_d.mean(): -8.488716125488281 
model_pd.lagr.mean(): -8.401442527770996 
model_pd.lambdas: dict_items([('pout', tensor([1.2805])), ('power', tensor([0.3791]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0713])), ('power', tensor([-22.5643]))])
epoch：522	 i:0 	 global-step:10440	 l-p:0.08727376908063889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[5.2080, 5.7804, 5.8819],
        [5.2080, 5.8631, 6.0266],
        [5.2080, 5.4187, 5.3561],
        [5.2080, 5.4782, 5.4260]], grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.08715655654668808 
model_pd.l_d.mean(): -8.463159561157227 
model_pd.lagr.mean(): -8.37600326538086 
model_pd.lambdas: dict_items([('pout', tensor([1.2806])), ('power', tensor([0.3780]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0684])), ('power', tensor([-22.5540]))])
epoch：523	 i:0 	 global-step:10460	 l-p:0.08715655654668808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[5.2196, 5.9504, 6.1747],
        [5.2196, 6.2428, 6.7435],
        [5.2196, 5.2220, 5.2198],
        [5.2196, 5.6022, 5.5899]], grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.08703956007957458 
model_pd.l_d.mean(): -8.437627792358398 
model_pd.lagr.mean(): -8.350587844848633 
model_pd.lambdas: dict_items([('pout', tensor([1.2806])), ('power', tensor([0.3769]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0654])), ('power', tensor([-22.5437]))])
epoch：524	 i:0 	 global-step:10480	 l-p:0.08703956007957458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[5.2314, 5.2478, 5.2340],
        [5.2314, 5.7286, 5.7783],
        [5.2314, 5.3892, 5.3251],
        [5.2314, 5.2314, 5.2314]], grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.0869227945804596 
model_pd.l_d.mean(): -8.41211986541748 
model_pd.lagr.mean(): -8.325197219848633 
model_pd.lambdas: dict_items([('pout', tensor([1.2807])), ('power', tensor([0.3757]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0624])), ('power', tensor([-22.5334]))])
epoch：525	 i:0 	 global-step:10500	 l-p:0.0869227945804596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[5.2431, 5.3619, 5.3029],
        [5.2431, 6.4839, 7.2161],
        [5.2431, 5.3785, 5.3166],
        [5.2431, 5.5505, 5.5077]], grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.0868062749505043 
model_pd.l_d.mean(): -8.386636734008789 
model_pd.lagr.mean(): -8.299830436706543 
model_pd.lambdas: dict_items([('pout', tensor([1.2808])), ('power', tensor([0.3746]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0594])), ('power', tensor([-22.5229]))])
epoch：526	 i:0 	 global-step:10520	 l-p:0.0868062749505043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[5.2550, 5.4578, 5.3935],
        [5.2550, 5.2551, 5.2550],
        [5.2550, 5.2550, 5.2550],
        [5.2550, 5.2880, 5.2628]], grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.08668999373912811 
model_pd.l_d.mean(): -8.361178398132324 
model_pd.lagr.mean(): -8.27448844909668 
model_pd.lambdas: dict_items([('pout', tensor([1.2808])), ('power', tensor([0.3735]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0564])), ('power', tensor([-22.5124]))])
epoch：527	 i:0 	 global-step:10540	 l-p:0.08668999373912811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.2669, 6.5156, 7.2530],
        [5.2669, 5.3001, 5.2748],
        [5.2669, 5.2669, 5.2669],
        [5.2669, 5.2669, 5.2669]], grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): 0.08657394349575043 
model_pd.l_d.mean(): -8.335742950439453 
model_pd.lagr.mean(): -8.24916934967041 
model_pd.lambdas: dict_items([('pout', tensor([1.2809])), ('power', tensor([0.3724]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0534])), ('power', tensor([-22.5019]))])
epoch：528	 i:0 	 global-step:10560	 l-p:0.08657394349575043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[5.2789, 5.6409, 5.6169],
        [5.2789, 5.3787, 5.3240],
        [5.2789, 5.2790, 5.2789],
        [5.2789, 6.6431, 7.5118]], grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.08645813167095184 
model_pd.l_d.mean(): -8.310331344604492 
model_pd.lagr.mean(): -8.223873138427734 
model_pd.lambdas: dict_items([('pout', tensor([1.2809])), ('power', tensor([0.3712]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0504])), ('power', tensor([-22.4912]))])
epoch：529	 i:0 	 global-step:10580	 l-p:0.08645813167095184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[5.2909, 5.2911, 5.2909],
        [5.2909, 6.3106, 6.7980],
        [5.2909, 5.2911, 5.2909],
        [5.2909, 5.3233, 5.2985]], grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.08634252846240997 
model_pd.l_d.mean(): -8.28494644165039 
model_pd.lagr.mean(): -8.198603630065918 
model_pd.lambdas: dict_items([('pout', tensor([1.2810])), ('power', tensor([0.3701]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0474])), ('power', tensor([-22.4805]))])
epoch：530	 i:0 	 global-step:10600	 l-p:0.08634252846240997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[5.3030, 6.3501, 6.8639],
        [5.3030, 6.3261, 6.8154],
        [5.3030, 6.0454, 6.2713],
        [5.3030, 5.3364, 5.3110]], grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.0862271711230278 
model_pd.l_d.mean(): -8.259584426879883 
model_pd.lagr.mean(): -8.173357009887695 
model_pd.lambdas: dict_items([('pout', tensor([1.2810])), ('power', tensor([0.3690]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0443])), ('power', tensor([-22.4698]))])
epoch：531	 i:0 	 global-step:10620	 l-p:0.0862271711230278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[5.3152, 5.9064, 6.0128],
        [5.3152, 5.5953, 5.5422],
        [5.3152, 5.3250, 5.3164],
        [5.3152, 5.3487, 5.3232]], grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.08611202239990234 
model_pd.l_d.mean(): -8.234247207641602 
model_pd.lagr.mean(): -8.1481351852417 
model_pd.lambdas: dict_items([('pout', tensor([1.2810])), ('power', tensor([0.3679]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0413])), ('power', tensor([-22.4589]))])
epoch：532	 i:0 	 global-step:10640	 l-p:0.08611202239990234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[5.3275, 5.3373, 5.3286],
        [5.3275, 5.5252, 5.4592],
        [5.3275, 5.3283, 5.3275],
        [5.3275, 5.3604, 5.3352]], grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.08599711209535599 
model_pd.l_d.mean(): -8.208937644958496 
model_pd.lagr.mean(): -8.122940063476562 
model_pd.lambdas: dict_items([('pout', tensor([1.2811])), ('power', tensor([0.3667]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0382])), ('power', tensor([-22.4480]))])
epoch：533	 i:0 	 global-step:10660	 l-p:0.08599711209535599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[5.3398, 5.5038, 5.4378],
        [5.3398, 5.7089, 5.6850],
        [5.3398, 5.3433, 5.3400],
        [5.3398, 5.3496, 5.3410]], grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.08588240295648575 
model_pd.l_d.mean(): -8.183650970458984 
model_pd.lagr.mean(): -8.097768783569336 
model_pd.lambdas: dict_items([('pout', tensor([1.2811])), ('power', tensor([0.3656]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0352])), ('power', tensor([-22.4371]))])
epoch：534	 i:0 	 global-step:10680	 l-p:0.08588240295648575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[5.3522, 5.3522, 5.3522],
        [5.3522, 5.6706, 5.6274],
        [5.3522, 5.3530, 5.3522],
        [5.3522, 6.4703, 7.0515]], grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.0857679471373558 
model_pd.l_d.mean(): -8.158390045166016 
model_pd.lagr.mean(): -8.072622299194336 
model_pd.lambdas: dict_items([('pout', tensor([1.2812])), ('power', tensor([0.3645]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0321])), ('power', tensor([-22.4261]))])
epoch：535	 i:0 	 global-step:10700	 l-p:0.0857679471373558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[5.3646, 5.3820, 5.3674],
        [5.3646, 5.9645, 6.0731],
        [5.3646, 5.3648, 5.3646],
        [5.3646, 5.3648, 5.3646]], grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.08565368503332138 
model_pd.l_d.mean(): -8.133153915405273 
model_pd.lagr.mean(): -8.047500610351562 
model_pd.lambdas: dict_items([('pout', tensor([1.2812])), ('power', tensor([0.3634]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0290])), ('power', tensor([-22.4150]))])
epoch：536	 i:0 	 global-step:10720	 l-p:0.08565368503332138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[5.3772, 5.3772, 5.3772],
        [5.3772, 5.6629, 5.6093],
        [5.3772, 5.3775, 5.3772],
        [5.3772, 5.3872, 5.3783]], grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.08553963899612427 
model_pd.l_d.mean(): -8.107943534851074 
model_pd.lagr.mean(): -8.022403717041016 
model_pd.lambdas: dict_items([('pout', tensor([1.2812])), ('power', tensor([0.3623]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0259])), ('power', tensor([-22.4038]))])
epoch：537	 i:0 	 global-step:10740	 l-p:0.08553963899612427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[5.3898, 5.9940, 6.1038],
        [5.3898, 5.6020, 5.5356],
        [5.3898, 5.3897, 5.3898],
        [5.3898, 5.5323, 5.4677]], grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.08542581647634506 
model_pd.l_d.mean(): -8.082759857177734 
model_pd.lagr.mean(): -7.997334003448486 
model_pd.lambdas: dict_items([('pout', tensor([1.2812])), ('power', tensor([0.3611]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0228])), ('power', tensor([-22.3925]))])
epoch：538	 i:0 	 global-step:10760	 l-p:0.08542581647634506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.4024, 5.7652, 5.7360],
        [5.4024, 5.4376, 5.4109],
        [5.4024, 5.4025, 5.4024],
        [5.4024, 5.4024, 5.4024]], grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.08531219512224197 
model_pd.l_d.mean(): -8.057600021362305 
model_pd.lagr.mean(): -7.972287654876709 
model_pd.lambdas: dict_items([('pout', tensor([1.2812])), ('power', tensor([0.3600]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0197])), ('power', tensor([-22.3812]))])
epoch：539	 i:0 	 global-step:10780	 l-p:0.08531219512224197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[5.4151, 5.4352, 5.4186],
        [5.4151, 6.5522, 7.1444],
        [5.4151, 5.5834, 5.5161],
        [5.4151, 5.4505, 5.4237]], grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.08519881218671799 
model_pd.l_d.mean(): -8.032466888427734 
model_pd.lagr.mean(): -7.947268009185791 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3589]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0165])), ('power', tensor([-22.3699]))])
epoch：540	 i:0 	 global-step:10800	 l-p:0.08519881218671799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[5.4280, 5.7936, 5.7644],
        [5.4280, 5.5121, 5.4616],
        [5.4280, 5.4280, 5.4280],
        [5.4280, 5.4633, 5.4365]], grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.08508563041687012 
model_pd.l_d.mean(): -8.00736141204834 
model_pd.lagr.mean(): -7.922275543212891 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3578]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0134])), ('power', tensor([-22.3584]))])
epoch：541	 i:0 	 global-step:10820	 l-p:0.08508563041687012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[5.4408, 5.4410, 5.4408],
        [5.4408, 5.8217, 5.7982],
        [5.4408, 5.8080, 5.7788],
        [5.4408, 5.4760, 5.4493]], grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.08497264236211777 
model_pd.l_d.mean(): -7.982280731201172 
model_pd.lagr.mean(): -7.897307872772217 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3567]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0103])), ('power', tensor([-22.3469]))])
epoch：542	 i:0 	 global-step:10840	 l-p:0.08497264236211777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.4538, 5.5601, 5.5023],
        [5.4538, 5.6827, 5.6164],
        [5.4538, 5.9855, 6.0416],
        [5.4538, 5.4538, 5.4538]], grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.08485988527536392 
model_pd.l_d.mean(): -7.957225322723389 
model_pd.lagr.mean(): -7.872365474700928 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3556]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0071])), ('power', tensor([-22.3353]))])
epoch：543	 i:0 	 global-step:10860	 l-p:0.08485988527536392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[5.4668, 5.4873, 5.4704],
        [5.4668, 5.4849, 5.4697],
        [5.4668, 5.4705, 5.4670],
        [5.4668, 5.8238, 5.7895]], grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.0847473219037056 
model_pd.l_d.mean(): -7.932197570800781 
model_pd.lagr.mean(): -7.847450256347656 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3544]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0039])), ('power', tensor([-22.3237]))])
epoch：544	 i:0 	 global-step:10880	 l-p:0.0847473219037056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[5.4799, 5.5656, 5.5142],
        [5.4799, 5.8114, 5.7676],
        [5.4799, 5.4808, 5.4799],
        [5.4799, 5.8653, 5.8420]], grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.08463498950004578 
model_pd.l_d.mean(): -7.907195091247559 
model_pd.lagr.mean(): -7.8225603103637695 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3533]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0008])), ('power', tensor([-22.3119]))])
epoch：545	 i:0 	 global-step:10900	 l-p:0.08463498950004578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[5.4931, 5.4980, 5.4934],
        [5.4931, 5.5938, 5.5373],
        [5.4931, 5.5289, 5.5017],
        [5.4931, 6.0767, 6.1641]], grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.08452283591032028 
model_pd.l_d.mean(): -7.88222074508667 
model_pd.lagr.mean(): -7.797698020935059 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3522]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0024])), ('power', tensor([-22.3001]))])
epoch：546	 i:0 	 global-step:10920	 l-p:0.08452283591032028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[5.5063, 5.5066, 5.5063],
        [5.5063, 5.8406, 5.7967],
        [5.5063, 5.5072, 5.5063],
        [5.5063, 6.2988, 6.5468]], grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.0844108983874321 
model_pd.l_d.mean(): -7.857272148132324 
model_pd.lagr.mean(): -7.772861480712891 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3511]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0056])), ('power', tensor([-22.2883]))])
epoch：547	 i:0 	 global-step:10940	 l-p:0.0844108983874321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[5.5196, 5.5196, 5.5196],
        [5.5196, 5.6506, 5.5865],
        [5.5196, 5.8824, 5.8481],
        [5.5196, 6.9712, 7.9007]], grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.08429917693138123 
model_pd.l_d.mean(): -7.832350730895996 
model_pd.lagr.mean(): -7.748051643371582 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3500]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0088])), ('power', tensor([-22.2763]))])
epoch：548	 i:0 	 global-step:10960	 l-p:0.08429917693138123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[5.5330, 5.5698, 5.5419],
        [5.5330, 5.5330, 5.5330],
        [5.5330, 5.8973, 5.8630],
        [5.5330, 6.3248, 6.5694]], grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.08418764173984528 
model_pd.l_d.mean(): -7.807455062866211 
model_pd.lagr.mean(): -7.723267555236816 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3489]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0121])), ('power', tensor([-22.2643]))])
epoch：549	 i:0 	 global-step:10980	 l-p:0.08418764173984528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[5.5465, 6.1784, 6.2955],
        [5.5465, 6.7231, 7.3384],
        [5.5465, 5.5515, 5.5468],
        [5.5465, 5.9256, 5.8966]], grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.08407630026340485 
model_pd.l_d.mean(): -7.782588005065918 
model_pd.lagr.mean(): -7.698511600494385 
model_pd.lambdas: dict_items([('pout', tensor([1.2813])), ('power', tensor([0.3478]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0153])), ('power', tensor([-22.2522]))])
epoch：550	 i:0 	 global-step:11000	 l-p:0.08407630026340485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[5.5600, 6.3652, 6.6186],
        [5.5600, 5.5628, 5.5601],
        [5.5600, 5.6928, 5.6279],
        [5.5600, 6.1943, 6.3121]], grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.08396516740322113 
model_pd.l_d.mean(): -7.757747650146484 
model_pd.lagr.mean(): -7.6737823486328125 
model_pd.lambdas: dict_items([('pout', tensor([1.2812])), ('power', tensor([0.3466]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0185])), ('power', tensor([-22.2400]))])
epoch：551	 i:0 	 global-step:11020	 l-p:0.08396516740322113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[5.5736, 5.6470, 5.6000],
        [5.5736, 5.5736, 5.5736],
        [5.5736, 5.7070, 5.6419],
        [5.5736, 7.0449, 7.9882]], grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.08385423570871353 
model_pd.l_d.mean(): -7.732934474945068 
model_pd.lagr.mean(): -7.649080276489258 
model_pd.lambdas: dict_items([('pout', tensor([1.2812])), ('power', tensor([0.3455]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0218])), ('power', tensor([-22.2278]))])
epoch：552	 i:0 	 global-step:11040	 l-p:0.08385423570871353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[5.5873, 5.8136, 5.7442],
        [5.5873, 5.6063, 5.5904],
        [5.5873, 5.8262, 5.7580],
        [5.5873, 6.7163, 7.2755]], grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.08374348282814026 
model_pd.l_d.mean(): -7.70814847946167 
model_pd.lagr.mean(): -7.6244049072265625 
model_pd.lambdas: dict_items([('pout', tensor([1.2812])), ('power', tensor([0.3444]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0250])), ('power', tensor([-22.2155]))])
epoch：553	 i:0 	 global-step:11060	 l-p:0.08374348282814026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[5.6011, 5.6011, 5.6011],
        [5.6011, 5.6376, 5.6098],
        [5.6011, 5.9076, 5.8521],
        [5.6011, 6.7340, 7.2955]], grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.0836329460144043 
model_pd.l_d.mean(): -7.683391094207764 
model_pd.lagr.mean(): -7.599758148193359 
model_pd.lambdas: dict_items([('pout', tensor([1.2812])), ('power', tensor([0.3433]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0283])), ('power', tensor([-22.2031]))])
epoch：554	 i:0 	 global-step:11080	 l-p:0.0836329460144043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.6149, 5.9604, 5.9161],
        [5.6149, 5.7502, 5.6843],
        [5.6149, 5.6341, 5.6180],
        [5.6149, 5.6151, 5.6149]], grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.08352259546518326 
model_pd.l_d.mean(): -7.658661365509033 
model_pd.lagr.mean(): -7.575138568878174 
model_pd.lambdas: dict_items([('pout', tensor([1.2811])), ('power', tensor([0.3422]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0316])), ('power', tensor([-22.1906]))])
epoch：555	 i:0 	 global-step:11100	 l-p:0.08352259546518326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[5.6288, 5.7834, 5.7143],
        [5.6288, 5.7344, 5.6756],
        [5.6288, 6.2754, 6.3964],
        [5.6288, 5.6298, 5.6289]], grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.08341243863105774 
model_pd.l_d.mean(): -7.633958339691162 
model_pd.lagr.mean(): -7.550545692443848 
model_pd.lambdas: dict_items([('pout', tensor([1.2811])), ('power', tensor([0.3411]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0349])), ('power', tensor([-22.1781]))])
epoch：556	 i:0 	 global-step:11120	 l-p:0.08341243863105774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[5.6428, 5.6430, 5.6428],
        [5.6428, 5.6808, 5.6520],
        [5.6428, 7.1394, 8.1003],
        [5.6428, 5.6438, 5.6429]], grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.08330246806144714 
model_pd.l_d.mean(): -7.6092848777771 
model_pd.lagr.mean(): -7.52598237991333 
model_pd.lambdas: dict_items([('pout', tensor([1.2811])), ('power', tensor([0.3400]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0382])), ('power', tensor([-22.1654]))])
epoch：557	 i:0 	 global-step:11140	 l-p:0.08330246806144714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[5.6569, 5.8392, 5.7674],
        [5.6569, 5.6609, 5.6572],
        [5.6569, 6.3085, 6.4308],
        [5.6569, 6.0067, 5.9623]], grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.08319269120693207 
model_pd.l_d.mean(): -7.584639072418213 
model_pd.lagr.mean(): -7.50144624710083 
model_pd.lambdas: dict_items([('pout', tensor([1.2810])), ('power', tensor([0.3389]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0415])), ('power', tensor([-22.1527]))])
epoch：558	 i:0 	 global-step:11160	 l-p:0.08319269120693207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[5.6711, 5.6713, 5.6711],
        [5.6711, 5.6711, 5.6711],
        [5.6711, 5.8923, 5.8206],
        [5.6711, 5.9163, 5.8469]], grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.08308311551809311 
model_pd.l_d.mean(): -7.56002140045166 
model_pd.lagr.mean(): -7.476938247680664 
model_pd.lambdas: dict_items([('pout', tensor([1.2810])), ('power', tensor([0.3378]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0448])), ('power', tensor([-22.1399]))])
epoch：559	 i:0 	 global-step:11180	 l-p:0.08308311551809311
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[5.6853, 5.8237, 5.7565],
        [5.6853, 5.6853, 5.6853],
        [5.6853, 5.9075, 5.8356],
        [5.6853, 6.5179, 6.7819]], grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.08297372609376907 
model_pd.l_d.mean(): -7.5354323387146 
model_pd.lagr.mean(): -7.452458381652832 
model_pd.lambdas: dict_items([('pout', tensor([1.2809])), ('power', tensor([0.3367]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0481])), ('power', tensor([-22.1271]))])
epoch：560	 i:0 	 global-step:11200	 l-p:0.08297372609376907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[5.6996, 6.0539, 6.0093],
        [5.6996, 6.8350, 7.3852],
        [5.6996, 6.3180, 6.4136],
        [5.6996, 5.8844, 5.8119]], grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.08286451548337936 
model_pd.l_d.mean(): -7.510870933532715 
model_pd.lagr.mean(): -7.428006649017334 
model_pd.lambdas: dict_items([('pout', tensor([1.2809])), ('power', tensor([0.3356]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0515])), ('power', tensor([-22.1141]))])
epoch：561	 i:0 	 global-step:11220	 l-p:0.08286451548337936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[5.7140, 6.1124, 6.0837],
        [5.7140, 5.7535, 5.7237],
        [5.7140, 5.8302, 5.7677],
        [5.7140, 7.2367, 8.2158]], grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.08275549113750458 
model_pd.l_d.mean(): -7.486340522766113 
model_pd.lagr.mean(): -7.403584957122803 
model_pd.lambdas: dict_items([('pout', tensor([1.2808])), ('power', tensor([0.3344]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0548])), ('power', tensor([-22.1011]))])
epoch：562	 i:0 	 global-step:11240	 l-p:0.08275549113750458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.7285, 5.7326, 5.7288],
        [5.7285, 5.7285, 5.7285],
        [5.7285, 6.5627, 6.8234],
        [5.7285, 5.7286, 5.7285]], grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.08264665305614471 
model_pd.l_d.mean(): -7.4618377685546875 
model_pd.lagr.mean(): -7.379190921783447 
model_pd.lambdas: dict_items([('pout', tensor([1.2808])), ('power', tensor([0.3333]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0582])), ('power', tensor([-22.0880]))])
epoch：563	 i:0 	 global-step:11260	 l-p:0.08264665305614471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[5.7431, 5.7632, 5.7464],
        [5.7431, 6.5042, 6.7028],
        [5.7431, 6.8908, 7.4478],
        [5.7431, 6.1307, 6.0964]], grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.08253799378871918 
model_pd.l_d.mean(): -7.43736457824707 
model_pd.lagr.mean(): -7.3548264503479 
model_pd.lambdas: dict_items([('pout', tensor([1.2807])), ('power', tensor([0.3322]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0616])), ('power', tensor([-22.0748]))])
epoch：564	 i:0 	 global-step:11280	 l-p:0.08253799378871918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[5.7577, 6.2075, 6.1997],
        [5.7577, 5.7580, 5.7577],
        [5.7577, 5.8994, 5.8309],
        [5.7577, 6.6051, 6.8743]], grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.08242955058813095 
model_pd.l_d.mean(): -7.412919998168945 
model_pd.lagr.mean(): -7.330490589141846 
model_pd.lambdas: dict_items([('pout', tensor([1.2806])), ('power', tensor([0.3311]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0650])), ('power', tensor([-22.0615]))])
epoch：565	 i:0 	 global-step:11300	 l-p:0.08242955058813095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[5.7724, 5.8909, 5.8273],
        [5.7724, 6.0007, 5.9274],
        [5.7724, 7.0177, 7.6731],
        [5.7724, 6.1775, 6.1489]], grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.08232126384973526 
model_pd.l_d.mean(): -7.388504981994629 
model_pd.lagr.mean(): -7.306183815002441 
model_pd.lambdas: dict_items([('pout', tensor([1.2806])), ('power', tensor([0.3300]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0683])), ('power', tensor([-22.0482]))])
epoch：566	 i:0 	 global-step:11320	 l-p:0.08232126384973526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228]])
 pt:tensor([[5.7873, 7.3368, 8.3346],
        [5.7873, 6.2090, 6.1869],
        [5.7873, 5.9499, 5.8779],
        [5.7873, 6.2408, 6.2332]], grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.08221316337585449 
model_pd.l_d.mean(): -7.364119052886963 
model_pd.lagr.mean(): -7.2819061279296875 
model_pd.lambdas: dict_items([('pout', tensor([1.2805])), ('power', tensor([0.3289]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0717])), ('power', tensor([-22.0347]))])
epoch：567	 i:0 	 global-step:11340	 l-p:0.08221316337585449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[5.8022, 6.2257, 6.2036],
        [5.8022, 5.8429, 5.8122],
        [5.8022, 6.2106, 6.1821],
        [5.8022, 5.9217, 5.8577]], grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.08210524916648865 
model_pd.l_d.mean(): -7.339763164520264 
model_pd.lagr.mean(): -7.257658004760742 
model_pd.lambdas: dict_items([('pout', tensor([1.2804])), ('power', tensor([0.3278]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0752])), ('power', tensor([-22.0212]))])
epoch：568	 i:0 	 global-step:11360	 l-p:0.08210524916648865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.8171, 6.0601, 5.9870],
        [5.8171, 5.8182, 5.8172],
        [5.8171, 5.9813, 5.9087],
        [5.8171, 5.8171, 5.8171]], grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.08199749886989594 
model_pd.l_d.mean(): -7.315436840057373 
model_pd.lagr.mean(): -7.2334394454956055 
model_pd.lambdas: dict_items([('pout', tensor([1.2803])), ('power', tensor([0.3267]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0786])), ('power', tensor([-22.0076]))])
epoch：569	 i:0 	 global-step:11380	 l-p:0.08199749886989594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.8322, 5.8322, 5.8322],
        [5.8322, 6.6971, 6.9735],
        [5.8322, 5.8333, 5.8323],
        [5.8322, 5.8322, 5.8322]], grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.08188994228839874 
model_pd.l_d.mean(): -7.291139125823975 
model_pd.lagr.mean(): -7.209249019622803 
model_pd.lambdas: dict_items([('pout', tensor([1.2803])), ('power', tensor([0.3256]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0820])), ('power', tensor([-21.9939]))])
epoch：570	 i:0 	 global-step:11400	 l-p:0.08188994228839874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[5.8474, 7.1155, 7.7841],
        [5.8474, 6.7155, 6.9933],
        [5.8474, 5.8474, 5.8474],
        [5.8474, 5.8594, 5.8488]], grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.08178255707025528 
model_pd.l_d.mean(): -7.266874313354492 
model_pd.lagr.mean(): -7.185091972351074 
model_pd.lambdas: dict_items([('pout', tensor([1.2802])), ('power', tensor([0.3245]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0855])), ('power', tensor([-21.9802]))])
epoch：571	 i:0 	 global-step:11420	 l-p:0.08178255707025528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[5.8626, 5.9040, 5.8728],
        [5.8626, 5.9845, 5.9193],
        [5.8626, 6.6477, 6.8544],
        [5.8626, 5.8626, 5.8626]], grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.08167535811662674 
model_pd.l_d.mean(): -7.2426371574401855 
model_pd.lagr.mean(): -7.160961627960205 
model_pd.lambdas: dict_items([('pout', tensor([1.2801])), ('power', tensor([0.3234]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0889])), ('power', tensor([-21.9663]))])
epoch：572	 i:0 	 global-step:11440	 l-p:0.08167535811662674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.8780, 5.8780, 5.8780],
        [5.8780, 6.0731, 5.9974],
        [5.8780, 6.7447, 7.0179],
        [5.8780, 5.8780, 5.8780]], grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.08156831562519073 
model_pd.l_d.mean(): -7.218430995941162 
model_pd.lagr.mean(): -7.136862754821777 
model_pd.lambdas: dict_items([('pout', tensor([1.2800])), ('power', tensor([0.3223]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0924])), ('power', tensor([-21.9523]))])
epoch：573	 i:0 	 global-step:11460	 l-p:0.08156831562519073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[5.8934, 5.9338, 5.9032],
        [5.8934, 6.5446, 6.6479],
        [5.8934, 6.2978, 6.2635],
        [5.8934, 6.2275, 6.1695]], grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.08146146684885025 
model_pd.l_d.mean(): -7.194255828857422 
model_pd.lagr.mean(): -7.112794399261475 
model_pd.lambdas: dict_items([('pout', tensor([1.2799])), ('power', tensor([0.3212]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0959])), ('power', tensor([-21.9383]))])
epoch：574	 i:0 	 global-step:11480	 l-p:0.08146146684885025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.9089, 5.9302, 5.9124],
        [5.9089, 5.9093, 5.9089],
        [5.9089, 5.9090, 5.9089],
        [5.9089, 5.9091, 5.9089]], grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.0813547894358635 
model_pd.l_d.mean(): -7.170109748840332 
model_pd.lagr.mean(): -7.088755130767822 
model_pd.lambdas: dict_items([('pout', tensor([1.2798])), ('power', tensor([0.3201]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0994])), ('power', tensor([-21.9242]))])
epoch：575	 i:0 	 global-step:11500	 l-p:0.0813547894358635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[5.9245, 5.9246, 5.9245],
        [5.9245, 6.3626, 6.3410],
        [5.9245, 6.1752, 6.1005],
        [5.9245, 6.8096, 7.0939]], grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.08124828338623047 
model_pd.l_d.mean(): -7.14599609375 
model_pd.lagr.mean(): -7.0647478103637695 
model_pd.lambdas: dict_items([('pout', tensor([1.2797])), ('power', tensor([0.3190]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1029])), ('power', tensor([-21.9100]))])
epoch：576	 i:0 	 global-step:11520	 l-p:0.08124828338623047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[5.9402, 5.9813, 5.9501],
        [5.9402, 7.4173, 8.3038],
        [5.9402, 6.4131, 6.4069],
        [5.9402, 7.1718, 7.7883]], grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.08114195615053177 
model_pd.l_d.mean(): -7.121912956237793 
model_pd.lagr.mean(): -7.040771007537842 
model_pd.lambdas: dict_items([('pout', tensor([1.2796])), ('power', tensor([0.3180]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1064])), ('power', tensor([-21.8957]))])
epoch：577	 i:0 	 global-step:11540	 l-p:0.08114195615053177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[5.9560, 5.9604, 5.9563],
        [5.9560, 5.9684, 5.9575],
        [5.9560, 6.0814, 6.0146],
        [5.9560, 6.8397, 7.1195]], grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.0810357853770256 
model_pd.l_d.mean(): -7.097861289978027 
model_pd.lagr.mean(): -7.0168256759643555 
model_pd.lambdas: dict_items([('pout', tensor([1.2795])), ('power', tensor([0.3169]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1099])), ('power', tensor([-21.8813]))])
epoch：578	 i:0 	 global-step:11560	 l-p:0.0810357853770256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[5.9718, 6.6364, 6.7428],
        [5.9718, 6.4488, 6.4429],
        [5.9718, 5.9722, 5.9718],
        [5.9718, 6.0556, 6.0026]], grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.08092979341745377 
model_pd.l_d.mean(): -7.073840141296387 
model_pd.lagr.mean(): -6.992910385131836 
model_pd.lambdas: dict_items([('pout', tensor([1.2794])), ('power', tensor([0.3158]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1134])), ('power', tensor([-21.8668]))])
epoch：579	 i:0 	 global-step:11580	 l-p:0.08092979341745377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[5.9878, 5.9923, 5.9881],
        [5.9878, 6.0312, 5.9986],
        [5.9878, 5.9882, 5.9878],
        [5.9878, 6.4668, 6.4610]], grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.08082397282123566 
model_pd.l_d.mean(): -7.049850940704346 
model_pd.lagr.mean(): -6.969027042388916 
model_pd.lambdas: dict_items([('pout', tensor([1.2792])), ('power', tensor([0.3147]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1170])), ('power', tensor([-21.8522]))])
epoch：580	 i:0 	 global-step:11600	 l-p:0.08082397282123566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[6.0038, 6.1312, 6.0635],
        [6.0038, 6.0039, 6.0039],
        [6.0038, 6.8981, 7.1818],
        [6.0038, 6.4849, 6.4793]], grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.08071832358837128 
model_pd.l_d.mean(): -7.0258917808532715 
model_pd.lagr.mean(): -6.945173263549805 
model_pd.lambdas: dict_items([('pout', tensor([1.2791])), ('power', tensor([0.3136]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1205])), ('power', tensor([-21.8375]))])
epoch：581	 i:0 	 global-step:11620	 l-p:0.08071832358837128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[6.0200, 6.4696, 6.4484],
        [6.0200, 6.0200, 6.0200],
        [6.0200, 6.0200, 6.0200],
        [6.0200, 6.7367, 6.8764]], grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.08061285316944122 
model_pd.l_d.mean(): -7.00196647644043 
model_pd.lagr.mean(): -6.921353816986084 
model_pd.lambdas: dict_items([('pout', tensor([1.2790])), ('power', tensor([0.3125]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1241])), ('power', tensor([-21.8228]))])
epoch：582	 i:0 	 global-step:11640	 l-p:0.08061285316944122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[6.0362, 6.0363, 6.0362],
        [6.0362, 6.2116, 6.1349],
        [6.0362, 6.2408, 6.1621],
        [6.0362, 6.0362, 6.0362]], grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.0805075466632843 
model_pd.l_d.mean(): -6.978072166442871 
model_pd.lagr.mean(): -6.89756441116333 
model_pd.lambdas: dict_items([('pout', tensor([1.2789])), ('power', tensor([0.3114]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1277])), ('power', tensor([-21.8079]))])
epoch：583	 i:0 	 global-step:11660	 l-p:0.0805075466632843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[6.0526, 6.0572, 6.0529],
        [6.0526, 7.5682, 8.4799],
        [6.0526, 6.5398, 6.5347],
        [6.0526, 6.7751, 6.9164]], grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.08040240406990051 
model_pd.l_d.mean(): -6.954209327697754 
model_pd.lagr.mean(): -6.873806953430176 
model_pd.lambdas: dict_items([('pout', tensor([1.2787])), ('power', tensor([0.3103]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1313])), ('power', tensor([-21.7929]))])
epoch：584	 i:0 	 global-step:11680	 l-p:0.08040240406990051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[6.0690, 6.3446, 6.2693],
        [6.0690, 6.1133, 6.0800],
        [6.0690, 6.4620, 6.4160],
        [6.0690, 6.0691, 6.0690]], grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.08029742538928986 
model_pd.l_d.mean(): -6.930378437042236 
model_pd.lagr.mean(): -6.850080966949463 
model_pd.lambdas: dict_items([('pout', tensor([1.2786])), ('power', tensor([0.3092]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1349])), ('power', tensor([-21.7779]))])
epoch：585	 i:0 	 global-step:11700	 l-p:0.08029742538928986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.0856, 7.3597, 7.9999],
        [6.0856, 6.0856, 6.0856],
        [6.0856, 6.0856, 6.0856],
        [6.0856, 6.1296, 6.0964]], grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.08019261062145233 
model_pd.l_d.mean(): -6.906582355499268 
model_pd.lagr.mean(): -6.826389789581299 
model_pd.lambdas: dict_items([('pout', tensor([1.2785])), ('power', tensor([0.3081]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1385])), ('power', tensor([-21.7628]))])
epoch：586	 i:0 	 global-step:11720	 l-p:0.08019261062145233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[6.1022, 6.1025, 6.1022],
        [6.1022, 7.7675, 8.8464],
        [6.1022, 6.1455, 6.1128],
        [6.1022, 6.2249, 6.1576]], grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.08008796721696854 
model_pd.l_d.mean(): -6.882816791534424 
model_pd.lagr.mean(): -6.802728652954102 
model_pd.lambdas: dict_items([('pout', tensor([1.2783])), ('power', tensor([0.3070]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1421])), ('power', tensor([-21.7475]))])
epoch：587	 i:0 	 global-step:11740	 l-p:0.08008796721696854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228]])
 pt:tensor([[6.1189, 6.2422, 6.1747],
        [6.1189, 7.6573, 8.5839],
        [6.1189, 7.4029, 8.0485],
        [6.1189, 6.2771, 6.2018]], grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.07998349517583847 
model_pd.l_d.mean(): -6.859084606170654 
model_pd.lagr.mean(): -6.7791008949279785 
model_pd.lambdas: dict_items([('pout', tensor([1.2782])), ('power', tensor([0.3060]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1457])), ('power', tensor([-21.7322]))])
epoch：588	 i:0 	 global-step:11760	 l-p:0.07998349517583847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[6.1357, 6.9759, 7.2010],
        [6.1357, 6.1588, 6.1396],
        [6.1357, 6.1357, 6.1357],
        [6.1357, 6.1357, 6.1357]], grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.07987918704748154 
model_pd.l_d.mean(): -6.835383415222168 
model_pd.lagr.mean(): -6.755504131317139 
model_pd.lambdas: dict_items([('pout', tensor([1.2780])), ('power', tensor([0.3049]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1494])), ('power', tensor([-21.7167]))])
epoch：589	 i:0 	 global-step:11780	 l-p:0.07987918704748154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[6.1527, 6.1527, 6.1527],
        [6.1527, 6.1977, 6.1638],
        [6.1527, 6.1981, 6.1640],
        [6.1527, 6.4201, 6.3417]], grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.07977503538131714 
model_pd.l_d.mean(): -6.8117170333862305 
model_pd.lagr.mean(): -6.731942176818848 
model_pd.lambdas: dict_items([('pout', tensor([1.2779])), ('power', tensor([0.3038]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1530])), ('power', tensor([-21.7012]))])
epoch：590	 i:0 	 global-step:11800	 l-p:0.07977503538131714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[6.1697, 6.1697, 6.1697],
        [6.1697, 6.1960, 6.1744],
        [6.1697, 6.3302, 6.2539],
        [6.1697, 6.1732, 6.1699]], grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.07967105507850647 
model_pd.l_d.mean(): -6.788083553314209 
model_pd.lagr.mean(): -6.7084126472473145 
model_pd.lambdas: dict_items([('pout', tensor([1.2777])), ('power', tensor([0.3027]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1567])), ('power', tensor([-21.6856]))])
epoch：591	 i:0 	 global-step:11820	 l-p:0.07967105507850647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.1868, 6.4440, 6.3638],
        [6.1868, 6.2330, 6.1984],
        [6.1868, 6.3481, 6.2715],
        [6.1868, 6.1871, 6.1868]], grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.07956722378730774 
model_pd.l_d.mean(): -6.764483451843262 
model_pd.lagr.mean(): -6.684916019439697 
model_pd.lambdas: dict_items([('pout', tensor([1.2776])), ('power', tensor([0.3016]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1604])), ('power', tensor([-21.6698]))])
epoch：592	 i:0 	 global-step:11840	 l-p:0.07956722378730774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[6.2041, 6.2940, 6.2374],
        [6.2041, 6.3305, 6.2614],
        [6.2041, 6.2053, 6.2041],
        [6.2041, 6.3661, 6.2892]], grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.07946356385946274 
model_pd.l_d.mean(): -6.740915298461914 
model_pd.lagr.mean(): -6.661451816558838 
model_pd.lambdas: dict_items([('pout', tensor([1.2774])), ('power', tensor([0.3005]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1641])), ('power', tensor([-21.6540]))])
epoch：593	 i:0 	 global-step:11860	 l-p:0.07946356385946274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[6.2214, 6.6627, 6.6285],
        [6.2214, 6.8743, 6.9540],
        [6.2214, 6.2350, 6.2231],
        [6.2214, 6.4062, 6.3261]], grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.07936007529497147 
model_pd.l_d.mean(): -6.717382431030273 
model_pd.lagr.mean(): -6.638022422790527 
model_pd.lambdas: dict_items([('pout', tensor([1.2772])), ('power', tensor([0.2995]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1678])), ('power', tensor([-21.6381]))])
epoch：594	 i:0 	 global-step:11880	 l-p:0.07936007529497147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.2388, 7.5579, 8.2232],
        [6.2388, 7.1934, 7.5046],
        [6.2388, 6.3752, 6.3033],
        [6.2388, 6.2391, 6.2388]], grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.07925673574209213 
model_pd.l_d.mean(): -6.693881988525391 
model_pd.lagr.mean(): -6.614625453948975 
model_pd.lambdas: dict_items([('pout', tensor([1.2771])), ('power', tensor([0.2984]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1715])), ('power', tensor([-21.6220]))])
epoch：595	 i:0 	 global-step:11900	 l-p:0.07925673574209213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[6.2564, 6.9695, 7.0873],
        [6.2564, 7.2059, 7.5110],
        [6.2564, 6.2568, 6.2564],
        [6.2564, 6.4739, 6.3912]], grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.07915356010198593 
model_pd.l_d.mean(): -6.670417308807373 
model_pd.lagr.mean(): -6.591263771057129 
model_pd.lambdas: dict_items([('pout', tensor([1.2769])), ('power', tensor([0.2973]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1752])), ('power', tensor([-21.6059]))])
epoch：596	 i:0 	 global-step:11920	 l-p:0.07915356010198593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[6.2740, 6.6886, 6.6418],
        [6.2740, 6.3214, 6.2859],
        [6.2740, 6.3846, 6.3199],
        [6.2740, 6.3658, 6.3081]], grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.07905054837465286 
model_pd.l_d.mean(): -6.64698600769043 
model_pd.lagr.mean(): -6.567935466766357 
model_pd.lambdas: dict_items([('pout', tensor([1.2767])), ('power', tensor([0.2962]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1789])), ('power', tensor([-21.5896]))])
epoch：597	 i:0 	 global-step:11940	 l-p:0.07905054837465286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.2918, 6.3191, 6.2967],
        [6.2918, 6.7739, 6.7539],
        [6.2918, 6.3395, 6.3037],
        [6.2918, 6.2921, 6.2918]], grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.07894767820835114 
model_pd.l_d.mean(): -6.623589515686035 
model_pd.lagr.mean(): -6.544641971588135 
model_pd.lambdas: dict_items([('pout', tensor([1.2765])), ('power', tensor([0.2951]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1827])), ('power', tensor([-21.5733]))])
epoch：598	 i:0 	 global-step:11960	 l-p:0.07894767820835114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[6.3096, 6.3371, 6.3146],
        [6.3096, 7.0785, 7.2323],
        [6.3096, 6.3099, 6.3096],
        [6.3096, 7.0319, 7.1519]], grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.07884498685598373 
model_pd.l_d.mean(): -6.600226402282715 
model_pd.lagr.mean(): -6.521381378173828 
model_pd.lambdas: dict_items([('pout', tensor([1.2763])), ('power', tensor([0.2941]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1865])), ('power', tensor([-21.5568]))])
epoch：599	 i:0 	 global-step:11980	 l-p:0.07884498685598373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[6.3276, 7.2928, 7.6038],
        [6.3276, 6.4953, 6.4161],
        [6.3276, 6.3758, 6.3397],
        [6.3276, 6.6078, 6.5267]], grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.07874244451522827 
model_pd.l_d.mean(): -6.576899528503418 
model_pd.lagr.mean(): -6.498157024383545 
model_pd.lambdas: dict_items([('pout', tensor([1.2761])), ('power', tensor([0.2930]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1902])), ('power', tensor([-21.5402]))])
epoch：600	 i:0 	 global-step:12000	 l-p:0.07874244451522827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[6.3457, 7.2283, 7.4676],
        [6.3457, 6.3523, 6.3462],
        [6.3457, 6.3734, 6.3507],
        [6.3457, 6.3493, 6.3459]], grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.07864005863666534 
model_pd.l_d.mean(): -6.553606033325195 
model_pd.lagr.mean(): -6.474966049194336 
model_pd.lambdas: dict_items([('pout', tensor([1.2760])), ('power', tensor([0.2919]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1940])), ('power', tensor([-21.5236]))])
epoch：601	 i:0 	 global-step:12020	 l-p:0.07864005863666534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[6.3639, 6.6467, 6.5650],
        [6.3639, 6.3639, 6.3639],
        [6.3639, 7.6897, 8.3439],
        [6.3639, 6.3886, 6.3680]], grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.07853783667087555 
model_pd.l_d.mean(): -6.53034782409668 
model_pd.lagr.mean(): -6.451809883117676 
model_pd.lambdas: dict_items([('pout', tensor([1.2758])), ('power', tensor([0.2908]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1978])), ('power', tensor([-21.5068]))])
epoch：602	 i:0 	 global-step:12040	 l-p:0.07853783667087555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[6.3822, 6.3826, 6.3822],
        [6.3822, 6.3859, 6.3824],
        [6.3822, 6.4311, 6.3945],
        [6.3822, 6.5753, 6.4921]], grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.07843577116727829 
model_pd.l_d.mean(): -6.507125377655029 
model_pd.lagr.mean(): -6.428689479827881 
model_pd.lambdas: dict_items([('pout', tensor([1.2756])), ('power', tensor([0.2898]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2016])), ('power', tensor([-21.4899]))])
epoch：603	 i:0 	 global-step:12060	 l-p:0.07843577116727829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.4006, 6.4006, 6.4006],
        [6.4006, 6.8285, 6.7813],
        [6.4006, 6.4018, 6.4006],
        [6.4006, 6.4006, 6.4006]], grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.07833384722471237 
model_pd.l_d.mean(): -6.483938217163086 
model_pd.lagr.mean(): -6.405604362487793 
model_pd.lambdas: dict_items([('pout', tensor([1.2753])), ('power', tensor([0.2887]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2054])), ('power', tensor([-21.4729]))])
epoch：604	 i:0 	 global-step:12080	 l-p:0.07833384722471237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[6.4191, 6.4243, 6.4194],
        [6.4191, 6.4191, 6.4191],
        [6.4191, 7.7608, 8.4237],
        [6.4191, 7.4044, 7.7231]], grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.07823210954666138 
model_pd.l_d.mean(): -6.460785865783691 
model_pd.lagr.mean(): -6.382553577423096 
model_pd.lambdas: dict_items([('pout', tensor([1.2751])), ('power', tensor([0.2876]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2093])), ('power', tensor([-21.4557]))])
epoch：605	 i:0 	 global-step:12100	 l-p:0.07823210954666138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[6.4377, 6.5726, 6.4994],
        [6.4377, 7.3390, 7.5845],
        [6.4377, 6.4377, 6.4377],
        [6.4377, 6.7124, 6.6282]], grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.07813049852848053 
model_pd.l_d.mean(): -6.43766975402832 
model_pd.lagr.mean(): -6.359539031982422 
model_pd.lambdas: dict_items([('pout', tensor([1.2749])), ('power', tensor([0.2865]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2131])), ('power', tensor([-21.4385]))])
epoch：606	 i:0 	 global-step:12120	 l-p:0.07813049852848053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[6.4564, 6.9409, 6.9138],
        [6.4564, 7.8394, 8.5403],
        [6.4564, 6.4565, 6.4564],
        [6.4564, 6.5062, 6.4690]], grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.07802905142307281 
model_pd.l_d.mean(): -6.414589881896973 
model_pd.lagr.mean(): -6.3365607261657715 
model_pd.lambdas: dict_items([('pout', tensor([1.2747])), ('power', tensor([0.2855]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2170])), ('power', tensor([-21.4212]))])
epoch：607	 i:0 	 global-step:12140	 l-p:0.07802905142307281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.4753, 6.4753, 6.4753],
        [6.4753, 7.8638, 8.5678],
        [6.4753, 6.8648, 6.8020],
        [6.4753, 6.4753, 6.4753]], grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.07792776077985764 
model_pd.l_d.mean(): -6.39154577255249 
model_pd.lagr.mean(): -6.313618183135986 
model_pd.lambdas: dict_items([('pout', tensor([1.2745])), ('power', tensor([0.2844]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2208])), ('power', tensor([-21.4037]))])
epoch：608	 i:0 	 global-step:12160	 l-p:0.07792776077985764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[6.4943, 6.5449, 6.5071],
        [6.4943, 7.1908, 7.2790],
        [6.4943, 7.5054, 7.8387],
        [6.4943, 6.8856, 6.8227]], grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.0778266116976738 
model_pd.l_d.mean(): -6.368537902832031 
model_pd.lagr.mean(): -6.290711402893066 
model_pd.lambdas: dict_items([('pout', tensor([1.2743])), ('power', tensor([0.2833]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2247])), ('power', tensor([-21.3861]))])
epoch：609	 i:0 	 global-step:12180	 l-p:0.0778266116976738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[6.5134, 6.5624, 6.5255],
        [6.5134, 8.3304, 9.5156],
        [6.5134, 6.5136, 6.5134],
        [6.5134, 6.5283, 6.5152]], grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.07772563397884369 
model_pd.l_d.mean(): -6.34556770324707 
model_pd.lagr.mean(): -6.2678422927856445 
model_pd.lambdas: dict_items([('pout', tensor([1.2740])), ('power', tensor([0.2823]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2286])), ('power', tensor([-21.3685]))])
epoch：610	 i:0 	 global-step:12200	 l-p:0.07772563397884369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[6.5326, 7.9070, 8.5878],
        [6.5326, 7.5509, 7.8864],
        [6.5326, 6.7335, 6.6475],
        [6.5326, 6.5818, 6.5448]], grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.07762481272220612 
model_pd.l_d.mean(): -6.322632789611816 
model_pd.lagr.mean(): -6.2450079917907715 
model_pd.lambdas: dict_items([('pout', tensor([1.2738])), ('power', tensor([0.2812]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2325])), ('power', tensor([-21.3506]))])
epoch：611	 i:0 	 global-step:12220	 l-p:0.07762481272220612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[6.5519, 6.8646, 6.7821],
        [6.5519, 6.5519, 6.5519],
        [6.5519, 7.3156, 7.4455],
        [6.5519, 6.6033, 6.5649]], grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.07752413302659988 
model_pd.l_d.mean(): -6.299735069274902 
model_pd.lagr.mean(): -6.222210884094238 
model_pd.lambdas: dict_items([('pout', tensor([1.2736])), ('power', tensor([0.2801]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2364])), ('power', tensor([-21.3327]))])
epoch：612	 i:0 	 global-step:12240	 l-p:0.07752413302659988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.5714, 6.5865, 6.5732],
        [6.5714, 6.5768, 6.5717],
        [6.5714, 7.2802, 7.3708],
        [6.5714, 6.6223, 6.5841]], grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.07742360979318619 
model_pd.l_d.mean(): -6.27687406539917 
model_pd.lagr.mean(): -6.199450492858887 
model_pd.lambdas: dict_items([('pout', tensor([1.2733])), ('power', tensor([0.2791]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2404])), ('power', tensor([-21.3147]))])
epoch：613	 i:0 	 global-step:12260	 l-p:0.07742360979318619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[6.5909, 7.3613, 7.4927],
        [6.5909, 7.4107, 7.5781],
        [6.5909, 6.5909, 6.5909],
        [6.5909, 6.5912, 6.5909]], grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.07732324302196503 
model_pd.l_d.mean(): -6.254051208496094 
model_pd.lagr.mean(): -6.176727771759033 
model_pd.lambdas: dict_items([('pout', tensor([1.2731])), ('power', tensor([0.2780]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2443])), ('power', tensor([-21.2965]))])
epoch：614	 i:0 	 global-step:12280	 l-p:0.07732324302196503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[6.6106, 7.3257, 7.4176],
        [6.6106, 6.6178, 6.6112],
        [6.6106, 6.7114, 6.6485],
        [6.6106, 8.0075, 8.7006]], grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.07722301781177521 
model_pd.l_d.mean(): -6.231264114379883 
model_pd.lagr.mean(): -6.154041290283203 
model_pd.lambdas: dict_items([('pout', tensor([1.2728])), ('power', tensor([0.2769]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2483])), ('power', tensor([-21.2782]))])
epoch：615	 i:0 	 global-step:12300	 l-p:0.07722301781177521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[6.6304, 7.6622, 7.9988],
        [6.6304, 7.6704, 8.0143],
        [6.6304, 6.6822, 6.6434],
        [6.6304, 6.6304, 6.6304]], grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.07712294161319733 
model_pd.l_d.mean(): -6.2085161209106445 
model_pd.lagr.mean(): -6.131392955780029 
model_pd.lambdas: dict_items([('pout', tensor([1.2726])), ('power', tensor([0.2759]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2522])), ('power', tensor([-21.2598]))])
epoch：616	 i:0 	 global-step:12320	 l-p:0.07712294161319733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[6.6504, 6.7024, 6.6635],
        [6.6504, 7.1757, 7.1573],
        [6.6504, 8.0902, 8.8230],
        [6.6504, 7.1401, 7.1060]], grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.07702302187681198 
model_pd.l_d.mean(): -6.185804843902588 
model_pd.lagr.mean(): -6.108781814575195 
model_pd.lambdas: dict_items([('pout', tensor([1.2723])), ('power', tensor([0.2748]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2562])), ('power', tensor([-21.2413]))])
epoch：617	 i:0 	 global-step:12340	 l-p:0.07702302187681198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[6.6704, 7.0786, 7.0142],
        [6.6704, 6.7232, 6.6838],
        [6.6704, 7.4544, 7.5891],
        [6.6704, 6.6707, 6.6704]], grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.07692325860261917 
model_pd.l_d.mean(): -6.163132667541504 
model_pd.lagr.mean(): -6.086209297180176 
model_pd.lambdas: dict_items([('pout', tensor([1.2721])), ('power', tensor([0.2737]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2602])), ('power', tensor([-21.2226]))])
epoch：618	 i:0 	 global-step:12360	 l-p:0.07692325860261917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[6.6906, 6.7420, 6.7034],
        [6.6906, 6.7440, 6.7042],
        [6.6906, 7.6431, 7.9059],
        [6.6906, 6.6980, 6.6912]], grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.0768236517906189 
model_pd.l_d.mean(): -6.14049768447876 
model_pd.lagr.mean(): -6.063673973083496 
model_pd.lambdas: dict_items([('pout', tensor([1.2718])), ('power', tensor([0.2727]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2642])), ('power', tensor([-21.2039]))])
epoch：619	 i:0 	 global-step:12380	 l-p:0.0768236517906189
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[6.7109, 6.7166, 6.7113],
        [6.7109, 7.1716, 7.1233],
        [6.7109, 7.7701, 8.1221],
        [6.7109, 6.7417, 6.7166]], grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.07672417908906937 
model_pd.l_d.mean(): -6.117901802062988 
model_pd.lagr.mean(): -6.041177749633789 
model_pd.lambdas: dict_items([('pout', tensor([1.2715])), ('power', tensor([0.2716]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2682])), ('power', tensor([-21.1850]))])
epoch：620	 i:0 	 global-step:12400	 l-p:0.07672417908906937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[6.7313, 6.7313, 6.7313],
        [6.7313, 6.7319, 6.7314],
        [6.7313, 6.7314, 6.7313],
        [6.7313, 6.7371, 6.7317]], grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.07662486284971237 
model_pd.l_d.mean(): -6.095344543457031 
model_pd.lagr.mean(): -6.018719673156738 
model_pd.lambdas: dict_items([('pout', tensor([1.2713])), ('power', tensor([0.2706]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2722])), ('power', tensor([-21.1659]))])
epoch：621	 i:0 	 global-step:12420	 l-p:0.07662486284971237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[6.7519, 6.8062, 6.7657],
        [6.7519, 6.7519, 6.7519],
        [6.7519, 7.5499, 7.6879],
        [6.7519, 7.7169, 7.9838]], grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.07652569562196732 
model_pd.l_d.mean(): -6.072824478149414 
model_pd.lagr.mean(): -5.996298789978027 
model_pd.lambdas: dict_items([('pout', tensor([1.2710])), ('power', tensor([0.2695]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2763])), ('power', tensor([-21.1468]))])
epoch：622	 i:0 	 global-step:12440	 l-p:0.07652569562196732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[6.7726, 7.3127, 7.2948],
        [6.7726, 6.9859, 6.8953],
        [6.7726, 6.9198, 6.8406],
        [6.7726, 6.7727, 6.7726]], grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.0764266848564148 
model_pd.l_d.mean(): -6.050344944000244 
model_pd.lagr.mean(): -5.973918437957764 
model_pd.lambdas: dict_items([('pout', tensor([1.2707])), ('power', tensor([0.2685]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2803])), ('power', tensor([-21.1275]))])
epoch：623	 i:0 	 global-step:12460	 l-p:0.0764266848564148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[6.7934, 6.7993, 6.7938],
        [6.7934, 6.8463, 6.8066],
        [6.7934, 7.2629, 7.2143],
        [6.7934, 6.7934, 6.7934]], grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.07632780820131302 
model_pd.l_d.mean(): -6.027904510498047 
model_pd.lagr.mean(): -5.9515767097473145 
model_pd.lambdas: dict_items([('pout', tensor([1.2704])), ('power', tensor([0.2674]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2844])), ('power', tensor([-21.1081]))])
epoch：624	 i:0 	 global-step:12480	 l-p:0.07632780820131302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[6.8144, 7.1304, 7.0416],
        [6.8144, 7.1474, 7.0610],
        [6.8144, 6.8687, 6.8281],
        [6.8144, 8.2700, 8.9953]], grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.07622910290956497 
model_pd.l_d.mean(): -6.005503177642822 
model_pd.lagr.mean(): -5.929274082183838 
model_pd.lambdas: dict_items([('pout', tensor([1.2701])), ('power', tensor([0.2663]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2885])), ('power', tensor([-21.0885]))])
epoch：625	 i:0 	 global-step:12500	 l-p:0.07622910290956497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[6.8355, 7.6996, 7.8789],
        [6.8355, 7.5865, 7.6854],
        [6.8355, 6.8355, 6.8355],
        [6.8355, 8.4064, 9.2525]], grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.07613053917884827 
model_pd.l_d.mean(): -5.98314094543457 
model_pd.lagr.mean(): -5.907010555267334 
model_pd.lambdas: dict_items([('pout', tensor([1.2698])), ('power', tensor([0.2653]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2926])), ('power', tensor([-21.0688]))])
epoch：626	 i:0 	 global-step:12520	 l-p:0.07613053917884827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[6.8567, 7.2827, 7.2168],
        [6.8567, 6.8567, 6.8567],
        [6.8567, 6.9122, 6.8708],
        [6.8567, 8.6485, 9.7413]], grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.0760321170091629 
model_pd.l_d.mean(): -5.960818290710449 
model_pd.lagr.mean(): -5.884786128997803 
model_pd.lambdas: dict_items([('pout', tensor([1.2695])), ('power', tensor([0.2642]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2967])), ('power', tensor([-21.0490]))])
epoch：627	 i:0 	 global-step:12540	 l-p:0.0760321170091629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[6.8781, 7.9730, 8.3382],
        [6.8781, 7.3061, 7.2400],
        [6.8781, 6.8824, 6.8783],
        [6.8781, 6.8796, 6.8781]], grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.07593384385108948 
model_pd.l_d.mean(): -5.938536643981934 
model_pd.lagr.mean(): -5.862602710723877 
model_pd.lambdas: dict_items([('pout', tensor([1.2692])), ('power', tensor([0.2632]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3008])), ('power', tensor([-21.0291]))])
epoch：628	 i:0 	 global-step:12560	 l-p:0.07593384385108948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[6.8996, 7.0936, 7.0036],
        [6.8996, 6.8996, 6.8996],
        [6.8996, 7.6608, 7.7618],
        [6.8996, 7.0617, 6.9777]], grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.07583572715520859 
model_pd.l_d.mean(): -5.916294574737549 
model_pd.lagr.mean(): -5.840458869934082 
model_pd.lambdas: dict_items([('pout', tensor([1.2689])), ('power', tensor([0.2621]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3049])), ('power', tensor([-21.0090]))])
epoch：629	 i:0 	 global-step:12580	 l-p:0.07583572715520859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[6.9212, 7.2451, 7.1546],
        [6.9212, 8.0270, 8.3972],
        [6.9212, 8.4407, 9.2180],
        [6.9212, 6.9501, 6.9261]], grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.07573775202035904 
model_pd.l_d.mean(): -5.894093036651611 
model_pd.lagr.mean(): -5.818355083465576 
model_pd.lambdas: dict_items([('pout', tensor([1.2686])), ('power', tensor([0.2611]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3091])), ('power', tensor([-20.9887]))])
epoch：630	 i:0 	 global-step:12600	 l-p:0.07573775202035904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[6.9430, 7.4283, 7.3792],
        [6.9430, 6.9433, 6.9430],
        [6.9430, 8.4357, 9.1812],
        [6.9430, 7.4659, 7.4319]], grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.07563991844654083 
model_pd.l_d.mean(): -5.871932029724121 
model_pd.lagr.mean(): -5.796292304992676 
model_pd.lambdas: dict_items([('pout', tensor([1.2683])), ('power', tensor([0.2600]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3132])), ('power', tensor([-20.9684]))])
epoch：631	 i:0 	 global-step:12620	 l-p:0.07563991844654083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[6.9649, 8.0790, 8.4518],
        [6.9649, 7.0222, 6.9796],
        [6.9649, 6.9649, 6.9649],
        [6.9649, 7.1191, 7.0364]], grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.07554224133491516 
model_pd.l_d.mean(): -5.849812030792236 
model_pd.lagr.mean(): -5.7742695808410645 
model_pd.lambdas: dict_items([('pout', tensor([1.2680])), ('power', tensor([0.2590]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3174])), ('power', tensor([-20.9479]))])
epoch：632	 i:0 	 global-step:12640	 l-p:0.07554224133491516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[6.9869, 7.1850, 7.0933],
        [6.9869, 7.0443, 7.0016],
        [6.9869, 7.5334, 7.5075],
        [6.9869, 6.9869, 6.9869]], grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.07544471323490143 
model_pd.l_d.mean(): -5.827732563018799 
model_pd.lagr.mean(): -5.752287864685059 
model_pd.lambdas: dict_items([('pout', tensor([1.2677])), ('power', tensor([0.2579]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3216])), ('power', tensor([-20.9272]))])
epoch：633	 i:0 	 global-step:12660	 l-p:0.07544471323490143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[7.0091, 8.8534, 9.9805],
        [7.0091, 7.1650, 7.0815],
        [7.0091, 7.0262, 7.0113],
        [7.0091, 7.0091, 7.0091]], grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.07534732669591904 
model_pd.l_d.mean(): -5.805695533752441 
model_pd.lagr.mean(): -5.730348110198975 
model_pd.lambdas: dict_items([('pout', tensor([1.2673])), ('power', tensor([0.2569]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3258])), ('power', tensor([-20.9064]))])
epoch：634	 i:0 	 global-step:12680	 l-p:0.07534732669591904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[7.0315, 7.6028, 7.5861],
        [7.0315, 7.2316, 7.1391],
        [7.0315, 7.0315, 7.0315],
        [7.0315, 7.0330, 7.0315]], grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.07525008171796799 
model_pd.l_d.mean(): -5.783699035644531 
model_pd.lagr.mean(): -5.708448886871338 
model_pd.lambdas: dict_items([('pout', tensor([1.2670])), ('power', tensor([0.2559]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3300])), ('power', tensor([-20.8855]))])
epoch：635	 i:0 	 global-step:12700	 l-p:0.07525008171796799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[7.0540, 7.1104, 7.0681],
        [7.0540, 8.1879, 8.5683],
        [7.0540, 7.0542, 7.0540],
        [7.0540, 7.1122, 7.0689]], grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.07515298575162888 
model_pd.l_d.mean(): -5.761744499206543 
model_pd.lagr.mean(): -5.686591625213623 
model_pd.lambdas: dict_items([('pout', tensor([1.2667])), ('power', tensor([0.2548]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3342])), ('power', tensor([-20.8644]))])
epoch：636	 i:0 	 global-step:12720	 l-p:0.07515298575162888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[7.0766, 7.6147, 7.5807],
        [7.0766, 7.0770, 7.0766],
        [7.0766, 7.1355, 7.0917],
        [7.0766, 7.2127, 7.1344]], grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.0750560462474823 
model_pd.l_d.mean(): -5.73983097076416 
model_pd.lagr.mean(): -5.6647748947143555 
model_pd.lambdas: dict_items([('pout', tensor([1.2663])), ('power', tensor([0.2538]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3385])), ('power', tensor([-20.8432]))])
epoch：637	 i:0 	 global-step:12740	 l-p:0.0750560462474823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[7.0994, 7.7209, 7.7253],
        [7.0994, 8.7514, 9.6450],
        [7.0994, 7.3670, 7.2689],
        [7.0994, 7.1565, 7.1137]], grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.07495925575494766 
model_pd.l_d.mean(): -5.717960357666016 
model_pd.lagr.mean(): -5.643001079559326 
model_pd.lambdas: dict_items([('pout', tensor([1.2660])), ('power', tensor([0.2527]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3427])), ('power', tensor([-20.8219]))])
epoch：638	 i:0 	 global-step:12760	 l-p:0.07495925575494766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[7.1223, 7.1223, 7.1223],
        [7.1223, 8.0383, 8.2316],
        [7.1223, 7.1227, 7.1223],
        [7.1223, 7.1223, 7.1223]], grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.07486259937286377 
model_pd.l_d.mean(): -5.696133136749268 
model_pd.lagr.mean(): -5.621270656585693 
model_pd.lambdas: dict_items([('pout', tensor([1.2657])), ('power', tensor([0.2517]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3470])), ('power', tensor([-20.8003]))])
epoch：639	 i:0 	 global-step:12780	 l-p:0.07486259937286377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[7.1454, 7.2049, 7.1606],
        [7.1454, 7.1454, 7.1454],
        [7.1454, 7.7304, 7.7142],
        [7.1454, 7.2031, 7.1599]], grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.07476609945297241 
model_pd.l_d.mean(): -5.674346923828125 
model_pd.lagr.mean(): -5.599580764770508 
model_pd.lambdas: dict_items([('pout', tensor([1.2653])), ('power', tensor([0.2506]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3513])), ('power', tensor([-20.7787]))])
epoch：640	 i:0 	 global-step:12800	 l-p:0.07476609945297241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[7.1686, 7.2033, 7.1750],
        [7.1686, 7.1686, 7.1686],
        [7.1686, 7.1686, 7.1686],
        [7.1686, 7.1686, 7.1686]], grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.0746697410941124 
model_pd.l_d.mean(): -5.652603626251221 
model_pd.lagr.mean(): -5.5779337882995605 
model_pd.lambdas: dict_items([('pout', tensor([1.2649])), ('power', tensor([0.2496]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3556])), ('power', tensor([-20.7569]))])
epoch：641	 i:0 	 global-step:12820	 l-p:0.0746697410941124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[7.1920, 8.2462, 8.5431],
        [7.1920, 7.5541, 7.4621],
        [7.1920, 7.1984, 7.1924],
        [7.1920, 7.1920, 7.1920]], grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.07457352429628372 
model_pd.l_d.mean(): -5.630903720855713 
model_pd.lagr.mean(): -5.55633020401001 
model_pd.lambdas: dict_items([('pout', tensor([1.2646])), ('power', tensor([0.2486]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3599])), ('power', tensor([-20.7349]))])
epoch：642	 i:0 	 global-step:12840	 l-p:0.07457352429628372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[7.2155, 7.7297, 7.6796],
        [7.2155, 7.2155, 7.2155],
        [7.2155, 7.2158, 7.2155],
        [7.2155, 7.3900, 7.3001]], grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.07447746396064758 
model_pd.l_d.mean(): -5.609246730804443 
model_pd.lagr.mean(): -5.534769058227539 
model_pd.lambdas: dict_items([('pout', tensor([1.2642])), ('power', tensor([0.2475]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3642])), ('power', tensor([-20.7128]))])
epoch：643	 i:0 	 global-step:12860	 l-p:0.07447746396064758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[7.2392, 7.8356, 7.8197],
        [7.2392, 7.2392, 7.2392],
        [7.2392, 7.3568, 7.2841],
        [7.2392, 7.2398, 7.2392]], grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.07438155263662338 
model_pd.l_d.mean(): -5.58763313293457 
model_pd.lagr.mean(): -5.513251781463623 
model_pd.lambdas: dict_items([('pout', tensor([1.2639])), ('power', tensor([0.2465]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3686])), ('power', tensor([-20.6905]))])
epoch：644	 i:0 	 global-step:12880	 l-p:0.07438155263662338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[7.2630, 7.7822, 7.7320],
        [7.2630, 7.2631, 7.2630],
        [7.2630, 8.4433, 8.8417],
        [7.2630, 7.6121, 7.5162]], grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.07428578287363052 
model_pd.l_d.mean(): -5.56606388092041 
model_pd.lagr.mean(): -5.4917778968811035 
model_pd.lambdas: dict_items([('pout', tensor([1.2635])), ('power', tensor([0.2455]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3729])), ('power', tensor([-20.6681]))])
epoch：645	 i:0 	 global-step:12900	 l-p:0.07428578287363052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[ 7.2870,  7.4531,  7.3646],
        [ 7.2870,  7.3227,  7.2937],
        [ 7.2870,  9.2269, 10.4167],
        [ 7.2870,  7.9327,  7.9388]], grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.0741901770234108 
model_pd.l_d.mean(): -5.544537544250488 
model_pd.lagr.mean(): -5.4703474044799805 
model_pd.lambdas: dict_items([('pout', tensor([1.2631])), ('power', tensor([0.2444]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3773])), ('power', tensor([-20.6455]))])
epoch：646	 i:0 	 global-step:12920	 l-p:0.0741901770234108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[7.3112, 7.3199, 7.3119],
        [7.3112, 7.9162, 7.9008],
        [7.3112, 7.3179, 7.3117],
        [7.3112, 7.3735, 7.3273]], grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.07409471273422241 
model_pd.l_d.mean(): -5.523056507110596 
model_pd.lagr.mean(): -5.4489617347717285 
model_pd.lambdas: dict_items([('pout', tensor([1.2627])), ('power', tensor([0.2434]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3817])), ('power', tensor([-20.6227]))])
epoch：647	 i:0 	 global-step:12940	 l-p:0.07409471273422241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[7.3355, 7.3443, 7.3362],
        [7.3355, 7.3355, 7.3355],
        [7.3355, 7.3359, 7.3355],
        [7.3355, 7.5034, 7.4140]], grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.07399939745664597 
model_pd.l_d.mean(): -5.501619338989258 
model_pd.lagr.mean(): -5.427619934082031 
model_pd.lambdas: dict_items([('pout', tensor([1.2623])), ('power', tensor([0.2424]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3861])), ('power', tensor([-20.5998]))])
epoch：648	 i:0 	 global-step:12960	 l-p:0.07399939745664597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[7.3600, 8.5618, 8.9685],
        [7.3600, 8.0150, 8.0218],
        [7.3600, 7.3600, 7.3600],
        [7.3600, 8.4483, 8.7567]], grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.07390423119068146 
model_pd.l_d.mean(): -5.480226516723633 
model_pd.lagr.mean(): -5.406322479248047 
model_pd.lambdas: dict_items([('pout', tensor([1.2619])), ('power', tensor([0.2413]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3905])), ('power', tensor([-20.5768]))])
epoch：649	 i:0 	 global-step:12980	 l-p:0.07390423119068146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[7.3846, 8.4780, 8.7880],
        [7.3846, 8.2912, 8.4548],
        [7.3846, 7.7261, 7.6262],
        [7.3846, 7.7427, 7.6448]], grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.0738091841340065 
model_pd.l_d.mean(): -5.458878993988037 
model_pd.lagr.mean(): -5.385069847106934 
model_pd.lambdas: dict_items([('pout', tensor([1.2616])), ('power', tensor([0.2403]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3949])), ('power', tensor([-20.5535]))])
epoch：650	 i:0 	 global-step:13000	 l-p:0.0738091841340065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[7.4095, 7.4095, 7.4095],
        [7.4095, 7.4095, 7.4095],
        [7.4095, 7.4732, 7.4259],
        [7.4095, 7.6558, 7.5532]], grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.07371428608894348 
model_pd.l_d.mean(): -5.437576770782471 
model_pd.lagr.mean(): -5.36386251449585 
model_pd.lambdas: dict_items([('pout', tensor([1.2612])), ('power', tensor([0.2393]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3993])), ('power', tensor([-20.5301]))])
epoch：651	 i:0 	 global-step:13020	 l-p:0.07371428608894348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[ 7.4344,  7.4345,  7.4344],
        [ 7.4344,  9.5921, 11.0161],
        [ 7.4344,  7.4348,  7.4344],
        [ 7.4344,  7.4976,  7.4506]], grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.07361951470375061 
model_pd.l_d.mean(): -5.416319847106934 
model_pd.lagr.mean(): -5.342700481414795 
model_pd.lambdas: dict_items([('pout', tensor([1.2607])), ('power', tensor([0.2383]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4038])), ('power', tensor([-20.5066]))])
epoch：652	 i:0 	 global-step:13040	 l-p:0.07361951470375061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[ 7.4596,  9.2223, 10.1808],
        [ 7.4596,  8.1275,  8.1352],
        [ 7.4596,  7.4596,  7.4596],
        [ 7.4596,  8.3790,  8.5457]], grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.07352489233016968 
model_pd.l_d.mean(): -5.395108222961426 
model_pd.lagr.mean(): -5.321583271026611 
model_pd.lambdas: dict_items([('pout', tensor([1.2603])), ('power', tensor([0.2372]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4083])), ('power', tensor([-20.4828]))])
epoch：653	 i:0 	 global-step:13060	 l-p:0.07352489233016968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[7.4849, 8.1561, 8.1641],
        [7.4849, 8.7144, 9.1320],
        [7.4849, 7.4852, 7.4849],
        [7.4849, 7.8335, 7.7320]], grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.0734303817152977 
model_pd.l_d.mean(): -5.3739423751831055 
model_pd.lagr.mean(): -5.300511837005615 
model_pd.lambdas: dict_items([('pout', tensor([1.2599])), ('power', tensor([0.2362]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4127])), ('power', tensor([-20.4589]))])
epoch：654	 i:0 	 global-step:13080	 l-p:0.0734303817152977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[ 7.5104,  7.8609,  7.7588],
        [ 7.5104,  9.5272, 10.7675],
        [ 7.5104,  7.5122,  7.5105],
        [ 7.5104,  7.8027,  7.6969]], grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.07333602756261826 
model_pd.l_d.mean(): -5.352823257446289 
model_pd.lagr.mean(): -5.279487133026123 
model_pd.lambdas: dict_items([('pout', tensor([1.2595])), ('power', tensor([0.2352]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4172])), ('power', tensor([-20.4348]))])
epoch：655	 i:0 	 global-step:13100	 l-p:0.07333602756261826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[ 7.5361,  9.2365, 10.1149],
        [ 7.5361,  7.6007,  7.5527],
        [ 7.5361,  7.9249,  7.8278],
        [ 7.5361,  8.0844,  8.0332]], grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.07324182987213135 
model_pd.l_d.mean(): -5.331750392913818 
model_pd.lagr.mean(): -5.258508682250977 
model_pd.lambdas: dict_items([('pout', tensor([1.2591])), ('power', tensor([0.2342]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4217])), ('power', tensor([-20.4106]))])
epoch：656	 i:0 	 global-step:13120	 l-p:0.07324182987213135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[7.5619, 7.8573, 7.7506],
        [7.5619, 8.8086, 9.2328],
        [7.5619, 7.9528, 7.8553],
        [7.5619, 8.4990, 8.6699]], grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.07314775884151459 
model_pd.l_d.mean(): -5.310723304748535 
model_pd.lagr.mean(): -5.237575531005859 
model_pd.lambdas: dict_items([('pout', tensor([1.2587])), ('power', tensor([0.2332]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4263])), ('power', tensor([-20.3861]))])
epoch：657	 i:0 	 global-step:13140	 l-p:0.07314775884151459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[7.5879, 7.5879, 7.5879],
        [7.5879, 8.8307, 9.2484],
        [7.5879, 7.5897, 7.5880],
        [7.5879, 7.7149, 7.6368]], grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.07305385172367096 
model_pd.l_d.mean(): -5.289743900299072 
model_pd.lagr.mean(): -5.2166900634765625 
model_pd.lambdas: dict_items([('pout', tensor([1.2582])), ('power', tensor([0.2321]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4308])), ('power', tensor([-20.3615]))])
epoch：658	 i:0 	 global-step:13160	 l-p:0.07305385172367096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[7.6141, 7.6235, 7.6149],
        [7.6141, 8.6195, 8.8368],
        [7.6141, 7.6482, 7.6201],
        [7.6141, 7.6808, 7.6314]], grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.07296010106801987 
model_pd.l_d.mean(): -5.26881217956543 
model_pd.lagr.mean(): -5.195852279663086 
model_pd.lambdas: dict_items([('pout', tensor([1.2578])), ('power', tensor([0.2311]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4354])), ('power', tensor([-20.3367]))])
epoch：659	 i:0 	 global-step:13180	 l-p:0.07296010106801987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[7.6405, 7.6408, 7.6405],
        [7.6405, 7.6748, 7.6465],
        [7.6405, 8.1999, 8.1484],
        [7.6405, 7.7947, 7.7067]], grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.07286649942398071 
model_pd.l_d.mean(): -5.247926235198975 
model_pd.lagr.mean(): -5.175059795379639 
model_pd.lambdas: dict_items([('pout', tensor([1.2574])), ('power', tensor([0.2301]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4400])), ('power', tensor([-20.3118]))])
epoch：660	 i:0 	 global-step:13200	 l-p:0.07286649942398071
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[7.6670, 8.9273, 9.3517],
        [7.6670, 7.6671, 7.6670],
        [7.6670, 7.9687, 7.8601],
        [7.6670, 7.6723, 7.6673]], grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.07277306914329529 
model_pd.l_d.mean(): -5.2270894050598145 
model_pd.lagr.mean(): -5.154316425323486 
model_pd.lambdas: dict_items([('pout', tensor([1.2569])), ('power', tensor([0.2291]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4445])), ('power', tensor([-20.2866]))])
epoch：661	 i:0 	 global-step:13220	 l-p:0.07277306914329529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[ 7.6938,  7.7616,  7.7114],
        [ 7.6938,  7.6990,  7.6941],
        [ 7.6938,  9.5286, 10.5294],
        [ 7.6938,  7.6938,  7.6938]], grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.072679802775383 
model_pd.l_d.mean(): -5.206298828125 
model_pd.lagr.mean(): -5.1336188316345215 
model_pd.lambdas: dict_items([('pout', tensor([1.2565])), ('power', tensor([0.2281]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4491])), ('power', tensor([-20.2613]))])
epoch：662	 i:0 	 global-step:13240	 l-p:0.072679802775383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[7.7207, 8.6852, 8.8626],
        [7.7207, 7.7225, 7.7207],
        [7.7207, 7.9833, 7.8747],
        [7.7207, 9.0027, 9.4406]], grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.07258670032024384 
model_pd.l_d.mean(): -5.185556411743164 
model_pd.lagr.mean(): -5.112969875335693 
model_pd.lambdas: dict_items([('pout', tensor([1.2560])), ('power', tensor([0.2271]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4538])), ('power', tensor([-20.2358]))])
epoch：663	 i:0 	 global-step:13260	 l-p:0.07258670032024384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[7.7477, 8.1328, 8.0292],
        [7.7477, 7.9053, 7.8156],
        [7.7477, 7.9811, 7.8750],
        [7.7477, 7.8154, 7.7652]], grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.0724937841296196 
model_pd.l_d.mean(): -5.164863586425781 
model_pd.lagr.mean(): -5.09237003326416 
model_pd.lambdas: dict_items([('pout', tensor([1.2555])), ('power', tensor([0.2261]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4584])), ('power', tensor([-20.2102]))])
epoch：664	 i:0 	 global-step:13280	 l-p:0.0724937841296196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[7.7750, 7.7768, 7.7751],
        [7.7750, 7.7754, 7.7750],
        [7.7750, 7.7757, 7.7750],
        [7.7750, 8.0405, 7.9309]], grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.0724010095000267 
model_pd.l_d.mean(): -5.144218444824219 
model_pd.lagr.mean(): -5.071817398071289 
model_pd.lambdas: dict_items([('pout', tensor([1.2551])), ('power', tensor([0.2251]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4630])), ('power', tensor([-20.1843]))])
epoch：665	 i:0 	 global-step:13300	 l-p:0.0724010095000267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[ 7.8024,  8.7810,  8.9619],
        [ 7.8024,  9.9199, 11.2262],
        [ 7.8024,  8.4231,  8.3895],
        [ 7.8024,  8.1123,  8.0011]], grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.07230840623378754 
model_pd.l_d.mean(): -5.123622894287109 
model_pd.lagr.mean(): -5.051314353942871 
model_pd.lambdas: dict_items([('pout', tensor([1.2546])), ('power', tensor([0.2240]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4677])), ('power', tensor([-20.1583]))])
epoch：666	 i:0 	 global-step:13320	 l-p:0.07230840623378754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[7.8300, 8.8134, 8.9954],
        [7.8300, 7.8300, 7.8300],
        [7.8300, 8.2418, 8.1403],
        [7.8300, 7.8995, 7.8481]], grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.0722159892320633 
model_pd.l_d.mean(): -5.1030755043029785 
model_pd.lagr.mean(): -5.030859470367432 
model_pd.lambdas: dict_items([('pout', tensor([1.2541])), ('power', tensor([0.2230]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4724])), ('power', tensor([-20.1321]))])
epoch：667	 i:0 	 global-step:13340	 l-p:0.0722159892320633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[7.8578, 7.8581, 7.8579],
        [7.8578, 8.5065, 8.4828],
        [7.8578, 8.2511, 8.1458],
        [7.8578, 7.8984, 7.8654]], grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.07212373614311218 
model_pd.l_d.mean(): -5.082578659057617 
model_pd.lagr.mean(): -5.010455131530762 
model_pd.lambdas: dict_items([('pout', tensor([1.2537])), ('power', tensor([0.2220]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4770])), ('power', tensor([-20.1057]))])
epoch：668	 i:0 	 global-step:13360	 l-p:0.07212373614311218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[ 7.8858,  8.4109,  8.3365],
        [ 7.8858,  7.8862,  7.8858],
        [ 7.8858,  8.2007,  8.0880],
        [ 7.8858, 10.2108, 11.7521]], grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.0720316469669342 
model_pd.l_d.mean(): -5.062129974365234 
model_pd.lagr.mean(): -4.990098476409912 
model_pd.lambdas: dict_items([('pout', tensor([1.2532])), ('power', tensor([0.2210]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4818])), ('power', tensor([-20.0791]))])
epoch：669	 i:0 	 global-step:13380	 l-p:0.0720316469669342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[ 7.9140,  7.9217,  7.9146],
        [ 7.9140,  9.6872, 10.5863],
        [ 7.9140,  7.9240,  7.9148],
        [ 7.9140,  7.9550,  7.9217]], grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.07193972170352936 
model_pd.l_d.mean(): -5.0417327880859375 
model_pd.lagr.mean(): -4.96979284286499 
model_pd.lambdas: dict_items([('pout', tensor([1.2527])), ('power', tensor([0.2200]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4865])), ('power', tensor([-20.0523]))])
epoch：670	 i:0 	 global-step:13400	 l-p:0.07193972170352936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[ 7.9424,  7.9501,  7.9429],
        [ 7.9424,  8.1455,  8.0423],
        [ 7.9424,  8.0113,  7.9600],
        [ 7.9424,  9.7238, 10.6273]], grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.07184796035289764 
model_pd.l_d.mean(): -5.021384239196777 
model_pd.lagr.mean(): -4.949536323547363 
model_pd.lambdas: dict_items([('pout', tensor([1.2522])), ('power', tensor([0.2190]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4912])), ('power', tensor([-20.0254]))])
epoch：671	 i:0 	 global-step:13420	 l-p:0.07184796035289764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[ 7.9710,  9.7997, 10.7499],
        [ 7.9710,  8.0124,  7.9787],
        [ 7.9710,  8.2147,  8.1044],
        [ 7.9710,  9.1839,  9.5342]], grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.07175637036561966 
model_pd.l_d.mean(): -5.001086235046387 
model_pd.lagr.mean(): -4.929329872131348 
model_pd.lambdas: dict_items([('pout', tensor([1.2517])), ('power', tensor([0.2180]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4959])), ('power', tensor([-19.9982]))])
epoch：672	 i:0 	 global-step:13440	 l-p:0.07175637036561966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[ 7.9997,  8.0720,  8.0186],
        [ 7.9997,  8.6651,  8.6417],
        [ 7.9997,  8.5975,  8.5448],
        [ 7.9997,  9.8369, 10.7919]], grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.07166494429111481 
model_pd.l_d.mean(): -4.980839252471924 
model_pd.lagr.mean(): -4.90917444229126 
model_pd.lambdas: dict_items([('pout', tensor([1.2512])), ('power', tensor([0.2170]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5007])), ('power', tensor([-19.9709]))])
epoch：673	 i:0 	 global-step:13460	 l-p:0.07166494429111481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[8.0287, 8.0287, 8.0287],
        [8.0287, 8.0365, 8.0292],
        [8.0287, 8.0287, 8.0287],
        [8.0287, 8.0706, 8.0365]], grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.0715736672282219 
model_pd.l_d.mean(): -4.9606428146362305 
model_pd.lagr.mean(): -4.889069080352783 
model_pd.lambdas: dict_items([('pout', tensor([1.2507])), ('power', tensor([0.2160]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5055])), ('power', tensor([-19.9433]))])
epoch：674	 i:0 	 global-step:13480	 l-p:0.0715736672282219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[ 8.0578,  9.0805,  9.2719],
        [ 8.0578,  9.8726, 10.7945],
        [ 8.0578,  8.1305,  8.0768],
        [ 8.0578,  9.4045,  9.8624]], grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.07148256152868271 
model_pd.l_d.mean(): -4.940497398376465 
model_pd.lagr.mean(): -4.869014739990234 
model_pd.lambdas: dict_items([('pout', tensor([1.2502])), ('power', tensor([0.2150]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5103])), ('power', tensor([-19.9156]))])
epoch：675	 i:0 	 global-step:13500	 l-p:0.07148256152868271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[8.0872, 8.2277, 8.1416],
        [8.0872, 8.7628, 8.7397],
        [8.0872, 8.0875, 8.0872],
        [8.0872, 9.1788, 9.4195]], grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.07139160484075546 
model_pd.l_d.mean(): -4.920403480529785 
model_pd.lagr.mean(): -4.8490118980407715 
model_pd.lambdas: dict_items([('pout', tensor([1.2497])), ('power', tensor([0.2140]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5151])), ('power', tensor([-19.8876]))])
epoch：676	 i:0 	 global-step:13520	 l-p:0.07139160484075546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[ 8.1167,  8.2580,  8.1715],
        [ 8.1167,  8.7959,  8.7728],
        [ 8.1167, 10.5274, 12.1287],
        [ 8.1167,  8.1546,  8.1234]], grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.07130080461502075 
model_pd.l_d.mean(): -4.90036153793335 
model_pd.lagr.mean(): -4.8290605545043945 
model_pd.lambdas: dict_items([('pout', tensor([1.2492])), ('power', tensor([0.2130]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5199])), ('power', tensor([-19.8594]))])
epoch：677	 i:0 	 global-step:13540	 l-p:0.07130080461502075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[ 8.1465,  8.3984,  8.2847],
        [ 8.1465,  8.4770,  8.3595],
        [ 8.1465, 10.3828, 11.7670],
        [ 8.1465, 10.0270, 11.0063]], grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): 0.07121015340089798 
model_pd.l_d.mean(): -4.880371570587158 
model_pd.lagr.mean(): -4.809161186218262 
model_pd.lambdas: dict_items([('pout', tensor([1.2486])), ('power', tensor([0.2121]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5247])), ('power', tensor([-19.8311]))])
epoch：678	 i:0 	 global-step:13560	 l-p:0.07121015340089798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[ 8.1765, 10.1603, 11.2484],
        [ 8.1765,  8.1765,  8.1765],
        [ 8.1765,  8.3755,  8.2710],
        [ 8.1765,  9.5495, 10.0174]], grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.07111965864896774 
model_pd.l_d.mean(): -4.8604326248168945 
model_pd.lagr.mean(): -4.789312839508057 
model_pd.lambdas: dict_items([('pout', tensor([1.2481])), ('power', tensor([0.2111]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5296])), ('power', tensor([-19.8025]))])
epoch：679	 i:0 	 global-step:13580	 l-p:0.07111965864896774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[ 8.2067,  9.5987, 10.0804],
        [ 8.2067,  8.3504,  8.2625],
        [ 8.2067,  8.2067,  8.2067],
        [ 8.2067,  8.8735,  8.8400]], grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.07102931290864944 
model_pd.l_d.mean(): -4.840547561645508 
model_pd.lagr.mean(): -4.7695183753967285 
model_pd.lambdas: dict_items([('pout', tensor([1.2476])), ('power', tensor([0.2101]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5345])), ('power', tensor([-19.7737]))])
epoch：680	 i:0 	 global-step:13600	 l-p:0.07102931290864944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[8.2371, 8.7959, 8.7188],
        [8.2371, 8.2376, 8.2371],
        [8.2371, 8.9073, 8.8739],
        [8.2371, 8.2808, 8.2453]], grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.07093912363052368 
model_pd.l_d.mean(): -4.820715427398682 
model_pd.lagr.mean(): -4.749776363372803 
model_pd.lambdas: dict_items([('pout', tensor([1.2470])), ('power', tensor([0.2091]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5393])), ('power', tensor([-19.7447]))])
epoch：681	 i:0 	 global-step:13620	 l-p:0.07093912363052368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[ 8.2677, 10.2796, 11.3843],
        [ 8.2677,  8.2677,  8.2677],
        [ 8.2677, 10.5458, 11.9575],
        [ 8.2677,  8.2736,  8.2681]], grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.07084908336400986 
model_pd.l_d.mean(): -4.800935745239258 
model_pd.lagr.mean(): -4.730086803436279 
model_pd.lambdas: dict_items([('pout', tensor([1.2465])), ('power', tensor([0.2081]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5442])), ('power', tensor([-19.7155]))])
epoch：682	 i:0 	 global-step:13640	 l-p:0.07084908336400986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[ 8.2986,  8.9758,  8.9424],
        [ 8.2986,  8.2990,  8.2986],
        [ 8.2986,  8.3068,  8.2991],
        [ 8.2986, 10.3200, 11.4302]], grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.07075918465852737 
model_pd.l_d.mean(): -4.7812089920043945 
model_pd.lagr.mean(): -4.710449695587158 
model_pd.lambdas: dict_items([('pout', tensor([1.2459])), ('power', tensor([0.2071]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5492])), ('power', tensor([-19.6861]))])
epoch：683	 i:0 	 global-step:13660	 l-p:0.07075918465852737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[8.3296, 8.3304, 8.3296],
        [8.3296, 8.3379, 8.3302],
        [8.3296, 9.3991, 9.6016],
        [8.3296, 8.3300, 8.3296]], grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.07066944241523743 
model_pd.l_d.mean(): -4.761537551879883 
model_pd.lagr.mean(): -4.690867900848389 
model_pd.lambdas: dict_items([('pout', tensor([1.2454])), ('power', tensor([0.2061]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5541])), ('power', tensor([-19.6564]))])
epoch：684	 i:0 	 global-step:13680	 l-p:0.07066944241523743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[8.3609, 8.3612, 8.3609],
        [8.3609, 9.0934, 9.0828],
        [8.3609, 9.3568, 9.5041],
        [8.3609, 8.3610, 8.3609]], grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.07057984173297882 
model_pd.l_d.mean(): -4.741918087005615 
model_pd.lagr.mean(): -4.671338081359863 
model_pd.lambdas: dict_items([('pout', tensor([1.2448])), ('power', tensor([0.2052]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5591])), ('power', tensor([-19.6265]))])
epoch：685	 i:0 	 global-step:13700	 l-p:0.07057984173297882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[ 8.3924,  9.0320,  8.9779],
        [ 8.3924,  8.4375,  8.4009],
        [ 8.3924,  8.4324,  8.3994],
        [ 8.3924, 10.3454, 11.3653]], grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.07049039751291275 
model_pd.l_d.mean(): -4.722353935241699 
model_pd.lagr.mean(): -4.6518635749816895 
model_pd.lambdas: dict_items([('pout', tensor([1.2443])), ('power', tensor([0.2042]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5640])), ('power', tensor([-19.5964]))])
epoch：686	 i:0 	 global-step:13720	 l-p:0.07049039751291275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[ 8.4242, 10.7561, 12.2033],
        [ 8.4242,  8.4243,  8.4242],
        [ 8.4242,  9.0671,  9.0129],
        [ 8.4242,  8.4249,  8.4242]], grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.07040110230445862 
model_pd.l_d.mean(): -4.702843189239502 
model_pd.lagr.mean(): -4.632441997528076 
model_pd.lambdas: dict_items([('pout', tensor([1.2437])), ('power', tensor([0.2032]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5690])), ('power', tensor([-19.5661]))])
epoch：687	 i:0 	 global-step:13740	 l-p:0.07040110230445862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[8.4561, 8.5350, 8.4769],
        [8.4561, 8.5321, 8.4757],
        [8.4561, 9.4673, 9.6176],
        [8.4561, 8.4562, 8.4561]], grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.07031195610761642 
model_pd.l_d.mean(): -4.6833882331848145 
model_pd.lagr.mean(): -4.613076210021973 
model_pd.lambdas: dict_items([('pout', tensor([1.2431])), ('power', tensor([0.2022]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5740])), ('power', tensor([-19.5355]))])
epoch：688	 i:0 	 global-step:13760	 l-p:0.07031195610761642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[8.4883, 8.5665, 8.5087],
        [8.4883, 8.4905, 8.4884],
        [8.4883, 9.5851, 9.7941],
        [8.4883, 8.4884, 8.4883]], grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.07022296637296677 
model_pd.l_d.mean(): -4.6639885902404785 
model_pd.lagr.mean(): -4.593765735626221 
model_pd.lambdas: dict_items([('pout', tensor([1.2425])), ('power', tensor([0.2012]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5790])), ('power', tensor([-19.5047]))])
epoch：689	 i:0 	 global-step:13780	 l-p:0.07022296637296677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[ 8.5208,  8.5208,  8.5208],
        [ 8.5208,  8.6005,  8.5417],
        [ 8.5208,  9.2726,  9.2627],
        [ 8.5208,  9.9826, 10.4916]], grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.07013411819934845 
model_pd.l_d.mean(): -4.644643306732178 
model_pd.lagr.mean(): -4.574509143829346 
model_pd.lambdas: dict_items([('pout', tensor([1.2420])), ('power', tensor([0.2003]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5841])), ('power', tensor([-19.4737]))])
epoch：690	 i:0 	 global-step:13800	 l-p:0.07013411819934845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[8.5534, 8.5534, 8.5534],
        [8.5534, 8.5534, 8.5534],
        [8.5534, 9.3092, 9.2995],
        [8.5534, 8.5538, 8.5534]], grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.07004542648792267 
model_pd.l_d.mean(): -4.625354290008545 
model_pd.lagr.mean(): -4.555308818817139 
model_pd.lambdas: dict_items([('pout', tensor([1.2414])), ('power', tensor([0.1993]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5891])), ('power', tensor([-19.4424]))])
epoch：691	 i:0 	 global-step:13820	 l-p:0.07004542648792267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[8.5863, 9.0334, 8.9166],
        [8.5863, 8.5951, 8.5870],
        [8.5863, 8.7402, 8.6464],
        [8.5863, 9.0568, 8.9441]], grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.06995689123868942 
model_pd.l_d.mean(): -4.60612154006958 
model_pd.lagr.mean(): -4.5361647605896 
model_pd.lambdas: dict_items([('pout', tensor([1.2408])), ('power', tensor([0.1983]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5942])), ('power', tensor([-19.4109]))])
epoch：692	 i:0 	 global-step:13840	 l-p:0.06995689123868942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[ 8.6195,  8.7002,  8.6407],
        [ 8.6195,  8.6195,  8.6195],
        [ 8.6195,  9.0486,  8.9282],
        [ 8.6195, 10.1033, 10.6208]], grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.06986851245164871 
model_pd.l_d.mean(): -4.586944103240967 
model_pd.lagr.mean(): -4.517075538635254 
model_pd.lambdas: dict_items([('pout', tensor([1.2402])), ('power', tensor([0.1974]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5993])), ('power', tensor([-19.3792]))])
epoch：693	 i:0 	 global-step:13860	 l-p:0.06986851245164871
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[8.6529, 8.6529, 8.6529],
        [8.6529, 8.6529, 8.6529],
        [8.6529, 9.3201, 9.2650],
        [8.6529, 8.6617, 8.6535]], grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.06978029012680054 
model_pd.l_d.mean(): -4.567823886871338 
model_pd.lagr.mean(): -4.498043537139893 
model_pd.lambdas: dict_items([('pout', tensor([1.2396])), ('power', tensor([0.1964]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6044])), ('power', tensor([-19.3472]))])
epoch：694	 i:0 	 global-step:13880	 l-p:0.06978029012680054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[ 8.6865,  8.6869,  8.6865],
        [ 8.6865, 10.1834, 10.7051],
        [ 8.6865, 10.8273, 12.0076],
        [ 8.6865,  8.7110,  8.6896]], grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.0696922242641449 
model_pd.l_d.mean(): -4.548760890960693 
model_pd.lagr.mean(): -4.479068756103516 
model_pd.lambdas: dict_items([('pout', tensor([1.2390])), ('power', tensor([0.1954]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6095])), ('power', tensor([-19.3149]))])
epoch：695	 i:0 	 global-step:13900	 l-p:0.0696922242641449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[ 8.7204, 10.7699, 11.8438],
        [ 8.7204,  8.8001,  8.7409],
        [ 8.7204,  9.4702,  9.4485],
        [ 8.7204, 10.7266, 11.7534]], grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.0696043148636818 
model_pd.l_d.mean(): -4.529754161834717 
model_pd.lagr.mean(): -4.460149765014648 
model_pd.lambdas: dict_items([('pout', tensor([1.2383])), ('power', tensor([0.1945]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6146])), ('power', tensor([-19.2825]))])
epoch：696	 i:0 	 global-step:13920	 l-p:0.0696043148636818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[ 8.7545,  8.9748,  8.8598],
        [ 8.7545,  8.9443,  8.8374],
        [ 8.7545,  8.8372,  8.7762],
        [ 8.7545, 10.2665, 10.7940]], grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.06951656192541122 
model_pd.l_d.mean(): -4.510805606842041 
model_pd.lagr.mean(): -4.441288948059082 
model_pd.lambdas: dict_items([('pout', tensor([1.2377])), ('power', tensor([0.1935]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6198])), ('power', tensor([-19.2498]))])
epoch：697	 i:0 	 global-step:13940	 l-p:0.06951656192541122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[ 8.7889,  8.8007,  8.7899],
        [ 8.7889,  9.1577,  9.0284],
        [ 8.7889,  9.8533, 10.0140],
        [ 8.7889,  8.7890,  8.7889]], grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.06942898035049438 
model_pd.l_d.mean(): -4.491914749145508 
model_pd.lagr.mean(): -4.422485828399658 
model_pd.lambdas: dict_items([('pout', tensor([1.2371])), ('power', tensor([0.1925]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6250])), ('power', tensor([-19.2168]))])
epoch：698	 i:0 	 global-step:13960	 l-p:0.06942898035049438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[ 8.8235, 10.9034, 11.9942],
        [ 8.8235,  9.2669,  9.1432],
        [ 8.8235,  8.8235,  8.8235],
        [ 8.8235, 11.4959, 13.2809]], grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.06934155523777008 
model_pd.l_d.mean(): -4.473082065582275 
model_pd.lagr.mean(): -4.403740406036377 
model_pd.lambdas: dict_items([('pout', tensor([1.2365])), ('power', tensor([0.1916]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6301])), ('power', tensor([-19.1836]))])
epoch：699	 i:0 	 global-step:13980	 l-p:0.06934155523777008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[8.8584, 9.0972, 8.9773],
        [8.8584, 8.8836, 8.8616],
        [8.8584, 8.9073, 8.8677],
        [8.8584, 8.8584, 8.8584]], grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.06925429403781891 
model_pd.l_d.mean(): -4.454307556152344 
model_pd.lagr.mean(): -4.3850531578063965 
model_pd.lambdas: dict_items([('pout', tensor([1.2358])), ('power', tensor([0.1906]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6354])), ('power', tensor([-19.1501]))])
epoch：700	 i:0 	 global-step:14000	 l-p:0.06925429403781891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[ 8.8935, 10.4381, 10.9793],
        [ 8.8935,  8.9786,  8.9160],
        [ 8.8935,  8.8936,  8.8935],
        [ 8.8935,  8.9055,  8.8945]], grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.06916719675064087 
model_pd.l_d.mean(): -4.435591220855713 
model_pd.lagr.mean(): -4.366424083709717 
model_pd.lambdas: dict_items([('pout', tensor([1.2352])), ('power', tensor([0.1897]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6406])), ('power', tensor([-19.1164]))])
epoch：701	 i:0 	 global-step:14020	 l-p:0.06916719675064087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[8.9289, 9.2543, 9.1228],
        [8.9289, 9.3797, 9.2542],
        [8.9289, 8.9382, 8.9296],
        [8.9289, 8.9289, 8.9289]], grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.06908025592565536 
model_pd.l_d.mean(): -4.416934490203857 
model_pd.lagr.mean(): -4.347854137420654 
model_pd.lambdas: dict_items([('pout', tensor([1.2345])), ('power', tensor([0.1887]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6458])), ('power', tensor([-19.0824]))])
epoch：702	 i:0 	 global-step:14040	 l-p:0.06908025592565536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[ 8.9646,  9.3438,  9.2112],
        [ 8.9646,  8.9646,  8.9646],
        [ 8.9646, 10.2153, 10.4989],
        [ 8.9646,  9.4394,  9.3166]], grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.06899348646402359 
model_pd.l_d.mean(): -4.398336410522461 
model_pd.lagr.mean(): -4.329342842102051 
model_pd.lambdas: dict_items([('pout', tensor([1.2339])), ('power', tensor([0.1878]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6511])), ('power', tensor([-19.0481]))])
epoch：703	 i:0 	 global-step:14060	 l-p:0.06899348646402359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[ 9.0005,  9.0871,  9.0234],
        [ 9.0005, 10.5550, 11.0929],
        [ 9.0005,  9.0866,  9.0232],
        [ 9.0005,  9.0072,  9.0009]], grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.06890687346458435 
model_pd.l_d.mean(): -4.379798412322998 
model_pd.lagr.mean(): -4.310891628265381 
model_pd.lambdas: dict_items([('pout', tensor([1.2332])), ('power', tensor([0.1868]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6563])), ('power', tensor([-19.0136]))])
epoch：704	 i:0 	 global-step:14080	 l-p:0.06890687346458435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[ 9.0366, 10.2274, 10.4586],
        [ 9.0366, 11.7878, 13.6281],
        [ 9.0366,  9.0366,  9.0366],
        [ 9.0366,  9.0375,  9.0367]], grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 0.06882043927907944 
model_pd.l_d.mean(): -4.361320495605469 
model_pd.lagr.mean(): -4.292500019073486 
model_pd.lambdas: dict_items([('pout', tensor([1.2326])), ('power', tensor([0.1859]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6616])), ('power', tensor([-18.9788]))])
epoch：705	 i:0 	 global-step:14100	 l-p:0.06882043927907944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[9.0731, 9.1608, 9.0963],
        [9.0731, 9.0736, 9.0731],
        [9.0731, 9.1603, 9.0961],
        [9.0731, 9.0735, 9.0731]], grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.06873416155576706 
model_pd.l_d.mean(): -4.342902660369873 
model_pd.lagr.mean(): -4.274168491363525 
model_pd.lambdas: dict_items([('pout', tensor([1.2319])), ('power', tensor([0.1849]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6669])), ('power', tensor([-18.9438]))])
epoch：706	 i:0 	 global-step:14120	 l-p:0.06873416155576706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[ 9.1098,  9.5954,  9.4702],
        [ 9.1098,  9.9051,  9.8842],
        [ 9.1098,  9.8789,  9.8457],
        [ 9.1098, 11.8880, 13.7472]], grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.06864805519580841 
model_pd.l_d.mean(): -4.324545383453369 
model_pd.lagr.mean(): -4.255897521972656 
model_pd.lambdas: dict_items([('pout', tensor([1.2312])), ('power', tensor([0.1840]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6723])), ('power', tensor([-18.9084]))])
epoch：707	 i:0 	 global-step:14140	 l-p:0.06864805519580841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[ 9.1468,  9.4834,  9.3478],
        [ 9.1468,  9.1492,  9.1469],
        [ 9.1468, 10.7456, 11.3069],
        [ 9.1468,  9.2355,  9.1703]], grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.0685621052980423 
model_pd.l_d.mean(): -4.306249618530273 
model_pd.lagr.mean(): -4.237687587738037 
model_pd.lambdas: dict_items([('pout', tensor([1.2306])), ('power', tensor([0.1830]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6776])), ('power', tensor([-18.8729]))])
epoch：708	 i:0 	 global-step:14160	 l-p:0.0685621052980423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[ 9.1841,  9.7005,  9.5789],
        [ 9.1841, 10.7930, 11.3593],
        [ 9.1841,  9.1841,  9.1841],
        [ 9.1841,  9.1845,  9.1841]], grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.06847634166479111 
model_pd.l_d.mean(): -4.2880144119262695 
model_pd.lagr.mean(): -4.21953821182251 
model_pd.lambdas: dict_items([('pout', tensor([1.2299])), ('power', tensor([0.1821]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6830])), ('power', tensor([-18.8370]))])
epoch：709	 i:0 	 global-step:14180	 l-p:0.06847634166479111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[ 9.2216, 10.6885, 11.1242],
        [ 9.2216,  9.2222,  9.2216],
        [ 9.2216, 10.3551, 10.5293],
        [ 9.2216, 10.4440, 10.6827]], grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.06839073449373245 
model_pd.l_d.mean(): -4.269840240478516 
model_pd.lagr.mean(): -4.201449394226074 
model_pd.lambdas: dict_items([('pout', tensor([1.2292])), ('power', tensor([0.1811]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6884])), ('power', tensor([-18.8008]))])
epoch：710	 i:0 	 global-step:14200	 l-p:0.06839073449373245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[ 9.2594,  9.3484,  9.2828],
        [ 9.2594,  9.7817,  9.6590],
        [ 9.2594,  9.4652,  9.3497],
        [ 9.2594, 11.4673, 12.6296]], grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.06830529868602753 
model_pd.l_d.mean(): -4.251728534698486 
model_pd.lagr.mean(): -4.183423042297363 
model_pd.lambdas: dict_items([('pout', tensor([1.2285])), ('power', tensor([0.1802]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6938])), ('power', tensor([-18.7644]))])
epoch：711	 i:0 	 global-step:14220	 l-p:0.06830529868602753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[ 9.2975,  9.3104,  9.2986],
        [ 9.2975,  9.2976,  9.2975],
        [ 9.2975, 10.0328,  9.9754],
        [ 9.2975,  9.2975,  9.2975]], grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 0.06822004169225693 
model_pd.l_d.mean(): -4.233678817749023 
model_pd.lagr.mean(): -4.165458679199219 
model_pd.lambdas: dict_items([('pout', tensor([1.2278])), ('power', tensor([0.1793]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6992])), ('power', tensor([-18.7277]))])
epoch：712	 i:0 	 global-step:14240	 l-p:0.06822004169225693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[ 9.3359, 10.9785, 11.5578],
        [ 9.3359,  9.9999,  9.9136],
        [ 9.3359,  9.3458,  9.3366],
        [ 9.3359,  9.3363,  9.3359]], grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.06813494116067886 
model_pd.l_d.mean(): -4.215691089630127 
model_pd.lagr.mean(): -4.147556304931641 
model_pd.lambdas: dict_items([('pout', tensor([1.2271])), ('power', tensor([0.1783]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7046])), ('power', tensor([-18.6907]))])
epoch：713	 i:0 	 global-step:14260	 l-p:0.06813494116067886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[ 9.3746, 10.6994, 11.0029],
        [ 9.3746,  9.3746,  9.3746],
        [ 9.3746,  9.8795,  9.7502],
        [ 9.3746,  9.5494,  9.4434]], grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.06805001944303513 
model_pd.l_d.mean(): -4.1977667808532715 
model_pd.lagr.mean(): -4.129716873168945 
model_pd.lambdas: dict_items([('pout', tensor([1.2264])), ('power', tensor([0.1774]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7100])), ('power', tensor([-18.6534]))])
epoch：714	 i:0 	 global-step:14280	 l-p:0.06805001944303513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[ 9.4136,  9.4267,  9.4146],
        [ 9.4136, 12.0858, 13.7560],
        [ 9.4136, 10.6687, 10.9152],
        [ 9.4136,  9.4611,  9.4220]], grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.06796527653932571 
model_pd.l_d.mean(): -4.179905414581299 
model_pd.lagr.mean(): -4.111939907073975 
model_pd.lambdas: dict_items([('pout', tensor([1.2257])), ('power', tensor([0.1765]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7155])), ('power', tensor([-18.6159]))])
epoch：715	 i:0 	 global-step:14300	 l-p:0.06796527653932571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[ 9.4528,  9.5425,  9.4761],
        [ 9.4528,  9.9899,  9.8643],
        [ 9.4528, 11.1212, 11.7106],
        [ 9.4528, 10.9664, 11.4178]], grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.06788069009780884 
model_pd.l_d.mean(): -4.162107467651367 
model_pd.lagr.mean(): -4.094226837158203 
model_pd.lambdas: dict_items([('pout', tensor([1.2249])), ('power', tensor([0.1755]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7210])), ('power', tensor([-18.5780]))])
epoch：716	 i:0 	 global-step:14320	 l-p:0.06788069009780884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[9.4924, 9.4924, 9.4924],
        [9.4924, 9.9028, 9.7606],
        [9.4924, 9.4924, 9.4924],
        [9.4924, 9.5860, 9.5172]], grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.06779627501964569 
model_pd.l_d.mean(): -4.144372940063477 
model_pd.lagr.mean(): -4.0765767097473145 
model_pd.lambdas: dict_items([('pout', tensor([1.2242])), ('power', tensor([0.1746]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7265])), ('power', tensor([-18.5398]))])
epoch：717	 i:0 	 global-step:14340	 l-p:0.06779627501964569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[ 9.5322, 12.4664, 14.4348],
        [ 9.5322, 12.2452, 13.9421],
        [ 9.5322, 11.9326, 13.2649],
        [ 9.5322, 10.2922, 10.2340]], grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.06771206110715866 
model_pd.l_d.mean(): -4.12670373916626 
model_pd.lagr.mean(): -4.0589919090271 
model_pd.lambdas: dict_items([('pout', tensor([1.2235])), ('power', tensor([0.1737]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7320])), ('power', tensor([-18.5014]))])
epoch：718	 i:0 	 global-step:14360	 l-p:0.06771206110715866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[9.5724, 9.8389, 9.7059],
        [9.5724, 9.6658, 9.5970],
        [9.5724, 9.6666, 9.5973],
        [9.5724, 9.6007, 9.5760]], grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.06762800365686417 
model_pd.l_d.mean(): -4.109097480773926 
model_pd.lagr.mean(): -4.041469573974609 
model_pd.lambdas: dict_items([('pout', tensor([1.2228])), ('power', tensor([0.1728]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7376])), ('power', tensor([-18.4626]))])
epoch：719	 i:0 	 global-step:14380	 l-p:0.06762800365686417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[ 9.6128, 10.5577, 10.5855],
        [ 9.6128, 10.4665, 10.4468],
        [ 9.6128,  9.6129,  9.6128],
        [ 9.6128,  9.6618,  9.6215]], grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.0675441175699234 
model_pd.l_d.mean(): -4.091557025909424 
model_pd.lagr.mean(): -4.024013042449951 
model_pd.lambdas: dict_items([('pout', tensor([1.2220])), ('power', tensor([0.1718]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7431])), ('power', tensor([-18.4235]))])
epoch：720	 i:0 	 global-step:14400	 l-p:0.0675441175699234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[ 9.6536, 11.9284, 13.1020],
        [ 9.6536, 10.5120, 10.4923],
        [ 9.6536, 10.5416, 10.5366],
        [ 9.6536,  9.6545,  9.6536]], grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.06746040284633636 
model_pd.l_d.mean(): -4.074080467224121 
model_pd.lagr.mean(): -4.006619930267334 
model_pd.lambdas: dict_items([('pout', tensor([1.2213])), ('power', tensor([0.1709]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7487])), ('power', tensor([-18.3842]))])
epoch：721	 i:0 	 global-step:14420	 l-p:0.06746040284633636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[ 9.6947,  9.6947,  9.6947],
        [ 9.6947, 10.0177,  9.8748],
        [ 9.6947,  9.6951,  9.6947],
        [ 9.6947,  9.6950,  9.6947]], grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.06737688183784485 
model_pd.l_d.mean(): -4.056670188903809 
model_pd.lagr.mean(): -3.989293336868286 
model_pd.lambdas: dict_items([('pout', tensor([1.2205])), ('power', tensor([0.1700]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7543])), ('power', tensor([-18.3445]))])
epoch：722	 i:0 	 global-step:14440	 l-p:0.06737688183784485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[ 9.7360,  9.7365,  9.7360],
        [ 9.7360, 12.0837, 13.3240],
        [ 9.7360,  9.7387,  9.7361],
        [ 9.7360,  9.9204,  9.8088]], grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.06729353219270706 
model_pd.l_d.mean(): -4.039324760437012 
model_pd.lagr.mean(): -3.9720311164855957 
model_pd.lambdas: dict_items([('pout', tensor([1.2197])), ('power', tensor([0.1691]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7599])), ('power', tensor([-18.3045]))])
epoch：723	 i:0 	 global-step:14460	 l-p:0.06729353219270706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[ 9.7777, 10.1468,  9.9993],
        [ 9.7777,  9.8719,  9.8022],
        [ 9.7777, 10.3121, 10.1764],
        [ 9.7777, 10.6221, 10.5890]], grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.0672103613615036 
model_pd.l_d.mean(): -4.022046089172363 
model_pd.lagr.mean(): -3.9548356533050537 
model_pd.lambdas: dict_items([('pout', tensor([1.2190])), ('power', tensor([0.1682]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7655])), ('power', tensor([-18.2642]))])
epoch：724	 i:0 	 global-step:14480	 l-p:0.0672103613615036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[ 9.8197, 10.0788,  9.9448],
        [ 9.8197, 10.0064,  9.8934],
        [ 9.8197,  9.8198,  9.8197],
        [ 9.8197, 10.1910, 10.0426]], grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.06712737679481506 
model_pd.l_d.mean(): -4.004834175109863 
model_pd.lagr.mean(): -3.937706708908081 
model_pd.lambdas: dict_items([('pout', tensor([1.2182])), ('power', tensor([0.1673]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7712])), ('power', tensor([-18.2235]))])
epoch：725	 i:0 	 global-step:14500	 l-p:0.06712737679481506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[ 9.8621,  9.8621,  9.8621],
        [ 9.8621, 11.4584, 11.9374],
        [ 9.8621, 10.0868,  9.9612],
        [ 9.8621, 12.2466, 13.5075]], grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.06704457104206085 
model_pd.l_d.mean(): -3.9876883029937744 
model_pd.lagr.mean(): -3.9206438064575195 
model_pd.lambdas: dict_items([('pout', tensor([1.2174])), ('power', tensor([0.1664]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7769])), ('power', tensor([-18.1826]))])
epoch：726	 i:0 	 global-step:14520	 l-p:0.06704457104206085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[ 9.9047,  9.9047,  9.9047],
        [ 9.9047, 11.5096, 11.9915],
        [ 9.9047,  9.9047,  9.9047],
        [ 9.9047, 10.4484, 10.3107]], grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.06696194410324097 
model_pd.l_d.mean(): -3.970609664916992 
model_pd.lagr.mean(): -3.9036476612091064 
model_pd.lambdas: dict_items([('pout', tensor([1.2167])), ('power', tensor([0.1654]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7825])), ('power', tensor([-18.1413]))])
epoch：727	 i:0 	 global-step:14540	 l-p:0.06696194410324097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[ 9.9477,  9.9477,  9.9477],
        [ 9.9477, 10.0442,  9.9729],
        [ 9.9477, 11.1967, 11.3934],
        [ 9.9477, 11.5613, 12.0461]], grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.066879503428936 
model_pd.l_d.mean(): -3.9535980224609375 
model_pd.lagr.mean(): -3.886718511581421 
model_pd.lambdas: dict_items([('pout', tensor([1.2159])), ('power', tensor([0.1645]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7882])), ('power', tensor([-18.0997]))])
epoch：728	 i:0 	 global-step:14560	 l-p:0.066879503428936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[ 9.9910,  9.9917,  9.9910],
        [ 9.9910, 10.7172, 10.6255],
        [ 9.9910, 12.8614, 14.6613],
        [ 9.9910,  9.9915,  9.9910]], grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.06679724901914597 
model_pd.l_d.mean(): -3.9366543292999268 
model_pd.lagr.mean(): -3.8698570728302 
model_pd.lambdas: dict_items([('pout', tensor([1.2151])), ('power', tensor([0.1636]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7940])), ('power', tensor([-18.0578]))])
epoch：729	 i:0 	 global-step:14580	 l-p:0.06679724901914597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[10.0347, 10.9683, 10.9649],
        [10.0347, 11.4784, 11.8137],
        [10.0347, 10.0868, 10.0439],
        [10.0347, 10.0650, 10.0386]], grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.06671518087387085 
model_pd.l_d.mean(): -3.919778823852539 
model_pd.lagr.mean(): -3.8530635833740234 
model_pd.lambdas: dict_items([('pout', tensor([1.2143])), ('power', tensor([0.1627]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7997])), ('power', tensor([-18.0155]))])
epoch：730	 i:0 	 global-step:14600	 l-p:0.06671518087387085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.0786, 11.5303, 11.8677],
        [10.0786, 11.8832, 12.5241],
        [10.0786, 10.6349, 10.4945],
        [10.0786, 10.0791, 10.0786]], grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.06663329899311066 
model_pd.l_d.mean(): -3.9029717445373535 
model_pd.lagr.mean(): -3.836338520050049 
model_pd.lambdas: dict_items([('pout', tensor([1.2135])), ('power', tensor([0.1618]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8055])), ('power', tensor([-17.9729]))])
epoch：731	 i:0 	 global-step:14620	 l-p:0.06663329899311066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[10.1229, 11.9394, 12.5861],
        [10.1229, 11.4991, 11.7739],
        [10.1229, 10.1257, 10.1230],
        [10.1229, 10.3559, 10.2259]], grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.06655159592628479 
model_pd.l_d.mean(): -3.886233329772949 
model_pd.lagr.mean(): -3.8196816444396973 
model_pd.lambdas: dict_items([('pout', tensor([1.2127])), ('power', tensor([0.1609]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8112])), ('power', tensor([-17.9300]))])
epoch：732	 i:0 	 global-step:14640	 l-p:0.06655159592628479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[10.1676, 10.1681, 10.1676],
        [10.1676, 10.7304, 10.5885],
        [10.1676, 10.1680, 10.1676],
        [10.1676, 10.1704, 10.1677]], grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.06647009402513504 
model_pd.l_d.mean(): -3.869563341140747 
model_pd.lagr.mean(): -3.803093194961548 
model_pd.lambdas: dict_items([('pout', tensor([1.2118])), ('power', tensor([0.1601]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8170])), ('power', tensor([-17.8867]))])
epoch：733	 i:0 	 global-step:14660	 l-p:0.06647009402513504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[10.2126, 10.2126, 10.2126],
        [10.2126, 11.5036, 11.7085],
        [10.2126, 11.8795, 12.3821],
        [10.2126, 11.6883, 12.0321]], grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.06638878583908081 
model_pd.l_d.mean(): -3.8529632091522217 
model_pd.lagr.mean(): -3.786574363708496 
model_pd.lambdas: dict_items([('pout', tensor([1.2110])), ('power', tensor([0.1592]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8228])), ('power', tensor([-17.8431]))])
epoch：734	 i:0 	 global-step:14680	 l-p:0.06638878583908081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[10.2579, 12.1019, 12.7582],
        [10.2579, 10.5508, 10.4054],
        [10.2579, 10.2589, 10.2579],
        [10.2579, 11.7418, 12.0878]], grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.06630765646696091 
model_pd.l_d.mean(): -3.836432933807373 
model_pd.lagr.mean(): -3.770125389099121 
model_pd.lambdas: dict_items([('pout', tensor([1.2102])), ('power', tensor([0.1583]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8287])), ('power', tensor([-17.7991]))])
epoch：735	 i:0 	 global-step:14700	 l-p:0.06630765646696091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[10.3036, 13.2811, 15.1511],
        [10.3036, 10.5029, 10.3826],
        [10.3036, 10.3350, 10.3076],
        [10.3036, 10.3185, 10.3048]], grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.06622672826051712 
model_pd.l_d.mean(): -3.819972038269043 
model_pd.lagr.mean(): -3.7537453174591064 
model_pd.lambdas: dict_items([('pout', tensor([1.2093])), ('power', tensor([0.1574]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8345])), ('power', tensor([-17.7548]))])
epoch：736	 i:0 	 global-step:14720	 l-p:0.06622672826051712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[10.3496, 10.7025, 10.5473],
        [10.3496, 10.3500, 10.3496],
        [10.3496, 10.5502, 10.4291],
        [10.3496, 11.1954, 11.1341]], grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.06614599376916885 
model_pd.l_d.mean(): -3.803582191467285 
model_pd.lagr.mean(): -3.737436294555664 
model_pd.lambdas: dict_items([('pout', tensor([1.2085])), ('power', tensor([0.1565]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8404])), ('power', tensor([-17.7102]))])
epoch：737	 i:0 	 global-step:14740	 l-p:0.06614599376916885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[10.3960, 10.4111, 10.3973],
        [10.3960, 10.4986, 10.4228],
        [10.3960, 11.3097, 11.2766],
        [10.3960, 10.3964, 10.3960]], grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.06606545299291611 
model_pd.l_d.mean(): -3.7872626781463623 
model_pd.lagr.mean(): -3.7211971282958984 
model_pd.lambdas: dict_items([('pout', tensor([1.2077])), ('power', tensor([0.1556]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8463])), ('power', tensor([-17.6652]))])
epoch：738	 i:0 	 global-step:14760	 l-p:0.06606545299291611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[10.4427, 10.4427, 10.4427],
        [10.4427, 11.2117, 11.1162],
        [10.4427, 12.9972, 14.3529],
        [10.4427, 12.1560, 12.6741]], grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.06598511338233948 
model_pd.l_d.mean(): -3.77101469039917 
model_pd.lagr.mean(): -3.7050294876098633 
model_pd.lambdas: dict_items([('pout', tensor([1.2068])), ('power', tensor([0.1547]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8522])), ('power', tensor([-17.6198]))])
epoch：739	 i:0 	 global-step:14780	 l-p:0.06598511338233948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.4898, 11.4141, 11.3810],
        [10.4898, 10.9590, 10.7985],
        [10.4898, 10.5937, 10.5170],
        [10.4898, 10.4903, 10.4898]], grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.06590496748685837 
model_pd.l_d.mean(): -3.7548372745513916 
model_pd.lagr.mean(): -3.688932418823242 
model_pd.lambdas: dict_items([('pout', tensor([1.2060])), ('power', tensor([0.1539]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8581])), ('power', tensor([-17.5741]))])
epoch：740	 i:0 	 global-step:14800	 l-p:0.06590496748685837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[10.5373, 11.9839, 12.2753],
        [10.5373, 10.5491, 10.5381],
        [10.5373, 13.0658, 14.3778],
        [10.5373, 13.2452, 14.7574]], grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.06582503020763397 
model_pd.l_d.mean(): -3.738731861114502 
model_pd.lagr.mean(): -3.6729068756103516 
model_pd.lambdas: dict_items([('pout', tensor([1.2051])), ('power', tensor([0.1530]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8641])), ('power', tensor([-17.5280]))])
epoch：741	 i:0 	 global-step:14820	 l-p:0.06582503020763397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[10.5851, 11.5514, 11.5336],
        [10.5851, 10.6484, 10.5972],
        [10.5851, 12.5012, 13.1853],
        [10.5851, 10.5881, 10.5852]], grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.0657452866435051 
model_pd.l_d.mean(): -3.722698211669922 
model_pd.lagr.mean(): -3.6569528579711914 
model_pd.lambdas: dict_items([('pout', tensor([1.2042])), ('power', tensor([0.1521]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8700])), ('power', tensor([-17.4815]))])
epoch：742	 i:0 	 global-step:14840	 l-p:0.0657452866435051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[10.6333, 10.7414, 10.6620],
        [10.6333, 13.2435, 14.6302],
        [10.6333, 12.5622, 13.2525],
        [10.6333, 11.2299, 11.0807]], grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.06566574424505234 
model_pd.l_d.mean(): -3.7067370414733887 
model_pd.lagr.mean(): -3.641071319580078 
model_pd.lambdas: dict_items([('pout', tensor([1.2033])), ('power', tensor([0.1512]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8760])), ('power', tensor([-17.4347]))])
epoch：743	 i:0 	 global-step:14860	 l-p:0.06566574424505234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[10.6818, 10.6818, 10.6818],
        [10.6818, 10.6905, 10.6823],
        [10.6818, 10.9720, 10.8228],
        [10.6818, 10.8911, 10.7650]], grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.06558641046285629 
model_pd.l_d.mean(): -3.6908485889434814 
model_pd.lagr.mean(): -3.6252622604370117 
model_pd.lambdas: dict_items([('pout', tensor([1.2025])), ('power', tensor([0.1504]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8820])), ('power', tensor([-17.3875]))])
epoch：744	 i:0 	 global-step:14880	 l-p:0.06558641046285629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[10.7308, 11.0419, 10.8879],
        [10.7308, 10.9414, 10.8144],
        [10.7308, 10.7429, 10.7316],
        [10.7308, 11.5270, 11.4291]], grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.06550728529691696 
model_pd.l_d.mean(): -3.6750328540802 
model_pd.lagr.mean(): -3.609525680541992 
model_pd.lambdas: dict_items([('pout', tensor([1.2016])), ('power', tensor([0.1495]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8880])), ('power', tensor([-17.3399]))])
epoch：745	 i:0 	 global-step:14900	 l-p:0.06550728529691696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[10.7801, 11.8736, 11.9116],
        [10.7801, 12.7391, 13.4398],
        [10.7801, 12.2680, 12.5690],
        [10.7801, 13.4332, 14.8438]], grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.06542836129665375 
model_pd.l_d.mean(): -3.6592905521392822 
model_pd.lagr.mean(): -3.593862295150757 
model_pd.lambdas: dict_items([('pout', tensor([1.2007])), ('power', tensor([0.1486]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8940])), ('power', tensor([-17.2920]))])
epoch：746	 i:0 	 global-step:14920	 l-p:0.06542836129665375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[10.8297, 11.8244, 11.8070],
        [10.8297, 10.8297, 10.8297],
        [10.8297, 11.2526, 11.0852],
        [10.8297, 10.9382, 10.8582]], grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.06534965336322784 
model_pd.l_d.mean(): -3.6436219215393066 
model_pd.lagr.mean(): -3.5782723426818848 
model_pd.lambdas: dict_items([('pout', tensor([1.1998])), ('power', tensor([0.1478]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9001])), ('power', tensor([-17.2436]))])
epoch：747	 i:0 	 global-step:14940	 l-p:0.06534965336322784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228]])
 pt:tensor([[10.8798, 11.3719, 11.2043],
        [10.8798, 11.8477, 11.8147],
        [10.8798, 11.1966, 11.0400],
        [10.8798, 12.8608, 13.5699]], grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.06527113914489746 
model_pd.l_d.mean(): -3.6280274391174316 
model_pd.lagr.mean(): -3.562756299972534 
model_pd.lambdas: dict_items([('pout', tensor([1.1989])), ('power', tensor([0.1469]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9062])), ('power', tensor([-17.1949]))])
epoch：748	 i:0 	 global-step:14960	 l-p:0.06527113914489746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[10.9302, 11.0436, 10.9605],
        [10.9302, 10.9302, 10.9303],
        [10.9302, 13.7584, 15.3407],
        [10.9302, 13.5715, 14.9449]], grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.06519283354282379 
model_pd.l_d.mean(): -3.61250638961792 
model_pd.lagr.mean(): -3.5473134517669678 
model_pd.lambdas: dict_items([('pout', tensor([1.1980])), ('power', tensor([0.1461]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9122])), ('power', tensor([-17.1458]))])
epoch：749	 i:0 	 global-step:14980	 l-p:0.06519283354282379
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[10.9811, 13.6369, 15.0182],
        [10.9811, 11.2821, 11.1275],
        [10.9811, 11.0155, 10.9855],
        [10.9811, 13.6930, 15.1363]], grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.06511475890874863 
model_pd.l_d.mean(): -3.5970606803894043 
model_pd.lagr.mean(): -3.5319459438323975 
model_pd.lambdas: dict_items([('pout', tensor([1.1970])), ('power', tensor([0.1452]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9183])), ('power', tensor([-17.0963]))])
epoch：750	 i:0 	 global-step:15000	 l-p:0.06511475890874863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[11.0323, 13.0468, 13.7689],
        [11.0323, 13.7592, 15.2109],
        [11.0323, 13.0492, 13.7735],
        [11.0323, 12.6554, 13.0384]], grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.06503688544034958 
model_pd.l_d.mean(): -3.5816893577575684 
model_pd.lagr.mean(): -3.5166525840759277 
model_pd.lambdas: dict_items([('pout', tensor([1.1961])), ('power', tensor([0.1443]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9245])), ('power', tensor([-17.0464]))])
epoch：751	 i:0 	 global-step:15020	 l-p:0.06503688544034958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[11.0839, 13.9590, 15.5687],
        [11.0839, 12.1079, 12.0910],
        [11.0839, 12.6235, 12.9366],
        [11.0839, 11.0847, 11.0839]], grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.06495922803878784 
model_pd.l_d.mean(): -3.5663933753967285 
model_pd.lagr.mean(): -3.501434087753296 
model_pd.lambdas: dict_items([('pout', tensor([1.1952])), ('power', tensor([0.1435]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9306])), ('power', tensor([-16.9962]))])
epoch：752	 i:0 	 global-step:15040	 l-p:0.06495922803878784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[11.1359, 11.6430, 11.4707],
        [11.1359, 11.4005, 11.2536],
        [11.1359, 11.4425, 11.2852],
        [11.1359, 11.1710, 11.1405]], grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.06488179415464401 
model_pd.l_d.mean(): -3.551173210144043 
model_pd.lagr.mean(): -3.4862914085388184 
model_pd.lambdas: dict_items([('pout', tensor([1.1942])), ('power', tensor([0.1426]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9368])), ('power', tensor([-16.9455]))])
epoch：753	 i:0 	 global-step:15060	 l-p:0.06488179415464401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[11.1883, 11.3059, 11.2199],
        [11.1883, 11.1884, 11.1883],
        [11.1883, 11.5170, 11.3547],
        [11.1883, 11.2489, 11.1992]], grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.0648045763373375 
model_pd.l_d.mean(): -3.5360288619995117 
model_pd.lagr.mean(): -3.471224308013916 
model_pd.lambdas: dict_items([('pout', tensor([1.1933])), ('power', tensor([0.1418]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9429])), ('power', tensor([-16.8944]))])
epoch：754	 i:0 	 global-step:15080	 l-p:0.0648045763373375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[11.2411, 13.2862, 14.0118],
        [11.2411, 11.2412, 11.2411],
        [11.2411, 11.2417, 11.2412],
        [11.2411, 12.0857, 11.9834]], grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.06472757458686829 
model_pd.l_d.mean(): -3.520960807800293 
model_pd.lagr.mean(): -3.456233263015747 
model_pd.lambdas: dict_items([('pout', tensor([1.1924])), ('power', tensor([0.1410]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9491])), ('power', tensor([-16.8429]))])
epoch：755	 i:0 	 global-step:15100	 l-p:0.06472757458686829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[11.2944, 12.3783, 12.3800],
        [11.2944, 11.2944, 11.2944],
        [11.2944, 11.6067, 11.4466],
        [11.2944, 11.2944, 11.2944]], grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.06465079635381699 
model_pd.l_d.mean(): -3.5059683322906494 
model_pd.lagr.mean(): -3.441317558288574 
model_pd.lambdas: dict_items([('pout', tensor([1.1914])), ('power', tensor([0.1401]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9553])), ('power', tensor([-16.7911]))])
epoch：756	 i:0 	 global-step:15120	 l-p:0.06465079635381699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[11.3480, 11.3513, 11.3481],
        [11.3480, 11.4097, 11.3590],
        [11.3480, 11.3575, 11.3485],
        [11.3480, 13.0277, 13.4258]], grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.0645742416381836 
model_pd.l_d.mean(): -3.491053581237793 
model_pd.lagr.mean(): -3.4264793395996094 
model_pd.lambdas: dict_items([('pout', tensor([1.1904])), ('power', tensor([0.1393]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9616])), ('power', tensor([-16.7388]))])
epoch：757	 i:0 	 global-step:15140	 l-p:0.0645742416381836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[11.4020, 12.3579, 12.2926],
        [11.4020, 11.8540, 11.6759],
        [11.4020, 11.4020, 11.4020],
        [11.4020, 11.5226, 11.4344]], grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.06449790298938751 
model_pd.l_d.mean(): -3.476215124130249 
model_pd.lagr.mean(): -3.411717176437378 
model_pd.lambdas: dict_items([('pout', tensor([1.1895])), ('power', tensor([0.1384]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9678])), ('power', tensor([-16.6861]))])
epoch：758	 i:0 	 global-step:15160	 l-p:0.06449790298938751
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[11.4565, 11.5190, 11.4677],
        [11.4565, 11.4565, 11.4565],
        [11.4565, 11.4570, 11.4565],
        [11.4565, 12.3213, 12.2172]], grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.06442180275917053 
model_pd.l_d.mean(): -3.461454391479492 
model_pd.lagr.mean(): -3.3970324993133545 
model_pd.lambdas: dict_items([('pout', tensor([1.1885])), ('power', tensor([0.1376]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9741])), ('power', tensor([-16.6330]))])
epoch：759	 i:0 	 global-step:15180	 l-p:0.06442180275917053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[11.5113, 11.5118, 11.5113],
        [11.5113, 11.5287, 11.5128],
        [11.5113, 12.6211, 12.6237],
        [11.5113, 13.0081, 13.2528]], grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.06434593349695206 
model_pd.l_d.mean(): -3.4467711448669434 
model_pd.lagr.mean(): -3.382425308227539 
model_pd.lambdas: dict_items([('pout', tensor([1.1875])), ('power', tensor([0.1368]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9804])), ('power', tensor([-16.5794]))])
epoch：760	 i:0 	 global-step:15200	 l-p:0.06434593349695206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[11.5666, 13.6986, 14.4661],
        [11.5666, 11.5672, 11.5666],
        [11.5666, 11.7991, 11.6594],
        [11.5666, 11.9097, 11.7406]], grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.06427028775215149 
model_pd.l_d.mean(): -3.43216609954834 
model_pd.lagr.mean(): -3.3678958415985107 
model_pd.lambdas: dict_items([('pout', tensor([1.1865])), ('power', tensor([0.1360]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9867])), ('power', tensor([-16.5255]))])
epoch：761	 i:0 	 global-step:15220	 l-p:0.06427028775215149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[11.6223, 13.7690, 14.5435],
        [11.6223, 11.6228, 11.6223],
        [11.6223, 11.7453, 11.6552],
        [11.6223, 11.6223, 11.6223]], grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.06419487297534943 
model_pd.l_d.mean(): -3.4176387786865234 
model_pd.lagr.mean(): -3.3534438610076904 
model_pd.lambdas: dict_items([('pout', tensor([1.1855])), ('power', tensor([0.1351]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9930])), ('power', tensor([-16.4711]))])
epoch：762	 i:0 	 global-step:15240	 l-p:0.06419487297534943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[11.6784, 11.8028, 11.7119],
        [11.6784, 13.8350, 14.6119],
        [11.6784, 11.6784, 11.6784],
        [11.6784, 11.6882, 11.6790]], grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.06411968916654587 
model_pd.l_d.mean(): -3.4031906127929688 
model_pd.lagr.mean(): -3.339071035385132 
model_pd.lambdas: dict_items([('pout', tensor([1.1845])), ('power', tensor([0.1343]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9993])), ('power', tensor([-16.4163]))])
epoch：763	 i:0 	 global-step:15260	 l-p:0.06411968916654587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[11.7349, 11.7349, 11.7349],
        [11.7349, 12.8341, 12.8183],
        [11.7349, 12.0183, 11.8613],
        [11.7349, 11.7354, 11.7349]], grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.06404474377632141 
model_pd.l_d.mean(): -3.3888211250305176 
model_pd.lagr.mean(): -3.3247764110565186 
model_pd.lambdas: dict_items([('pout', tensor([1.1835])), ('power', tensor([0.1335]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0057])), ('power', tensor([-16.3611]))])
epoch：764	 i:0 	 global-step:15280	 l-p:0.06404474377632141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[11.7919, 14.8833, 16.6192],
        [11.7919, 11.7932, 11.7919],
        [11.7919, 15.2790, 17.4820],
        [11.7919, 12.1437, 11.9705]], grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.06397003680467606 
model_pd.l_d.mean(): -3.37453031539917 
model_pd.lagr.mean(): -3.3105602264404297 
model_pd.lambdas: dict_items([('pout', tensor([1.1825])), ('power', tensor([0.1327]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0120])), ('power', tensor([-16.3054]))])
epoch：765	 i:0 	 global-step:15300	 l-p:0.06397003680467606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[11.8493, 11.8528, 11.8494],
        [11.8493, 11.8499, 11.8493],
        [11.8493, 12.9616, 12.9461],
        [11.8493, 12.5036, 12.3298]], grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.0638955757021904 
model_pd.l_d.mean(): -3.360319137573242 
model_pd.lagr.mean(): -3.2964236736297607 
model_pd.lambdas: dict_items([('pout', tensor([1.1815])), ('power', tensor([0.1319]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0184])), ('power', tensor([-16.2493]))])
epoch：766	 i:0 	 global-step:15320	 l-p:0.0638955757021904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[11.9071, 14.1140, 14.9103],
        [11.9071, 15.0337, 16.7902],
        [11.9071, 13.0261, 13.0107],
        [11.9071, 11.9071, 11.9071]], grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.06382135301828384 
model_pd.l_d.mean(): -3.3461875915527344 
model_pd.lagr.mean(): -3.2823662757873535 
model_pd.lambdas: dict_items([('pout', tensor([1.1805])), ('power', tensor([0.1311]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0248])), ('power', tensor([-16.1928]))])
epoch：767	 i:0 	 global-step:15340	 l-p:0.06382135301828384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[11.9654, 14.9032, 16.4382],
        [11.9654, 11.9654, 11.9654],
        [11.9654, 13.6546, 14.0027],
        [11.9654, 11.9654, 11.9654]], grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.06374738365411758 
model_pd.l_d.mean(): -3.332136392593384 
model_pd.lagr.mean(): -3.2683889865875244 
model_pd.lambdas: dict_items([('pout', tensor([1.1794])), ('power', tensor([0.1303]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0312])), ('power', tensor([-16.1359]))])
epoch：768	 i:0 	 global-step:15360	 l-p:0.06374738365411758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[12.0241, 12.5078, 12.3180],
        [12.0241, 12.3627, 12.1896],
        [12.0241, 14.0556, 14.6796],
        [12.0241, 12.0249, 12.0241]], grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.06367364525794983 
model_pd.l_d.mean(): -3.318164825439453 
model_pd.lagr.mean(): -3.254491090774536 
model_pd.lambdas: dict_items([('pout', tensor([1.1784])), ('power', tensor([0.1295]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0377])), ('power', tensor([-16.0785]))])
epoch：769	 i:0 	 global-step:15380	 l-p:0.06367364525794983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[12.0832, 12.3293, 12.1816],
        [12.0832, 15.1172, 16.7398],
        [12.0832, 14.3314, 15.1452],
        [12.0832, 12.1588, 12.0978]], grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): 0.06360017508268356 
model_pd.l_d.mean(): -3.3042750358581543 
model_pd.lagr.mean(): -3.2406749725341797 
model_pd.lambdas: dict_items([('pout', tensor([1.1774])), ('power', tensor([0.1287]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0441])), ('power', tensor([-16.0206]))])
epoch：770	 i:0 	 global-step:15400	 l-p:0.06360017508268356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[12.1428, 16.0387, 18.6794],
        [12.1428, 12.1428, 12.1428],
        [12.1428, 12.1615, 12.1443],
        [12.1428, 12.8490, 12.6760]], grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.0635269358754158 
model_pd.l_d.mean(): -3.2904653549194336 
model_pd.lagr.mean(): -3.226938486099243 
model_pd.lambdas: dict_items([('pout', tensor([1.1763])), ('power', tensor([0.1279]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0506])), ('power', tensor([-15.9623]))])
epoch：771	 i:0 	 global-step:15420	 l-p:0.0635269358754158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[12.2028, 12.2708, 12.2150],
        [12.2028, 13.1383, 13.0279],
        [12.2028, 12.2028, 12.2028],
        [12.2028, 14.2703, 14.9064]], grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.06345394253730774 
model_pd.l_d.mean(): -3.2767364978790283 
model_pd.lagr.mean(): -3.213282585144043 
model_pd.lambdas: dict_items([('pout', tensor([1.1753])), ('power', tensor([0.1271]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0571])), ('power', tensor([-15.9036]))])
epoch：772	 i:0 	 global-step:15440	 l-p:0.06345394253730774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[12.2633, 14.5513, 15.3803],
        [12.2633, 12.2639, 12.2633],
        [12.2633, 13.2045, 13.0936],
        [12.2633, 12.7031, 12.5121]], grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.06338120251893997 
model_pd.l_d.mean(): -3.263089179992676 
model_pd.lagr.mean(): -3.1997079849243164 
model_pd.lambdas: dict_items([('pout', tensor([1.1742])), ('power', tensor([0.1263]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0636])), ('power', tensor([-15.8444]))])
epoch：773	 i:0 	 global-step:15460	 l-p:0.06338120251893997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[12.3242, 13.0804, 12.9121],
        [12.3242, 12.3348, 12.3248],
        [12.3242, 12.3242, 12.3242],
        [12.3242, 12.3247, 12.3242]], grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.0633087083697319 
model_pd.l_d.mean(): -3.249523639678955 
model_pd.lagr.mean(): -3.1862149238586426 
model_pd.lambdas: dict_items([('pout', tensor([1.1731])), ('power', tensor([0.1255]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0701])), ('power', tensor([-15.7848]))])
epoch：774	 i:0 	 global-step:15480	 l-p:0.0633087083697319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[12.3856, 12.3857, 12.3856],
        [12.3856, 12.7602, 12.5763],
        [12.3856, 14.4900, 15.1383],
        [12.3856, 12.3962, 12.3862]], grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.06323646754026413 
model_pd.l_d.mean(): -3.236039638519287 
model_pd.lagr.mean(): -3.1728031635284424 
model_pd.lambdas: dict_items([('pout', tensor([1.1720])), ('power', tensor([0.1247]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0766])), ('power', tensor([-15.7247]))])
epoch：775	 i:0 	 global-step:15500	 l-p:0.06323646754026413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[12.4474, 12.8956, 12.7011],
        [12.4474, 12.7531, 12.5842],
        [12.4474, 13.7530, 13.8056],
        [12.4474, 14.5643, 15.2168]], grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.06316448003053665 
model_pd.l_d.mean(): -3.22263765335083 
model_pd.lagr.mean(): -3.159473180770874 
model_pd.lambdas: dict_items([('pout', tensor([1.1710])), ('power', tensor([0.1239]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0831])), ('power', tensor([-15.6641]))])
epoch：776	 i:0 	 global-step:15520	 l-p:0.06316448003053665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[12.5097, 16.2427, 18.6065],
        [12.5097, 13.4743, 13.3614],
        [12.5097, 14.2917, 14.6615],
        [12.5097, 12.8659, 12.6842]], grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.06309278309345245 
model_pd.l_d.mean(): -3.209317684173584 
model_pd.lagr.mean(): -3.1462249755859375 
model_pd.lambdas: dict_items([('pout', tensor([1.1699])), ('power', tensor([0.1231]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0897])), ('power', tensor([-15.6031]))])
epoch：777	 i:0 	 global-step:15540	 l-p:0.06309278309345245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[12.5724, 12.6521, 12.5878],
        [12.5724, 14.4722, 14.9289],
        [12.5724, 13.8939, 13.9476],
        [12.5724, 13.1639, 12.9653]], grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.06302133947610855 
model_pd.l_d.mean(): -3.196079730987549 
model_pd.lagr.mean(): -3.1330583095550537 
model_pd.lambdas: dict_items([('pout', tensor([1.1688])), ('power', tensor([0.1223]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0963])), ('power', tensor([-15.5416]))])
epoch：778	 i:0 	 global-step:15560	 l-p:0.06302133947610855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[12.6356, 13.0199, 12.8314],
        [12.6356, 15.8314, 17.5441],
        [12.6356, 12.7686, 12.6707],
        [12.6356, 15.9851, 17.8717]], grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.06295017898082733 
model_pd.l_d.mean(): -3.182924747467041 
model_pd.lagr.mean(): -3.1199746131896973 
model_pd.lambdas: dict_items([('pout', tensor([1.1677])), ('power', tensor([0.1216]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1029])), ('power', tensor([-15.4797]))])
epoch：779	 i:0 	 global-step:15580	 l-p:0.06295017898082733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[12.6992, 13.2175, 13.0150],
        [12.6992, 15.9136, 17.6367],
        [12.6992, 14.8669, 15.5365],
        [12.6992, 13.4128, 13.2250]], grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.0628792941570282 
model_pd.l_d.mean(): -3.1698522567749023 
model_pd.lagr.mean(): -3.1069729328155518 
model_pd.lambdas: dict_items([('pout', tensor([1.1666])), ('power', tensor([0.1208]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1095])), ('power', tensor([-15.4174]))])
epoch：780	 i:0 	 global-step:15600	 l-p:0.0628792941570282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[12.7633, 15.9965, 17.7300],
        [12.7633, 13.7520, 13.6369],
        [12.7633, 12.7786, 12.7644],
        [12.7633, 13.0273, 12.8692]], grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.06280869245529175 
model_pd.l_d.mean(): -3.156862735748291 
model_pd.lagr.mean(): -3.0940539836883545 
model_pd.lambdas: dict_items([('pout', tensor([1.1654])), ('power', tensor([0.1200]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1161])), ('power', tensor([-15.3545]))])
epoch：781	 i:0 	 global-step:15620	 l-p:0.06280869245529175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[12.8278, 13.2195, 13.0275],
        [12.8278, 13.2934, 13.0917],
        [12.8278, 14.0949, 14.1029],
        [12.8278, 15.2199, 16.0780]], grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.06273838877677917 
model_pd.l_d.mean(): -3.143956184387207 
model_pd.lagr.mean(): -3.0812177658081055 
model_pd.lambdas: dict_items([('pout', tensor([1.1643])), ('power', tensor([0.1193]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1227])), ('power', tensor([-15.2913]))])
epoch：782	 i:0 	 global-step:15640	 l-p:0.06273838877677917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[12.8927, 12.8929, 12.8927],
        [12.8927, 14.7400, 15.1253],
        [12.8927, 13.0343, 12.9310],
        [12.8927, 12.8928, 12.8927]], grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.06266836822032928 
model_pd.l_d.mean(): -3.1311328411102295 
model_pd.lagr.mean(): -3.068464517593384 
model_pd.lambdas: dict_items([('pout', tensor([1.1632])), ('power', tensor([0.1185]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1293])), ('power', tensor([-15.2276]))])
epoch：783	 i:0 	 global-step:15660	 l-p:0.06266836822032928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[12.9581, 13.9654, 13.8487],
        [12.9581, 12.9582, 12.9581],
        [12.9581, 14.1592, 14.1260],
        [12.9581, 13.3306, 13.1410]], grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.06259864568710327 
model_pd.l_d.mean(): -3.1183929443359375 
model_pd.lagr.mean(): -3.0557942390441895 
model_pd.lambdas: dict_items([('pout', tensor([1.1621])), ('power', tensor([0.1178]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1360])), ('power', tensor([-15.1634]))])
epoch：784	 i:0 	 global-step:15680	 l-p:0.06259864568710327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[13.0240, 13.1074, 13.0402],
        [13.0240, 14.2725, 14.2594],
        [13.0240, 13.0242, 13.0240],
        [13.0240, 13.1624, 13.0606]], grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.06252921372652054 
model_pd.l_d.mean(): -3.105736494064331 
model_pd.lagr.mean(): -3.0432071685791016 
model_pd.lambdas: dict_items([('pout', tensor([1.1609])), ('power', tensor([0.1170]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1426])), ('power', tensor([-15.0988]))])
epoch：785	 i:0 	 global-step:15700	 l-p:0.06252921372652054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[13.0903, 13.4921, 13.2954],
        [13.0903, 13.0903, 13.0903],
        [13.0903, 17.3357, 20.2207],
        [13.0903, 15.0834, 15.5650]], grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.06246007978916168 
model_pd.l_d.mean(): -3.09316349029541 
model_pd.lagr.mean(): -3.03070330619812 
model_pd.lambdas: dict_items([('pout', tensor([1.1598])), ('power', tensor([0.1162]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1493])), ('power', tensor([-15.0337]))])
epoch：786	 i:0 	 global-step:15720	 l-p:0.06246007978916168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[13.1571, 13.2972, 13.1941],
        [13.1571, 13.9373, 13.7482],
        [13.1571, 13.1571, 13.1571],
        [13.1571, 13.1580, 13.1571]], grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.062391236424446106 
model_pd.l_d.mean(): -3.0806756019592285 
model_pd.lagr.mean(): -3.018284320831299 
model_pd.lambdas: dict_items([('pout', tensor([1.1586])), ('power', tensor([0.1155]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1560])), ('power', tensor([-14.9682]))])
epoch：787	 i:0 	 global-step:15740	 l-p:0.062391236424446106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[13.2243, 13.2250, 13.2243],
        [13.2243, 16.5236, 18.2557],
        [13.2243, 13.2252, 13.2243],
        [13.2243, 15.7216, 16.6303]], grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.06232268363237381 
model_pd.l_d.mean(): -3.068270444869995 
model_pd.lagr.mean(): -3.0059478282928467 
model_pd.lambdas: dict_items([('pout', tensor([1.1575])), ('power', tensor([0.1148]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1627])), ('power', tensor([-14.9022]))])
epoch：788	 i:0 	 global-step:15760	 l-p:0.06232268363237381
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[13.2920, 14.7052, 14.7653],
        [13.2920, 16.8424, 18.8466],
        [13.2920, 15.8071, 16.7243],
        [13.2920, 13.2935, 13.2920]], grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.062254440039396286 
model_pd.l_d.mean(): -3.0559496879577637 
model_pd.lagr.mean(): -2.9936952590942383 
model_pd.lambdas: dict_items([('pout', tensor([1.1563])), ('power', tensor([0.1140]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1694])), ('power', tensor([-14.8357]))])
epoch：789	 i:0 	 global-step:15780	 l-p:0.062254440039396286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[13.3601, 13.5062, 13.3993],
        [13.3601, 15.2870, 15.6909],
        [13.3601, 14.1955, 14.0118],
        [13.3601, 13.5083, 13.4002]], grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.06218649074435234 
model_pd.l_d.mean(): -3.0437135696411133 
model_pd.lagr.mean(): -2.981527090072632 
model_pd.lambdas: dict_items([('pout', tensor([1.1551])), ('power', tensor([0.1133]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1761])), ('power', tensor([-14.7688]))])
epoch：790	 i:0 	 global-step:15800	 l-p:0.06218649074435234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[13.4287, 13.4297, 13.4287],
        [13.4287, 14.7677, 14.7782],
        [13.4287, 13.4501, 13.4305],
        [13.4287, 13.4302, 13.4288]], grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.062118835747241974 
model_pd.l_d.mean(): -3.0315613746643066 
model_pd.lagr.mean(): -2.96944260597229 
model_pd.lambdas: dict_items([('pout', tensor([1.1539])), ('power', tensor([0.1125]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1828])), ('power', tensor([-14.7014]))])
epoch：791	 i:0 	 global-step:15820	 l-p:0.062118835747241974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[13.4978, 14.0570, 13.8395],
        [13.4978, 13.4988, 13.4978],
        [13.4978, 17.5696, 20.1550],
        [13.4978, 14.8451, 14.8558]], grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.06205147132277489 
model_pd.l_d.mean(): -3.01949405670166 
model_pd.lagr.mean(): -2.95744252204895 
model_pd.lambdas: dict_items([('pout', tensor([1.1527])), ('power', tensor([0.1118]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1896])), ('power', tensor([-14.6336]))])
epoch：792	 i:0 	 global-step:15840	 l-p:0.06205147132277489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[13.5673, 13.5673, 13.5673],
        [13.5673, 13.7131, 13.6059],
        [13.5673, 13.6454, 13.5814],
        [13.5673, 15.6462, 16.1507]], grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.061984408646821976 
model_pd.l_d.mean(): -3.0075111389160156 
model_pd.lagr.mean(): -2.9455268383026123 
model_pd.lambdas: dict_items([('pout', tensor([1.1515])), ('power', tensor([0.1111]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1963])), ('power', tensor([-14.5652]))])
epoch：793	 i:0 	 global-step:15860	 l-p:0.061984408646821976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[13.6373, 16.2287, 17.1753],
        [13.6373, 13.6374, 13.6374],
        [13.6373, 13.7886, 13.6782],
        [13.6373, 14.9569, 14.9449]], grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.06191764771938324 
model_pd.l_d.mean(): -2.995612621307373 
model_pd.lagr.mean(): -2.933695077896118 
model_pd.lambdas: dict_items([('pout', tensor([1.1503])), ('power', tensor([0.1104]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2031])), ('power', tensor([-14.4964]))])
epoch：794	 i:0 	 global-step:15880	 l-p:0.06191764771938324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[13.7078, 15.8120, 16.3232],
        [13.7078, 13.7086, 13.7079],
        [13.7078, 15.6939, 16.1117],
        [13.7078, 13.7078, 13.7078]], grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.061851177364587784 
model_pd.l_d.mean(): -2.983799695968628 
model_pd.lagr.mean(): -2.9219484329223633 
model_pd.lambdas: dict_items([('pout', tensor([1.1491])), ('power', tensor([0.1096]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2099])), ('power', tensor([-14.4271]))])
epoch：795	 i:0 	 global-step:15900	 l-p:0.061851177364587784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[13.7788, 15.8957, 16.4103],
        [13.7788, 16.3790, 17.3168],
        [13.7788, 14.1810, 13.9768],
        [13.7788, 15.6355, 15.9500]], grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.061785005033016205 
model_pd.l_d.mean(): -2.9720711708068848 
model_pd.lagr.mean(): -2.9102861881256104 
model_pd.lambdas: dict_items([('pout', tensor([1.1479])), ('power', tensor([0.1089]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2166])), ('power', tensor([-14.3574]))])
epoch：796	 i:0 	 global-step:15920	 l-p:0.061785005033016205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[13.8502, 14.0031, 13.8913],
        [13.8502, 14.0053, 13.8923],
        [13.8502, 16.2500, 16.9970],
        [13.8502, 14.0045, 13.8919]], grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.0617191307246685 
model_pd.l_d.mean(): -2.9604275226593018 
model_pd.lagr.mean(): -2.8987083435058594 
model_pd.lambdas: dict_items([('pout', tensor([1.1467])), ('power', tensor([0.1082]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2234])), ('power', tensor([-14.2871]))])
epoch：797	 i:0 	 global-step:15940	 l-p:0.0617191307246685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[13.9222, 17.6653, 19.7821],
        [13.9222, 15.9447, 16.3710],
        [13.9222, 13.9692, 13.9283],
        [13.9222, 13.9222, 13.9222]], grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.061653558164834976 
model_pd.l_d.mean(): -2.948869228363037 
model_pd.lagr.mean(): -2.8872156143188477 
model_pd.lambdas: dict_items([('pout', tensor([1.1455])), ('power', tensor([0.1075]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2302])), ('power', tensor([-14.2164]))])
epoch：798	 i:0 	 global-step:15960	 l-p:0.061653558164834976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[13.9946, 14.0758, 14.0092],
        [13.9946, 16.1502, 16.6752],
        [13.9946, 16.4234, 17.1801],
        [13.9946, 14.7985, 14.5892]], grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.06158828362822533 
model_pd.l_d.mean(): -2.9373958110809326 
model_pd.lagr.mean(): -2.875807523727417 
model_pd.lambdas: dict_items([('pout', tensor([1.1442])), ('power', tensor([0.1068]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2370])), ('power', tensor([-14.1452]))])
epoch：799	 i:0 	 global-step:15980	 l-p:0.06158828362822533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[14.0675, 18.3344, 21.0474],
        [14.0675, 14.2248, 14.1100],
        [14.0675, 14.0800, 14.0682],
        [14.0675, 16.2362, 16.7647]], grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.06152331456542015 
model_pd.l_d.mean(): -2.9260077476501465 
model_pd.lagr.mean(): -2.8644845485687256 
model_pd.lambdas: dict_items([('pout', tensor([1.1430])), ('power', tensor([0.1061]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2439])), ('power', tensor([-14.0735]))])
epoch：800	 i:0 	 global-step:16000	 l-p:0.06152331456542015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[14.1408, 14.2992, 14.1836],
        [14.1408, 15.5649, 15.5782],
        [14.1408, 14.4412, 14.2619],
        [14.1408, 14.5561, 14.3454]], grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.06145863980054855 
model_pd.l_d.mean(): -2.9147047996520996 
model_pd.lagr.mean(): -2.8532462120056152 
model_pd.lambdas: dict_items([('pout', tensor([1.1417])), ('power', tensor([0.1054]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2507])), ('power', tensor([-14.0013]))])
epoch：801	 i:0 	 global-step:16020	 l-p:0.06145863980054855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[14.2147, 14.3741, 14.2578],
        [14.2147, 14.6597, 14.4427],
        [14.2147, 14.2274, 14.2154],
        [14.2147, 16.6878, 17.4593]], grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 0.06139427050948143 
model_pd.l_d.mean(): -2.903486728668213 
model_pd.lagr.mean(): -2.842092514038086 
model_pd.lambdas: dict_items([('pout', tensor([1.1405])), ('power', tensor([0.1047]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2575])), ('power', tensor([-13.9286]))])
epoch：802	 i:0 	 global-step:16040	 l-p:0.06139427050948143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[14.2890, 14.2906, 14.2890],
        [14.2890, 14.2890, 14.2890],
        [14.2890, 15.1953, 14.9977],
        [14.2890, 15.7307, 15.7447]], grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.06133021041750908 
model_pd.l_d.mean(): -2.8923540115356445 
model_pd.lagr.mean(): -2.831023693084717 
model_pd.lambdas: dict_items([('pout', tensor([1.1392])), ('power', tensor([0.1040]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2644])), ('power', tensor([-13.8554]))])
epoch：803	 i:0 	 global-step:16060	 l-p:0.06133021041750908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[14.3638, 14.3683, 14.3640],
        [14.3638, 14.4128, 14.3702],
        [14.3638, 14.3646, 14.3638],
        [14.3638, 14.3641, 14.3638]], grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.061266474425792694 
model_pd.l_d.mean(): -2.8813066482543945 
model_pd.lagr.mean(): -2.820040225982666 
model_pd.lambdas: dict_items([('pout', tensor([1.1379])), ('power', tensor([0.1033]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2712])), ('power', tensor([-13.7818]))])
epoch：804	 i:0 	 global-step:16080	 l-p:0.061266474425792694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[14.4391, 14.6025, 14.4835],
        [14.4391, 15.3568, 15.1571],
        [14.4391, 18.3403, 20.5494],
        [14.4391, 14.6017, 14.4831]], grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.06120303273200989 
model_pd.l_d.mean(): -2.8703441619873047 
model_pd.lagr.mean(): -2.809141159057617 
model_pd.lambdas: dict_items([('pout', tensor([1.1367])), ('power', tensor([0.1026]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2781])), ('power', tensor([-13.7076]))])
epoch：805	 i:0 	 global-step:16100	 l-p:0.06120303273200989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[14.5149, 15.7971, 15.7201],
        [14.5149, 17.0485, 17.8401],
        [14.5149, 14.8251, 14.6400],
        [14.5149, 15.9836, 15.9984]], grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 0.06113990768790245 
model_pd.l_d.mean(): -2.8594675064086914 
model_pd.lagr.mean(): -2.798327684402466 
model_pd.lambdas: dict_items([('pout', tensor([1.1354])), ('power', tensor([0.1019]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2850])), ('power', tensor([-13.6330]))])
epoch：806	 i:0 	 global-step:16120	 l-p:0.06113990768790245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[14.5912, 15.3014, 15.0658],
        [14.5912, 18.2825, 20.2283],
        [14.5912, 16.8540, 17.4075],
        [14.5912, 14.7508, 14.6336]], grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.06107710674405098 
model_pd.l_d.mean(): -2.848675489425659 
model_pd.lagr.mean(): -2.7875983715057373 
model_pd.lambdas: dict_items([('pout', tensor([1.1341])), ('power', tensor([0.1013]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2918])), ('power', tensor([-13.5579]))])
epoch：807	 i:0 	 global-step:16140	 l-p:0.06107710674405098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[14.6679, 15.2868, 15.0473],
        [14.6679, 14.8286, 14.7106],
        [14.6679, 16.9445, 17.5017],
        [14.6679, 14.6863, 14.6692]], grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.06101462244987488 
model_pd.l_d.mean(): -2.8379688262939453 
model_pd.lagr.mean(): -2.776954174041748 
model_pd.lambdas: dict_items([('pout', tensor([1.1328])), ('power', tensor([0.1006]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2987])), ('power', tensor([-13.4823]))])
epoch：808	 i:0 	 global-step:16160	 l-p:0.06101462244987488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[14.7451, 14.7497, 14.7452],
        [14.7451, 15.4644, 15.2260],
        [14.7451, 17.3250, 18.1321],
        [14.7451, 15.2105, 14.9838]], grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): 0.06095247343182564 
model_pd.l_d.mean(): -2.8273468017578125 
model_pd.lagr.mean(): -2.7663943767547607 
model_pd.lambdas: dict_items([('pout', tensor([1.1315])), ('power', tensor([0.0999]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3056])), ('power', tensor([-13.4063]))])
epoch：809	 i:0 	 global-step:16180	 l-p:0.06095247343182564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[14.8228, 14.8228, 14.8228],
        [14.8228, 16.0066, 15.8742],
        [14.8228, 18.6584, 20.7277],
        [14.8228, 19.7063, 23.0375]], grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.06089062988758087 
model_pd.l_d.mean(): -2.81680965423584 
model_pd.lagr.mean(): -2.7559189796447754 
model_pd.lambdas: dict_items([('pout', tensor([1.1302])), ('power', tensor([0.0992]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3125])), ('power', tensor([-13.3297]))])
epoch：810	 i:0 	 global-step:16200	 l-p:0.06089062988758087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[14.9009, 16.2235, 16.1450],
        [14.9009, 15.2835, 15.0734],
        [14.9009, 16.0921, 15.9591],
        [14.9009, 17.2194, 17.7877]], grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.06082913279533386 
model_pd.l_d.mean(): -2.8063578605651855 
model_pd.lagr.mean(): -2.7455286979675293 
model_pd.lambdas: dict_items([('pout', tensor([1.1288])), ('power', tensor([0.0986]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3194])), ('power', tensor([-13.2527]))])
epoch：811	 i:0 	 global-step:16220	 l-p:0.06082913279533386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[14.9795, 15.3019, 15.1097],
        [14.9795, 19.0458, 21.3514],
        [14.9795, 16.4071, 16.3740],
        [14.9795, 14.9797, 14.9795]], grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.060767970979213715 
model_pd.l_d.mean(): -2.795990467071533 
model_pd.lagr.mean(): -2.735222578048706 
model_pd.lambdas: dict_items([('pout', tensor([1.1275])), ('power', tensor([0.0979]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3263])), ('power', tensor([-13.1752]))])
epoch：812	 i:0 	 global-step:16240	 l-p:0.060767970979213715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[15.0585, 18.8838, 20.9027],
        [15.0585, 15.1476, 15.0747],
        [15.0585, 15.5070, 15.2800],
        [15.0585, 15.0594, 15.0586]], grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.06070714071393013 
model_pd.l_d.mean(): -2.785707473754883 
model_pd.lagr.mean(): -2.7250003814697266 
model_pd.lambdas: dict_items([('pout', tensor([1.1262])), ('power', tensor([0.0973]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3332])), ('power', tensor([-13.0973]))])
epoch：813	 i:0 	 global-step:16260	 l-p:0.06070714071393013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[15.1381, 15.1381, 15.1381],
        [15.1381, 16.3517, 16.2167],
        [15.1381, 15.1398, 15.1381],
        [15.1381, 16.0623, 15.8419]], grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.060646649450063705 
model_pd.l_d.mean(): -2.7755088806152344 
model_pd.lagr.mean(): -2.71486234664917 
model_pd.lambdas: dict_items([('pout', tensor([1.1248])), ('power', tensor([0.0966]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3401])), ('power', tensor([-13.0189]))])
epoch：814	 i:0 	 global-step:16280	 l-p:0.060646649450063705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[15.2180, 15.5467, 15.3508],
        [15.2180, 19.1693, 21.3029],
        [15.2180, 15.3082, 15.2343],
        [15.2180, 15.2189, 15.2180]], grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.06058651581406593 
model_pd.l_d.mean(): -2.765395164489746 
model_pd.lagr.mean(): -2.7048087120056152 
model_pd.lambdas: dict_items([('pout', tensor([1.1235])), ('power', tensor([0.0960]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3470])), ('power', tensor([-12.9401]))])
epoch：815	 i:0 	 global-step:16300	 l-p:0.06058651581406593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[15.2984, 15.9495, 15.6982],
        [15.2984, 16.8100, 16.8013],
        [15.2984, 19.4623, 21.8248],
        [15.2984, 16.0502, 15.8017]], grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.060526713728904724 
model_pd.l_d.mean(): -2.7553653717041016 
model_pd.lagr.mean(): -2.694838762283325 
model_pd.lambdas: dict_items([('pout', tensor([1.1221])), ('power', tensor([0.0953]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3539])), ('power', tensor([-12.8607]))])
epoch：816	 i:0 	 global-step:16320	 l-p:0.060526713728904724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[15.3793, 15.3988, 15.3807],
        [15.3793, 16.7520, 16.6717],
        [15.3793, 15.5497, 15.4246],
        [15.3793, 18.3293, 19.4007]], grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): 0.06046726554632187 
model_pd.l_d.mean(): -2.745419502258301 
model_pd.lagr.mean(): -2.6849522590637207 
model_pd.lambdas: dict_items([('pout', tensor([1.1208])), ('power', tensor([0.0947]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3609])), ('power', tensor([-12.7810]))])
epoch：817	 i:0 	 global-step:16340	 l-p:0.06046726554632187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[15.4606, 16.4561, 16.2412],
        [15.4606, 16.9909, 16.9825],
        [15.4606, 15.4606, 15.4606],
        [15.4606, 15.4606, 15.4606]], grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.06040817126631737 
model_pd.l_d.mean(): -2.7355575561523438 
model_pd.lagr.mean(): -2.675149440765381 
model_pd.lambdas: dict_items([('pout', tensor([1.1194])), ('power', tensor([0.0941]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3678])), ('power', tensor([-12.7008]))])
epoch：818	 i:0 	 global-step:16360	 l-p:0.06040817126631737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[15.5423, 15.5621, 15.5437],
        [15.5423, 17.8402, 18.3311],
        [15.5423, 15.5423, 15.5423],
        [15.5423, 15.9450, 15.7241]], grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.06034943461418152 
model_pd.l_d.mean(): -2.725778579711914 
model_pd.lagr.mean(): -2.66542911529541 
model_pd.lambdas: dict_items([('pout', tensor([1.1180])), ('power', tensor([0.0934]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3747])), ('power', tensor([-12.6202]))])
epoch：819	 i:0 	 global-step:16380	 l-p:0.06034943461418152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[15.6244, 17.7736, 18.1448],
        [15.6244, 17.1737, 17.1656],
        [15.6244, 15.9638, 15.7617],
        [15.6244, 16.2922, 16.0347]], grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.06029105186462402 
model_pd.l_d.mean(): -2.716083526611328 
model_pd.lagr.mean(): -2.655792474746704 
model_pd.lambdas: dict_items([('pout', tensor([1.1167])), ('power', tensor([0.0928]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3816])), ('power', tensor([-12.5391]))])
epoch：820	 i:0 	 global-step:16400	 l-p:0.06029105186462402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228]])
 pt:tensor([[15.7070, 17.8693, 18.2429],
        [15.7070, 16.7214, 16.5027],
        [15.7070, 16.2093, 15.9653],
        [15.7070, 19.7182, 21.8384]], grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.06023304536938667 
model_pd.l_d.mean(): -2.7064714431762695 
model_pd.lagr.mean(): -2.646238327026367 
model_pd.lambdas: dict_items([('pout', tensor([1.1153])), ('power', tensor([0.0922]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3885])), ('power', tensor([-12.4576]))])
epoch：821	 i:0 	 global-step:16420	 l-p:0.06023304536938667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[15.7900, 15.8045, 15.7908],
        [15.7900, 15.7918, 15.7900],
        [15.7900, 17.2057, 17.1238],
        [15.7900, 15.8843, 15.8071]], grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.06017540022730827 
model_pd.l_d.mean(): -2.69694185256958 
model_pd.lagr.mean(): -2.6367664337158203 
model_pd.lambdas: dict_items([('pout', tensor([1.1139])), ('power', tensor([0.0916]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3954])), ('power', tensor([-12.3757]))])
epoch：822	 i:0 	 global-step:16440	 l-p:0.06017540022730827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[15.8733, 15.9683, 15.8905],
        [15.8733, 15.8733, 15.8733],
        [15.8733, 15.9287, 15.8806],
        [15.8733, 15.8784, 15.8735]], grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.06011812388896942 
model_pd.l_d.mean(): -2.687495231628418 
model_pd.lagr.mean(): -2.6273770332336426 
model_pd.lambdas: dict_items([('pout', tensor([1.1125])), ('power', tensor([0.0909]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4023])), ('power', tensor([-12.2934]))])
epoch：823	 i:0 	 global-step:16460	 l-p:0.06011812388896942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[15.9571, 19.0596, 20.2041],
        [15.9571, 15.9571, 15.9571],
        [15.9571, 15.9590, 15.9572],
        [15.9571, 15.9579, 15.9571]], grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.06006121262907982 
model_pd.l_d.mean(): -2.678130626678467 
model_pd.lagr.mean(): -2.6180694103240967 
model_pd.lambdas: dict_items([('pout', tensor([1.1111])), ('power', tensor([0.0903]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4092])), ('power', tensor([-12.2107]))])
epoch：824	 i:0 	 global-step:16480	 l-p:0.06006121262907982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[16.0413, 17.6388, 17.6315],
        [16.0413, 16.2263, 16.0915],
        [16.0413, 21.3736, 25.0184],
        [16.0413, 18.2566, 18.6405]], grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.06000467762351036 
model_pd.l_d.mean(): -2.6688480377197266 
model_pd.lagr.mean(): -2.6088433265686035 
model_pd.lambdas: dict_items([('pout', tensor([1.1096])), ('power', tensor([0.0897]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4161])), ('power', tensor([-12.1276]))])
epoch：825	 i:0 	 global-step:16500	 l-p:0.06000467762351036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[16.1259, 16.2227, 16.1434],
        [16.1259, 18.5231, 19.0372],
        [16.1259, 19.2620, 20.4174],
        [16.1259, 16.1259, 16.1259]], grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.05994851887226105 
model_pd.l_d.mean(): -2.659646987915039 
model_pd.lagr.mean(): -2.599698543548584 
model_pd.lambdas: dict_items([('pout', tensor([1.1082])), ('power', tensor([0.0891]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4230])), ('power', tensor([-12.0440]))])
epoch：826	 i:0 	 global-step:16520	 l-p:0.05994851887226105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[16.2108, 16.3992, 16.2621],
        [16.2108, 18.7646, 19.3957],
        [16.2108, 21.2116, 24.4045],
        [16.2108, 16.3083, 16.2285]], grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.05989273637533188 
model_pd.l_d.mean(): -2.650527238845825 
model_pd.lagr.mean(): -2.59063458442688 
model_pd.lambdas: dict_items([('pout', tensor([1.1068])), ('power', tensor([0.0885]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4299])), ('power', tensor([-11.9601]))])
epoch：827	 i:0 	 global-step:16540	 l-p:0.05989273637533188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[16.2961, 16.2961, 16.2961],
        [16.2961, 16.2962, 16.2961],
        [16.2961, 16.6532, 16.4407],
        [16.2961, 17.1065, 16.8398]], grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.05983734130859375 
model_pd.l_d.mean(): -2.641488790512085 
model_pd.lagr.mean(): -2.581651449203491 
model_pd.lambdas: dict_items([('pout', tensor([1.1054])), ('power', tensor([0.0879]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4368])), ('power', tensor([-11.8758]))])
epoch：828	 i:0 	 global-step:16560	 l-p:0.05983734130859375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[16.3818, 16.5716, 16.4334],
        [16.3818, 18.0187, 18.0120],
        [16.3818, 16.5698, 16.4326],
        [16.3818, 16.3830, 16.3818]], grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.05978232994675636 
model_pd.l_d.mean(): -2.632530450820923 
model_pd.lagr.mean(): -2.5727481842041016 
model_pd.lambdas: dict_items([('pout', tensor([1.1039])), ('power', tensor([0.0873]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4437])), ('power', tensor([-11.7912]))])
epoch：829	 i:0 	 global-step:16580	 l-p:0.05978232994675636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[16.4679, 16.4691, 16.4679],
        [16.4679, 20.9892, 23.5605],
        [16.4679, 19.3947, 20.3174],
        [16.4679, 19.0679, 19.7113]], grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.059727706015110016 
model_pd.l_d.mean(): -2.623652458190918 
model_pd.lagr.mean(): -2.563924789428711 
model_pd.lambdas: dict_items([('pout', tensor([1.1025])), ('power', tensor([0.0868]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4506])), ('power', tensor([-11.7061]))])
epoch：830	 i:0 	 global-step:16600	 l-p:0.059727706015110016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[16.5542, 16.7447, 16.6057],
        [16.5542, 16.7474, 16.6069],
        [16.5542, 21.6727, 24.9425],
        [16.5542, 16.5543, 16.5542]], grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.05967346206307411 
model_pd.l_d.mean(): -2.614854335784912 
model_pd.lagr.mean(): -2.555180788040161 
model_pd.lambdas: dict_items([('pout', tensor([1.1010])), ('power', tensor([0.0862]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4574])), ('power', tensor([-11.6208]))])
epoch：831	 i:0 	 global-step:16620	 l-p:0.05967346206307411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[16.6410, 16.8344, 16.6936],
        [16.6410, 16.6429, 16.6410],
        [16.6410, 16.6410, 16.6410],
        [16.6410, 16.7543, 16.6631]], grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.05961961671710014 
model_pd.l_d.mean(): -2.606135368347168 
model_pd.lagr.mean(): -2.546515703201294 
model_pd.lambdas: dict_items([('pout', tensor([1.0995])), ('power', tensor([0.0856]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4643])), ('power', tensor([-11.5350]))])
epoch：832	 i:0 	 global-step:16640	 l-p:0.05961961671710014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[16.7281, 16.7871, 16.7358],
        [16.7281, 21.3290, 23.9468],
        [16.7281, 16.9237, 16.7814],
        [16.7281, 22.3133, 26.1350]], grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.05956616252660751 
model_pd.l_d.mean(): -2.5974955558776855 
model_pd.lagr.mean(): -2.5379292964935303 
model_pd.lambdas: dict_items([('pout', tensor([1.0981])), ('power', tensor([0.0850]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4711])), ('power', tensor([-11.4490]))])
epoch：833	 i:0 	 global-step:16660	 l-p:0.05956616252660751
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[16.8154, 16.8372, 16.8170],
        [16.8154, 16.8749, 16.8232],
        [16.8154, 16.8154, 16.8154],
        [16.8154, 16.8154, 16.8155]], grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.05951310694217682 
model_pd.l_d.mean(): -2.5889337062835693 
model_pd.lagr.mean(): -2.5294206142425537 
model_pd.lambdas: dict_items([('pout', tensor([1.0966])), ('power', tensor([0.0845]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4780])), ('power', tensor([-11.3626]))])
epoch：834	 i:0 	 global-step:16680	 l-p:0.05951310694217682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[16.9032, 17.0186, 16.9257],
        [16.9032, 21.5576, 24.2067],
        [16.9032, 16.9087, 16.9034],
        [16.9032, 16.9032, 16.9032]], grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.05946044996380806 
model_pd.l_d.mean(): -2.5804498195648193 
model_pd.lagr.mean(): -2.520989418029785 
model_pd.lambdas: dict_items([('pout', tensor([1.0951])), ('power', tensor([0.0839]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4848])), ('power', tensor([-11.2758]))])
epoch：835	 i:0 	 global-step:16700	 l-p:0.05946044996380806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[16.9912, 17.4393, 17.1941],
        [16.9912, 17.0514, 16.9991],
        [16.9912, 17.1835, 17.0425],
        [16.9912, 18.6986, 18.6931]], grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.05940817669034004 
model_pd.l_d.mean(): -2.5720438957214355 
model_pd.lagr.mean(): -2.5126357078552246 
model_pd.lambdas: dict_items([('pout', tensor([1.0936])), ('power', tensor([0.0833]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4916])), ('power', tensor([-11.1888]))])
epoch：836	 i:0 	 global-step:16720	 l-p:0.05940817669034004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[17.0795, 17.0805, 17.0795],
        [17.0795, 19.6390, 20.1911],
        [17.0795, 18.9746, 19.0679],
        [17.0795, 17.8216, 17.5367]], grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.05935632809996605 
model_pd.l_d.mean(): -2.5637145042419434 
model_pd.lagr.mean(): -2.5043582916259766 
model_pd.lambdas: dict_items([('pout', tensor([1.0921])), ('power', tensor([0.0828]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4984])), ('power', tensor([-11.1015]))])
epoch：837	 i:0 	 global-step:16740	 l-p:0.05935632809996605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[17.1681, 17.1737, 17.1683],
        [17.1681, 18.8411, 18.8079],
        [17.1681, 17.1681, 17.1681],
        [17.1681, 18.8960, 18.8909]], grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 0.059304870665073395 
model_pd.l_d.mean(): -2.555461883544922 
model_pd.lagr.mean(): -2.496156930923462 
model_pd.lambdas: dict_items([('pout', tensor([1.0906])), ('power', tensor([0.0822]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5052])), ('power', tensor([-11.0138]))])
epoch：838	 i:0 	 global-step:16760	 l-p:0.059304870665073395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[17.2570, 17.2794, 17.2586],
        [17.2570, 17.2590, 17.2570],
        [17.2570, 18.9400, 18.9068],
        [17.2570, 17.2569, 17.2570]], grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.05925382301211357 
model_pd.l_d.mean(): -2.5472846031188965 
model_pd.lagr.mean(): -2.4880306720733643 
model_pd.lambdas: dict_items([('pout', tensor([1.0891])), ('power', tensor([0.0817]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5119])), ('power', tensor([-10.9259]))])
epoch：839	 i:0 	 global-step:16780	 l-p:0.05925382301211357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[17.3461, 17.5475, 17.4006],
        [17.3461, 17.8772, 17.6096],
        [17.3461, 18.1018, 17.8120],
        [17.3461, 21.8275, 24.2040]], grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.05920318141579628 
model_pd.l_d.mean(): -2.5391833782196045 
model_pd.lagr.mean(): -2.479980230331421 
model_pd.lambdas: dict_items([('pout', tensor([1.0876])), ('power', tensor([0.0811]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5187])), ('power', tensor([-10.8377]))])
epoch：840	 i:0 	 global-step:16800	 l-p:0.05920318141579628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[17.4355, 17.5419, 17.4548],
        [17.4355, 22.8559, 26.3231],
        [17.4355, 20.8601, 22.1271],
        [17.4355, 19.1943, 19.1898]], grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): 0.059152960777282715 
model_pd.l_d.mean(): -2.531156539916992 
model_pd.lagr.mean(): -2.47200345993042 
model_pd.lambdas: dict_items([('pout', tensor([1.0860])), ('power', tensor([0.0806]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5254])), ('power', tensor([-10.7493]))])
epoch：841	 i:0 	 global-step:16820	 l-p:0.059152960777282715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[17.5251, 17.5251, 17.5251],
        [17.5251, 22.9763, 26.4636],
        [17.5251, 20.9444, 22.1952],
        [17.5251, 17.5252, 17.5251]], grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): 0.05910314619541168 
model_pd.l_d.mean(): -2.5232038497924805 
model_pd.lagr.mean(): -2.4641005992889404 
model_pd.lambdas: dict_items([('pout', tensor([1.0845])), ('power', tensor([0.0801]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5321])), ('power', tensor([-10.6606]))])
epoch：842	 i:0 	 global-step:16840	 l-p:0.05910314619541168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[17.6150, 17.6150, 17.6150],
        [17.6150, 17.6150, 17.6150],
        [17.6150, 18.7194, 18.4598],
        [17.6150, 17.6779, 17.6232]], grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.05905374512076378 
model_pd.l_d.mean(): -2.515324354171753 
model_pd.lagr.mean(): -2.456270694732666 
model_pd.lambdas: dict_items([('pout', tensor([1.0830])), ('power', tensor([0.0795]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5388])), ('power', tensor([-10.5717]))])
epoch：843	 i:0 	 global-step:16860	 l-p:0.05905374512076378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[17.7050, 18.8720, 18.6236],
        [17.7050, 18.7672, 18.4966],
        [17.7050, 19.4951, 19.4911],
        [17.7050, 17.8272, 17.7289]], grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): 0.059004753828048706 
model_pd.l_d.mean(): -2.5075182914733887 
model_pd.lagr.mean(): -2.4485135078430176 
model_pd.lambdas: dict_items([('pout', tensor([1.0814])), ('power', tensor([0.0790]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5455])), ('power', tensor([-10.4825]))])
epoch：844	 i:0 	 global-step:16880	 l-p:0.059004753828048706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[17.7953, 17.9044, 17.8151],
        [17.7953, 18.6939, 18.3997],
        [17.7953, 17.8590, 17.8037],
        [17.7953, 17.8012, 17.7955]], grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.05895619094371796 
model_pd.l_d.mean(): -2.499783992767334 
model_pd.lagr.mean(): -2.4408278465270996 
model_pd.lambdas: dict_items([('pout', tensor([1.0799])), ('power', tensor([0.0785]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5522])), ('power', tensor([-10.3931]))])
epoch：845	 i:0 	 global-step:16900	 l-p:0.05895619094371796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[17.8858, 17.9956, 17.9057],
        [17.8858, 21.4137, 22.7229],
        [17.8858, 23.8978, 28.0178],
        [17.8858, 20.5826, 21.1670]], grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.05890804901719093 
model_pd.l_d.mean(): -2.492122173309326 
model_pd.lagr.mean(): -2.4332141876220703 
model_pd.lambdas: dict_items([('pout', tensor([1.0783])), ('power', tensor([0.0780]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5588])), ('power', tensor([-10.3036]))])
epoch：846	 i:0 	 global-step:16920	 l-p:0.05890804901719093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[17.9764, 20.6887, 21.2767],
        [17.9764, 18.0409, 17.9849],
        [17.9764, 18.1897, 18.0347],
        [17.9764, 19.7980, 19.7945]], grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.05886032059788704 
model_pd.l_d.mean(): -2.4845309257507324 
model_pd.lagr.mean(): -2.425670623779297 
model_pd.lambdas: dict_items([('pout', tensor([1.0768])), ('power', tensor([0.0775]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5655])), ('power', tensor([-10.2138]))])
epoch：847	 i:0 	 global-step:16940	 l-p:0.05886032059788704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[18.0672, 23.7043, 27.3132],
        [18.0672, 18.2787, 18.1245],
        [18.0672, 21.6052, 22.9015],
        [18.0672, 18.1784, 18.0874]], grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.05881302431225777 
model_pd.l_d.mean(): -2.4770097732543945 
model_pd.lagr.mean(): -2.418196678161621 
model_pd.lambdas: dict_items([('pout', tensor([1.0752])), ('power', tensor([0.0770]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5721])), ('power', tensor([-10.1239]))])
epoch：848	 i:0 	 global-step:16960	 l-p:0.05881302431225777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[18.1582, 18.1893, 18.1608],
        [18.1582, 19.0782, 18.7773],
        [18.1582, 18.1585, 18.1582],
        [18.1582, 18.7548, 18.4664]], grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.05876614898443222 
model_pd.l_d.mean(): -2.4695582389831543 
model_pd.lagr.mean(): -2.410792112350464 
model_pd.lambdas: dict_items([('pout', tensor([1.0736])), ('power', tensor([0.0765]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5787])), ('power', tensor([-10.0338]))])
epoch：849	 i:0 	 global-step:16980	 l-p:0.05876614898443222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[18.2493, 18.4653, 18.3081],
        [18.2493, 18.2496, 18.2493],
        [18.2493, 18.3149, 18.2579],
        [18.2493, 21.1699, 21.8988]], grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.058719709515571594 
model_pd.l_d.mean(): -2.4621756076812744 
model_pd.lagr.mean(): -2.403455972671509 
model_pd.lambdas: dict_items([('pout', tensor([1.0720])), ('power', tensor([0.0760]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5852])), ('power', tensor([-9.9436]))])
epoch：850	 i:0 	 global-step:17000	 l-p:0.058719709515571594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[18.3405, 19.4470, 19.1659],
        [18.3405, 19.8577, 19.6958],
        [18.3405, 21.9384, 23.2575],
        [18.3405, 20.9208, 21.3755]], grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.05867370218038559 
model_pd.l_d.mean(): -2.4548611640930176 
model_pd.lagr.mean(): -2.3961875438690186 
model_pd.lambdas: dict_items([('pout', tensor([1.0704])), ('power', tensor([0.0755]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5918])), ('power', tensor([-9.8533]))])
epoch：851	 i:0 	 global-step:17020	 l-p:0.05867370218038559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[18.4318, 19.1523, 18.8456],
        [18.4318, 22.0497, 23.3765],
        [18.4318, 19.0023, 18.7154],
        [18.4318, 21.0266, 21.4842]], grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.05862811952829361 
model_pd.l_d.mean(): -2.4476141929626465 
model_pd.lagr.mean(): -2.388986110687256 
model_pd.lambdas: dict_items([('pout', tensor([1.0688])), ('power', tensor([0.0750]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5983])), ('power', tensor([-9.7628]))])
epoch：852	 i:0 	 global-step:17040	 l-p:0.05862811952829361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[18.5232, 22.1611, 23.4956],
        [18.5232, 18.5233, 18.5232],
        [18.5232, 18.7364, 18.5802],
        [18.5232, 21.1326, 21.5930]], grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.05858297273516655 
model_pd.l_d.mean(): -2.440433979034424 
model_pd.lagr.mean(): -2.3818509578704834 
model_pd.lambdas: dict_items([('pout', tensor([1.0672])), ('power', tensor([0.0745]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6047])), ('power', tensor([-9.6723]))])
epoch：853	 i:0 	 global-step:17060	 l-p:0.05858297273516655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[18.6147, 18.6157, 18.6147],
        [18.6147, 23.4604, 26.0357],
        [18.6147, 19.8512, 19.5893],
        [18.6147, 24.8955, 29.2035]], grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 0.05853826552629471 
model_pd.l_d.mean(): -2.433319568634033 
model_pd.lagr.mean(): -2.374781370162964 
model_pd.lambdas: dict_items([('pout', tensor([1.0656])), ('power', tensor([0.0740]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6112])), ('power', tensor([-9.5817]))])
epoch：854	 i:0 	 global-step:17080	 l-p:0.05853826552629471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[18.7062, 22.4111, 23.7865],
        [18.7062, 19.2082, 18.9342],
        [18.7062, 18.7062, 18.7062],
        [18.7062, 22.3842, 23.7341]], grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.05849399417638779 
model_pd.l_d.mean(): -2.426270008087158 
model_pd.lagr.mean(): -2.3677759170532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0640])), ('power', tensor([0.0735]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6176])), ('power', tensor([-9.4910]))])
epoch：855	 i:0 	 global-step:17100	 l-p:0.05849399417638779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[18.7978, 18.8158, 18.7988],
        [18.7978, 22.5271, 23.9144],
        [18.7978, 23.6961, 26.3001],
        [18.7978, 19.4191, 19.1190]], grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.058450158685445786 
model_pd.l_d.mean(): -2.4192843437194824 
model_pd.lagr.mean(): -2.3608341217041016 
model_pd.lambdas: dict_items([('pout', tensor([1.0624])), ('power', tensor([0.0731]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6240])), ('power', tensor([-9.4003]))])
epoch：856	 i:0 	 global-step:17120	 l-p:0.058450158685445786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[18.8893, 22.6347, 24.0257],
        [18.8893, 20.8168, 20.8152],
        [18.8893, 19.1143, 18.9507],
        [18.8893, 20.0342, 19.7440]], grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.0584067739546299 
model_pd.l_d.mean(): -2.412362813949585 
model_pd.lagr.mean(): -2.3539559841156006 
model_pd.lambdas: dict_items([('pout', tensor([1.0607])), ('power', tensor([0.0726]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6304])), ('power', tensor([-9.3095]))])
epoch：857	 i:0 	 global-step:17140	 l-p:0.0584067739546299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[18.9809, 19.4915, 19.2130],
        [18.9809, 20.1322, 19.8405],
        [18.9809, 20.7316, 20.6376],
        [18.9809, 18.9873, 18.9811]], grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.058363817632198334 
model_pd.l_d.mean(): -2.405503988265991 
model_pd.lagr.mean(): -2.347140073776245 
model_pd.lambdas: dict_items([('pout', tensor([1.0591])), ('power', tensor([0.0721]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6367])), ('power', tensor([-9.2187]))])
epoch：858	 i:0 	 global-step:17160	 l-p:0.058363817632198334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[19.0725, 21.0860, 21.1195],
        [19.0725, 21.2218, 21.3328],
        [19.0725, 19.0748, 19.0725],
        [19.0725, 19.0908, 19.0736]], grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.058321308344602585 
model_pd.l_d.mean(): -2.3987066745758057 
model_pd.lagr.mean(): -2.3403854370117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0575])), ('power', tensor([0.0717]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6430])), ('power', tensor([-9.1280]))])
epoch：859	 i:0 	 global-step:17180	 l-p:0.058321308344602585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[19.1640, 19.6804, 19.3988],
        [19.1640, 19.1650, 19.1640],
        [19.1640, 19.1655, 19.1641],
        [19.1640, 22.9424, 24.3307]], grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.05827925726771355 
model_pd.l_d.mean(): -2.391970157623291 
model_pd.lagr.mean(): -2.333690881729126 
model_pd.lambdas: dict_items([('pout', tensor([1.0558])), ('power', tensor([0.0712]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6493])), ('power', tensor([-9.0372]))])
epoch：860	 i:0 	 global-step:17200	 l-p:0.05827925726771355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[19.2555, 23.0817, 24.5041],
        [19.2555, 20.4797, 20.1942],
        [19.2555, 19.2555, 19.2555],
        [19.2555, 20.5412, 20.2697]], grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.05823763459920883 
model_pd.l_d.mean(): -2.385293960571289 
model_pd.lagr.mean(): -2.327056407928467 
model_pd.lambdas: dict_items([('pout', tensor([1.0542])), ('power', tensor([0.0708]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6556])), ('power', tensor([-8.9465]))])
epoch：861	 i:0 	 global-step:17220	 l-p:0.05823763459920883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[19.3469, 25.8980, 30.3951],
        [19.3469, 23.1977, 24.6321],
        [19.3469, 24.4032, 27.0933],
        [19.3469, 22.8547, 23.9714]], grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.058196473866701126 
model_pd.l_d.mean(): -2.3786771297454834 
model_pd.lagr.mean(): -2.3204805850982666 
model_pd.lambdas: dict_items([('pout', tensor([1.0525])), ('power', tensor([0.0703]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6618])), ('power', tensor([-8.8559]))])
epoch：862	 i:0 	 global-step:17240	 l-p:0.058196473866701126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[19.4383, 19.6723, 19.5024],
        [19.4383, 19.5595, 19.4603],
        [19.4383, 19.4406, 19.4383],
        [19.4383, 20.2048, 19.8792]], grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.05815575271844864 
model_pd.l_d.mean(): -2.3721184730529785 
model_pd.lagr.mean(): -2.313962697982788 
model_pd.lambdas: dict_items([('pout', tensor([1.0508])), ('power', tensor([0.0699]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6679])), ('power', tensor([-8.7653]))])
epoch：863	 i:0 	 global-step:17260	 l-p:0.05815575271844864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[19.5295, 19.5295, 19.5295],
        [19.5295, 19.7614, 19.5925],
        [19.5295, 20.3002, 19.9729],
        [19.5295, 21.5978, 21.6333]], grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.058115482330322266 
model_pd.l_d.mean(): -2.365617513656616 
model_pd.lagr.mean(): -2.307502031326294 
model_pd.lambdas: dict_items([('pout', tensor([1.0492])), ('power', tensor([0.0695]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6741])), ('power', tensor([-8.6749]))])
epoch：864	 i:0 	 global-step:17280	 l-p:0.058115482330322266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[19.6206, 19.6272, 19.6208],
        [19.6206, 19.6230, 19.6206],
        [19.6206, 19.6206, 19.6206],
        [19.6206, 20.2343, 19.9262]], grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.058075662702322006 
model_pd.l_d.mean(): -2.359172821044922 
model_pd.lagr.mean(): -2.3010971546173096 
model_pd.lambdas: dict_items([('pout', tensor([1.0475])), ('power', tensor([0.0690]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6802])), ('power', tensor([-8.5845]))])
epoch：865	 i:0 	 global-step:17300	 l-p:0.058075662702322006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[19.7115, 19.7115, 19.7115],
        [19.7115, 21.3592, 21.1858],
        [19.7115, 20.3682, 20.0515],
        [19.7115, 21.8017, 21.8380]], grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.05803629755973816 
model_pd.l_d.mean(): -2.3527841567993164 
model_pd.lagr.mean(): -2.294747829437256 
model_pd.lambdas: dict_items([('pout', tensor([1.0458])), ('power', tensor([0.0686]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6862])), ('power', tensor([-8.4943]))])
epoch：866	 i:0 	 global-step:17320	 l-p:0.05803629755973816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[19.8023, 20.4226, 20.1113],
        [19.8023, 19.8027, 19.8023],
        [19.8023, 26.0348, 30.0334],
        [19.8023, 19.8039, 19.8024]], grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.05799737945199013 
model_pd.l_d.mean(): -2.3464503288269043 
model_pd.lagr.mean(): -2.2884528636932373 
model_pd.lambdas: dict_items([('pout', tensor([1.0441])), ('power', tensor([0.0682]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6922])), ('power', tensor([-8.4043]))])
epoch：867	 i:0 	 global-step:17340	 l-p:0.05799737945199013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[19.8929, 19.8945, 19.8930],
        [19.8929, 23.8313, 25.2809],
        [19.8929, 19.9196, 19.8949],
        [19.8929, 19.8930, 19.8929]], grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.05795891582965851 
model_pd.l_d.mean(): -2.340169906616211 
model_pd.lagr.mean(): -2.2822110652923584 
model_pd.lambdas: dict_items([('pout', tensor([1.0424])), ('power', tensor([0.0678]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6982])), ('power', tensor([-8.3144]))])
epoch：868	 i:0 	 global-step:17360	 l-p:0.05795891582965851
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[19.9833, 19.9858, 19.9834],
        [19.9833, 20.1244, 20.0110],
        [19.9833, 20.0027, 19.9845],
        [19.9833, 25.5813, 28.7819]], grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.0579209141433239 
model_pd.l_d.mean(): -2.333942413330078 
model_pd.lagr.mean(): -2.2760214805603027 
model_pd.lambdas: dict_items([('pout', tensor([1.0407])), ('power', tensor([0.0674]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7042])), ('power', tensor([-8.2247]))])
epoch：869	 i:0 	 global-step:17380	 l-p:0.0579209141433239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[20.0735, 21.9392, 21.8413],
        [20.0735, 20.0760, 20.0736],
        [20.0735, 20.6186, 20.3216],
        [20.0735, 23.3232, 24.1401]], grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.0578833632171154 
model_pd.l_d.mean(): -2.3277668952941895 
model_pd.lagr.mean(): -2.269883632659912 
model_pd.lambdas: dict_items([('pout', tensor([1.0390])), ('power', tensor([0.0670]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7101])), ('power', tensor([-8.1353]))])
epoch：870	 i:0 	 global-step:17400	 l-p:0.0578833632171154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[20.1634, 22.3079, 22.3460],
        [20.1634, 20.1645, 20.1635],
        [20.1634, 20.1646, 20.1635],
        [20.1634, 20.1650, 20.1635]], grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.05784625932574272 
model_pd.l_d.mean(): -2.3216428756713867 
model_pd.lagr.mean(): -2.26379656791687 
model_pd.lambdas: dict_items([('pout', tensor([1.0373])), ('power', tensor([0.0666]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7159])), ('power', tensor([-8.0460]))])
epoch：871	 i:0 	 global-step:17420	 l-p:0.05784625932574272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[20.2531, 20.2531, 20.2531],
        [20.2531, 20.2531, 20.2531],
        [20.2531, 24.3044, 25.8166],
        [20.2531, 21.5503, 21.2490]], grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): 0.05780962482094765 
model_pd.l_d.mean(): -2.3155689239501953 
model_pd.lagr.mean(): -2.2577593326568604 
model_pd.lambdas: dict_items([('pout', tensor([1.0355])), ('power', tensor([0.0662]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7217])), ('power', tensor([-7.9571]))])
epoch：872	 i:0 	 global-step:17440	 l-p:0.05780962482094765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[20.3425, 20.8065, 20.5316],
        [20.3425, 20.3428, 20.3425],
        [20.3425, 22.6542, 22.7767],
        [20.3425, 20.3425, 20.3425]], grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.05777343362569809 
model_pd.l_d.mean(): -2.309544086456299 
model_pd.lagr.mean(): -2.2517707347869873 
model_pd.lambdas: dict_items([('pout', tensor([1.0338])), ('power', tensor([0.0658]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7275])), ('power', tensor([-7.8684]))])
epoch：873	 i:0 	 global-step:17460	 l-p:0.05777343362569809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[20.4315, 23.3449, 23.8645],
        [20.4315, 20.4591, 20.4336],
        [20.4315, 23.7459, 24.5802],
        [20.4315, 20.4328, 20.4316]], grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): 0.05773770436644554 
model_pd.l_d.mean(): -2.30356764793396 
model_pd.lagr.mean(): -2.2458300590515137 
model_pd.lambdas: dict_items([('pout', tensor([1.0321])), ('power', tensor([0.0654]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7332])), ('power', tensor([-7.7800]))])
epoch：874	 i:0 	 global-step:17480	 l-p:0.05773770436644554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[20.5203, 20.6496, 20.5439],
        [20.5203, 20.7684, 20.5881],
        [20.5203, 21.8371, 21.5315],
        [20.5203, 21.7794, 21.4622]], grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): 0.05770243704319 
model_pd.l_d.mean(): -2.297638416290283 
model_pd.lagr.mean(): -2.239935874938965 
model_pd.lambdas: dict_items([('pout', tensor([1.0303])), ('power', tensor([0.0650]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7389])), ('power', tensor([-7.6919]))])
epoch：875	 i:0 	 global-step:17500	 l-p:0.05770243704319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[20.6087, 20.6157, 20.6089],
        [20.6087, 22.8067, 22.8467],
        [20.6087, 20.6087, 20.6087],
        [20.6087, 20.8505, 20.6735]], grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.05766761675477028 
model_pd.l_d.mean(): -2.291755199432373 
model_pd.lagr.mean(): -2.2340874671936035 
model_pd.lambdas: dict_items([('pout', tensor([1.0286])), ('power', tensor([0.0646]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7445])), ('power', tensor([-7.6042]))])
epoch：876	 i:0 	 global-step:17520	 l-p:0.05766761675477028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[20.6967, 20.6967, 20.6967],
        [20.6967, 20.7331, 20.6998],
        [20.6967, 21.9683, 21.6481],
        [20.6967, 21.5209, 21.1716]], grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.05763325095176697 
model_pd.l_d.mean(): -2.2859179973602295 
model_pd.lagr.mean(): -2.2282848358154297 
model_pd.lambdas: dict_items([('pout', tensor([1.0269])), ('power', tensor([0.0642]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7501])), ('power', tensor([-7.5168]))])
epoch：877	 i:0 	 global-step:17540	 l-p:0.05763325095176697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[20.7844, 21.6126, 21.2616],
        [20.7844, 23.1528, 23.2792],
        [20.7844, 20.7860, 20.7844],
        [20.7844, 24.5830, 25.7971]], grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.057599350810050964 
model_pd.l_d.mean(): -2.2801246643066406 
model_pd.lagr.mean(): -2.2225253582000732 
model_pd.lambdas: dict_items([('pout', tensor([1.0251])), ('power', tensor([0.0639]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7557])), ('power', tensor([-7.4298]))])
epoch：878	 i:0 	 global-step:17560	 l-p:0.057599350810050964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[20.8716, 21.5309, 21.2005],
        [20.8716, 20.8742, 20.8717],
        [20.8716, 20.8729, 20.8716],
        [20.8716, 22.6299, 22.4469]], grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.05756589397788048 
model_pd.l_d.mean(): -2.2743752002716064 
model_pd.lagr.mean(): -2.2168092727661133 
model_pd.lambdas: dict_items([('pout', tensor([1.0233])), ('power', tensor([0.0635]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7611])), ('power', tensor([-7.3432]))])
epoch：879	 i:0 	 global-step:17580	 l-p:0.05756589397788048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[20.9584, 21.1076, 20.9877],
        [20.9584, 23.1985, 23.2401],
        [20.9584, 20.9610, 20.9585],
        [20.9584, 26.5914, 29.6626]], grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.0575329065322876 
model_pd.l_d.mean(): -2.2686681747436523 
model_pd.lagr.mean(): -2.2111353874206543 
model_pd.lambdas: dict_items([('pout', tensor([1.0216])), ('power', tensor([0.0631]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7666])), ('power', tensor([-7.2570]))])
epoch：880	 i:0 	 global-step:17600	 l-p:0.0575329065322876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[21.0448, 21.2980, 21.1137],
        [21.0448, 21.0465, 21.0448],
        [21.0448, 21.1226, 21.0550],
        [21.0448, 28.2231, 33.1594]], grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.057500362396240234 
model_pd.l_d.mean(): -2.263002634048462 
model_pd.lagr.mean(): -2.2055022716522217 
model_pd.lambdas: dict_items([('pout', tensor([1.0198])), ('power', tensor([0.0628]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7720])), ('power', tensor([-7.1713]))])
epoch：881	 i:0 	 global-step:17620	 l-p:0.057500362396240234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[21.1307, 22.4923, 22.1771],
        [21.1307, 26.8143, 29.9138],
        [21.1307, 22.0809, 21.7196],
        [21.1307, 21.3888, 21.2015]], grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): 0.05746827647089958 
model_pd.l_d.mean(): -2.257378101348877 
model_pd.lagr.mean(): -2.1999099254608154 
model_pd.lambdas: dict_items([('pout', tensor([1.0180])), ('power', tensor([0.0624]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7773])), ('power', tensor([-7.0860]))])
epoch：882	 i:0 	 global-step:17640	 l-p:0.05746827647089958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[21.2161, 21.2234, 21.2164],
        [21.2161, 21.2161, 21.2161],
        [21.2161, 27.9345, 32.2514],
        [21.2161, 21.2161, 21.2161]], grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.057436637580394745 
model_pd.l_d.mean(): -2.251793384552002 
model_pd.lagr.mean(): -2.194356679916382 
model_pd.lambdas: dict_items([('pout', tensor([1.0162])), ('power', tensor([0.0621]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7826])), ('power', tensor([-7.0012]))])
epoch：883	 i:0 	 global-step:17660	 l-p:0.057436637580394745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[21.3010, 21.5578, 21.3709],
        [21.3010, 21.3037, 21.3011],
        [21.3010, 25.5846, 27.1871],
        [21.3010, 23.1003, 22.9137]], grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.05740546062588692 
model_pd.l_d.mean(): -2.2462470531463623 
model_pd.lagr.mean(): -2.1888415813446045 
model_pd.lambdas: dict_items([('pout', tensor([1.0144])), ('power', tensor([0.0617]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7878])), ('power', tensor([-6.9169]))])
epoch：884	 i:0 	 global-step:17680	 l-p:0.05740546062588692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[21.3854, 21.4145, 21.3875],
        [21.3854, 25.6829, 27.2879],
        [21.3854, 24.4510, 25.0004],
        [21.3854, 21.8772, 21.5861]], grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.057374730706214905 
model_pd.l_d.mean(): -2.240739107131958 
model_pd.lagr.mean(): -2.1833643913269043 
model_pd.lambdas: dict_items([('pout', tensor([1.0127])), ('power', tensor([0.0614]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7930])), ('power', tensor([-6.8331]))])
epoch：885	 i:0 	 global-step:17700	 l-p:0.057374730706214905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[21.4692, 21.6227, 21.4994],
        [21.4692, 25.4066, 26.6673],
        [21.4692, 21.4709, 21.4692],
        [21.4692, 22.0585, 21.7379]], grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.05734444782137871 
model_pd.l_d.mean(): -2.2352678775787354 
model_pd.lagr.mean(): -2.1779234409332275 
model_pd.lambdas: dict_items([('pout', tensor([1.0109])), ('power', tensor([0.0611]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7981])), ('power', tensor([-6.7498]))])
epoch：886	 i:0 	 global-step:17720	 l-p:0.05734444782137871
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[21.5525, 21.5528, 21.5525],
        [21.5525, 21.5525, 21.5525],
        [21.5525, 22.1444, 21.8224],
        [21.5525, 22.2367, 21.8940]], grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.05731461942195892 
model_pd.l_d.mean(): -2.229832649230957 
model_pd.lagr.mean(): -2.172518014907837 
model_pd.lambdas: dict_items([('pout', tensor([1.0091])), ('power', tensor([0.0607]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8032])), ('power', tensor([-6.6671]))])
epoch：887	 i:0 	 global-step:17740	 l-p:0.05731461942195892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[21.6352, 28.4978, 32.9092],
        [21.6352, 21.7155, 21.6457],
        [21.6352, 25.9566, 27.5532],
        [21.6352, 25.6062, 26.8782]], grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.05728524178266525 
model_pd.l_d.mean(): -2.2244324684143066 
model_pd.lagr.mean(): -2.167147159576416 
model_pd.lambdas: dict_items([('pout', tensor([1.0072])), ('power', tensor([0.0604]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8082])), ('power', tensor([-6.5850]))])
epoch：888	 i:0 	 global-step:17760	 l-p:0.05728524178266525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[21.7172, 21.7173, 21.7172],
        [21.7172, 21.9837, 21.7904],
        [21.7172, 26.0567, 27.6602],
        [21.7172, 21.7172, 21.7172]], grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.0572563074529171 
model_pd.l_d.mean(): -2.219066619873047 
model_pd.lagr.mean(): -2.1618103981018066 
model_pd.lambdas: dict_items([('pout', tensor([1.0054])), ('power', tensor([0.0601]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8132])), ('power', tensor([-6.5035]))])
epoch：889	 i:0 	 global-step:17780	 l-p:0.0572563074529171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[21.7987, 22.0663, 21.8722],
        [21.7987, 24.9304, 25.4927],
        [21.7987, 21.8202, 21.8000],
        [21.7987, 22.3984, 22.0723]], grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): 0.057227812707424164 
model_pd.l_d.mean(): -2.2137341499328613 
model_pd.lagr.mean(): -2.156506299972534 
model_pd.lambdas: dict_items([('pout', tensor([1.0036])), ('power', tensor([0.0597]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8181])), ('power', tensor([-6.4226]))])
epoch：890	 i:0 	 global-step:17800	 l-p:0.057227812707424164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[21.8795, 22.5757, 22.2272],
        [21.8795, 22.8684, 22.4929],
        [21.8795, 22.0189, 21.9050],
        [21.8795, 27.6652, 30.7540]], grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.05719976872205734 
model_pd.l_d.mean(): -2.2084341049194336 
model_pd.lagr.mean(): -2.1512343883514404 
model_pd.lambdas: dict_items([('pout', tensor([1.0018])), ('power', tensor([0.0594]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8229])), ('power', tensor([-6.3423]))])
epoch：891	 i:0 	 global-step:17820	 l-p:0.05719976872205734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[21.9597, 23.3201, 22.9790],
        [21.9597, 21.9608, 21.9597],
        [21.9597, 23.1048, 22.7341],
        [21.9597, 28.9340, 33.4186]], grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.05717216804623604 
model_pd.l_d.mean(): -2.2031655311584473 
model_pd.lagr.mean(): -2.14599347114563 
model_pd.lambdas: dict_items([('pout', tensor([1.0000])), ('power', tensor([0.0591]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8277])), ('power', tensor([-6.2627]))])
epoch：892	 i:0 	 global-step:17840	 l-p:0.05717216804623604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[22.0392, 26.4816, 28.1430],
        [22.0392, 23.9091, 23.7164],
        [22.0392, 25.6445, 26.5569],
        [22.0392, 26.0923, 27.3917]], grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): 0.057145003229379654 
model_pd.l_d.mean(): -2.197927474975586 
model_pd.lagr.mean(): -2.140782356262207 
model_pd.lambdas: dict_items([('pout', tensor([0.9981])), ('power', tensor([0.0588]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8324])), ('power', tensor([-6.1837]))])
epoch：893	 i:0 	 global-step:17860	 l-p:0.057145003229379654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[22.1180, 22.1180, 22.1180],
        [22.1180, 22.1197, 22.1180],
        [22.1180, 26.5829, 28.2559],
        [22.1180, 24.4216, 24.4269]], grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.057118285447359085 
model_pd.l_d.mean(): -2.1927194595336914 
model_pd.lagr.mean(): -2.135601282119751 
model_pd.lambdas: dict_items([('pout', tensor([0.9963])), ('power', tensor([0.0585]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8371])), ('power', tensor([-6.1054]))])
epoch：894	 i:0 	 global-step:17880	 l-p:0.057118285447359085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[22.1960, 22.2037, 22.1963],
        [22.1960, 25.3914, 25.9662],
        [22.1960, 23.3552, 22.9802],
        [22.1960, 22.4655, 22.2694]], grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): 0.05709198862314224 
model_pd.l_d.mean(): -2.1875405311584473 
model_pd.lagr.mean(): -2.130448579788208 
model_pd.lambdas: dict_items([('pout', tensor([0.9945])), ('power', tensor([0.0582]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8417])), ('power', tensor([-6.0278]))])
epoch：895	 i:0 	 global-step:17900	 l-p:0.05709198862314224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[22.2734, 22.2762, 22.2735],
        [22.2734, 22.9840, 22.6285],
        [22.2734, 23.0296, 22.6662],
        [22.2734, 22.3566, 22.2844]], grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.05706613510847092 
model_pd.l_d.mean(): -2.182389497756958 
model_pd.lagr.mean(): -2.1253232955932617 
model_pd.lambdas: dict_items([('pout', tensor([0.9926])), ('power', tensor([0.0579]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8463])), ('power', tensor([-5.9510]))])
epoch：896	 i:0 	 global-step:17920	 l-p:0.05706613510847092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[22.3500, 22.4929, 22.3761],
        [22.3500, 22.3502, 22.3501],
        [22.3500, 24.7579, 24.8056],
        [22.3500, 22.3500, 22.3500]], grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.05704072117805481 
model_pd.l_d.mean(): -2.1772661209106445 
model_pd.lagr.mean(): -2.120225429534912 
model_pd.lambdas: dict_items([('pout', tensor([0.9908])), ('power', tensor([0.0576]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8508])), ('power', tensor([-5.8748]))])
epoch：897	 i:0 	 global-step:17940	 l-p:0.05704072117805481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[22.4259, 22.4659, 22.4293],
        [22.4259, 25.0051, 25.1466],
        [22.4259, 23.8829, 23.5471],
        [22.4259, 25.6581, 26.2401]], grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.05701572075486183 
model_pd.l_d.mean(): -2.1721692085266113 
model_pd.lagr.mean(): -2.1151535511016846 
model_pd.lambdas: dict_items([('pout', tensor([0.9889])), ('power', tensor([0.0573]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8552])), ('power', tensor([-5.7994]))])
epoch：898	 i:0 	 global-step:17960	 l-p:0.05701572075486183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[22.5010, 22.5010, 22.5010],
        [22.5010, 24.7756, 24.7432],
        [22.5010, 23.9635, 23.6265],
        [22.5010, 23.6784, 23.2978]], grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.056991156190633774 
model_pd.l_d.mean(): -2.167097806930542 
model_pd.lagr.mean(): -2.1101067066192627 
model_pd.lambdas: dict_items([('pout', tensor([0.9870])), ('power', tensor([0.0570]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8596])), ('power', tensor([-5.7248]))])
epoch：899	 i:0 	 global-step:17980	 l-p:0.056991156190633774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[22.5753, 28.6840, 32.0211],
        [22.5753, 23.0989, 22.7893],
        [22.5753, 24.8584, 24.8260],
        [22.5753, 30.3201, 35.6534]], grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.05696702003479004 
model_pd.l_d.mean(): -2.162051200866699 
model_pd.lagr.mean(): -2.105084180831909 
model_pd.lambdas: dict_items([('pout', tensor([0.9852])), ('power', tensor([0.0568]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8639])), ('power', tensor([-5.6509]))])
epoch：900	 i:0 	 global-step:18000	 l-p:0.05696702003479004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[22.6489, 22.6489, 22.6489],
        [22.6489, 23.1744, 22.8637],
        [22.6489, 22.6507, 22.6489],
        [22.6489, 22.9272, 22.7251]], grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.05694330111145973 
model_pd.l_d.mean(): -2.157028913497925 
model_pd.lagr.mean(): -2.100085496902466 
model_pd.lambdas: dict_items([('pout', tensor([0.9833])), ('power', tensor([0.0565]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8682])), ('power', tensor([-5.5778]))])
epoch：901	 i:0 	 global-step:18020	 l-p:0.05694330111145973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[22.7216, 23.7541, 23.3627],
        [22.7216, 28.7505, 31.9724],
        [22.7216, 22.7442, 22.7229],
        [22.7216, 27.3208, 29.0461]], grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.05692000314593315 
model_pd.l_d.mean(): -2.152029514312744 
model_pd.lagr.mean(): -2.095109462738037 
model_pd.lambdas: dict_items([('pout', tensor([0.9814])), ('power', tensor([0.0562]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8724])), ('power', tensor([-5.5056]))])
epoch：902	 i:0 	 global-step:18040	 l-p:0.05692000314593315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[22.7935, 23.4249, 23.0818],
        [22.7935, 26.0846, 26.6782],
        [22.7935, 25.2550, 25.3046],
        [22.7935, 22.8343, 22.7970]], grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.0568971186876297 
model_pd.l_d.mean(): -2.1470530033111572 
model_pd.lagr.mean(): -2.090155839920044 
model_pd.lambdas: dict_items([('pout', tensor([0.9796])), ('power', tensor([0.0559]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8765])), ('power', tensor([-5.4341]))])
epoch：903	 i:0 	 global-step:18060	 l-p:0.0568971186876297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[22.8646, 29.0584, 32.4432],
        [22.8646, 30.1507, 34.8398],
        [22.8646, 26.1671, 26.7629],
        [22.8646, 23.5969, 23.2307]], grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.056874651461839676 
model_pd.l_d.mean(): -2.1420986652374268 
model_pd.lagr.mean(): -2.085223913192749 
model_pd.lambdas: dict_items([('pout', tensor([0.9777])), ('power', tensor([0.0557]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8806])), ('power', tensor([-5.3635]))])
epoch：904	 i:0 	 global-step:18080	 l-p:0.056874651461839676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[22.9348, 23.2187, 23.0129],
        [22.9348, 25.3341, 25.3413],
        [22.9348, 22.9427, 22.9351],
        [22.9348, 24.5039, 24.1777]], grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.056852590292692184 
model_pd.l_d.mean(): -2.137165069580078 
model_pd.lagr.mean(): -2.080312490463257 
model_pd.lambdas: dict_items([('pout', tensor([0.9758])), ('power', tensor([0.0554]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8846])), ('power', tensor([-5.2937]))])
epoch：905	 i:0 	 global-step:18100	 l-p:0.056852590292692184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[23.0041, 23.0060, 23.0042],
        [23.0041, 23.7416, 23.3729],
        [23.0041, 23.6423, 23.2956],
        [23.0041, 25.4911, 25.5417]], grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.056830938905477524 
model_pd.l_d.mean(): -2.1322519779205322 
model_pd.lagr.mean(): -2.075421094894409 
model_pd.lambdas: dict_items([('pout', tensor([0.9739])), ('power', tensor([0.0551]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8886])), ('power', tensor([-5.2247]))])
epoch：906	 i:0 	 global-step:18120	 l-p:0.056830938905477524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[23.0727, 23.1593, 23.0841],
        [23.0727, 27.3359, 28.7060],
        [23.0727, 23.0727, 23.0726],
        [23.0727, 23.3586, 23.1513]], grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.056809693574905396 
model_pd.l_d.mean(): -2.1273586750030518 
model_pd.lagr.mean(): -2.0705490112304688 
model_pd.lambdas: dict_items([('pout', tensor([0.9720])), ('power', tensor([0.0549]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8925])), ('power', tensor([-5.1566]))])
epoch：907	 i:0 	 global-step:18140	 l-p:0.056809693574905396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[23.1403, 23.1403, 23.1403],
        [23.1403, 23.1483, 23.1406],
        [23.1403, 23.1415, 23.1403],
        [23.1403, 23.4171, 23.2147]], grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.0567888468503952 
model_pd.l_d.mean(): -2.1224842071533203 
model_pd.lagr.mean(): -2.065695285797119 
model_pd.lambdas: dict_items([('pout', tensor([0.9701])), ('power', tensor([0.0546]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8963])), ('power', tensor([-5.0894]))])
epoch：908	 i:0 	 global-step:18160	 l-p:0.0567888468503952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[23.2070, 23.4847, 23.2817],
        [23.2070, 23.2084, 23.2071],
        [23.2070, 25.4045, 25.2953],
        [23.2070, 23.2070, 23.2070]], grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.056768398731946945 
model_pd.l_d.mean(): -2.117628574371338 
model_pd.lagr.mean(): -2.0608601570129395 
model_pd.lambdas: dict_items([('pout', tensor([0.9682])), ('power', tensor([0.0544]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9001])), ('power', tensor([-5.0230]))])
epoch：909	 i:0 	 global-step:18180	 l-p:0.056768398731946945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[23.2729, 30.6999, 35.4815],
        [23.2729, 23.2729, 23.2729],
        [23.2729, 24.4964, 24.1015],
        [23.2729, 25.7924, 25.8443]], grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.05674833804368973 
model_pd.l_d.mean(): -2.112790107727051 
model_pd.lagr.mean(): -2.056041717529297 
model_pd.lambdas: dict_items([('pout', tensor([0.9663])), ('power', tensor([0.0541]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9038])), ('power', tensor([-4.9576]))])
epoch：910	 i:0 	 global-step:18200	 l-p:0.05674833804368973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[23.3378, 23.3379, 23.3378],
        [23.3378, 24.0876, 23.7129],
        [23.3378, 23.3460, 23.3381],
        [23.3378, 25.3325, 25.1291]], grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.05672866478562355 
model_pd.l_d.mean(): -2.107969045639038 
model_pd.lagr.mean(): -2.0512404441833496 
model_pd.lambdas: dict_items([('pout', tensor([0.9644])), ('power', tensor([0.0539]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9074])), ('power', tensor([-4.8930]))])
epoch：911	 i:0 	 global-step:18220	 l-p:0.05672866478562355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[23.4019, 27.0432, 27.8506],
        [23.4019, 23.4031, 23.4019],
        [23.4019, 31.4531, 37.0014],
        [23.4019, 30.0511, 33.8677]], grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.05670937895774841 
model_pd.l_d.mean(): -2.1031641960144043 
model_pd.lagr.mean(): -2.046454906463623 
model_pd.lambdas: dict_items([('pout', tensor([0.9625])), ('power', tensor([0.0536]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9110])), ('power', tensor([-4.8293]))])
epoch：912	 i:0 	 global-step:18240	 l-p:0.05670937895774841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[23.4650, 23.6163, 23.4927],
        [23.4650, 25.4720, 25.2675],
        [23.4650, 24.2679, 23.8826],
        [23.4650, 24.2195, 23.8425]], grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.05669047310948372 
model_pd.l_d.mean(): -2.0983753204345703 
model_pd.lagr.mean(): -2.041684865951538 
model_pd.lambdas: dict_items([('pout', tensor([0.9606])), ('power', tensor([0.0534]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9145])), ('power', tensor([-4.7665]))])
epoch：913	 i:0 	 global-step:18260	 l-p:0.05669047310948372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[23.5273, 24.7660, 24.3664],
        [23.5273, 28.3006, 30.0907],
        [23.5273, 23.8094, 23.6032],
        [23.5273, 25.5402, 25.3352]], grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.05667196214199066 
model_pd.l_d.mean(): -2.093601703643799 
model_pd.lagr.mean(): -2.0369298458099365 
model_pd.lambdas: dict_items([('pout', tensor([0.9587])), ('power', tensor([0.0532]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9180])), ('power', tensor([-4.7046]))])
epoch：914	 i:0 	 global-step:18280	 l-p:0.05667196214199066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[23.5886, 24.6662, 24.2583],
        [23.5886, 28.3809, 30.1815],
        [23.5886, 23.5915, 23.5886],
        [23.5886, 23.7407, 23.6164]], grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.056653812527656555 
model_pd.l_d.mean(): -2.0888428688049316 
model_pd.lagr.mean(): -2.032189130783081 
model_pd.lambdas: dict_items([('pout', tensor([0.9567])), ('power', tensor([0.0529]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9214])), ('power', tensor([-4.6437]))])
epoch：915	 i:0 	 global-step:18300	 l-p:0.056653812527656555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[23.6489, 23.6519, 23.6490],
        [23.6489, 24.4590, 24.0703],
        [23.6489, 23.7381, 23.6607],
        [23.6489, 29.9461, 33.3150]], grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.0566360279917717 
model_pd.l_d.mean(): -2.0840981006622314 
model_pd.lagr.mean(): -2.0274620056152344 
model_pd.lambdas: dict_items([('pout', tensor([0.9548])), ('power', tensor([0.0527]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9248])), ('power', tensor([-4.5837]))])
epoch：916	 i:0 	 global-step:18320	 l-p:0.0566360279917717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[23.7084, 23.7084, 23.7084],
        [23.7084, 23.7103, 23.7084],
        [23.7084, 23.7084, 23.7084],
        [23.7084, 23.7084, 23.7084]], grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.05661862716078758 
model_pd.l_d.mean(): -2.07936692237854 
model_pd.lagr.mean(): -2.0227482318878174 
model_pd.lambdas: dict_items([('pout', tensor([0.9529])), ('power', tensor([0.0525]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9280])), ('power', tensor([-4.5245]))])
epoch：917	 i:0 	 global-step:18340	 l-p:0.05661862716078758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[23.7669, 23.7752, 23.7672],
        [23.7669, 25.2551, 24.8841],
        [23.7669, 23.7688, 23.7669],
        [23.7669, 25.3231, 24.9660]], grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): 0.05660157650709152 
model_pd.l_d.mean(): -2.0746490955352783 
model_pd.lagr.mean(): -2.018047571182251 
model_pd.lambdas: dict_items([('pout', tensor([0.9510])), ('power', tensor([0.0522]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9313])), ('power', tensor([-4.4663]))])
epoch：918	 i:0 	 global-step:18360	 l-p:0.05660157650709152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[23.8245, 27.2813, 27.9076],
        [23.8245, 30.1726, 33.5693],
        [23.8245, 24.7931, 24.3846],
        [23.8245, 24.1108, 23.9015]], grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.05658488720655441 
model_pd.l_d.mean(): -2.069943428039551 
model_pd.lagr.mean(): -2.0133585929870605 
model_pd.lambdas: dict_items([('pout', tensor([0.9490])), ('power', tensor([0.0520]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9344])), ('power', tensor([-4.4091]))])
epoch：919	 i:0 	 global-step:18380	 l-p:0.05658488720655441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[23.8811, 23.9714, 23.8930],
        [23.8811, 23.9051, 23.8826],
        [23.8811, 24.0549, 23.9154],
        [23.8811, 24.1787, 23.9630]], grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.05656854808330536 
model_pd.l_d.mean(): -2.0652496814727783 
model_pd.lagr.mean(): -2.008681058883667 
model_pd.lambdas: dict_items([('pout', tensor([0.9471])), ('power', tensor([0.0518]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9376])), ('power', tensor([-4.3527]))])
epoch：920	 i:0 	 global-step:18400	 l-p:0.05656854808330536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[23.9368, 24.7088, 24.3232],
        [23.9368, 26.7111, 26.8668],
        [23.9368, 24.2352, 24.0190],
        [23.9368, 30.7512, 34.6647]], grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.05655256286263466 
model_pd.l_d.mean(): -2.060567617416382 
model_pd.lagr.mean(): -2.0040149688720703 
model_pd.lambdas: dict_items([('pout', tensor([0.9451])), ('power', tensor([0.0516]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9406])), ('power', tensor([-4.2973]))])
epoch：921	 i:0 	 global-step:18420	 l-p:0.05655256286263466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[23.9916, 24.0249, 23.9941],
        [23.9916, 23.9916, 23.9916],
        [23.9916, 24.1664, 24.0260],
        [23.9916, 28.8740, 30.7098]], grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.05653691291809082 
model_pd.l_d.mean(): -2.055896759033203 
model_pd.lagr.mean(): -1.9993598461151123 
model_pd.lambdas: dict_items([('pout', tensor([0.9432])), ('power', tensor([0.0514]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9436])), ('power', tensor([-4.2429]))])
epoch：922	 i:0 	 global-step:18440	 l-p:0.05653691291809082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[24.0455, 24.0455, 24.0455],
        [24.0455, 24.0469, 24.0455],
        [24.0455, 26.4960, 26.4642],
        [24.0455, 26.3324, 26.2203]], grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): 0.05652160197496414 
model_pd.l_d.mean(): -2.051236391067505 
model_pd.lagr.mean(): -1.9947147369384766 
model_pd.lambdas: dict_items([('pout', tensor([0.9413])), ('power', tensor([0.0512]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9465])), ('power', tensor([-4.1893]))])
epoch：923	 i:0 	 global-step:18460	 l-p:0.05652160197496414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[24.0984, 28.9639, 30.7699],
        [24.0984, 24.0984, 24.0984],
        [24.0984, 24.0996, 24.0984],
        [24.0984, 24.0985, 24.0984]], grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.05650663748383522 
model_pd.l_d.mean(): -2.046586513519287 
model_pd.lagr.mean(): -1.9900798797607422 
model_pd.lambdas: dict_items([('pout', tensor([0.9393])), ('power', tensor([0.0510]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9494])), ('power', tensor([-4.1367]))])
epoch：924	 i:0 	 global-step:18480	 l-p:0.05650663748383522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[24.1503, 32.4796, 38.2231],
        [24.1503, 24.1940, 24.1541],
        [24.1503, 24.1518, 24.1504],
        [24.1503, 31.8805, 36.8613]], grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.05649199336767197 
model_pd.l_d.mean(): -2.0419459342956543 
model_pd.lagr.mean(): -1.9854539632797241 
model_pd.lambdas: dict_items([('pout', tensor([0.9374])), ('power', tensor([0.0508]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9522])), ('power', tensor([-4.0850]))])
epoch：925	 i:0 	 global-step:18500	 l-p:0.05649199336767197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[24.2014, 24.2028, 24.2014],
        [24.2014, 30.7900, 34.3959],
        [24.2014, 26.8338, 26.8899],
        [24.2014, 24.8779, 24.5108]], grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.05647768825292587 
model_pd.l_d.mean(): -2.0373151302337646 
model_pd.lagr.mean(): -1.9808374643325806 
model_pd.lambdas: dict_items([('pout', tensor([0.9354])), ('power', tensor([0.0506]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9550])), ('power', tensor([-4.0342]))])
epoch：926	 i:0 	 global-step:18520	 l-p:0.05647768825292587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[24.2515, 29.1509, 30.9701],
        [24.2515, 24.2527, 24.2515],
        [24.2515, 29.1920, 31.0505],
        [24.2515, 25.8437, 25.4790]], grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.05646369978785515 
model_pd.l_d.mean(): -2.0326931476593018 
model_pd.lagr.mean(): -1.9762294292449951 
model_pd.lambdas: dict_items([('pout', tensor([0.9334])), ('power', tensor([0.0504]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9577])), ('power', tensor([-3.9844]))])
epoch：927	 i:0 	 global-step:18540	 l-p:0.05646369978785515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[24.3007, 24.3345, 24.3031],
        [24.3007, 25.8965, 25.5310],
        [24.3007, 28.0973, 28.9417],
        [24.3007, 26.8608, 26.8713]], grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.056450020521879196 
model_pd.l_d.mean(): -2.0280795097351074 
model_pd.lagr.mean(): -1.9716295003890991 
model_pd.lambdas: dict_items([('pout', tensor([0.9315])), ('power', tensor([0.0502]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9604])), ('power', tensor([-3.9355]))])
epoch：928	 i:0 	 global-step:18560	 l-p:0.056450020521879196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[24.3489, 25.4664, 25.0440],
        [24.3489, 24.6426, 24.4280],
        [24.3489, 24.4412, 24.3611],
        [24.3489, 26.6684, 26.5552]], grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.05643666163086891 
model_pd.l_d.mean(): -2.0234742164611816 
model_pd.lagr.mean(): -1.967037558555603 
model_pd.lambdas: dict_items([('pout', tensor([0.9295])), ('power', tensor([0.0500]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9630])), ('power', tensor([-3.8875]))])
epoch：929	 i:0 	 global-step:18580	 l-p:0.05643666163086891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[24.3963, 31.0427, 34.6809],
        [24.3963, 26.8870, 26.8554],
        [24.3963, 24.3963, 24.3963],
        [24.3963, 25.0791, 24.7086]], grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.0564236119389534 
model_pd.l_d.mean(): -2.018876552581787 
model_pd.lagr.mean(): -1.9624528884887695 
model_pd.lambdas: dict_items([('pout', tensor([0.9276])), ('power', tensor([0.0498]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9655])), ('power', tensor([-3.8404]))])
epoch：930	 i:0 	 global-step:18600	 l-p:0.0564236119389534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[24.4427, 29.4204, 31.2903],
        [24.4427, 24.4447, 24.4427],
        [24.4427, 27.1046, 27.1619],
        [24.4427, 24.4870, 24.4465]], grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.056410856544971466 
model_pd.l_d.mean(): -2.014286518096924 
model_pd.lagr.mean(): -1.9578756093978882 
model_pd.lambdas: dict_items([('pout', tensor([0.9256])), ('power', tensor([0.0496]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9680])), ('power', tensor([-3.7942]))])
epoch：931	 i:0 	 global-step:18620	 l-p:0.056410856544971466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[24.4882, 24.4914, 24.4883],
        [24.4882, 27.0706, 27.0816],
        [24.4882, 25.6130, 25.1879],
        [24.4882, 24.7947, 24.5726]], grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.056398406624794006 
model_pd.l_d.mean(): -2.0097036361694336 
model_pd.lagr.mean(): -1.9533052444458008 
model_pd.lambdas: dict_items([('pout', tensor([0.9236])), ('power', tensor([0.0494]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9704])), ('power', tensor([-3.7489]))])
epoch：932	 i:0 	 global-step:18640	 l-p:0.056398406624794006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[24.5329, 25.3270, 24.9306],
        [24.5329, 24.7122, 24.5682],
        [24.5329, 25.5346, 25.1125],
        [24.5329, 27.3846, 27.5459]], grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): 0.056386251002550125 
model_pd.l_d.mean(): -2.0051279067993164 
model_pd.lagr.mean(): -1.9487416744232178 
model_pd.lambdas: dict_items([('pout', tensor([0.9216])), ('power', tensor([0.0492]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9728])), ('power', tensor([-3.7045]))])
epoch：933	 i:0 	 global-step:18660	 l-p:0.056386251002550125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[24.5766, 31.5889, 35.6188],
        [24.5766, 24.8799, 24.6594],
        [24.5766, 26.6910, 26.4775],
        [24.5766, 26.1931, 25.8232]], grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.05637437850236893 
model_pd.l_d.mean(): -2.0005581378936768 
model_pd.lagr.mean(): -1.9441837072372437 
model_pd.lambdas: dict_items([('pout', tensor([0.9197])), ('power', tensor([0.0490]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9752])), ('power', tensor([-3.6610]))])
epoch：934	 i:0 	 global-step:18680	 l-p:0.05637437850236893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[24.6194, 25.1981, 24.8565],
        [24.6194, 24.6207, 24.6195],
        [24.6194, 26.1685, 25.7832],
        [24.6194, 24.9261, 24.7036]], grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): 0.05636279284954071 
model_pd.l_d.mean(): -1.9959945678710938 
model_pd.lagr.mean(): -1.9396318197250366 
model_pd.lambdas: dict_items([('pout', tensor([0.9177])), ('power', tensor([0.0489]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9774])), ('power', tensor([-3.6184]))])
epoch：935	 i:0 	 global-step:18700	 l-p:0.05636279284954071
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[24.6614, 24.8217, 24.6907],
        [24.6614, 25.6691, 25.2446],
        [24.6614, 24.6645, 24.6614],
        [24.6614, 24.6627, 24.6614]], grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.05635148659348488 
model_pd.l_d.mean(): -1.991437315940857 
model_pd.lagr.mean(): -1.9350857734680176 
model_pd.lambdas: dict_items([('pout', tensor([0.9157])), ('power', tensor([0.0487]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9797])), ('power', tensor([-3.5766]))])
epoch：936	 i:0 	 global-step:18720	 l-p:0.05635148659348488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[24.7025, 26.4094, 26.0569],
        [24.7025, 31.4397, 35.1289],
        [24.7025, 25.2834, 24.9405],
        [24.7025, 28.5688, 29.4299]], grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.05634044483304024 
model_pd.l_d.mean(): -1.9868853092193604 
model_pd.lagr.mean(): -1.9305448532104492 
model_pd.lambdas: dict_items([('pout', tensor([0.9137])), ('power', tensor([0.0485]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9818])), ('power', tensor([-3.5357]))])
epoch：937	 i:0 	 global-step:18740	 l-p:0.05634044483304024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[24.7427, 24.7427, 24.7427],
        [24.7427, 27.1044, 26.9900],
        [24.7427, 27.2732, 27.2418],
        [24.7427, 25.5447, 25.1445]], grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.05632968246936798 
model_pd.l_d.mean(): -1.9823390245437622 
model_pd.lagr.mean(): -1.9260092973709106 
model_pd.lambdas: dict_items([('pout', tensor([0.9117])), ('power', tensor([0.0483]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9840])), ('power', tensor([-3.4957]))])
epoch：938	 i:0 	 global-step:18760	 l-p:0.05632968246936798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[24.7821, 26.4953, 26.1415],
        [24.7821, 26.3428, 25.9548],
        [24.7821, 24.8071, 24.7836],
        [24.7821, 32.7312, 37.8560]], grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.056319184601306915 
model_pd.l_d.mean(): -1.9777977466583252 
model_pd.lagr.mean(): -1.921478509902954 
model_pd.lambdas: dict_items([('pout', tensor([0.9098])), ('power', tensor([0.0481]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9861])), ('power', tensor([-3.4565]))])
epoch：939	 i:0 	 global-step:18780	 l-p:0.056319184601306915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[24.8206, 25.8358, 25.4082],
        [24.8206, 24.8456, 24.8221],
        [24.8206, 24.8219, 24.8206],
        [24.8206, 31.9086, 35.9831]], grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.05630895122885704 
model_pd.l_d.mean(): -1.9732611179351807 
model_pd.lagr.mean(): -1.916952133178711 
model_pd.lambdas: dict_items([('pout', tensor([0.9078])), ('power', tensor([0.0480]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9881])), ('power', tensor([-3.4182]))])
epoch：940	 i:0 	 global-step:18800	 l-p:0.05630895122885704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228]])
 pt:tensor([[24.8583, 25.6645, 25.2622],
        [24.8583, 28.4823, 29.1418],
        [24.8583, 26.0026, 25.5704],
        [24.8583, 29.9292, 31.8356]], grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): 0.056298960000276566 
model_pd.l_d.mean(): -1.9687293767929077 
model_pd.lagr.mean(): -1.9124304056167603 
model_pd.lambdas: dict_items([('pout', tensor([0.9058])), ('power', tensor([0.0478]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9901])), ('power', tensor([-3.3807]))])
epoch：941	 i:0 	 global-step:18820	 l-p:0.056298960000276566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[24.8951, 27.0406, 26.8245],
        [24.8951, 26.4640, 26.0741],
        [24.8951, 25.5941, 25.2150],
        [24.8951, 25.2059, 24.9804]], grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.056289225816726685 
model_pd.l_d.mean(): -1.964201807975769 
model_pd.lagr.mean(): -1.9079126119613647 
model_pd.lambdas: dict_items([('pout', tensor([0.9038])), ('power', tensor([0.0476]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9920])), ('power', tensor([-3.3441]))])
epoch：942	 i:0 	 global-step:18840	 l-p:0.056289225816726685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[24.9311, 24.9765, 24.9350],
        [24.9311, 27.4834, 27.4521],
        [24.9311, 24.9311, 24.9311],
        [24.9311, 25.2396, 25.0154]], grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.0562797412276268 
model_pd.l_d.mean(): -1.9596790075302124 
model_pd.lagr.mean(): -1.9033992290496826 
model_pd.lambdas: dict_items([('pout', tensor([0.9018])), ('power', tensor([0.0475]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9939])), ('power', tensor([-3.3082]))])
epoch：943	 i:0 	 global-step:18860	 l-p:0.0562797412276268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[24.9663, 28.6079, 29.2710],
        [24.9663, 27.1188, 26.9022],
        [24.9663, 27.6924, 27.7523],
        [24.9663, 25.2781, 25.0520]], grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.05627049133181572 
model_pd.l_d.mean(): -1.9551599025726318 
model_pd.lagr.mean(): -1.898889422416687 
model_pd.lambdas: dict_items([('pout', tensor([0.8998])), ('power', tensor([0.0473]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9958])), ('power', tensor([-3.2732]))])
epoch：944	 i:0 	 global-step:18880	 l-p:0.05627049133181572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[25.0008, 25.0463, 25.0046],
        [25.0008, 27.7310, 27.7911],
        [25.0008, 25.3147, 25.0873],
        [25.0008, 25.0023, 25.0008]], grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.05626148730516434 
model_pd.l_d.mean(): -1.9506444931030273 
model_pd.lagr.mean(): -1.8943829536437988 
model_pd.lambdas: dict_items([('pout', tensor([0.8978])), ('power', tensor([0.0472]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9976])), ('power', tensor([-3.2389]))])
epoch：945	 i:0 	 global-step:18900	 l-p:0.05626148730516434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[25.0344, 25.0800, 25.0383],
        [25.0344, 25.3472, 25.1203],
        [25.0344, 26.6133, 26.2211],
        [25.0344, 30.1507, 32.0781]], grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.056252703070640564 
model_pd.l_d.mean(): -1.9461326599121094 
model_pd.lagr.mean(): -1.8898799419403076 
model_pd.lambdas: dict_items([('pout', tensor([0.8958])), ('power', tensor([0.0470]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9993])), ('power', tensor([-3.2055]))])
epoch：946	 i:0 	 global-step:18920	 l-p:0.056252703070640564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[25.0672, 25.7719, 25.3898],
        [25.0672, 25.3777, 25.1520],
        [25.0672, 25.0688, 25.0673],
        [25.0672, 27.8057, 27.8661]], grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.056244153529405594 
model_pd.l_d.mean(): -1.941624402999878 
model_pd.lagr.mean(): -1.8853802680969238 
model_pd.lambdas: dict_items([('pout', tensor([0.8938])), ('power', tensor([0.0468]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0010])), ('power', tensor([-3.1728]))])
epoch：947	 i:0 	 global-step:18940	 l-p:0.056244153529405594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[25.0993, 25.6912, 25.3419],
        [25.0993, 25.4036, 25.1813],
        [25.0993, 29.7774, 31.2876],
        [25.0993, 27.4996, 27.3840]], grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.05623582750558853 
model_pd.l_d.mean(): -1.9371196031570435 
model_pd.lagr.mean(): -1.880883812904358 
model_pd.lambdas: dict_items([('pout', tensor([0.8918])), ('power', tensor([0.0467]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0027])), ('power', tensor([-3.1409]))])
epoch：948	 i:0 	 global-step:18960	 l-p:0.05623582750558853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[25.1306, 26.1605, 25.7270],
        [25.1306, 28.7990, 29.4674],
        [25.1306, 25.1306, 25.1306],
        [25.1306, 26.7165, 26.3227]], grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.05622771754860878 
model_pd.l_d.mean(): -1.9326171875 
model_pd.lagr.mean(): -1.876389503479004 
model_pd.lambdas: dict_items([('pout', tensor([0.8898])), ('power', tensor([0.0465]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0044])), ('power', tensor([-3.1097]))])
epoch：949	 i:0 	 global-step:18980	 l-p:0.05622771754860878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[25.1612, 28.0951, 28.2626],
        [25.1612, 27.3327, 27.1146],
        [25.1612, 25.1616, 25.1612],
        [25.1612, 25.3254, 25.1913]], grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): 0.05621982738375664 
model_pd.l_d.mean(): -1.9281184673309326 
model_pd.lagr.mean(): -1.8718986511230469 
model_pd.lambdas: dict_items([('pout', tensor([0.8878])), ('power', tensor([0.0464]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0059])), ('power', tensor([-3.0793]))])
epoch：950	 i:0 	 global-step:19000	 l-p:0.05621982738375664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[25.1910, 26.0623, 25.6450],
        [25.1910, 33.2824, 38.5010],
        [25.1910, 25.2165, 25.1926],
        [25.1910, 25.2263, 25.1936]], grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.056212134659290314 
model_pd.l_d.mean(): -1.9236226081848145 
model_pd.lagr.mean(): -1.86741042137146 
model_pd.lambdas: dict_items([('pout', tensor([0.8858])), ('power', tensor([0.0462]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0075])), ('power', tensor([-3.0496]))])
epoch：951	 i:0 	 global-step:19020	 l-p:0.056212134659290314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[25.2201, 25.5373, 25.3076],
        [25.2201, 25.2202, 25.2202],
        [25.2201, 25.5357, 25.3068],
        [25.2201, 26.8851, 26.5049]], grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): 0.05620464310050011 
model_pd.l_d.mean(): -1.9191287755966187 
model_pd.lagr.mean(): -1.8629240989685059 
model_pd.lambdas: dict_items([('pout', tensor([0.8838])), ('power', tensor([0.0461]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0090])), ('power', tensor([-3.0206]))])
epoch：952	 i:0 	 global-step:19040	 l-p:0.05620464310050011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[25.2485, 25.2485, 25.2486],
        [25.2485, 25.2486, 25.2486],
        [25.2485, 25.2518, 25.2486],
        [25.2485, 28.9362, 29.6085]], grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): 0.05619737133383751 
model_pd.l_d.mean(): -1.914637804031372 
model_pd.lagr.mean(): -1.8584403991699219 
model_pd.lambdas: dict_items([('pout', tensor([0.8818])), ('power', tensor([0.0459]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0105])), ('power', tensor([-2.9923]))])
epoch：953	 i:0 	 global-step:19060	 l-p:0.05619737133383751
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[25.2762, 25.4620, 25.3129],
        [25.2762, 25.9877, 25.6020],
        [25.2762, 25.5943, 25.3639],
        [25.2762, 25.2783, 25.2763]], grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 0.056190282106399536 
model_pd.l_d.mean(): -1.910149097442627 
model_pd.lagr.mean(): -1.8539588451385498 
model_pd.lambdas: dict_items([('pout', tensor([0.8797])), ('power', tensor([0.0458]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0119])), ('power', tensor([-2.9648]))])
epoch：954	 i:0 	 global-step:19080	 l-p:0.056190282106399536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[25.3033, 26.1790, 25.7596],
        [25.3033, 26.9745, 26.5930],
        [25.3033, 25.3033, 25.3033],
        [25.3033, 25.3494, 25.3072]], grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.05618339031934738 
model_pd.l_d.mean(): -1.9056625366210938 
model_pd.lagr.mean(): -1.8494791984558105 
model_pd.lambdas: dict_items([('pout', tensor([0.8777])), ('power', tensor([0.0456]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0133])), ('power', tensor([-2.9379]))])
epoch：955	 i:0 	 global-step:19100	 l-p:0.05618339031934738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[25.3296, 28.2856, 28.4549],
        [25.3296, 26.3690, 25.9315],
        [25.3296, 27.7549, 27.6386],
        [25.3296, 25.3312, 25.3296]], grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.05617668107151985 
model_pd.l_d.mean(): -1.9011785984039307 
model_pd.lagr.mean(): -1.8450019359588623 
model_pd.lambdas: dict_items([('pout', tensor([0.8757])), ('power', tensor([0.0455]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0147])), ('power', tensor([-2.9117]))])
epoch：956	 i:0 	 global-step:19120	 l-p:0.05617668107151985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[25.3552, 28.3147, 28.4842],
        [25.3552, 29.0604, 29.7362],
        [25.3552, 25.6699, 25.4412],
        [25.3552, 25.4015, 25.3592]], grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.056170158088207245 
model_pd.l_d.mean(): -1.8966963291168213 
model_pd.lagr.mean(): -1.8405262231826782 
model_pd.lambdas: dict_items([('pout', tensor([0.8737])), ('power', tensor([0.0453]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0160])), ('power', tensor([-2.8861]))])
epoch：957	 i:0 	 global-step:19140	 l-p:0.056170158088207245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[25.3803, 25.3807, 25.3803],
        [25.3803, 25.4266, 25.3842],
        [25.3803, 34.1685, 40.2351],
        [25.3803, 29.0895, 29.7662]], grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): 0.05616381764411926 
model_pd.l_d.mean(): -1.8922160863876343 
model_pd.lagr.mean(): -1.8360522985458374 
model_pd.lambdas: dict_items([('pout', tensor([0.8717])), ('power', tensor([0.0452]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0173])), ('power', tensor([-2.8612]))])
epoch：958	 i:0 	 global-step:19160	 l-p:0.05616381764411926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[25.4046, 25.4403, 25.4072],
        [25.4046, 26.2845, 25.8632],
        [25.4046, 26.7571, 26.3227],
        [25.4046, 30.5988, 32.5537]], grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.05615765601396561 
model_pd.l_d.mean(): -1.8877372741699219 
model_pd.lagr.mean(): -1.831579566001892 
model_pd.lambdas: dict_items([('pout', tensor([0.8697])), ('power', tensor([0.0450]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0186])), ('power', tensor([-2.8370]))])
epoch：959	 i:0 	 global-step:19180	 l-p:0.05615765601396561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[25.4284, 25.4284, 25.4284],
        [25.4284, 32.2437, 35.8974],
        [25.4284, 25.5255, 25.4412],
        [25.4284, 25.4541, 25.4299]], grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.056151650846004486 
model_pd.l_d.mean(): -1.8832603693008423 
model_pd.lagr.mean(): -1.8271087408065796 
model_pd.lambdas: dict_items([('pout', tensor([0.8676])), ('power', tensor([0.0449]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0198])), ('power', tensor([-2.8134]))])
epoch：960	 i:0 	 global-step:19200	 l-p:0.056151650846004486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[25.4515, 32.2736, 35.9311],
        [25.4515, 25.4516, 25.4515],
        [25.4515, 25.4536, 25.4515],
        [25.4515, 28.4237, 28.5942]], grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.05614582449197769 
model_pd.l_d.mean(): -1.8787853717803955 
model_pd.lagr.mean(): -1.8226395845413208 
model_pd.lambdas: dict_items([('pout', tensor([0.8656])), ('power', tensor([0.0448]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0210])), ('power', tensor([-2.7904]))])
epoch：961	 i:0 	 global-step:19220	 l-p:0.05614582449197769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228]])
 pt:tensor([[25.4740, 26.1920, 25.8028],
        [25.4740, 29.7071, 30.7895],
        [25.4740, 27.1582, 26.7741],
        [25.4740, 27.2421, 26.8781]], grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.05614016577601433 
model_pd.l_d.mean(): -1.8743120431900024 
model_pd.lagr.mean(): -1.8181718587875366 
model_pd.lambdas: dict_items([('pout', tensor([0.8636])), ('power', tensor([0.0446]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0221])), ('power', tensor([-2.7680]))])
epoch：962	 i:0 	 global-step:19240	 l-p:0.05614016577601433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[25.4959, 30.7167, 32.6854],
        [25.4959, 32.7944, 36.9932],
        [25.4959, 25.4973, 25.4959],
        [25.4959, 26.2147, 25.8251]], grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.056134652346372604 
model_pd.l_d.mean(): -1.8698396682739258 
model_pd.lagr.mean(): -1.8137049674987793 
model_pd.lambdas: dict_items([('pout', tensor([0.8616])), ('power', tensor([0.0445]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0233])), ('power', tensor([-2.7461]))])
epoch：963	 i:0 	 global-step:19260	 l-p:0.056134652346372604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[25.5172, 25.5431, 25.5188],
        [25.5172, 30.7429, 32.7135],
        [25.5172, 26.3485, 25.9341],
        [25.5172, 25.8276, 25.6009]], grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.05612931400537491 
model_pd.l_d.mean(): -1.8653688430786133 
model_pd.lagr.mean(): -1.8092395067214966 
model_pd.lambdas: dict_items([('pout', tensor([0.8596])), ('power', tensor([0.0444]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0244])), ('power', tensor([-2.7249]))])
epoch：964	 i:0 	 global-step:19280	 l-p:0.05612931400537491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[25.5380, 29.5506, 30.4471],
        [25.5380, 30.3071, 31.8484],
        [25.5380, 32.8497, 37.0563],
        [25.5380, 25.6357, 25.5509]], grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.056124113500118256 
model_pd.l_d.mean(): -1.8608992099761963 
model_pd.lagr.mean(): -1.8047751188278198 
model_pd.lambdas: dict_items([('pout', tensor([0.8575])), ('power', tensor([0.0442]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0254])), ('power', tensor([-2.7042]))])
epoch：965	 i:0 	 global-step:19300	 l-p:0.056124113500118256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[25.5582, 25.5672, 25.5585],
        [25.5582, 30.7932, 32.7675],
        [25.5582, 25.8759, 25.6450],
        [25.5582, 25.5582, 25.5582]], grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.05611906945705414 
model_pd.l_d.mean(): -1.856431007385254 
model_pd.lagr.mean(): -1.8003119230270386 
model_pd.lambdas: dict_items([('pout', tensor([0.8555])), ('power', tensor([0.0441]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0265])), ('power', tensor([-2.6841]))])
epoch：966	 i:0 	 global-step:19320	 l-p:0.05611906945705414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[25.5778, 25.5783, 25.5778],
        [25.5778, 29.3197, 30.0030],
        [25.5778, 28.2054, 28.1748],
        [25.5778, 26.9411, 26.5034]], grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.05611416697502136 
model_pd.l_d.mean(): -1.8519642353057861 
model_pd.lagr.mean(): -1.7958500385284424 
model_pd.lambdas: dict_items([('pout', tensor([0.8535])), ('power', tensor([0.0439]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0275])), ('power', tensor([-2.6646]))])
epoch：967	 i:0 	 global-step:19340	 l-p:0.05611416697502136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[25.5969, 26.9614, 26.5234],
        [25.5969, 25.7645, 25.6277],
        [25.5969, 25.6002, 25.5970],
        [25.5969, 25.6060, 25.5973]], grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): 0.05610940605401993 
model_pd.l_d.mean(): -1.8474980592727661 
model_pd.lagr.mean(): -1.7913886308670044 
model_pd.lambdas: dict_items([('pout', tensor([0.8514])), ('power', tensor([0.0438]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0284])), ('power', tensor([-2.6456]))])
epoch：968	 i:0 	 global-step:19360	 l-p:0.05610940605401993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[25.6155, 29.3636, 30.0482],
        [25.6155, 30.8201, 32.7579],
        [25.6155, 25.9273, 25.6996],
        [25.6155, 25.6155, 25.6155]], grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.056104790419340134 
model_pd.l_d.mean(): -1.8430330753326416 
model_pd.lagr.mean(): -1.7869282960891724 
model_pd.lambdas: dict_items([('pout', tensor([0.8494])), ('power', tensor([0.0437]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0294])), ('power', tensor([-2.6271]))])
epoch：969	 i:0 	 global-step:19380	 l-p:0.056104790419340134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[25.6336, 27.4146, 27.0482],
        [25.6336, 25.6805, 25.6376],
        [25.6336, 30.8423, 32.7817],
        [25.6336, 25.9524, 25.7207]], grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.05610029399394989 
model_pd.l_d.mean(): -1.8385690450668335 
model_pd.lagr.mean(): -1.7824687957763672 
model_pd.lambdas: dict_items([('pout', tensor([0.8474])), ('power', tensor([0.0436]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0303])), ('power', tensor([-2.6091]))])
epoch：970	 i:0 	 global-step:19400	 l-p:0.05610029399394989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[25.6512, 25.6512, 25.6512],
        [25.6512, 28.4622, 28.5258],
        [25.6512, 32.9985, 37.2262],
        [25.6512, 27.3490, 26.9621]], grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.05609594285488129 
model_pd.l_d.mean(): -1.8341060876846313 
model_pd.lagr.mean(): -1.7780101299285889 
model_pd.lambdas: dict_items([('pout', tensor([0.8454])), ('power', tensor([0.0434]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0312])), ('power', tensor([-2.5916]))])
epoch：971	 i:0 	 global-step:19420	 l-p:0.05609594285488129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[25.6682, 25.8575, 25.7056],
        [25.6682, 27.0372, 26.5978],
        [25.6682, 30.9225, 32.9012],
        [25.6682, 25.6684, 25.6682]], grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.05609171465039253 
model_pd.l_d.mean(): -1.829643726348877 
model_pd.lagr.mean(): -1.7735520601272583 
model_pd.lambdas: dict_items([('pout', tensor([0.8433])), ('power', tensor([0.0433]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0321])), ('power', tensor([-2.5746]))])
epoch：972	 i:0 	 global-step:19440	 l-p:0.05609171465039253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[25.6848, 26.4099, 26.0170],
        [25.6848, 25.7209, 25.6875],
        [25.6848, 28.5002, 28.5639],
        [25.6848, 25.8743, 25.7222]], grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.056087613105773926 
model_pd.l_d.mean(): -1.8251824378967285 
model_pd.lagr.mean(): -1.7690948247909546 
model_pd.lambdas: dict_items([('pout', tensor([0.8413])), ('power', tensor([0.0432]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0329])), ('power', tensor([-2.5581]))])
epoch：973	 i:0 	 global-step:19460	 l-p:0.056087613105773926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[25.7009, 25.8905, 25.7383],
        [25.7009, 29.4632, 30.1508],
        [25.7009, 28.7065, 28.8797],
        [25.7009, 25.7010, 25.7009]], grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.05608362704515457 
model_pd.l_d.mean(): -1.820721983909607 
model_pd.lagr.mean(): -1.7646383047103882 
model_pd.lambdas: dict_items([('pout', tensor([0.8393])), ('power', tensor([0.0430]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0338])), ('power', tensor([-2.5420]))])
epoch：974	 i:0 	 global-step:19480	 l-p:0.05608362704515457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[25.7166, 25.7167, 25.7166],
        [25.7166, 27.5044, 27.1367],
        [25.7166, 32.7570, 36.6171],
        [25.7166, 27.9433, 27.7208]], grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.056079767644405365 
model_pd.l_d.mean(): -1.816261887550354 
model_pd.lagr.mean(): -1.7601821422576904 
model_pd.lambdas: dict_items([('pout', tensor([0.8372])), ('power', tensor([0.0429]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0346])), ('power', tensor([-2.5264]))])
epoch：975	 i:0 	 global-step:19500	 l-p:0.056079767644405365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[25.7318, 25.7318, 25.7318],
        [25.7318, 25.7334, 25.7318],
        [25.7318, 25.7319, 25.7318],
        [25.7318, 26.0567, 25.8214]], grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.05607602000236511 
model_pd.l_d.mean(): -1.8118029832839966 
model_pd.lagr.mean(): -1.755726933479309 
model_pd.lambdas: dict_items([('pout', tensor([0.8352])), ('power', tensor([0.0428]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0353])), ('power', tensor([-2.5113]))])
epoch：976	 i:0 	 global-step:19520	 l-p:0.05607602000236511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[25.7466, 25.9366, 25.7840],
        [25.7466, 25.7467, 25.7466],
        [25.7466, 25.7557, 25.7469],
        [25.7466, 25.7479, 25.7466]], grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.05607238784432411 
model_pd.l_d.mean(): -1.8073445558547974 
model_pd.lagr.mean(): -1.751272201538086 
model_pd.lambdas: dict_items([('pout', tensor([0.8332])), ('power', tensor([0.0427]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0361])), ('power', tensor([-2.4966]))])
epoch：977	 i:0 	 global-step:19540	 l-p:0.05607238784432411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[25.7609, 27.1357, 26.6946],
        [25.7609, 34.6927, 40.8609],
        [25.7609, 28.4959, 28.5108],
        [25.7609, 25.8596, 25.7740]], grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.05606886371970177 
model_pd.l_d.mean(): -1.8028863668441772 
model_pd.lagr.mean(): -1.7468174695968628 
model_pd.lambdas: dict_items([('pout', tensor([0.8311])), ('power', tensor([0.0425]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0368])), ('power', tensor([-2.4823]))])
epoch：978	 i:0 	 global-step:19560	 l-p:0.05606886371970177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[25.7748, 27.1505, 26.7091],
        [25.7748, 28.5116, 28.5265],
        [25.7748, 25.7748, 25.7748],
        [25.7748, 33.1613, 37.4123]], grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.056065451353788376 
model_pd.l_d.mean(): -1.798429250717163 
model_pd.lagr.mean(): -1.7423638105392456 
model_pd.lambdas: dict_items([('pout', tensor([0.8291])), ('power', tensor([0.0424]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0375])), ('power', tensor([-2.4685]))])
epoch：979	 i:0 	 global-step:19580	 l-p:0.056065451353788376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[25.7883, 31.0762, 33.0716],
        [25.7883, 25.7975, 25.7887],
        [25.7883, 25.7883, 25.7883],
        [25.7883, 26.6302, 26.2106]], grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.05606213957071304 
model_pd.l_d.mean(): -1.7939724922180176 
model_pd.lagr.mean(): -1.7379103899002075 
model_pd.lambdas: dict_items([('pout', tensor([0.8270])), ('power', tensor([0.0423]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0382])), ('power', tensor([-2.4550]))])
epoch：980	 i:0 	 global-step:19600	 l-p:0.05606213957071304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[25.8014, 28.6316, 28.6961],
        [25.8014, 25.8378, 25.8041],
        [25.8014, 29.5806, 30.2717],
        [25.8014, 26.8637, 26.4171]], grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.05605892091989517 
model_pd.l_d.mean(): -1.7895163297653198 
model_pd.lagr.mean(): -1.7334574460983276 
model_pd.lambdas: dict_items([('pout', tensor([0.8250])), ('power', tensor([0.0422]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0389])), ('power', tensor([-2.4420]))])
epoch：981	 i:0 	 global-step:19620	 l-p:0.05605892091989517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[25.8142, 27.4503, 27.0451],
        [25.8142, 25.8146, 25.8142],
        [25.8142, 26.0048, 25.8518],
        [25.8142, 27.5248, 27.1352]], grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.05605581775307655 
model_pd.l_d.mean(): -1.7850605249404907 
model_pd.lagr.mean(): -1.7290047407150269 
model_pd.lambdas: dict_items([('pout', tensor([0.8230])), ('power', tensor([0.0421]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0395])), ('power', tensor([-2.4293]))])
epoch：982	 i:0 	 global-step:19640	 l-p:0.05605581775307655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[25.8265, 25.8298, 25.8266],
        [25.8265, 32.9003, 36.7795],
        [25.8265, 26.1483, 25.9145],
        [25.8265, 25.8265, 25.8265]], grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): 0.0560528002679348 
model_pd.l_d.mean(): -1.7806053161621094 
model_pd.lagr.mean(): -1.7245525121688843 
model_pd.lambdas: dict_items([('pout', tensor([0.8209])), ('power', tensor([0.0419]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0402])), ('power', tensor([-2.4170]))])
epoch：983	 i:0 	 global-step:19660	 l-p:0.0560528002679348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[25.8385, 25.8749, 25.8411],
        [25.8385, 25.8385, 25.8385],
        [25.8385, 26.7363, 26.3067],
        [25.8385, 25.9376, 25.8516]], grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.05604987591505051 
model_pd.l_d.mean(): -1.7761503458023071 
model_pd.lagr.mean(): -1.7201005220413208 
model_pd.lambdas: dict_items([('pout', tensor([0.8189])), ('power', tensor([0.0418]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0408])), ('power', tensor([-2.4051]))])
epoch：984	 i:0 	 global-step:19680	 l-p:0.05604987591505051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[25.8501, 29.9186, 30.8289],
        [25.8501, 27.2306, 26.7877],
        [25.8501, 25.8534, 25.8501],
        [25.8501, 25.8501, 25.8501]], grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.05604703724384308 
model_pd.l_d.mean(): -1.771695852279663 
model_pd.lagr.mean(): -1.7156487703323364 
model_pd.lambdas: dict_items([('pout', tensor([0.8168])), ('power', tensor([0.0417]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0414])), ('power', tensor([-2.3935]))])
epoch：985	 i:0 	 global-step:19700	 l-p:0.05604703724384308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[25.8613, 33.2754, 37.5428],
        [25.8613, 25.9605, 25.8744],
        [25.8613, 26.9266, 26.4788],
        [25.8613, 25.8876, 25.8629]], grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.05604429543018341 
model_pd.l_d.mean(): -1.7672417163848877 
model_pd.lagr.mean(): -1.7111973762512207 
model_pd.lambdas: dict_items([('pout', tensor([0.8148])), ('power', tensor([0.0416]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0419])), ('power', tensor([-2.3823]))])
epoch：986	 i:0 	 global-step:19720	 l-p:0.05604429543018341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[25.8722, 25.8724, 25.8723],
        [25.8722, 34.8465, 41.0450],
        [25.8722, 25.9197, 25.8763],
        [25.8722, 27.5128, 27.1066]], grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.056041643023490906 
model_pd.l_d.mean(): -1.76278817653656 
model_pd.lagr.mean(): -1.7067465782165527 
model_pd.lambdas: dict_items([('pout', tensor([0.8128])), ('power', tensor([0.0415]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0425])), ('power', tensor([-2.3715]))])
epoch：987	 i:0 	 global-step:19740	 l-p:0.056041643023490906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[25.8828, 28.9131, 29.0885],
        [25.8828, 26.7825, 26.3521],
        [25.8828, 25.9193, 25.8855],
        [25.8828, 25.8830, 25.8828]], grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.05603907257318497 
model_pd.l_d.mean(): -1.7583346366882324 
model_pd.lagr.mean(): -1.7022955417633057 
model_pd.lambdas: dict_items([('pout', tensor([0.8107])), ('power', tensor([0.0413]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0430])), ('power', tensor([-2.3609]))])
epoch：988	 i:0 	 global-step:19760	 l-p:0.05603907257318497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[25.8931, 31.1614, 33.1243],
        [25.8931, 25.9194, 25.8947],
        [25.8931, 27.2764, 26.8327],
        [25.8931, 26.2188, 25.9826]], grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.056036576628685 
model_pd.l_d.mean(): -1.7538816928863525 
model_pd.lagr.mean(): -1.6978451013565063 
model_pd.lambdas: dict_items([('pout', tensor([0.8087])), ('power', tensor([0.0412]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0435])), ('power', tensor([-2.3507]))])
epoch：989	 i:0 	 global-step:19780	 l-p:0.056036576628685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[25.9030, 34.2448, 39.6293],
        [25.9030, 26.5176, 26.1551],
        [25.9030, 29.6994, 30.3941],
        [25.9030, 26.0731, 25.9342]], grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): 0.056034162640571594 
model_pd.l_d.mean(): -1.7494289875030518 
model_pd.lagr.mean(): -1.6933947801589966 
model_pd.lambdas: dict_items([('pout', tensor([0.8066])), ('power', tensor([0.0411]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0440])), ('power', tensor([-2.3408]))])
epoch：990	 i:0 	 global-step:19800	 l-p:0.056034162640571594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[25.9126, 28.9471, 29.1228],
        [25.9126, 25.9142, 25.9127],
        [25.9126, 26.1042, 25.9504],
        [25.9126, 34.2578, 39.6447]], grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.05603182688355446 
model_pd.l_d.mean(): -1.744976282119751 
model_pd.lagr.mean(): -1.6889444589614868 
model_pd.lambdas: dict_items([('pout', tensor([0.8046])), ('power', tensor([0.0410]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0445])), ('power', tensor([-2.3312]))])
epoch：991	 i:0 	 global-step:19820	 l-p:0.05603182688355446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[25.9219, 25.9220, 25.9220],
        [25.9219, 30.0036, 30.9172],
        [25.9219, 26.9903, 26.5412],
        [25.9219, 26.0922, 25.9531]], grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.056029561907052994 
model_pd.l_d.mean(): -1.7405245304107666 
model_pd.lagr.mean(): -1.684494972229004 
model_pd.lambdas: dict_items([('pout', tensor([0.8025])), ('power', tensor([0.0409]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0450])), ('power', tensor([-2.3220]))])
epoch：992	 i:0 	 global-step:19840	 l-p:0.056029561907052994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[25.9310, 25.9402, 25.9313],
        [25.9310, 25.9310, 25.9310],
        [25.9310, 26.6646, 26.2672],
        [25.9310, 25.9314, 25.9310]], grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.056027378886938095 
model_pd.l_d.mean(): -1.7360725402832031 
model_pd.lagr.mean(): -1.6800451278686523 
model_pd.lambdas: dict_items([('pout', tensor([0.8005])), ('power', tensor([0.0408]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0455])), ('power', tensor([-2.3130]))])
epoch：993	 i:0 	 global-step:19860	 l-p:0.056027378886938095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[25.9397, 26.7877, 26.3652],
        [25.9397, 26.2564, 26.0251],
        [25.9397, 25.9411, 25.9397],
        [25.9397, 26.6737, 26.2761]], grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.05602525547146797 
model_pd.l_d.mean(): -1.7316209077835083 
model_pd.lagr.mean(): -1.6755956411361694 
model_pd.lambdas: dict_items([('pout', tensor([0.7984])), ('power', tensor([0.0406]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0459])), ('power', tensor([-2.3043]))])
epoch：994	 i:0 	 global-step:19880	 l-p:0.05602525547146797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[25.9482, 33.3902, 37.6744],
        [25.9482, 25.9498, 25.9482],
        [25.9482, 26.2650, 26.0336],
        [25.9482, 25.9958, 25.9523]], grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.05602320656180382 
model_pd.l_d.mean(): -1.727169156074524 
model_pd.lagr.mean(): -1.6711459159851074 
model_pd.lambdas: dict_items([('pout', tensor([0.7964])), ('power', tensor([0.0405]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0463])), ('power', tensor([-2.2958]))])
epoch：995	 i:0 	 global-step:19900	 l-p:0.05602320656180382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[25.9564, 26.0561, 25.9696],
        [25.9564, 28.4516, 28.3338],
        [25.9564, 25.9565, 25.9564],
        [25.9564, 26.2802, 26.0449]], grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): 0.056021228432655334 
model_pd.l_d.mean(): -1.7227180004119873 
model_pd.lagr.mean(): -1.6666967868804932 
model_pd.lambdas: dict_items([('pout', tensor([0.7943])), ('power', tensor([0.0404]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0467])), ('power', tensor([-2.2877]))])
epoch：996	 i:0 	 global-step:19920	 l-p:0.056021228432655334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[25.9643, 25.9643, 25.9643],
        [25.9643, 28.8157, 28.8813],
        [25.9643, 30.2904, 31.3990],
        [25.9643, 26.1349, 25.9956]], grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.05601930618286133 
model_pd.l_d.mean(): -1.7182669639587402 
model_pd.lagr.mean(): -1.662247657775879 
model_pd.lambdas: dict_items([('pout', tensor([0.7923])), ('power', tensor([0.0403]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0471])), ('power', tensor([-2.2797]))])
epoch：997	 i:0 	 global-step:19940	 l-p:0.05601930618286133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[25.9720, 30.8333, 32.4067],
        [25.9720, 25.9720, 25.9720],
        [25.9720, 25.9720, 25.9720],
        [25.9720, 25.9734, 25.9720]], grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.056017450988292694 
model_pd.l_d.mean(): -1.7138159275054932 
model_pd.lagr.mean(): -1.6577985286712646 
model_pd.lambdas: dict_items([('pout', tensor([0.7903])), ('power', tensor([0.0402]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0475])), ('power', tensor([-2.2721]))])
epoch：998	 i:0 	 global-step:19960	 l-p:0.056017450988292694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[25.9794, 26.0161, 25.9821],
        [25.9794, 26.5963, 26.2325],
        [25.9794, 31.3062, 33.3140],
        [25.9794, 32.9591, 36.7043]], grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.056015659123659134 
model_pd.l_d.mean(): -1.7093654870986938 
model_pd.lagr.mean(): -1.6533498764038086 
model_pd.lambdas: dict_items([('pout', tensor([0.7882])), ('power', tensor([0.0401]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0479])), ('power', tensor([-2.2647]))])
epoch：999	 i:0 	 global-step:19980	 l-p:0.056015659123659134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[25.9866, 28.8410, 28.9068],
        [25.9866, 30.8511, 32.4257],
        [25.9866, 30.3171, 31.4270],
        [25.9866, 25.9868, 25.9866]], grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.05601393058896065 
model_pd.l_d.mean(): -1.704914927482605 
model_pd.lagr.mean(): -1.6489009857177734 
model_pd.lambdas: dict_items([('pout', tensor([0.7862])), ('power', tensor([0.0400]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0483])), ('power', tensor([-2.2575]))])
epoch：1000	 i:0 	 global-step:20000	 l-p:0.05601393058896065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[25.9936, 27.0657, 26.6151],
        [25.9936, 26.6109, 26.2469],
        [25.9936, 30.8596, 32.4348],
        [25.9936, 26.0303, 25.9963]], grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): 0.05601225048303604 
model_pd.l_d.mean(): -1.7004646062850952 
model_pd.lagr.mean(): -1.6444523334503174 
model_pd.lambdas: dict_items([('pout', tensor([0.7841])), ('power', tensor([0.0398]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0486])), ('power', tensor([-2.2506]))])
epoch：1001	 i:0 	 global-step:20020	 l-p:0.05601225048303604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.0003, 28.5007, 28.3828],
        [26.0003, 26.0003, 26.0003],
        [26.0003, 26.1002, 26.0135],
        [26.0003, 26.0025, 26.0003]], grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.05601062625646591 
model_pd.l_d.mean(): -1.696014404296875 
model_pd.lagr.mean(): -1.6400038003921509 
model_pd.lambdas: dict_items([('pout', tensor([0.7821])), ('power', tensor([0.0397]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0490])), ('power', tensor([-2.2439]))])
epoch：1002	 i:0 	 global-step:20040	 l-p:0.05601062625646591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.0068, 26.0068, 26.0068],
        [26.0068, 35.0331, 41.2689],
        [26.0068, 26.6245, 26.2603],
        [26.0068, 31.3462, 33.3624]], grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.05600905790925026 
model_pd.l_d.mean(): -1.6915643215179443 
model_pd.lagr.mean(): -1.6355552673339844 
model_pd.lambdas: dict_items([('pout', tensor([0.7800])), ('power', tensor([0.0396]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0493])), ('power', tensor([-2.2374]))])
epoch：1003	 i:0 	 global-step:20060	 l-p:0.05600905790925026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.0131, 26.0224, 26.0134],
        [26.0131, 31.3480, 33.3591],
        [26.0131, 33.4763, 37.7734],
        [26.0131, 26.0147, 26.0131]], grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.05600754916667938 
model_pd.l_d.mean(): -1.6871142387390137 
model_pd.lagr.mean(): -1.6311067342758179 
model_pd.lambdas: dict_items([('pout', tensor([0.7780])), ('power', tensor([0.0395]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0496])), ('power', tensor([-2.2312]))])
epoch：1004	 i:0 	 global-step:20080	 l-p:0.05600754916667938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.0192, 26.1192, 26.0324],
        [26.0192, 26.0670, 26.0233],
        [26.0192, 30.1190, 31.0374],
        [26.0192, 30.3562, 31.4679]], grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.05600608512759209 
model_pd.l_d.mean(): -1.6826642751693726 
model_pd.lagr.mean(): -1.6266582012176514 
model_pd.lambdas: dict_items([('pout', tensor([0.7759])), ('power', tensor([0.0394]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0499])), ('power', tensor([-2.2251]))])
epoch：1005	 i:0 	 global-step:20100	 l-p:0.05600608512759209
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.0251, 26.0284, 26.0251],
        [26.0251, 33.0188, 36.7721],
        [26.0251, 33.4923, 37.7917],
        [26.0251, 26.0251, 26.0251]], grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.056004662066698074 
model_pd.l_d.mean(): -1.678214430809021 
model_pd.lagr.mean(): -1.6222097873687744 
model_pd.lambdas: dict_items([('pout', tensor([0.7739])), ('power', tensor([0.0393]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0502])), ('power', tensor([-2.2192]))])
epoch：1006	 i:0 	 global-step:20120	 l-p:0.056004662066698074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.0308, 28.7134, 28.6838],
        [26.0308, 26.0309, 26.0308],
        [26.0308, 28.8001, 28.8163],
        [26.0308, 27.4233, 26.9770]], grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): 0.05600329861044884 
model_pd.l_d.mean(): -1.6737650632858276 
model_pd.lagr.mean(): -1.6177617311477661 
model_pd.lambdas: dict_items([('pout', tensor([0.7718])), ('power', tensor([0.0392]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0505])), ('power', tensor([-2.2136]))])
epoch：1007	 i:0 	 global-step:20140	 l-p:0.05600329861044884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.0362, 26.0363, 26.0363],
        [26.0362, 31.3768, 33.3903],
        [26.0362, 26.0384, 26.0363],
        [26.0362, 29.0883, 29.2657]], grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.05600198358297348 
model_pd.l_d.mean(): -1.6693154573440552 
model_pd.lagr.mean(): -1.6133134365081787 
model_pd.lambdas: dict_items([('pout', tensor([0.7698])), ('power', tensor([0.0391]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0508])), ('power', tensor([-2.2081]))])
epoch：1008	 i:0 	 global-step:20160	 l-p:0.05600198358297348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.0416, 29.0944, 29.2718],
        [26.0416, 27.6955, 27.2864],
        [26.0416, 26.0416, 26.0415],
        [26.0416, 30.1457, 31.0652]], grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.05600070580840111 
model_pd.l_d.mean(): -1.664866328239441 
model_pd.lagr.mean(): -1.6088656187057495 
model_pd.lambdas: dict_items([('pout', tensor([0.7677])), ('power', tensor([0.0390]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0511])), ('power', tensor([-2.2028]))])
epoch：1009	 i:0 	 global-step:20180	 l-p:0.05600070580840111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.0467, 26.8992, 26.4746],
        [26.0467, 26.0488, 26.0467],
        [26.0467, 28.5527, 28.4348],
        [26.0467, 26.9537, 26.5199]], grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.05599946901202202 
model_pd.l_d.mean(): -1.6604169607162476 
model_pd.lagr.mean(): -1.6044174432754517 
model_pd.lambdas: dict_items([('pout', tensor([0.7657])), ('power', tensor([0.0388]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0513])), ('power', tensor([-2.1977]))])
epoch：1010	 i:0 	 global-step:20200	 l-p:0.05599946901202202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.0516, 33.0538, 36.8118],
        [26.0516, 26.0533, 26.0517],
        [26.0516, 28.9149, 28.9813],
        [26.0516, 26.3800, 26.1419]], grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.05599828064441681 
model_pd.l_d.mean(): -1.6559677124023438 
model_pd.lagr.mean(): -1.5999693870544434 
model_pd.lambdas: dict_items([('pout', tensor([0.7636])), ('power', tensor([0.0387]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0516])), ('power', tensor([-2.1928]))])
epoch：1011	 i:0 	 global-step:20220	 l-p:0.05599828064441681
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.0564, 26.7947, 26.3949],
        [26.0564, 26.0657, 26.0567],
        [26.0564, 26.3819, 26.1455],
        [26.0564, 26.2494, 26.0945]], grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.05599713325500488 
model_pd.l_d.mean(): -1.65151846408844 
model_pd.lagr.mean(): -1.595521330833435 
model_pd.lambdas: dict_items([('pout', tensor([0.7616])), ('power', tensor([0.0386]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0518])), ('power', tensor([-2.1880]))])
epoch：1012	 i:0 	 global-step:20240	 l-p:0.05599713325500488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.0610, 26.0703, 26.0614],
        [26.0610, 30.9423, 32.5229],
        [26.0610, 31.4138, 33.4355],
        [26.0610, 26.0610, 26.0610]], grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): 0.05599602311849594 
model_pd.l_d.mean(): -1.6470692157745361 
model_pd.lagr.mean(): -1.5910731554031372 
model_pd.lambdas: dict_items([('pout', tensor([0.7595])), ('power', tensor([0.0385]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0520])), ('power', tensor([-2.1834]))])
epoch：1013	 i:0 	 global-step:20260	 l-p:0.05599602311849594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.0655, 27.1416, 26.6894],
        [26.0655, 26.1023, 26.0682],
        [26.0655, 26.8042, 26.4042],
        [26.0655, 27.7215, 27.3120]], grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.05599495396018028 
model_pd.l_d.mean(): -1.6426199674606323 
model_pd.lagr.mean(): -1.5866249799728394 
model_pd.lambdas: dict_items([('pout', tensor([0.7575])), ('power', tensor([0.0384]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0523])), ('power', tensor([-2.1789]))])
epoch：1014	 i:0 	 global-step:20280	 l-p:0.05599495396018028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.0698, 26.1177, 26.0739],
        [26.0698, 31.4247, 33.4474],
        [26.0698, 29.1268, 29.3048],
        [26.0698, 31.4187, 33.4357]], grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.05599392578005791 
model_pd.l_d.mean(): -1.6381713151931763 
model_pd.lagr.mean(): -1.5821774005889893 
model_pd.lambdas: dict_items([('pout', tensor([0.7554])), ('power', tensor([0.0383]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0525])), ('power', tensor([-2.1747]))])
epoch：1015	 i:0 	 global-step:20300	 l-p:0.05599392578005791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.0740, 26.0740, 26.0740],
        [26.0740, 26.1743, 26.0872],
        [26.0740, 26.0740, 26.0740],
        [26.0740, 28.8492, 28.8657]], grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.05599292740225792 
model_pd.l_d.mean(): -1.6337224245071411 
model_pd.lagr.mean(): -1.5777294635772705 
model_pd.lambdas: dict_items([('pout', tensor([0.7533])), ('power', tensor([0.0382]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0527])), ('power', tensor([-2.1705]))])
epoch：1016	 i:0 	 global-step:20320	 l-p:0.05599292740225792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.0780, 26.0873, 26.0783],
        [26.0780, 33.0885, 36.8514],
        [26.0780, 29.9055, 30.6070],
        [26.0780, 30.9632, 32.5453]], grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.05599197372794151 
model_pd.l_d.mean(): -1.6292734146118164 
model_pd.lagr.mean(): -1.5732814073562622 
model_pd.lambdas: dict_items([('pout', tensor([0.7513])), ('power', tensor([0.0381]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0529])), ('power', tensor([-2.1665]))])
epoch：1017	 i:0 	 global-step:20340	 l-p:0.05599197372794151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.0818, 26.1085, 26.0834],
        [26.0818, 33.2356, 37.1610],
        [26.0818, 26.0818, 26.0818],
        [26.0818, 26.0840, 26.0819]], grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.0559910386800766 
model_pd.l_d.mean(): -1.62482488155365 
model_pd.lagr.mean(): -1.568833827972412 
model_pd.lambdas: dict_items([('pout', tensor([0.7492])), ('power', tensor([0.0380]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0531])), ('power', tensor([-2.1627]))])
epoch：1018	 i:0 	 global-step:20360	 l-p:0.0559910386800766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[26.0856, 33.2406, 37.1667],
        [26.0856, 26.4116, 26.1748],
        [26.0856, 27.8188, 27.4248],
        [26.0856, 33.0986, 36.8629]], grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.05599014833569527 
model_pd.l_d.mean(): -1.6203759908676147 
model_pd.lagr.mean(): -1.5643858909606934 
model_pd.lambdas: dict_items([('pout', tensor([0.7472])), ('power', tensor([0.0379]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0533])), ('power', tensor([-2.1589]))])
epoch：1019	 i:0 	 global-step:20380	 l-p:0.05599014833569527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.0892, 29.9188, 30.6209],
        [26.0892, 31.4490, 33.4739],
        [26.0892, 26.1896, 26.1025],
        [26.0892, 26.8288, 26.4283]], grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.055989284068346024 
model_pd.l_d.mean(): -1.6159273386001587 
model_pd.lagr.mean(): -1.5599380731582642 
model_pd.lambdas: dict_items([('pout', tensor([0.7451])), ('power', tensor([0.0378]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0535])), ('power', tensor([-2.1553]))])
epoch：1020	 i:0 	 global-step:20400	 l-p:0.055989284068346024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.0926, 26.9473, 26.5217],
        [26.0926, 26.4218, 26.1832],
        [26.0926, 26.1406, 26.0967],
        [26.0926, 35.1533, 41.4139]], grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.05598844960331917 
model_pd.l_d.mean(): -1.6114788055419922 
model_pd.lagr.mean(): -1.5554903745651245 
model_pd.lambdas: dict_items([('pout', tensor([0.7431])), ('power', tensor([0.0376]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0536])), ('power', tensor([-2.1519]))])
epoch：1021	 i:0 	 global-step:20420	 l-p:0.05598844960331917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.0960, 26.0960, 26.0960],
        [26.0960, 26.1440, 26.1001],
        [26.0960, 27.1738, 26.7211],
        [26.0960, 26.7167, 26.3508]], grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.055987648665905 
model_pd.l_d.mean(): -1.6070302724838257 
model_pd.lagr.mean(): -1.5510426759719849 
model_pd.lambdas: dict_items([('pout', tensor([0.7410])), ('power', tensor([0.0375]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0538])), ('power', tensor([-2.1485]))])
epoch：1022	 i:0 	 global-step:20440	 l-p:0.055987648665905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.0992, 31.4174, 33.4006],
        [26.0992, 27.3120, 26.8554],
        [26.0992, 28.7911, 28.7618],
        [26.0992, 26.4302, 26.1906]], grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.05598687753081322 
model_pd.l_d.mean(): -1.6025818586349487 
model_pd.lagr.mean(): -1.5465949773788452 
model_pd.lambdas: dict_items([('pout', tensor([0.7390])), ('power', tensor([0.0374]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0540])), ('power', tensor([-2.1453]))])
epoch：1023	 i:0 	 global-step:20460	 l-p:0.05598687753081322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[26.1023, 26.7233, 26.3572],
        [26.1023, 26.4218, 26.1885],
        [26.1023, 27.3153, 26.8586],
        [26.1023, 33.1209, 36.8884]], grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.055986128747463226 
model_pd.l_d.mean(): -1.5981330871582031 
model_pd.lagr.mean(): -1.542146921157837 
model_pd.lambdas: dict_items([('pout', tensor([0.7369])), ('power', tensor([0.0373]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0541])), ('power', tensor([-2.1422]))])
epoch：1024	 i:0 	 global-step:20480	 l-p:0.055986128747463226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.1054, 26.4347, 26.1960],
        [26.1054, 28.9764, 29.0434],
        [26.1054, 28.6189, 28.5010],
        [26.1054, 26.1534, 26.1095]], grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.05598541721701622 
model_pd.l_d.mean(): -1.5936845541000366 
model_pd.lagr.mean(): -1.5376991033554077 
model_pd.lambdas: dict_items([('pout', tensor([0.7349])), ('power', tensor([0.0372]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0543])), ('power', tensor([-2.1392]))])
epoch：1025	 i:0 	 global-step:20500	 l-p:0.05598541721701622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.1082, 27.5066, 27.0586],
        [26.1082, 26.1104, 26.1083],
        [26.1082, 26.2088, 26.1215],
        [26.1082, 28.9798, 29.0468]], grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.05598472058773041 
model_pd.l_d.mean(): -1.5892362594604492 
model_pd.lagr.mean(): -1.5332515239715576 
model_pd.lambdas: dict_items([('pout', tensor([0.7328])), ('power', tensor([0.0371]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0544])), ('power', tensor([-2.1363]))])
epoch：1026	 i:0 	 global-step:20520	 l-p:0.05598472058773041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.1110, 28.6254, 28.5075],
        [26.1110, 27.8466, 27.4523],
        [26.1110, 26.1124, 26.1111],
        [26.1110, 34.5296, 39.9662]], grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.05598405748605728 
model_pd.l_d.mean(): -1.5847878456115723 
model_pd.lagr.mean(): -1.528803825378418 
model_pd.lambdas: dict_items([('pout', tensor([0.7308])), ('power', tensor([0.0370]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0546])), ('power', tensor([-2.1335]))])
epoch：1027	 i:0 	 global-step:20540	 l-p:0.05598405748605728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.1137, 26.1159, 26.1138],
        [26.1137, 31.0075, 32.5929],
        [26.1137, 26.1618, 26.1178],
        [26.1137, 26.1137, 26.1137]], grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.05598340928554535 
model_pd.l_d.mean(): -1.5803394317626953 
model_pd.lagr.mean(): -1.5243560075759888 
model_pd.lambdas: dict_items([('pout', tensor([0.7287])), ('power', tensor([0.0369]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0547])), ('power', tensor([-2.1308]))])
epoch：1028	 i:0 	 global-step:20560	 l-p:0.05598340928554535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.1163, 26.1163, 26.1163],
        [26.1163, 28.3858, 28.1605],
        [26.1163, 35.1868, 41.4547],
        [26.1163, 26.1179, 26.1164]], grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.0559827946126461 
model_pd.l_d.mean(): -1.5758910179138184 
model_pd.lagr.mean(): -1.5199081897735596 
model_pd.lambdas: dict_items([('pout', tensor([0.7266])), ('power', tensor([0.0368]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0548])), ('power', tensor([-2.1283]))])
epoch：1029	 i:0 	 global-step:20580	 l-p:0.0559827946126461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[26.1188, 28.9006, 28.9175],
        [26.1188, 26.4455, 26.2082],
        [26.1188, 27.7796, 27.3692],
        [26.1188, 28.3886, 28.1634]], grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.05598220229148865 
model_pd.l_d.mean(): -1.5714426040649414 
model_pd.lagr.mean(): -1.5154603719711304 
model_pd.lambdas: dict_items([('pout', tensor([0.7246])), ('power', tensor([0.0367]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0550])), ('power', tensor([-2.1258]))])
epoch：1030	 i:0 	 global-step:20600	 l-p:0.05598220229148865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[26.1212, 34.5438, 39.9832],
        [26.1212, 33.1461, 36.9174],
        [26.1212, 27.2006, 26.7473],
        [26.1212, 28.3913, 28.1661]], grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.05598162114620209 
model_pd.l_d.mean(): -1.5669941902160645 
model_pd.lagr.mean(): -1.5110125541687012 
model_pd.lambdas: dict_items([('pout', tensor([0.7225])), ('power', tensor([0.0366]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0551])), ('power', tensor([-2.1234]))])
epoch：1031	 i:0 	 global-step:20620	 l-p:0.05598162114620209
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.1235, 35.1971, 41.4673],
        [26.1235, 26.1235, 26.1235],
        [26.1235, 30.4827, 31.6012],
        [26.1235, 27.7848, 27.3743]], grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.055981073528528214 
model_pd.l_d.mean(): -1.562545895576477 
model_pd.lagr.mean(): -1.5065648555755615 
model_pd.lambdas: dict_items([('pout', tensor([0.7205])), ('power', tensor([0.0365]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0552])), ('power', tensor([-2.1211]))])
epoch：1032	 i:0 	 global-step:20640	 l-p:0.055981073528528214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.1258, 26.1351, 26.1261],
        [26.1258, 26.9821, 26.5557],
        [26.1258, 27.7873, 27.3767],
        [26.1258, 31.4894, 33.5127]], grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): 0.05598054453730583 
model_pd.l_d.mean(): -1.5580976009368896 
model_pd.lagr.mean(): -1.5021170377731323 
model_pd.lambdas: dict_items([('pout', tensor([0.7184])), ('power', tensor([0.0364]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0553])), ('power', tensor([-2.1188]))])
epoch：1033	 i:0 	 global-step:20660	 l-p:0.05598054453730583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.1279, 26.1279, 26.1279],
        [26.1279, 30.4880, 31.6069],
        [26.1279, 26.4578, 26.2186],
        [26.1279, 26.8692, 26.4679]], grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.055980030447244644 
model_pd.l_d.mean(): -1.5536490678787231 
model_pd.lagr.mean(): -1.497668981552124 
model_pd.lambdas: dict_items([('pout', tensor([0.7164])), ('power', tensor([0.0363]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0554])), ('power', tensor([-2.1167]))])
epoch：1034	 i:0 	 global-step:20680	 l-p:0.055980030447244644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.1300, 26.4569, 26.2194],
        [26.1300, 28.4012, 28.1759],
        [26.1300, 34.5561, 39.9980],
        [26.1300, 29.0050, 29.0723]], grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.05597953870892525 
model_pd.l_d.mean(): -1.5492011308670044 
model_pd.lagr.mean(): -1.493221640586853 
model_pd.lambdas: dict_items([('pout', tensor([0.7143])), ('power', tensor([0.0362]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0555])), ('power', tensor([-2.1146]))])
epoch：1035	 i:0 	 global-step:20700	 l-p:0.05597953870892525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1319, 26.1324, 26.1320],
        [26.1319, 27.9561, 27.5823],
        [26.1319, 26.1333, 26.1320],
        [26.1319, 27.5322, 27.0837]], grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): 0.055979061871767044 
model_pd.l_d.mean(): -1.544752836227417 
model_pd.lagr.mean(): -1.4887738227844238 
model_pd.lambdas: dict_items([('pout', tensor([0.7123])), ('power', tensor([0.0361]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0556])), ('power', tensor([-2.1127]))])
epoch：1036	 i:0 	 global-step:20720	 l-p:0.055979061871767044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.1339, 31.0328, 32.6202],
        [26.1339, 30.2571, 31.1819],
        [26.1339, 28.9181, 28.9352],
        [26.1339, 26.1343, 26.1339]], grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): 0.05597860366106033 
model_pd.l_d.mean(): -1.5403045415878296 
model_pd.lagr.mean(): -1.484325885772705 
model_pd.lambdas: dict_items([('pout', tensor([0.7102])), ('power', tensor([0.0359]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0557])), ('power', tensor([-2.1108]))])
epoch：1037	 i:0 	 global-step:20740	 l-p:0.05597860366106033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.1357, 26.4657, 26.2265],
        [26.1357, 29.2035, 29.3827],
        [26.1357, 31.4637, 33.4513],
        [26.1357, 26.1838, 26.1398]], grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.05597816780209541 
model_pd.l_d.mean(): -1.5358563661575317 
model_pd.lagr.mean(): -1.4798781871795654 
model_pd.lambdas: dict_items([('pout', tensor([0.7081])), ('power', tensor([0.0358]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0558])), ('power', tensor([-2.1089]))])
epoch：1038	 i:0 	 global-step:20760	 l-p:0.05597816780209541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.1375, 26.1375, 26.1375],
        [26.1375, 27.2180, 26.7642],
        [26.1375, 26.1375, 26.1375],
        [26.1375, 26.7597, 26.3929]], grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): 0.05597774684429169 
model_pd.l_d.mean(): -1.5314081907272339 
model_pd.lagr.mean(): -1.4754304885864258 
model_pd.lambdas: dict_items([('pout', tensor([0.7061])), ('power', tensor([0.0357]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0559])), ('power', tensor([-2.1072]))])
epoch：1039	 i:0 	 global-step:20780	 l-p:0.05597774684429169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1392, 26.2399, 26.1525],
        [26.1392, 30.2636, 31.1889],
        [26.1392, 26.1392, 26.1392],
        [26.1392, 27.0510, 26.6151]], grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.055977340787649155 
model_pd.l_d.mean(): -1.526959776878357 
model_pd.lagr.mean(): -1.4709824323654175 
model_pd.lambdas: dict_items([('pout', tensor([0.7040])), ('power', tensor([0.0356]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0560])), ('power', tensor([-2.1055]))])
epoch：1040	 i:0 	 global-step:20800	 l-p:0.055977340787649155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.1408, 26.1409, 26.1408],
        [26.1408, 28.4136, 28.1883],
        [26.1408, 26.3348, 26.1791],
        [26.1408, 26.1430, 26.1408]], grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.05597695708274841 
model_pd.l_d.mean(): -1.5225119590759277 
model_pd.lagr.mean(): -1.466534972190857 
model_pd.lambdas: dict_items([('pout', tensor([0.7020])), ('power', tensor([0.0355]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0561])), ('power', tensor([-2.1038]))])
epoch：1041	 i:0 	 global-step:20820	 l-p:0.05597695708274841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1424, 26.1424, 26.1424],
        [26.1424, 26.3147, 26.1740],
        [26.1424, 28.6613, 28.5436],
        [26.1424, 26.1425, 26.1424]], grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.05597657337784767 
model_pd.l_d.mean(): -1.5180635452270508 
model_pd.lagr.mean(): -1.4620869159698486 
model_pd.lambdas: dict_items([('pout', tensor([0.6999])), ('power', tensor([0.0354]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0562])), ('power', tensor([-2.1023]))])
epoch：1042	 i:0 	 global-step:20840	 l-p:0.05597657337784767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.1439, 30.5080, 31.6281],
        [26.1439, 26.1439, 26.1439],
        [26.1439, 29.9848, 30.6897],
        [26.1439, 27.0012, 26.5744]], grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.05597621574997902 
model_pd.l_d.mean(): -1.513615369796753 
model_pd.lagr.mean(): -1.4576390981674194 
model_pd.lambdas: dict_items([('pout', tensor([0.6979])), ('power', tensor([0.0353]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0562])), ('power', tensor([-2.1007]))])
epoch：1043	 i:0 	 global-step:20860	 l-p:0.05597621574997902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.1453, 28.4188, 28.1935],
        [26.1453, 31.4762, 33.4651],
        [26.1453, 26.1453, 26.1453],
        [26.1453, 26.3394, 26.1836]], grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.05597586929798126 
model_pd.l_d.mean(): -1.5091673135757446 
model_pd.lagr.mean(): -1.4531913995742798 
model_pd.lambdas: dict_items([('pout', tensor([0.6958])), ('power', tensor([0.0352]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0563])), ('power', tensor([-2.0993]))])
epoch：1044	 i:0 	 global-step:20880	 l-p:0.05597586929798126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.1467, 26.4770, 26.2376],
        [26.1467, 28.4204, 28.1951],
        [26.1467, 26.4788, 26.2384],
        [26.1467, 26.1472, 26.1467]], grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.055975541472435 
model_pd.l_d.mean(): -1.5047191381454468 
model_pd.lagr.mean(): -1.4487435817718506 
model_pd.lambdas: dict_items([('pout', tensor([0.6938])), ('power', tensor([0.0351]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0564])), ('power', tensor([-2.0979]))])
epoch：1045	 i:0 	 global-step:20900	 l-p:0.055975541472435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1481, 31.5182, 33.5445],
        [26.1481, 26.1502, 26.1481],
        [26.1481, 29.9899, 30.6952],
        [26.1481, 26.4685, 26.2345]], grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): 0.05597522109746933 
model_pd.l_d.mean(): -1.5002710819244385 
model_pd.lagr.mean(): -1.444295883178711 
model_pd.lambdas: dict_items([('pout', tensor([0.6917])), ('power', tensor([0.0350]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0565])), ('power', tensor([-2.0966]))])
epoch：1046	 i:0 	 global-step:20920	 l-p:0.05597522109746933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.1493, 26.1495, 26.1494],
        [26.1493, 27.9756, 27.6015],
        [26.1493, 26.3218, 26.1810],
        [26.1493, 26.1587, 26.1497]], grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.05597491189837456 
model_pd.l_d.mean(): -1.495822787284851 
model_pd.lagr.mean(): -1.4398478269577026 
model_pd.lambdas: dict_items([('pout', tensor([0.6896])), ('power', tensor([0.0349]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0565])), ('power', tensor([-2.0953]))])
epoch：1047	 i:0 	 global-step:20940	 l-p:0.05597491189837456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1506, 26.1876, 26.1533],
        [26.1506, 29.0293, 29.0970],
        [26.1506, 26.3447, 26.1889],
        [26.1506, 26.1507, 26.1506]], grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.05597461387515068 
model_pd.l_d.mean(): -1.4913747310638428 
model_pd.lagr.mean(): -1.435400128364563 
model_pd.lambdas: dict_items([('pout', tensor([0.6876])), ('power', tensor([0.0348]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0566])), ('power', tensor([-2.0940]))])
epoch：1048	 i:0 	 global-step:20960	 l-p:0.05597461387515068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[26.1518, 29.0307, 29.0985],
        [26.1518, 27.2334, 26.7793],
        [26.1518, 28.4263, 28.2009],
        [26.1518, 31.4847, 33.4745]], grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.055974334478378296 
model_pd.l_d.mean(): -1.486926555633545 
model_pd.lagr.mean(): -1.4309521913528442 
model_pd.lambdas: dict_items([('pout', tensor([0.6855])), ('power', tensor([0.0347]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0567])), ('power', tensor([-2.0929]))])
epoch：1049	 i:0 	 global-step:20980	 l-p:0.055974334478378296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.1529, 26.2011, 26.1570],
        [26.1529, 31.5247, 33.5517],
        [26.1529, 28.9405, 28.9580],
        [26.1529, 30.2808, 31.2072]], grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 0.05597406253218651 
model_pd.l_d.mean(): -1.4824784994125366 
model_pd.lagr.mean(): -1.4265044927597046 
model_pd.lambdas: dict_items([('pout', tensor([0.6835])), ('power', tensor([0.0346]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0567])), ('power', tensor([-2.0917]))])
epoch：1050	 i:0 	 global-step:21000	 l-p:0.05597406253218651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[26.1540, 28.8546, 28.8259],
        [26.1540, 26.8968, 26.4947],
        [26.1540, 27.5564, 27.1074],
        [26.1540, 29.9974, 30.7030]], grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.05597379803657532 
model_pd.l_d.mean(): -1.4780302047729492 
model_pd.lagr.mean(): -1.4220564365386963 
model_pd.lambdas: dict_items([('pout', tensor([0.6814])), ('power', tensor([0.0345]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0568])), ('power', tensor([-2.0906]))])
epoch：1051	 i:0 	 global-step:21020	 l-p:0.05597379803657532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.1551, 26.1551, 26.1551],
        [26.1551, 31.5275, 33.5549],
        [26.1551, 26.1567, 26.1551],
        [26.1551, 26.1551, 26.1550]], grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.055973559617996216 
model_pd.l_d.mean(): -1.473582148551941 
model_pd.lagr.mean(): -1.417608618736267 
model_pd.lambdas: dict_items([('pout', tensor([0.6794])), ('power', tensor([0.0344]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0568])), ('power', tensor([-2.0896]))])
epoch：1052	 i:0 	 global-step:21040	 l-p:0.055973559617996216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[26.1561, 30.5235, 31.6449],
        [26.1561, 26.1582, 26.1561],
        [26.1561, 26.1561, 26.1561],
        [26.1561, 28.4313, 28.2059]], grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.05597331002354622 
model_pd.l_d.mean(): -1.4691338539123535 
model_pd.lagr.mean(): -1.4131605625152588 
model_pd.lambdas: dict_items([('pout', tensor([0.6773])), ('power', tensor([0.0343]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0569])), ('power', tensor([-2.0886]))])
epoch：1053	 i:0 	 global-step:21060	 l-p:0.05597331002354622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.1570, 28.4324, 28.2071],
        [26.1570, 26.1570, 26.1570],
        [26.1570, 26.3513, 26.1954],
        [26.1570, 26.1570, 26.1570]], grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.05597308278083801 
model_pd.l_d.mean(): -1.4646857976913452 
model_pd.lagr.mean(): -1.4087127447128296 
model_pd.lambdas: dict_items([('pout', tensor([0.6752])), ('power', tensor([0.0342]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0569])), ('power', tensor([-2.0876]))])
epoch：1054	 i:0 	 global-step:21080	 l-p:0.05597308278083801
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.1580, 27.3754, 26.9173],
        [26.1580, 26.1601, 26.1580],
        [26.1580, 26.1580, 26.1580],
        [26.1580, 30.2873, 31.2141]], grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.0559728629887104 
model_pd.l_d.mean(): -1.4602376222610474 
model_pd.lagr.mean(): -1.4042648077011108 
model_pd.lambdas: dict_items([('pout', tensor([0.6732])), ('power', tensor([0.0341]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0570])), ('power', tensor([-2.0867]))])
epoch：1055	 i:0 	 global-step:21100	 l-p:0.0559728629887104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.1588, 26.1588, 26.1588],
        [26.1588, 26.1959, 26.1616],
        [26.1588, 26.1593, 26.1588],
        [26.1588, 33.3403, 37.2827]], grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.05597264692187309 
model_pd.l_d.mean(): -1.4557896852493286 
model_pd.lagr.mean(): -1.3998169898986816 
model_pd.lambdas: dict_items([('pout', tensor([0.6711])), ('power', tensor([0.0340]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0570])), ('power', tensor([-2.0858]))])
epoch：1056	 i:0 	 global-step:21120	 l-p:0.05597264692187309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.1597, 28.9487, 28.9663],
        [26.1597, 30.5282, 31.6500],
        [26.1597, 26.1597, 26.1597],
        [26.1597, 35.2499, 41.5331]], grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.05597245320677757 
model_pd.l_d.mean(): -1.4513415098190308 
model_pd.lagr.mean(): -1.395369052886963 
model_pd.lambdas: dict_items([('pout', tensor([0.6691])), ('power', tensor([0.0338]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0571])), ('power', tensor([-2.0849]))])
epoch：1057	 i:0 	 global-step:21140	 l-p:0.05597245320677757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.1605, 27.2430, 26.7885],
        [26.1605, 34.6002, 40.0521],
        [26.1605, 27.0190, 26.5917],
        [26.1605, 26.1976, 26.1632]], grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.05597224831581116 
model_pd.l_d.mean(): -1.446893334388733 
model_pd.lagr.mean(): -1.3909211158752441 
model_pd.lambdas: dict_items([('pout', tensor([0.6670])), ('power', tensor([0.0337]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0571])), ('power', tensor([-2.0841]))])
epoch：1058	 i:0 	 global-step:21160	 l-p:0.05597224831581116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.1613, 27.9027, 27.5075],
        [26.1613, 26.2622, 26.1746],
        [26.1613, 34.6014, 40.0536],
        [26.1613, 27.3791, 26.9209]], grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.05597206950187683 
model_pd.l_d.mean(): -1.4424453973770142 
model_pd.lagr.mean(): -1.386473298072815 
model_pd.lambdas: dict_items([('pout', tensor([0.6650])), ('power', tensor([0.0336]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0571])), ('power', tensor([-2.0833]))])
epoch：1059	 i:0 	 global-step:21180	 l-p:0.05597206950187683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.1620, 28.4384, 28.2130],
        [26.1620, 30.0076, 30.7140],
        [26.1620, 26.1621, 26.1620],
        [26.1620, 26.3347, 26.1937]], grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.05597187951207161 
model_pd.l_d.mean(): -1.4379971027374268 
model_pd.lagr.mean(): -1.3820252418518066 
model_pd.lambdas: dict_items([('pout', tensor([0.6629])), ('power', tensor([0.0335]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0572])), ('power', tensor([-2.0826]))])
epoch：1060	 i:0 	 global-step:21200	 l-p:0.05597187951207161
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.1628, 27.9044, 27.5092],
        [26.1628, 29.0441, 29.1122],
        [26.1628, 26.1629, 26.1628],
        [26.1628, 31.5380, 33.5668]], grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.055971719324588776 
model_pd.l_d.mean(): -1.433549165725708 
model_pd.lagr.mean(): -1.3775774240493774 
model_pd.lambdas: dict_items([('pout', tensor([0.6608])), ('power', tensor([0.0334]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0572])), ('power', tensor([-2.0818]))])
epoch：1061	 i:0 	 global-step:21220	 l-p:0.055971719324588776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.1635, 26.1649, 26.1635],
        [26.1635, 26.1634, 26.1635],
        [26.1635, 26.1635, 26.1635],
        [26.1635, 26.3578, 26.2018]], grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.055971551686525345 
model_pd.l_d.mean(): -1.4291009902954102 
model_pd.lagr.mean(): -1.3731294870376587 
model_pd.lambdas: dict_items([('pout', tensor([0.6588])), ('power', tensor([0.0333]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0573])), ('power', tensor([-2.0811]))])
epoch：1062	 i:0 	 global-step:21240	 l-p:0.055971551686525345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.1641, 31.0723, 32.6636],
        [26.1641, 28.9542, 28.9719],
        [26.1641, 33.3480, 37.2920],
        [26.1641, 26.4920, 26.2538]], grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.05597138777375221 
model_pd.l_d.mean(): -1.4246528148651123 
model_pd.lagr.mean(): -1.3686814308166504 
model_pd.lambdas: dict_items([('pout', tensor([0.6567])), ('power', tensor([0.0332]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0573])), ('power', tensor([-2.0805]))])
epoch：1063	 i:0 	 global-step:21260	 l-p:0.05597138777375221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.1647, 26.2018, 26.1675],
        [26.1647, 29.0466, 29.1148],
        [26.1647, 27.3829, 26.9246],
        [26.1647, 33.6826, 38.0142]], grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.05597124248743057 
model_pd.l_d.mean(): -1.420204520225525 
model_pd.lagr.mean(): -1.3642332553863525 
model_pd.lambdas: dict_items([('pout', tensor([0.6547])), ('power', tensor([0.0331]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0573])), ('power', tensor([-2.0799]))])
epoch：1064	 i:0 	 global-step:21280	 l-p:0.05597124248743057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.1653, 26.2136, 26.1695],
        [26.1653, 26.1921, 26.1670],
        [26.1653, 27.9941, 27.6197],
        [26.1653, 26.4933, 26.2551]], grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.05597109720110893 
model_pd.l_d.mean(): -1.4157567024230957 
model_pd.lagr.mean(): -1.359785556793213 
model_pd.lambdas: dict_items([('pout', tensor([0.6526])), ('power', tensor([0.0330]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0574])), ('power', tensor([-2.0792]))])
epoch：1065	 i:0 	 global-step:21300	 l-p:0.05597109720110893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1659, 26.2030, 26.1686],
        [26.1659, 26.1927, 26.1675],
        [26.1659, 26.1660, 26.1659],
        [26.1659, 27.0798, 26.6431]], grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.05597095564007759 
model_pd.l_d.mean(): -1.4113082885742188 
model_pd.lagr.mean(): -1.355337381362915 
model_pd.lambdas: dict_items([('pout', tensor([0.6506])), ('power', tensor([0.0329]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0574])), ('power', tensor([-2.0787]))])
epoch：1066	 i:0 	 global-step:21320	 l-p:0.05597095564007759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.1665, 26.2036, 26.1692],
        [26.1665, 27.9954, 27.6210],
        [26.1665, 26.1665, 26.1665],
        [26.1665, 26.2675, 26.1798]], grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.055970825254917145 
model_pd.l_d.mean(): -1.4068603515625 
model_pd.lagr.mean(): -1.3508895635604858 
model_pd.lambdas: dict_items([('pout', tensor([0.6485])), ('power', tensor([0.0328]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0574])), ('power', tensor([-2.0781]))])
epoch：1067	 i:0 	 global-step:21340	 l-p:0.055970825254917145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.1670, 31.0764, 32.6682],
        [26.1670, 26.1670, 26.1670],
        [26.1670, 26.1670, 26.1670],
        [26.1670, 34.6102, 40.0648]], grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.055970698595047 
model_pd.l_d.mean(): -1.4024121761322021 
model_pd.lagr.mean(): -1.3464415073394775 
model_pd.lambdas: dict_items([('pout', tensor([0.6464])), ('power', tensor([0.0327]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0574])), ('power', tensor([-2.0776]))])
epoch：1068	 i:0 	 global-step:21360	 l-p:0.055970698595047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[26.1675, 28.8712, 28.8429],
        [26.1675, 29.0501, 29.1185],
        [26.1675, 30.5388, 31.6617],
        [26.1675, 27.9100, 27.5147]], grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.05597057193517685 
model_pd.l_d.mean(): -1.3979641199111938 
model_pd.lagr.mean(): -1.3419935703277588 
model_pd.lambdas: dict_items([('pout', tensor([0.6444])), ('power', tensor([0.0326]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0575])), ('power', tensor([-2.0771]))])
epoch：1069	 i:0 	 global-step:21380	 l-p:0.05597057193517685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.1680, 33.6877, 38.0205],
        [26.1680, 26.1680, 26.1680],
        [26.1680, 28.8718, 28.8435],
        [26.1680, 30.5395, 31.6624]], grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): 0.055970460176467896 
model_pd.l_d.mean(): -1.3935155868530273 
model_pd.lagr.mean(): -1.3375451564788818 
model_pd.lambdas: dict_items([('pout', tensor([0.6423])), ('power', tensor([0.0325]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0575])), ('power', tensor([-2.0766]))])
epoch：1070	 i:0 	 global-step:21400	 l-p:0.055970460176467896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.1685, 34.6126, 40.0679],
        [26.1685, 26.1685, 26.1685],
        [26.1685, 26.1685, 26.1685],
        [26.1685, 35.2638, 41.5511]], grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.05597035586833954 
model_pd.l_d.mean(): -1.3890676498413086 
model_pd.lagr.mean(): -1.3330973386764526 
model_pd.lambdas: dict_items([('pout', tensor([0.6403])), ('power', tensor([0.0324]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0575])), ('power', tensor([-2.0761]))])
epoch：1071	 i:0 	 global-step:21420	 l-p:0.05597035586833954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.1689, 26.2060, 26.1716],
        [26.1689, 26.9128, 26.5102],
        [26.1689, 27.9984, 27.6239],
        [26.1689, 28.8730, 28.8447]], grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.055970244109630585 
model_pd.l_d.mean(): -1.3846195936203003 
model_pd.lagr.mean(): -1.3286494016647339 
model_pd.lambdas: dict_items([('pout', tensor([0.6382])), ('power', tensor([0.0323]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0575])), ('power', tensor([-2.0757]))])
epoch：1072	 i:0 	 global-step:21440	 l-p:0.055970244109630585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.1693, 28.6937, 28.5764],
        [26.1693, 31.5535, 33.5895],
        [26.1693, 28.4473, 28.2221],
        [26.1693, 26.2704, 26.1827]], grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.05597013980150223 
model_pd.l_d.mean(): -1.380171298980713 
model_pd.lagr.mean(): -1.3242011070251465 
model_pd.lambdas: dict_items([('pout', tensor([0.6362])), ('power', tensor([0.0322]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0576])), ('power', tensor([-2.0752]))])
epoch：1073	 i:0 	 global-step:21460	 l-p:0.05597013980150223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.1697, 33.6904, 38.0240],
        [26.1697, 33.3567, 37.3028],
        [26.1697, 30.3031, 31.2314],
        [26.1697, 26.1731, 26.1698]], grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): 0.055970050394535065 
model_pd.l_d.mean(): -1.3757230043411255 
model_pd.lagr.mean(): -1.3197529315948486 
model_pd.lambdas: dict_items([('pout', tensor([0.6341])), ('power', tensor([0.0321]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0576])), ('power', tensor([-2.0748]))])
epoch：1074	 i:0 	 global-step:21480	 l-p:0.055970050394535065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.1701, 26.1969, 26.1717],
        [26.1701, 35.2665, 41.5548],
        [26.1701, 26.2184, 26.1743],
        [26.1701, 26.1795, 26.1705]], grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.055969953536987305 
model_pd.l_d.mean(): -1.3712748289108276 
model_pd.lagr.mean(): -1.3153048753738403 
model_pd.lambdas: dict_items([('pout', tensor([0.6320])), ('power', tensor([0.0320]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0576])), ('power', tensor([-2.0744]))])
epoch：1075	 i:0 	 global-step:21500	 l-p:0.055969953536987305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1705, 31.5492, 33.5798],
        [26.1705, 27.0850, 26.6480],
        [26.1705, 26.1727, 26.1705],
        [26.1705, 26.1705, 26.1705]], grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.05596986785531044 
model_pd.l_d.mean(): -1.3668265342712402 
model_pd.lagr.mean(): -1.3108566999435425 
model_pd.lambdas: dict_items([('pout', tensor([0.6300])), ('power', tensor([0.0319]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0576])), ('power', tensor([-2.0740]))])
epoch：1076	 i:0 	 global-step:21520	 l-p:0.05596986785531044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.1709, 26.1713, 26.1709],
        [26.1709, 31.5498, 33.5805],
        [26.1709, 26.1730, 26.1709],
        [26.1709, 27.8383, 27.4268]], grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.055969782173633575 
model_pd.l_d.mean(): -1.3623783588409424 
model_pd.lagr.mean(): -1.3064085245132446 
model_pd.lambdas: dict_items([('pout', tensor([0.6279])), ('power', tensor([0.0318]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0577])), ('power', tensor([-2.0737]))])
epoch：1077	 i:0 	 global-step:21540	 l-p:0.055969782173633575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.1712, 26.4924, 26.2579],
        [26.1712, 33.6928, 38.0271],
        [26.1712, 26.7955, 26.4276],
        [26.1712, 26.1746, 26.1713]], grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): 0.05596970021724701 
model_pd.l_d.mean(): -1.357930302619934 
model_pd.lagr.mean(): -1.3019605875015259 
model_pd.lambdas: dict_items([('pout', tensor([0.6259])), ('power', tensor([0.0317]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0577])), ('power', tensor([-2.0734]))])
epoch：1078	 i:0 	 global-step:21560	 l-p:0.05596970021724701
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[26.1715, 27.2554, 26.8005],
        [26.1715, 27.0312, 26.6034],
        [26.1715, 31.5508, 33.5817],
        [26.1715, 31.5123, 33.5062]], grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.05596962571144104 
model_pd.l_d.mean(): -1.3534821271896362 
model_pd.lagr.mean(): -1.2975125312805176 
model_pd.lambdas: dict_items([('pout', tensor([0.6238])), ('power', tensor([0.0316]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0577])), ('power', tensor([-2.0730]))])
epoch：1079	 i:0 	 global-step:21580	 l-p:0.05596962571144104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.1718, 27.0865, 26.6494],
        [26.1718, 26.1718, 26.1718],
        [26.1718, 28.6970, 28.5798],
        [26.1718, 31.0835, 32.6765]], grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.05596955120563507 
model_pd.l_d.mean(): -1.3490338325500488 
model_pd.lagr.mean(): -1.2930642366409302 
model_pd.lambdas: dict_items([('pout', tensor([0.6217])), ('power', tensor([0.0315]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0577])), ('power', tensor([-2.0727]))])
epoch：1080	 i:0 	 global-step:21600	 l-p:0.05596955120563507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.1721, 26.3450, 26.2038],
        [26.1721, 33.3606, 37.3079],
        [26.1721, 27.9157, 27.5203],
        [26.1721, 35.2700, 41.5596]], grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.0559694766998291 
model_pd.l_d.mean(): -1.3445855379104614 
model_pd.lagr.mean(): -1.2886160612106323 
model_pd.lambdas: dict_items([('pout', tensor([0.6197])), ('power', tensor([0.0314]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0577])), ('power', tensor([-2.0724]))])
epoch：1081	 i:0 	 global-step:21620	 l-p:0.0559694766998291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.1724, 26.1992, 26.1740],
        [26.1724, 26.5036, 26.2636],
        [26.1724, 29.0567, 29.1254],
        [26.1724, 31.5522, 33.5834]], grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.05596941336989403 
model_pd.l_d.mean(): -1.3401374816894531 
model_pd.lagr.mean(): -1.2841681241989136 
model_pd.lambdas: dict_items([('pout', tensor([0.6176])), ('power', tensor([0.0313]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0577])), ('power', tensor([-2.0721]))])
epoch：1082	 i:0 	 global-step:21640	 l-p:0.05596941336989403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.1727, 26.1820, 26.1730],
        [26.1727, 26.1727, 26.1727],
        [26.1727, 26.1728, 26.1727],
        [26.1727, 26.2098, 26.1754]], grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.05596934258937836 
model_pd.l_d.mean(): -1.3356891870498657 
model_pd.lagr.mean(): -1.2797198295593262 
model_pd.lambdas: dict_items([('pout', tensor([0.6156])), ('power', tensor([0.0311]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0578])), ('power', tensor([-2.0719]))])
epoch：1083	 i:0 	 global-step:21660	 l-p:0.05596934258937836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.1729, 26.1729, 26.1729],
        [26.1729, 26.5042, 26.2641],
        [26.1729, 26.1763, 26.1730],
        [26.1729, 26.5060, 26.2649]], grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.05596928671002388 
model_pd.l_d.mean(): -1.3312410116195679 
model_pd.lagr.mean(): -1.2752717733383179 
model_pd.lambdas: dict_items([('pout', tensor([0.6135])), ('power', tensor([0.0310]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0578])), ('power', tensor([-2.0716]))])
epoch：1084	 i:0 	 global-step:21680	 l-p:0.05596928671002388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[26.1732, 33.2199, 37.0052],
        [26.1732, 30.3082, 31.2373],
        [26.1732, 27.2574, 26.8024],
        [26.1732, 27.3928, 26.9341]], grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.055969227105379105 
model_pd.l_d.mean(): -1.3267927169799805 
model_pd.lagr.mean(): -1.2708234786987305 
model_pd.lambdas: dict_items([('pout', tensor([0.6115])), ('power', tensor([0.0309]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0578])), ('power', tensor([-2.0714]))])
epoch：1085	 i:0 	 global-step:21700	 l-p:0.055969227105379105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1734, 28.0041, 27.6296],
        [26.1734, 27.5789, 27.1292],
        [26.1734, 26.4948, 26.2602],
        [26.1734, 26.1734, 26.1734]], grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.05596918240189552 
model_pd.l_d.mean(): -1.322344422340393 
model_pd.lagr.mean(): -1.266375184059143 
model_pd.lambdas: dict_items([('pout', tensor([0.6094])), ('power', tensor([0.0308]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0578])), ('power', tensor([-2.0711]))])
epoch：1086	 i:0 	 global-step:21720	 l-p:0.05596918240189552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.1736, 26.1741, 26.1736],
        [26.1736, 27.0886, 26.6514],
        [26.1736, 33.2208, 37.0063],
        [26.1736, 26.1736, 26.1736]], grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.05596911907196045 
model_pd.l_d.mean(): -1.3178961277008057 
model_pd.lagr.mean(): -1.2619270086288452 
model_pd.lambdas: dict_items([('pout', tensor([0.6073])), ('power', tensor([0.0307]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0578])), ('power', tensor([-2.0709]))])
epoch：1087	 i:0 	 global-step:21740	 l-p:0.05596911907196045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[26.1738, 33.3637, 37.3119],
        [26.1738, 26.1738, 26.1738],
        [26.1738, 26.1772, 26.1739],
        [26.1738, 33.2211, 37.0069]], grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.05596906691789627 
model_pd.l_d.mean(): -1.3134477138519287 
model_pd.lagr.mean(): -1.2574785947799683 
model_pd.lambdas: dict_items([('pout', tensor([0.6053])), ('power', tensor([0.0306]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0578])), ('power', tensor([-2.0707]))])
epoch：1088	 i:0 	 global-step:21760	 l-p:0.05596906691789627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1740, 26.1740, 26.1740],
        [26.1740, 27.5797, 27.1300],
        [26.1740, 29.2514, 29.4324],
        [26.1740, 26.1742, 26.1740]], grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.05596902221441269 
model_pd.l_d.mean(): -1.3089996576309204 
model_pd.lagr.mean(): -1.2530306577682495 
model_pd.lambdas: dict_items([('pout', tensor([0.6032])), ('power', tensor([0.0305]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0578])), ('power', tensor([-2.0705]))])
epoch：1089	 i:0 	 global-step:21780	 l-p:0.05596902221441269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[26.1742, 33.2219, 37.0079],
        [26.1742, 28.4539, 28.2288],
        [26.1742, 28.7004, 28.5833],
        [26.1742, 35.2740, 41.5652]], grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.05596897751092911 
model_pd.l_d.mean(): -1.304551362991333 
model_pd.lagr.mean(): -1.248582363128662 
model_pd.lambdas: dict_items([('pout', tensor([0.6012])), ('power', tensor([0.0304]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0578])), ('power', tensor([-2.0703]))])
epoch：1090	 i:0 	 global-step:21800	 l-p:0.05596897751092911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1744, 26.1745, 26.1744],
        [26.1744, 26.1749, 26.1744],
        [26.1744, 26.1758, 26.1744],
        [26.1744, 27.5802, 27.1305]], grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.055968932807445526 
model_pd.l_d.mean(): -1.3001030683517456 
model_pd.lagr.mean(): -1.2441341876983643 
model_pd.lambdas: dict_items([('pout', tensor([0.5991])), ('power', tensor([0.0303]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0701]))])
epoch：1091	 i:0 	 global-step:21820	 l-p:0.055968932807445526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.1746, 26.1750, 26.1746],
        [26.1746, 26.5077, 26.2666],
        [26.1746, 27.3946, 26.9358],
        [26.1746, 31.5620, 33.5999]], grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.05596889182925224 
model_pd.l_d.mean(): -1.2956546545028687 
model_pd.lagr.mean(): -1.2396857738494873 
model_pd.lambdas: dict_items([('pout', tensor([0.5971])), ('power', tensor([0.0302]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0699]))])
epoch：1092	 i:0 	 global-step:21840	 l-p:0.05596889182925224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.1747, 26.1841, 26.1750],
        [26.1747, 26.5062, 26.2660],
        [26.1747, 33.6993, 38.0358],
        [26.1747, 35.2750, 41.5668]], grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.05596885830163956 
model_pd.l_d.mean(): -1.2912063598632812 
model_pd.lagr.mean(): -1.2352374792099 
model_pd.lambdas: dict_items([('pout', tensor([0.5950])), ('power', tensor([0.0301]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0697]))])
epoch：1093	 i:0 	 global-step:21860	 l-p:0.05596885830163956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.1749, 26.4964, 26.2617],
        [26.1749, 29.0605, 29.1295],
        [26.1749, 27.0902, 26.6529],
        [26.1749, 29.2528, 29.4340]], grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.055968817323446274 
model_pd.l_d.mean(): -1.2867581844329834 
model_pd.lagr.mean(): -1.2307894229888916 
model_pd.lambdas: dict_items([('pout', tensor([0.5929])), ('power', tensor([0.0300]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0696]))])
epoch：1094	 i:0 	 global-step:21880	 l-p:0.055968817323446274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1750, 31.5569, 33.5893],
        [26.1750, 26.1844, 26.1754],
        [26.1750, 26.1767, 26.1751],
        [26.1750, 26.1752, 26.1750]], grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.05596878007054329 
model_pd.l_d.mean(): -1.282309651374817 
model_pd.lagr.mean(): -1.226340889930725 
model_pd.lambdas: dict_items([('pout', tensor([0.5909])), ('power', tensor([0.0299]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0694]))])
epoch：1095	 i:0 	 global-step:21900	 l-p:0.05596878007054329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228]])
 pt:tensor([[26.1752, 31.5632, 33.6015],
        [26.1752, 27.2600, 26.8048],
        [26.1752, 33.7002, 38.0371],
        [26.1752, 26.8000, 26.4319]], grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.0559687465429306 
model_pd.l_d.mean(): -1.2778613567352295 
model_pd.lagr.mean(): -1.2218925952911377 
model_pd.lambdas: dict_items([('pout', tensor([0.5888])), ('power', tensor([0.0298]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0693]))])
epoch：1096	 i:0 	 global-step:21920	 l-p:0.0559687465429306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1753, 26.1753, 26.1753],
        [26.1753, 30.0267, 30.7352],
        [26.1753, 28.8821, 28.8544],
        [26.1753, 27.0907, 26.6534]], grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.05596871301531792 
model_pd.l_d.mean(): -1.2734129428863525 
model_pd.lagr.mean(): -1.2174441814422607 
model_pd.lambdas: dict_items([('pout', tensor([0.5868])), ('power', tensor([0.0297]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0691]))])
epoch：1097	 i:0 	 global-step:21940	 l-p:0.05596871301531792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.1754, 26.2023, 26.1770],
        [26.1754, 27.5816, 27.1318],
        [26.1754, 26.2766, 26.1888],
        [26.1754, 26.3485, 26.2072]], grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.05596868693828583 
model_pd.l_d.mean(): -1.2689646482467651 
model_pd.lagr.mean(): -1.212996006011963 
model_pd.lambdas: dict_items([('pout', tensor([0.5847])), ('power', tensor([0.0296]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0690]))])
epoch：1098	 i:0 	 global-step:21960	 l-p:0.05596868693828583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1756, 33.7010, 38.0384],
        [26.1756, 31.5195, 33.5151],
        [26.1756, 27.0361, 26.6079],
        [26.1756, 26.1757, 26.1756]], grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.05596865713596344 
model_pd.l_d.mean(): -1.2645163536071777 
model_pd.lagr.mean(): -1.2085477113723755 
model_pd.lambdas: dict_items([('pout', tensor([0.5827])), ('power', tensor([0.0295]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0689]))])
epoch：1099	 i:0 	 global-step:21980	 l-p:0.05596865713596344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1757, 28.7028, 28.5858],
        [26.1757, 26.5072, 26.2669],
        [26.1757, 26.1791, 26.1757],
        [26.1757, 26.1758, 26.1757]], grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): 0.05596862733364105 
model_pd.l_d.mean(): -1.2600679397583008 
model_pd.lagr.mean(): -1.2040992975234985 
model_pd.lambdas: dict_items([('pout', tensor([0.5806])), ('power', tensor([0.0294]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0688]))])
epoch：1100	 i:0 	 global-step:22000	 l-p:0.05596862733364105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.1758, 26.1758, 26.1758],
        [26.1758, 31.5585, 33.5913],
        [26.1758, 26.1792, 26.1758],
        [26.1758, 26.2026, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.055968597531318665 
model_pd.l_d.mean(): -1.255619764328003 
model_pd.lagr.mean(): -1.1996511220932007 
model_pd.lambdas: dict_items([('pout', tensor([0.5785])), ('power', tensor([0.0293]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0687]))])
epoch：1101	 i:0 	 global-step:22020	 l-p:0.055968597531318665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.1759, 29.2546, 29.4360],
        [26.1759, 31.5587, 33.5917],
        [26.1759, 26.5045, 26.2659],
        [26.1759, 26.9209, 26.5178]], grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.055968575179576874 
model_pd.l_d.mean(): -1.2511712312698364 
model_pd.lagr.mean(): -1.1952027082443237 
model_pd.lambdas: dict_items([('pout', tensor([0.5765])), ('power', tensor([0.0292]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0686]))])
epoch：1102	 i:0 	 global-step:22040	 l-p:0.055968575179576874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.1760, 28.7033, 28.5864],
        [26.1760, 27.0366, 26.6084],
        [26.1760, 28.4568, 28.2318],
        [26.1760, 26.1794, 26.1760]], grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.055968545377254486 
model_pd.l_d.mean(): -1.246722936630249 
model_pd.lagr.mean(): -1.1907544136047363 
model_pd.lambdas: dict_items([('pout', tensor([0.5744])), ('power', tensor([0.0291]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0579])), ('power', tensor([-2.0685]))])
epoch：1103	 i:0 	 global-step:22060	 l-p:0.055968545377254486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.1761, 26.1795, 26.1761],
        [26.1761, 27.2612, 26.8059],
        [26.1761, 34.6268, 40.0878],
        [26.1761, 27.9213, 27.5258]], grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.05596853420138359 
model_pd.l_d.mean(): -1.242274522781372 
model_pd.lagr.mean(): -1.1863059997558594 
model_pd.lambdas: dict_items([('pout', tensor([0.5724])), ('power', tensor([0.0290]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0684]))])
epoch：1104	 i:0 	 global-step:22080	 l-p:0.05596853420138359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.1762, 29.0628, 29.1320],
        [26.1762, 26.9213, 26.5181],
        [26.1762, 26.1761, 26.1762],
        [26.1762, 30.0285, 30.7373]], grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.0559685081243515 
model_pd.l_d.mean(): -1.2378259897232056 
model_pd.lagr.mean(): -1.1818574666976929 
model_pd.lambdas: dict_items([('pout', tensor([0.5703])), ('power', tensor([0.0289]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0683]))])
epoch：1105	 i:0 	 global-step:22100	 l-p:0.0559685081243515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.1762, 30.3139, 31.2441],
        [26.1762, 33.2264, 37.0142],
        [26.1762, 33.3689, 37.3193],
        [26.1762, 26.5079, 26.2675]], grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.05596848949790001 
model_pd.l_d.mean(): -1.2333775758743286 
model_pd.lagr.mean(): -1.177409052848816 
model_pd.lambdas: dict_items([('pout', tensor([0.5682])), ('power', tensor([0.0288]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0682]))])
epoch：1106	 i:0 	 global-step:22120	 l-p:0.05596848949790001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.1763, 31.0918, 32.6868],
        [26.1763, 26.1763, 26.1763],
        [26.1763, 26.5080, 26.2676],
        [26.1763, 26.1780, 26.1763]], grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.05596847087144852 
model_pd.l_d.mean(): -1.2289291620254517 
model_pd.lagr.mean(): -1.172960638999939 
model_pd.lambdas: dict_items([('pout', tensor([0.5662])), ('power', tensor([0.0287]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0681]))])
epoch：1107	 i:0 	 global-step:22140	 l-p:0.05596847087144852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[26.1764, 28.4576, 28.2326],
        [26.1764, 31.5660, 33.6052],
        [26.1764, 27.2617, 26.8064],
        [26.1764, 28.8842, 28.8566]], grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.05596845597028732 
model_pd.l_d.mean(): -1.2244806289672852 
model_pd.lagr.mean(): -1.168512225151062 
model_pd.lambdas: dict_items([('pout', tensor([0.5641])), ('power', tensor([0.0286]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0680]))])
epoch：1108	 i:0 	 global-step:22160	 l-p:0.05596845597028732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.1765, 26.2777, 26.1899],
        [26.1765, 28.0088, 27.6342],
        [26.1765, 26.1765, 26.1765],
        [26.1765, 33.7034, 38.0419]], grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.05596843361854553 
model_pd.l_d.mean(): -1.2200323343276978 
model_pd.lagr.mean(): -1.1640639305114746 
model_pd.lambdas: dict_items([('pout', tensor([0.5621])), ('power', tensor([0.0285]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0679]))])
epoch：1109	 i:0 	 global-step:22180	 l-p:0.05596843361854553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.1765, 28.4579, 28.2329],
        [26.1765, 27.9221, 27.5266],
        [26.1765, 27.3974, 26.9384],
        [26.1765, 26.5100, 26.2686]], grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.05596842244267464 
model_pd.l_d.mean(): -1.2155838012695312 
model_pd.lagr.mean(): -1.159615397453308 
model_pd.lambdas: dict_items([('pout', tensor([0.5600])), ('power', tensor([0.0284]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0679]))])
epoch：1110	 i:0 	 global-step:22200	 l-p:0.05596842244267464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1766, 33.7038, 38.0425],
        [26.1766, 26.1767, 26.1766],
        [26.1766, 27.0375, 26.6092],
        [26.1766, 26.4984, 26.2635]], grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.05596840754151344 
model_pd.l_d.mean(): -1.2111355066299438 
model_pd.lagr.mean(): -1.1551671028137207 
model_pd.lambdas: dict_items([('pout', tensor([0.5580])), ('power', tensor([0.0283]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0678]))])
epoch：1111	 i:0 	 global-step:22220	 l-p:0.05596840754151344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.1766, 26.1783, 26.1767],
        [26.1766, 31.5222, 33.5189],
        [26.1766, 26.5084, 26.2680],
        [26.1766, 28.0091, 27.6346]], grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.05596838518977165 
model_pd.l_d.mean(): -1.2066869735717773 
model_pd.lagr.mean(): -1.1507185697555542 
model_pd.lambdas: dict_items([('pout', tensor([0.5559])), ('power', tensor([0.0281]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0677]))])
epoch：1112	 i:0 	 global-step:22240	 l-p:0.05596838518977165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.1767, 26.5055, 26.2667],
        [26.1767, 29.0641, 29.1335],
        [26.1767, 26.1772, 26.1767],
        [26.1767, 35.2800, 41.5748]], grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.05596837401390076 
model_pd.l_d.mean(): -1.20223867893219 
model_pd.lagr.mean(): -1.1462702751159668 
model_pd.lambdas: dict_items([('pout', tensor([0.5538])), ('power', tensor([0.0280]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0677]))])
epoch：1113	 i:0 	 global-step:22260	 l-p:0.05596837401390076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.1767, 26.5056, 26.2668],
        [26.1767, 26.5103, 26.2689],
        [26.1767, 26.1768, 26.1768],
        [26.1767, 26.2139, 26.1795]], grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.05596836656332016 
model_pd.l_d.mean(): -1.1977901458740234 
model_pd.lagr.mean(): -1.1418217420578003 
model_pd.lambdas: dict_items([('pout', tensor([0.5518])), ('power', tensor([0.0279]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0676]))])
epoch：1114	 i:0 	 global-step:22280	 l-p:0.05596836656332016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1768, 26.1769, 26.1768],
        [26.1768, 26.2781, 26.1902],
        [26.1768, 26.8022, 26.4337],
        [26.1768, 26.1768, 26.1768]], grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.055968351662158966 
model_pd.l_d.mean(): -1.1933417320251465 
model_pd.lagr.mean(): -1.1373733282089233 
model_pd.lambdas: dict_items([('pout', tensor([0.5497])), ('power', tensor([0.0278]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0676]))])
epoch：1115	 i:0 	 global-step:22300	 l-p:0.055968351662158966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.1768, 28.7052, 28.5885],
        [26.1768, 26.1769, 26.1768],
        [26.1768, 26.2781, 26.1902],
        [26.1768, 29.2568, 29.4386]], grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.05596834048628807 
model_pd.l_d.mean(): -1.18889319896698 
model_pd.lagr.mean(): -1.1329249143600464 
model_pd.lambdas: dict_items([('pout', tensor([0.5477])), ('power', tensor([0.0277]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0675]))])
epoch：1116	 i:0 	 global-step:22320	 l-p:0.05596834048628807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.1769, 28.9728, 28.9917],
        [26.1769, 26.1803, 26.1770],
        [26.1769, 27.0380, 26.6096],
        [26.1769, 28.0097, 27.6352]], grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.055968329310417175 
model_pd.l_d.mean(): -1.1844446659088135 
model_pd.lagr.mean(): -1.1284763813018799 
model_pd.lambdas: dict_items([('pout', tensor([0.5456])), ('power', tensor([0.0276]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0675]))])
epoch：1117	 i:0 	 global-step:22340	 l-p:0.055968329310417175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1769, 28.8855, 28.8581],
        [26.1769, 28.4588, 28.2339],
        [26.1769, 26.5058, 26.2670],
        [26.1769, 26.4988, 26.2639]], grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.05596831813454628 
model_pd.l_d.mean(): -1.179996132850647 
model_pd.lagr.mean(): -1.1240278482437134 
model_pd.lambdas: dict_items([('pout', tensor([0.5435])), ('power', tensor([0.0275]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0674]))])
epoch：1118	 i:0 	 global-step:22360	 l-p:0.05596831813454628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 35.2811, 41.5767],
        [26.1770, 26.1774, 26.1770],
        [26.1770, 26.5106, 26.2691],
        [26.1770, 28.4590, 28.2340]], grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 0.05596831440925598 
model_pd.l_d.mean(): -1.175547480583191 
model_pd.lagr.mean(): -1.1195791959762573 
model_pd.lambdas: dict_items([('pout', tensor([0.5415])), ('power', tensor([0.0274]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0674]))])
epoch：1119	 i:0 	 global-step:22380	 l-p:0.05596831440925598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.2254, 26.1811],
        [26.1770, 26.4989, 26.2639],
        [26.1770, 26.8025, 26.4340],
        [26.1770, 29.2573, 29.4392]], grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 0.055968303233385086 
model_pd.l_d.mean(): -1.1710988283157349 
model_pd.lagr.mean(): -1.1151305437088013 
model_pd.lambdas: dict_items([('pout', tensor([0.5394])), ('power', tensor([0.0273]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0673]))])
epoch：1120	 i:0 	 global-step:22400	 l-p:0.055968303233385086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 30.3163, 31.2472],
        [26.1770, 29.2575, 29.4394],
        [26.1770, 27.8474, 27.4357],
        [26.1770, 26.1864, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): 0.05596828833222389 
model_pd.l_d.mean(): -1.166650414466858 
model_pd.lagr.mean(): -1.1106821298599243 
model_pd.lambdas: dict_items([('pout', tensor([0.5374])), ('power', tensor([0.0272]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0673]))])
epoch：1121	 i:0 	 global-step:22420	 l-p:0.05596828833222389
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.5107, 26.2692],
        [26.1771, 30.3164, 31.2474],
        [26.1771, 29.0652, 29.1348],
        [26.1771, 26.1787, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.055968284606933594 
model_pd.l_d.mean(): -1.1622018814086914 
model_pd.lagr.mean(): -1.1062335968017578 
model_pd.lambdas: dict_items([('pout', tensor([0.5353])), ('power', tensor([0.0271]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0673]))])
epoch：1122	 i:0 	 global-step:22440	 l-p:0.055968284606933594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.3504, 26.2089],
        [26.1771, 28.0103, 27.6357],
        [26.1771, 26.1805, 26.1772],
        [26.1771, 30.3165, 31.2476]], grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): 0.0559682734310627 
model_pd.l_d.mean(): -1.157753348350525 
model_pd.lagr.mean(): -1.1017850637435913 
model_pd.lambdas: dict_items([('pout', tensor([0.5333])), ('power', tensor([0.0270]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0672]))])
epoch：1123	 i:0 	 global-step:22460	 l-p:0.0559682734310627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228]])
 pt:tensor([[26.1771, 27.2631, 26.8076],
        [26.1771, 27.9236, 27.5280],
        [26.1771, 31.5240, 33.5215],
        [26.1771, 31.5625, 33.5971]], grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.0559682659804821 
model_pd.l_d.mean(): -1.1533046960830688 
model_pd.lagr.mean(): -1.0973364114761353 
model_pd.lambdas: dict_items([('pout', tensor([0.5312])), ('power', tensor([0.0269]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0672]))])
epoch：1124	 i:0 	 global-step:22480	 l-p:0.0559682659804821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.5108, 26.2693],
        [26.1771, 31.5241, 33.5217],
        [26.1771, 30.0314, 30.7411],
        [26.1771, 27.0936, 26.6559]], grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.0559682622551918 
model_pd.l_d.mean(): -1.1488561630249023 
model_pd.lagr.mean(): -1.0928878784179688 
model_pd.lambdas: dict_items([('pout', tensor([0.5291])), ('power', tensor([0.0268]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0672]))])
epoch：1125	 i:0 	 global-step:22500	 l-p:0.0559682622551918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[26.1772, 28.7063, 28.5897],
        [26.1772, 27.0937, 26.6559],
        [26.1772, 31.0947, 32.6910],
        [26.1772, 27.3988, 26.9396]], grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.05596825107932091 
model_pd.l_d.mean(): -1.1444075107574463 
model_pd.lagr.mean(): -1.0884392261505127 
model_pd.lambdas: dict_items([('pout', tensor([0.5271])), ('power', tensor([0.0267]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0671]))])
epoch：1126	 i:0 	 global-step:22520	 l-p:0.05596825107932091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 27.0387, 26.6101],
        [26.1772, 33.3725, 37.3251],
        [26.1772, 33.2299, 37.0199],
        [26.1772, 26.1786, 26.1772]], grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.05596825107932091 
model_pd.l_d.mean(): -1.1399590969085693 
model_pd.lagr.mean(): -1.0839908123016357 
model_pd.lambdas: dict_items([('pout', tensor([0.5250])), ('power', tensor([0.0266]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0671]))])
epoch：1127	 i:0 	 global-step:22540	 l-p:0.05596825107932091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.3723, 26.2158],
        [26.1772, 26.5092, 26.2686],
        [26.1772, 26.1772, 26.1772],
        [26.1772, 34.6310, 40.0949]], grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -1.1355104446411133 
model_pd.lagr.mean(): -1.0795421600341797 
model_pd.lambdas: dict_items([('pout', tensor([0.5230])), ('power', tensor([0.0265]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0671]))])
epoch：1128	 i:0 	 global-step:22560	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.1772, 30.0319, 30.7417],
        [26.1772, 27.0388, 26.6102],
        [26.1772, 26.3506, 26.2090],
        [26.1772, 26.1773, 26.1772]], grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -1.1310620307922363 
model_pd.lagr.mean(): -1.0750937461853027 
model_pd.lambdas: dict_items([('pout', tensor([0.5209])), ('power', tensor([0.0264]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0580])), ('power', tensor([-2.0671]))])
epoch：1129	 i:0 	 global-step:22580	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 30.5564, 31.6830],
        [26.1772, 28.7067, 28.5901],
        [26.1772, 27.8481, 27.4363],
        [26.1772, 26.5063, 26.2674]], grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -1.1266132593154907 
model_pd.lagr.mean(): -1.0706449747085571 
model_pd.lambdas: dict_items([('pout', tensor([0.5189])), ('power', tensor([0.0263]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0670]))])
epoch：1130	 i:0 	 global-step:22600	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.1773, 30.3175, 31.2489],
        [26.1773, 28.7068, 28.5902],
        [26.1773, 30.0321, 30.7420],
        [26.1773, 33.3730, 37.3260]], grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -1.1221647262573242 
model_pd.lagr.mean(): -1.0661964416503906 
model_pd.lambdas: dict_items([('pout', tensor([0.5168])), ('power', tensor([0.0262]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0670]))])
epoch：1131	 i:0 	 global-step:22620	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 34.6315, 40.0959],
        [26.1773, 26.1795, 26.1773],
        [26.1773, 33.3731, 37.3262],
        [26.1773, 26.1774, 26.1773]], grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -1.1177160739898682 
model_pd.lagr.mean(): -1.0617477893829346 
model_pd.lambdas: dict_items([('pout', tensor([0.5147])), ('power', tensor([0.0261]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0670]))])
epoch：1132	 i:0 	 global-step:22640	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.5064, 26.2674],
        [26.1773, 27.8483, 27.4365],
        [26.1773, 26.2042, 26.1789],
        [26.1773, 33.3732, 37.3264]], grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -1.1132675409317017 
model_pd.lagr.mean(): -1.057299256324768 
model_pd.lambdas: dict_items([('pout', tensor([0.5127])), ('power', tensor([0.0260]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0670]))])
epoch：1133	 i:0 	 global-step:22660	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.1773, 31.5698, 33.6109],
        [26.1773, 33.2308, 37.0214],
        [26.1773, 28.7070, 28.5905],
        [26.1773, 33.3734, 37.3266]], grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -1.1088188886642456 
model_pd.lagr.mean(): -1.0528507232666016 
model_pd.lambdas: dict_items([('pout', tensor([0.5106])), ('power', tensor([0.0259]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0670]))])
epoch：1134	 i:0 	 global-step:22680	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.1773, 34.6319, 40.0966],
        [26.1773, 26.1867, 26.1777],
        [26.1773, 26.2146, 26.1801],
        [26.1773, 26.1773, 26.1773]], grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): 0.05596821382641792 
model_pd.l_d.mean(): -1.1043702363967896 
model_pd.lagr.mean(): -1.0484020709991455 
model_pd.lambdas: dict_items([('pout', tensor([0.5086])), ('power', tensor([0.0258]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0669]))])
epoch：1135	 i:0 	 global-step:22700	 l-p:0.05596821382641792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 27.0942, 26.6563],
        [26.1773, 27.3994, 26.9401],
        [26.1773, 26.1795, 26.1774],
        [26.1773, 26.1867, 26.1777]], grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): 0.05596821382641792 
model_pd.l_d.mean(): -1.099921464920044 
model_pd.lagr.mean(): -1.0439532995224 
model_pd.lambdas: dict_items([('pout', tensor([0.5065])), ('power', tensor([0.0257]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0669]))])
epoch：1136	 i:0 	 global-step:22720	 l-p:0.05596821382641792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 28.7073, 28.5908],
        [26.1774, 26.1788, 26.1774],
        [26.1774, 33.3737, 37.3273],
        [26.1774, 27.5855, 27.1354]], grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.055968210101127625 
model_pd.l_d.mean(): -1.0954726934432983 
model_pd.lagr.mean(): -1.0395045280456543 
model_pd.lambdas: dict_items([('pout', tensor([0.5044])), ('power', tensor([0.0256]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0669]))])
epoch：1137	 i:0 	 global-step:22740	 l-p:0.055968210101127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1774, 26.1774],
        [26.1774, 27.8486, 27.4368],
        [26.1774, 26.9235, 26.5199],
        [26.1774, 26.1774, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.05596820265054703 
model_pd.l_d.mean(): -1.0910241603851318 
model_pd.lagr.mean(): -1.0350559949874878 
model_pd.lambdas: dict_items([('pout', tensor([0.5024])), ('power', tensor([0.0255]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0669]))])
epoch：1138	 i:0 	 global-step:22760	 l-p:0.05596820265054703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 27.8487, 27.4369],
        [26.1774, 27.9246, 27.5290],
        [26.1774, 26.1778, 26.1774],
        [26.1774, 26.3725, 26.2159]], grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -1.0865752696990967 
model_pd.lagr.mean(): -1.0306071043014526 
model_pd.lambdas: dict_items([('pout', tensor([0.5003])), ('power', tensor([0.0254]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0669]))])
epoch：1139	 i:0 	 global-step:22780	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 31.5704, 33.6119],
        [26.1774, 28.8876, 28.8606],
        [26.1774, 26.1796, 26.1774],
        [26.1774, 26.1788, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.05596820265054703 
model_pd.l_d.mean(): -1.0821266174316406 
model_pd.lagr.mean(): -1.0261584520339966 
model_pd.lambdas: dict_items([('pout', tensor([0.4983])), ('power', tensor([0.0253]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0669]))])
epoch：1140	 i:0 	 global-step:22800	 l-p:0.05596820265054703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228]])
 pt:tensor([[26.1774, 27.9247, 27.5291],
        [26.1774, 33.3742, 37.3281],
        [26.1774, 28.0116, 27.6370],
        [26.1774, 27.2640, 26.8083]], grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -1.077677845954895 
model_pd.lagr.mean(): -1.021709680557251 
model_pd.lambdas: dict_items([('pout', tensor([0.4962])), ('power', tensor([0.0252]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0669]))])
epoch：1141	 i:0 	 global-step:22820	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.9236, 26.5200],
        [26.1774, 26.1775, 26.1774],
        [26.1774, 27.9248, 27.5292],
        [26.1774, 27.0944, 26.6565]], grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -1.0732290744781494 
model_pd.lagr.mean(): -1.0172609090805054 
model_pd.lambdas: dict_items([('pout', tensor([0.4942])), ('power', tensor([0.0250]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1142	 i:0 	 global-step:22840	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 34.6328, 40.0984],
        [26.1774, 27.3997, 26.9404],
        [26.1774, 26.1774, 26.1774],
        [26.1774, 29.2596, 29.4420]], grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -1.068780541419983 
model_pd.lagr.mean(): -1.0128123760223389 
model_pd.lambdas: dict_items([('pout', tensor([0.4921])), ('power', tensor([0.0249]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1143	 i:0 	 global-step:22860	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 27.0945, 26.6565],
        [26.1774, 26.9237, 26.5200],
        [26.1774, 29.2596, 29.4421],
        [26.1774, 26.3508, 26.2092]], grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -1.0643316507339478 
model_pd.lagr.mean(): -1.0083634853363037 
model_pd.lambdas: dict_items([('pout', tensor([0.4900])), ('power', tensor([0.0248]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1144	 i:0 	 global-step:22880	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.1774, 29.0673, 29.1373],
        [26.1774, 26.1790, 26.1774],
        [26.1774, 26.5096, 26.2689],
        [26.1774, 26.1774, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -1.0598828792572021 
model_pd.lagr.mean(): -1.003914713859558 
model_pd.lambdas: dict_items([('pout', tensor([0.4880])), ('power', tensor([0.0247]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1145	 i:0 	 global-step:22900	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.5114, 26.2697],
        [26.1774, 34.6332, 40.0990],
        [26.1774, 26.1788, 26.1774],
        [26.1774, 26.8035, 26.4347]], grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -1.055434226989746 
model_pd.lagr.mean(): -0.999466061592102 
model_pd.lambdas: dict_items([('pout', tensor([0.4859])), ('power', tensor([0.0246]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1146	 i:0 	 global-step:22920	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 33.7087, 38.0511],
        [26.1774, 30.5581, 31.6856],
        [26.1774, 26.4997, 26.2645],
        [26.1774, 26.1776, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -1.0509854555130005 
model_pd.lagr.mean(): -0.9950172901153564 
model_pd.lambdas: dict_items([('pout', tensor([0.4839])), ('power', tensor([0.0245]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1147	 i:0 	 global-step:22940	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.1774, 34.6334, 40.0994],
        [26.1774, 31.5652, 33.6014],
        [26.1774, 26.8036, 26.4348],
        [26.1774, 26.1774, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -1.0465365648269653 
model_pd.lagr.mean(): -0.9905683994293213 
model_pd.lambdas: dict_items([('pout', tensor([0.4818])), ('power', tensor([0.0244]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1148	 i:0 	 global-step:22960	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 31.5268, 33.5260],
        [26.1774, 26.5114, 26.2697],
        [26.1774, 34.6335, 40.0996],
        [26.1774, 26.2259, 26.1816]], grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -1.0420879125595093 
model_pd.lagr.mean(): -0.9861197471618652 
model_pd.lambdas: dict_items([('pout', tensor([0.4797])), ('power', tensor([0.0243]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1149	 i:0 	 global-step:22980	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.1774, 27.9253, 27.5296],
        [26.1774, 29.2601, 29.4427],
        [26.1774, 27.0396, 26.6108],
        [26.1774, 26.1774, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -1.0376390218734741 
model_pd.lagr.mean(): -0.9816708564758301 
model_pd.lambdas: dict_items([('pout', tensor([0.4777])), ('power', tensor([0.0242]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1150	 i:0 	 global-step:23000	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 30.0340, 30.7446],
        [26.1774, 33.3753, 37.3300],
        [26.1774, 26.1796, 26.1775],
        [26.1774, 26.1776, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -1.0331902503967285 
model_pd.lagr.mean(): -0.9772220849990845 
model_pd.lambdas: dict_items([('pout', tensor([0.4756])), ('power', tensor([0.0241]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1151	 i:0 	 global-step:23020	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 27.5862, 27.1360],
        [26.1774, 28.0122, 27.6376],
        [26.1774, 26.1808, 26.1775],
        [26.1774, 31.5656, 33.6020]], grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -1.028741478919983 
model_pd.lagr.mean(): -0.9727733135223389 
model_pd.lambdas: dict_items([('pout', tensor([0.4736])), ('power', tensor([0.0240]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1152	 i:0 	 global-step:23040	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.3727, 26.2160],
        [26.1774, 26.2789, 26.1909],
        [26.1774, 26.5097, 26.2689],
        [26.1774, 26.1776, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -1.0242927074432373 
model_pd.lagr.mean(): -0.9683245420455933 
model_pd.lambdas: dict_items([('pout', tensor([0.4715])), ('power', tensor([0.0239]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1153	 i:0 	 global-step:23060	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 27.2645, 26.8087],
        [26.1774, 34.6340, 40.1006],
        [26.1774, 26.1868, 26.1778],
        [26.1774, 31.5718, 33.6142]], grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -1.0198438167572021 
model_pd.lagr.mean(): -0.9638756513595581 
model_pd.lambdas: dict_items([('pout', tensor([0.4695])), ('power', tensor([0.0238]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0668]))])
epoch：1154	 i:0 	 global-step:23080	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.2043, 26.1791],
        [26.1774, 27.8495, 27.4377],
        [26.1774, 26.4998, 26.2645],
        [26.1774, 26.9240, 26.5202]], grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -1.0153950452804565 
model_pd.lagr.mean(): -0.9594268798828125 
model_pd.lambdas: dict_items([('pout', tensor([0.4674])), ('power', tensor([0.0237]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1155	 i:0 	 global-step:23100	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.2259, 26.1816],
        [26.1774, 31.5275, 33.5270],
        [26.1774, 26.1808, 26.1775],
        [26.1774, 28.8887, 28.8620]], grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -1.010946273803711 
model_pd.lagr.mean(): -0.9549781084060669 
model_pd.lambdas: dict_items([('pout', tensor([0.4653])), ('power', tensor([0.0236]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1156	 i:0 	 global-step:23120	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1791, 26.1775],
        [26.1774, 35.2857, 41.5856],
        [26.1774, 26.5068, 26.2676],
        [26.1774, 26.4998, 26.2645]], grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -1.0064972639083862 
model_pd.lagr.mean(): -0.9505290985107422 
model_pd.lambdas: dict_items([('pout', tensor([0.4633])), ('power', tensor([0.0235]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1157	 i:0 	 global-step:23140	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.2789, 26.1909],
        [26.1774, 26.2147, 26.1802],
        [26.1774, 26.3727, 26.2160],
        [26.1774, 28.8889, 28.8621]], grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -1.0020484924316406 
model_pd.lagr.mean(): -0.9460803270339966 
model_pd.lambdas: dict_items([('pout', tensor([0.4612])), ('power', tensor([0.0234]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1158	 i:0 	 global-step:23160	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1779, 26.1774],
        [26.1774, 26.1774, 26.1774],
        [26.1774, 28.9764, 28.9961],
        [26.1774, 26.1788, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -0.9975997805595398 
model_pd.lagr.mean(): -0.9416316151618958 
model_pd.lambdas: dict_items([('pout', tensor([0.4592])), ('power', tensor([0.0233]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1159	 i:0 	 global-step:23180	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1774, 26.1774],
        [26.1774, 30.3201, 31.2528],
        [26.1774, 26.3728, 26.2160],
        [26.1774, 31.5663, 33.6032]], grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -0.9931508302688599 
model_pd.lagr.mean(): -0.9371826648712158 
model_pd.lambdas: dict_items([('pout', tensor([0.4571])), ('power', tensor([0.0232]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1160	 i:0 	 global-step:23200	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1868, 26.1777],
        [26.1774, 28.9766, 28.9963],
        [26.1774, 26.1774, 26.1774],
        [26.1774, 30.3202, 31.2529]], grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.9887019991874695 
model_pd.lagr.mean(): -0.9327338337898254 
model_pd.lambdas: dict_items([('pout', tensor([0.4551])), ('power', tensor([0.0231]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1161	 i:0 	 global-step:23220	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1791, 26.1775],
        [26.1774, 26.5098, 26.2690],
        [26.1774, 26.5069, 26.2677],
        [26.1774, 26.3510, 26.2093]], grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -0.9842531681060791 
model_pd.lagr.mean(): -0.9282850027084351 
model_pd.lambdas: dict_items([('pout', tensor([0.4530])), ('power', tensor([0.0230]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1162	 i:0 	 global-step:23240	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1774, 26.1774],
        [26.1774, 26.4999, 26.2645],
        [26.1774, 28.9767, 28.9964],
        [26.1774, 26.2789, 26.1908]], grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -0.979804277420044 
model_pd.lagr.mean(): -0.9238361120223999 
model_pd.lambdas: dict_items([('pout', tensor([0.4509])), ('power', tensor([0.0229]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1163	 i:0 	 global-step:23260	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 28.4622, 28.2377],
        [26.1774, 26.5069, 26.2677],
        [26.1774, 26.1788, 26.1774],
        [26.1774, 30.3205, 31.2533]], grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.9753554463386536 
model_pd.lagr.mean(): -0.9193872809410095 
model_pd.lambdas: dict_items([('pout', tensor([0.4489])), ('power', tensor([0.0228]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1164	 i:0 	 global-step:23280	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.1774, 26.8040, 26.4349],
        [26.1774, 31.0986, 32.6971],
        [26.1774, 26.2147, 26.1801],
        [26.1774, 26.1774, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -0.9709065556526184 
model_pd.lagr.mean(): -0.9149383902549744 
model_pd.lambdas: dict_items([('pout', tensor([0.4468])), ('power', tensor([0.0227]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1165	 i:0 	 global-step:23300	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[26.1774, 31.5729, 33.6160],
        [26.1774, 26.8040, 26.4350],
        [26.1774, 27.9260, 27.5303],
        [26.1774, 30.5597, 31.6880]], grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -0.9664574861526489 
model_pd.lagr.mean(): -0.9104893207550049 
model_pd.lambdas: dict_items([('pout', tensor([0.4448])), ('power', tensor([0.0226]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1166	 i:0 	 global-step:23320	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.8040, 26.4350],
        [26.1774, 26.1796, 26.1774],
        [26.1774, 30.5598, 31.6881],
        [26.1774, 26.1788, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.962008535861969 
model_pd.lagr.mean(): -0.906040370464325 
model_pd.lambdas: dict_items([('pout', tensor([0.4427])), ('power', tensor([0.0225]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1167	 i:0 	 global-step:23340	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1808, 26.1775],
        [26.1774, 26.9243, 26.5203],
        [26.1774, 26.1776, 26.1774],
        [26.1774, 27.0952, 26.6570]], grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -0.9575595855712891 
model_pd.lagr.mean(): -0.901591420173645 
model_pd.lambdas: dict_items([('pout', tensor([0.4406])), ('power', tensor([0.0224]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1168	 i:0 	 global-step:23360	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 31.5671, 33.6046],
        [26.1774, 27.2650, 26.8090],
        [26.1774, 26.1779, 26.1774],
        [26.1774, 26.3728, 26.2160]], grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.9531107544898987 
model_pd.lagr.mean(): -0.8971425890922546 
model_pd.lambdas: dict_items([('pout', tensor([0.4386])), ('power', tensor([0.0223]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1169	 i:0 	 global-step:23380	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.3728, 26.2160],
        [26.1774, 28.9771, 28.9970],
        [26.1774, 27.4007, 26.9412],
        [26.1774, 27.5870, 27.1367]], grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.9486616849899292 
model_pd.lagr.mean(): -0.8926935195922852 
model_pd.lambdas: dict_items([('pout', tensor([0.4365])), ('power', tensor([0.0222]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1170	 i:0 	 global-step:23400	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 27.5870, 27.1367],
        [26.1774, 31.5733, 33.6167],
        [26.1774, 26.4999, 26.2645],
        [26.1774, 30.0355, 30.7469]], grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -0.9442126154899597 
model_pd.lagr.mean(): -0.8882444500923157 
model_pd.lambdas: dict_items([('pout', tensor([0.4345])), ('power', tensor([0.0221]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1171	 i:0 	 global-step:23420	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.1774, 29.2616, 29.4447],
        [26.1774, 30.3211, 31.2542],
        [26.1774, 31.5289, 33.5294],
        [26.1774, 33.3772, 37.3337]], grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.9397638440132141 
model_pd.lagr.mean(): -0.8837956786155701 
model_pd.lambdas: dict_items([('pout', tensor([0.4324])), ('power', tensor([0.0219]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1172	 i:0 	 global-step:23440	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.2789, 26.1908],
        [26.1774, 28.7096, 28.5936],
        [26.1774, 31.0993, 32.6981],
        [26.1774, 27.4008, 26.9413]], grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.05596817657351494 
model_pd.l_d.mean(): -0.9353148341178894 
model_pd.lagr.mean(): -0.8793466687202454 
model_pd.lambdas: dict_items([('pout', tensor([0.4304])), ('power', tensor([0.0218]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1173	 i:0 	 global-step:23460	 l-p:0.05596817657351494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 27.4009, 26.9413],
        [26.1774, 27.0402, 26.6112],
        [26.1774, 26.5100, 26.2690],
        [26.1774, 27.5871, 27.1368]], grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -0.9308658242225647 
model_pd.lagr.mean(): -0.8748976588249207 
model_pd.lambdas: dict_items([('pout', tensor([0.4283])), ('power', tensor([0.0217]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1174	 i:0 	 global-step:23480	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 30.3213, 31.2545],
        [26.1774, 31.5291, 33.5298],
        [26.1774, 26.3728, 26.2160],
        [26.1774, 26.5000, 26.2646]], grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.9264167547225952 
model_pd.lagr.mean(): -0.8704485893249512 
model_pd.lambdas: dict_items([('pout', tensor([0.4262])), ('power', tensor([0.0216]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1175	 i:0 	 global-step:23500	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1808, 26.1775],
        [26.1774, 28.9775, 28.9974],
        [26.1774, 33.3776, 37.3344],
        [26.1774, 26.5070, 26.2677]], grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.05596818029880524 
model_pd.l_d.mean(): -0.9219678640365601 
model_pd.lagr.mean(): -0.865999698638916 
model_pd.lambdas: dict_items([('pout', tensor([0.4242])), ('power', tensor([0.0215]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1176	 i:0 	 global-step:23520	 l-p:0.05596818029880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.2789, 26.1908],
        [26.1774, 30.5606, 31.6893],
        [26.1774, 26.9245, 26.5204],
        [26.1774, 31.0996, 32.6986]], grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.9175188541412354 
model_pd.lagr.mean(): -0.8615506887435913 
model_pd.lambdas: dict_items([('pout', tensor([0.4221])), ('power', tensor([0.0214]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1177	 i:0 	 global-step:23540	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1796, 26.1774],
        [26.1774, 29.0694, 29.1401],
        [26.1774, 26.2043, 26.1790],
        [26.1774, 27.9265, 27.5309]], grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.9130699634552002 
model_pd.lagr.mean(): -0.8571017980575562 
model_pd.lambdas: dict_items([('pout', tensor([0.4201])), ('power', tensor([0.0213]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1178	 i:0 	 global-step:23560	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1808, 26.1774],
        [26.1774, 31.5294, 33.5303],
        [26.1774, 26.2147, 26.1801],
        [26.1774, 28.4630, 28.2386]], grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.9086207747459412 
model_pd.lagr.mean(): -0.8526526093482971 
model_pd.lambdas: dict_items([('pout', tensor([0.4180])), ('power', tensor([0.0212]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1179	 i:0 	 global-step:23580	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 31.5741, 33.6179],
        [26.1774, 29.2621, 29.4454],
        [26.1774, 26.1808, 26.1774],
        [26.1774, 31.0998, 32.6990]], grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.9041718244552612 
model_pd.lagr.mean(): -0.8482036590576172 
model_pd.lambdas: dict_items([('pout', tensor([0.4159])), ('power', tensor([0.0211]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1180	 i:0 	 global-step:23600	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1774, 26.1774],
        [26.1774, 26.1774, 26.1774],
        [26.1774, 27.5874, 27.1370],
        [26.1774, 26.1788, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.8997228145599365 
model_pd.lagr.mean(): -0.8437546491622925 
model_pd.lambdas: dict_items([('pout', tensor([0.4139])), ('power', tensor([0.0210]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1181	 i:0 	 global-step:23620	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 28.4632, 28.2388],
        [26.1774, 27.8507, 27.4388],
        [26.1774, 26.5000, 26.2646],
        [26.1774, 26.3511, 26.2093]], grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.8952738046646118 
model_pd.lagr.mean(): -0.8393056392669678 
model_pd.lambdas: dict_items([('pout', tensor([0.4118])), ('power', tensor([0.0209]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1182	 i:0 	 global-step:23640	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1868, 26.1777],
        [26.1774, 31.5683, 33.6065],
        [26.1774, 27.4012, 26.9415],
        [26.1774, 27.0404, 26.6113]], grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.8908247351646423 
model_pd.lagr.mean(): -0.8348565697669983 
model_pd.lambdas: dict_items([('pout', tensor([0.4098])), ('power', tensor([0.0208]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1183	 i:0 	 global-step:23660	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1790, 26.1774],
        [26.1774, 26.1868, 26.1777],
        [26.1774, 26.2043, 26.1790],
        [26.1774, 28.9779, 28.9980]], grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.8863756656646729 
model_pd.lagr.mean(): -0.8304075002670288 
model_pd.lambdas: dict_items([('pout', tensor([0.4077])), ('power', tensor([0.0207]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1184	 i:0 	 global-step:23680	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.5001, 26.2646],
        [26.1774, 27.9268, 27.5311],
        [26.1774, 28.9780, 28.9981],
        [26.1774, 26.1788, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.8819265365600586 
model_pd.lagr.mean(): -0.8259583711624146 
model_pd.lambdas: dict_items([('pout', tensor([0.4057])), ('power', tensor([0.0206]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1185	 i:0 	 global-step:23700	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.5118, 26.2698],
        [26.1774, 31.5300, 33.5313],
        [26.1774, 27.9269, 27.5312],
        [26.1774, 26.8043, 26.4351]], grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.8774776458740234 
model_pd.lagr.mean(): -0.8215094804763794 
model_pd.lambdas: dict_items([('pout', tensor([0.4036])), ('power', tensor([0.0205]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1186	 i:0 	 global-step:23720	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 28.8906, 28.8643],
        [26.1774, 26.1773, 26.1774],
        [26.1774, 35.2884, 41.5913],
        [26.1774, 28.9781, 28.9982]], grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.873028576374054 
model_pd.lagr.mean(): -0.8170604109764099 
model_pd.lambdas: dict_items([('pout', tensor([0.4015])), ('power', tensor([0.0204]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1187	 i:0 	 global-step:23740	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.1774, 26.1774, 26.1773],
        [26.1774, 26.3511, 26.2092],
        [26.1774, 30.0367, 30.7486],
        [26.1774, 31.5687, 33.6071]], grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.055968184024095535 
model_pd.l_d.mean(): -0.8685794472694397 
model_pd.lagr.mean(): -0.8126112818717957 
model_pd.lambdas: dict_items([('pout', tensor([0.3995])), ('power', tensor([0.0203]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1188	 i:0 	 global-step:23760	 l-p:0.055968184024095535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.1773, 26.8044, 26.4351],
        [26.1773, 29.2627, 29.4461],
        [26.1773, 27.0957, 26.6573],
        [26.1773, 26.1774, 26.1773]], grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.8641303777694702 
model_pd.lagr.mean(): -0.8081622123718262 
model_pd.lambdas: dict_items([('pout', tensor([0.3974])), ('power', tensor([0.0202]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1189	 i:0 	 global-step:23780	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1775, 26.1774],
        [26.1773, 27.5877, 27.1372],
        [26.1773, 26.2043, 26.1790],
        [26.1773, 30.5615, 31.6908]], grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.8596814274787903 
model_pd.lagr.mean(): -0.8037132620811462 
model_pd.lambdas: dict_items([('pout', tensor([0.3954])), ('power', tensor([0.0201]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1190	 i:0 	 global-step:23800	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1808, 26.1774],
        [26.1773, 26.1795, 26.1774],
        [26.1773, 35.2888, 41.5920],
        [26.1773, 30.3224, 31.2563]], grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.855232298374176 
model_pd.lagr.mean(): -0.799264132976532 
model_pd.lambdas: dict_items([('pout', tensor([0.3933])), ('power', tensor([0.0200]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1191	 i:0 	 global-step:23820	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1790, 26.1774],
        [26.1773, 26.1787, 26.1774],
        [26.1773, 26.5101, 26.2690],
        [26.1773, 27.0406, 26.6114]], grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.8507831692695618 
model_pd.lagr.mean(): -0.7948150038719177 
model_pd.lambdas: dict_items([('pout', tensor([0.3913])), ('power', tensor([0.0199]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1192	 i:0 	 global-step:23840	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1790, 26.1774],
        [26.1773, 26.1795, 26.1774],
        [26.1773, 26.5071, 26.2677],
        [26.1773, 30.5617, 31.6911]], grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.8463341593742371 
model_pd.lagr.mean(): -0.7903659343719482 
model_pd.lambdas: dict_items([('pout', tensor([0.3892])), ('power', tensor([0.0198]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1193	 i:0 	 global-step:23860	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 28.8910, 28.8648],
        [26.1773, 26.1773, 26.1773],
        [26.1773, 31.5691, 33.6079],
        [26.1773, 29.2630, 29.4465]], grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.8418849110603333 
model_pd.lagr.mean(): -0.7859166860580444 
model_pd.lambdas: dict_items([('pout', tensor([0.3871])), ('power', tensor([0.0197]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1194	 i:0 	 global-step:23880	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 31.5692, 33.6080],
        [26.1773, 26.1773, 26.1773],
        [26.1773, 26.1773, 26.1773],
        [26.1773, 30.3227, 31.2567]], grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.8374359011650085 
model_pd.lagr.mean(): -0.7814677357673645 
model_pd.lambdas: dict_items([('pout', tensor([0.3851])), ('power', tensor([0.0196]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1195	 i:0 	 global-step:23900	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.1773, 26.5001, 26.2646],
        [26.1773, 30.0372, 30.7494],
        [26.1773, 31.5307, 33.5325],
        [26.1773, 26.1773, 26.1773]], grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.8329866528511047 
model_pd.lagr.mean(): -0.7770184874534607 
model_pd.lambdas: dict_items([('pout', tensor([0.3830])), ('power', tensor([0.0195]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1196	 i:0 	 global-step:23920	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.3729, 26.2160],
        [26.1773, 33.7132, 38.0597],
        [26.1773, 27.0959, 26.6574],
        [26.1773, 29.0705, 29.1415]], grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.8285374641418457 
model_pd.lagr.mean(): -0.7725692987442017 
model_pd.lambdas: dict_items([('pout', tensor([0.3810])), ('power', tensor([0.0194]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1197	 i:0 	 global-step:23940	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 33.7133, 38.0599],
        [26.1773, 27.0408, 26.6115],
        [26.1773, 34.6379, 40.1085],
        [26.1773, 26.5119, 26.2698]], grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.8240883946418762 
model_pd.lagr.mean(): -0.7681201696395874 
model_pd.lambdas: dict_items([('pout', tensor([0.3789])), ('power', tensor([0.0193]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1198	 i:0 	 global-step:23960	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1773, 26.1773],
        [26.1773, 31.5310, 33.5329],
        [26.1773, 31.5755, 33.6204],
        [26.1773, 26.5002, 26.2646]], grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.8196391463279724 
model_pd.lagr.mean(): -0.7636709213256836 
model_pd.lambdas: dict_items([('pout', tensor([0.3768])), ('power', tensor([0.0192]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1199	 i:0 	 global-step:23980	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 27.4017, 26.9419],
        [26.1773, 26.5102, 26.2690],
        [26.1773, 33.2370, 37.0329],
        [26.1773, 26.2043, 26.1789]], grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.8151898980140686 
model_pd.lagr.mean(): -0.7592216730117798 
model_pd.lambdas: dict_items([('pout', tensor([0.3748])), ('power', tensor([0.0191]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1200	 i:0 	 global-step:24000	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.3729, 26.2160],
        [26.1773, 31.5311, 33.5332],
        [26.1773, 27.4017, 26.9419],
        [26.1773, 30.0376, 30.7499]], grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.055968187749385834 
model_pd.l_d.mean(): -0.8107407689094543 
model_pd.lagr.mean(): -0.7547726035118103 
model_pd.lambdas: dict_items([('pout', tensor([0.3727])), ('power', tensor([0.0190]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1201	 i:0 	 global-step:24020	 l-p:0.055968187749385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1773, 26.5072, 26.2677],
        [26.1773, 26.1773, 26.1773],
        [26.1773, 27.5881, 27.1376],
        [26.1773, 26.1773, 26.1773]], grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.8062915205955505 
model_pd.lagr.mean(): -0.7503232955932617 
model_pd.lambdas: dict_items([('pout', tensor([0.3707])), ('power', tensor([0.0188]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1202	 i:0 	 global-step:24040	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.9250, 26.5207],
        [26.1773, 31.5312, 33.5334],
        [26.1773, 26.2043, 26.1789],
        [26.1773, 26.5119, 26.2698]], grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.8018423318862915 
model_pd.lagr.mean(): -0.7458741664886475 
model_pd.lambdas: dict_items([('pout', tensor([0.3686])), ('power', tensor([0.0187]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1203	 i:0 	 global-step:24060	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 30.0377, 30.7502],
        [26.1773, 27.0960, 26.6575],
        [26.1773, 26.1807, 26.1774],
        [26.1773, 26.5002, 26.2646]], grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.7973932027816772 
model_pd.lagr.mean(): -0.7414250373840332 
model_pd.lambdas: dict_items([('pout', tensor([0.3666])), ('power', tensor([0.0186]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1204	 i:0 	 global-step:24080	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[26.1773, 30.3234, 31.2577],
        [26.1773, 26.9250, 26.5207],
        [26.1773, 31.5314, 33.5336],
        [26.1773, 27.0409, 26.6116]], grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7929438352584839 
model_pd.lagr.mean(): -0.7369756102561951 
model_pd.lambdas: dict_items([('pout', tensor([0.3645])), ('power', tensor([0.0185]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1205	 i:0 	 global-step:24100	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.5120, 26.2698],
        [26.1773, 26.1807, 26.1774],
        [26.1773, 26.3729, 26.2160],
        [26.1773, 33.7139, 38.0611]], grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.7884947061538696 
model_pd.lagr.mean(): -0.7325265407562256 
model_pd.lambdas: dict_items([('pout', tensor([0.3624])), ('power', tensor([0.0184]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1206	 i:0 	 global-step:24120	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 28.9792, 28.9996],
        [26.1773, 26.2146, 26.1800],
        [26.1773, 26.8046, 26.4352],
        [26.1773, 31.5315, 33.5339]], grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7840455174446106 
model_pd.lagr.mean(): -0.7280772924423218 
model_pd.lambdas: dict_items([('pout', tensor([0.3604])), ('power', tensor([0.0183]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1207	 i:0 	 global-step:24140	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 27.0410, 26.6116],
        [26.1773, 26.5120, 26.2698],
        [26.1773, 26.1778, 26.1773],
        [26.1773, 27.5882, 27.1377]], grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7795963287353516 
model_pd.lagr.mean(): -0.7236281037330627 
model_pd.lambdas: dict_items([('pout', tensor([0.3583])), ('power', tensor([0.0182]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1208	 i:0 	 global-step:24160	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.5120, 26.2698],
        [26.1773, 33.3803, 37.3395],
        [26.1773, 26.5072, 26.2677],
        [26.1773, 30.0380, 30.7506]], grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7751470804214478 
model_pd.lagr.mean(): -0.7191788554191589 
model_pd.lambdas: dict_items([('pout', tensor([0.3563])), ('power', tensor([0.0181]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1209	 i:0 	 global-step:24180	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.2043, 26.1789],
        [26.1773, 26.2146, 26.1800],
        [26.1773, 26.2259, 26.1814],
        [26.1773, 26.8047, 26.4352]], grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7706978917121887 
model_pd.lagr.mean(): -0.7147296667098999 
model_pd.lambdas: dict_items([('pout', tensor([0.3542])), ('power', tensor([0.0180]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0581])), ('power', tensor([-2.0667]))])
epoch：1210	 i:0 	 global-step:24200	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1773, 26.1773],
        [26.1773, 30.0382, 30.7508],
        [26.1773, 26.5102, 26.2690],
        [26.1773, 28.7116, 28.5959]], grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7662484645843506 
model_pd.lagr.mean(): -0.7102802395820618 
model_pd.lambdas: dict_items([('pout', tensor([0.3521])), ('power', tensor([0.0179]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1211	 i:0 	 global-step:24220	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 27.0962, 26.6576],
        [26.1773, 26.1867, 26.1776],
        [26.1773, 26.1774, 26.1773],
        [26.1773, 30.0382, 30.7509]], grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7617994546890259 
model_pd.lagr.mean(): -0.7058312296867371 
model_pd.lambdas: dict_items([('pout', tensor([0.3501])), ('power', tensor([0.0178]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1212	 i:0 	 global-step:24240	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1777, 26.1773],
        [26.1773, 28.8920, 28.8660],
        [26.1773, 26.2259, 26.1814],
        [26.1773, 27.8518, 27.4399]], grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7573500275611877 
model_pd.lagr.mean(): -0.7013818025588989 
model_pd.lambdas: dict_items([('pout', tensor([0.3480])), ('power', tensor([0.0177]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1213	 i:0 	 global-step:24260	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1774, 26.1773],
        [26.1773, 26.5103, 26.2690],
        [26.1773, 30.5631, 31.6933],
        [26.1773, 26.1773, 26.1773]], grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7529008388519287 
model_pd.lagr.mean(): -0.6969326138496399 
model_pd.lambdas: dict_items([('pout', tensor([0.3460])), ('power', tensor([0.0176]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1214	 i:0 	 global-step:24280	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 31.1023, 32.7031],
        [26.1773, 30.0384, 30.7512],
        [26.1773, 26.1773, 26.1773],
        [26.1773, 33.7146, 38.0624]], grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7484516501426697 
model_pd.lagr.mean(): -0.6924834251403809 
model_pd.lambdas: dict_items([('pout', tensor([0.3439])), ('power', tensor([0.0175]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1215	 i:0 	 global-step:24300	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 33.2382, 37.0351],
        [26.1773, 31.5767, 33.6224],
        [26.1773, 26.3512, 26.2092],
        [26.1773, 26.5073, 26.2677]], grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.7440023422241211 
model_pd.lagr.mean(): -0.688034176826477 
model_pd.lambdas: dict_items([('pout', tensor([0.3419])), ('power', tensor([0.0174]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1216	 i:0 	 global-step:24320	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 28.4648, 28.2406],
        [26.1773, 33.2383, 37.0353],
        [26.1773, 28.8921, 28.8662],
        [26.1773, 26.5003, 26.2646]], grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7395530939102173 
model_pd.lagr.mean(): -0.6835848689079285 
model_pd.lambdas: dict_items([('pout', tensor([0.3398])), ('power', tensor([0.0173]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1217	 i:0 	 global-step:24340	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 31.1025, 32.7034],
        [26.1773, 26.1787, 26.1773],
        [26.1773, 26.1773, 26.1773],
        [26.1773, 35.2908, 41.5964]], grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.05596820265054703 
model_pd.l_d.mean(): -0.7351038455963135 
model_pd.lagr.mean(): -0.6791356205940247 
model_pd.lambdas: dict_items([('pout', tensor([0.3377])), ('power', tensor([0.0172]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1218	 i:0 	 global-step:24360	 l-p:0.05596820265054703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1867, 26.1776],
        [26.1773, 27.0964, 26.6577],
        [26.1773, 26.1773, 26.1773],
        [26.1773, 31.5769, 33.6228]], grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7306545376777649 
model_pd.lagr.mean(): -0.6746863126754761 
model_pd.lambdas: dict_items([('pout', tensor([0.3357])), ('power', tensor([0.0171]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1219	 i:0 	 global-step:24380	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1773, 26.1773],
        [26.1773, 26.1773, 26.1773],
        [26.1773, 27.2663, 26.8099],
        [26.1773, 27.5886, 27.1380]], grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7262052297592163 
model_pd.lagr.mean(): -0.6702370047569275 
model_pd.lambdas: dict_items([('pout', tensor([0.3336])), ('power', tensor([0.0170]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1220	 i:0 	 global-step:24400	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.1773, 26.1807, 26.1773],
        [26.1773, 35.2911, 41.5968],
        [26.1773, 26.5121, 26.2698],
        [26.1773, 27.4022, 26.9423]], grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): 0.05596820265054703 
model_pd.l_d.mean(): -0.721756100654602 
model_pd.lagr.mean(): -0.6657878756523132 
model_pd.lambdas: dict_items([('pout', tensor([0.3316])), ('power', tensor([0.0169]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1221	 i:0 	 global-step:24420	 l-p:0.05596820265054703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.1773, 30.5636, 31.6941],
        [26.1773, 30.0388, 30.7518],
        [26.1773, 27.9282, 27.5325],
        [26.1773, 26.1773, 26.1772]], grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.05596819519996643 
model_pd.l_d.mean(): -0.7173067331314087 
model_pd.lagr.mean(): -0.6613385677337646 
model_pd.lambdas: dict_items([('pout', tensor([0.3295])), ('power', tensor([0.0168]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1222	 i:0 	 global-step:24440	 l-p:0.05596819519996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.1772, 26.1774, 26.1773],
        [26.1772, 27.0413, 26.6118],
        [26.1772, 29.2645, 29.4486],
        [26.1772, 26.1773, 26.1773]], grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.05596820265054703 
model_pd.l_d.mean(): -0.7128574848175049 
model_pd.lagr.mean(): -0.6568892598152161 
model_pd.lambdas: dict_items([('pout', tensor([0.3274])), ('power', tensor([0.0167]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1223	 i:0 	 global-step:24460	 l-p:0.05596820265054703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.1773, 26.1772],
        [26.1772, 28.4651, 28.2410],
        [26.1772, 26.5003, 26.2646],
        [26.1772, 26.9253, 26.5209]], grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.7084081172943115 
model_pd.lagr.mean(): -0.6524398922920227 
model_pd.lambdas: dict_items([('pout', tensor([0.3254])), ('power', tensor([0.0166]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1224	 i:0 	 global-step:24480	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 27.0413, 26.6118],
        [26.1772, 26.2259, 26.1814],
        [26.1772, 35.2913, 41.5974],
        [26.1772, 28.0153, 27.6407]], grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): 0.055968210101127625 
model_pd.l_d.mean(): -0.7039588689804077 
model_pd.lagr.mean(): -0.6479906439781189 
model_pd.lambdas: dict_items([('pout', tensor([0.3233])), ('power', tensor([0.0165]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1225	 i:0 	 global-step:24500	 l-p:0.055968210101127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 27.9284, 27.5326],
        [26.1772, 26.1789, 26.1773],
        [26.1772, 28.0153, 27.6407],
        [26.1772, 26.5121, 26.2698]], grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.05596819892525673 
model_pd.l_d.mean(): -0.6995095610618591 
model_pd.lagr.mean(): -0.6435413360595703 
model_pd.lambdas: dict_items([('pout', tensor([0.3213])), ('power', tensor([0.0164]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1226	 i:0 	 global-step:24520	 l-p:0.05596819892525673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 33.3816, 37.3419],
        [26.1772, 33.7155, 38.0641],
        [26.1772, 26.5074, 26.2677],
        [26.1772, 31.5714, 33.6118]], grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.05596820265054703 
model_pd.l_d.mean(): -0.695060133934021 
model_pd.lagr.mean(): -0.6390919089317322 
model_pd.lambdas: dict_items([('pout', tensor([0.3192])), ('power', tensor([0.0163]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1227	 i:0 	 global-step:24540	 l-p:0.05596820265054703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.9254, 26.5209],
        [26.1772, 26.1777, 26.1772],
        [26.1772, 26.1772, 26.1772],
        [26.1772, 28.9802, 29.0009]], grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.055968210101127625 
model_pd.l_d.mean(): -0.6906108260154724 
model_pd.lagr.mean(): -0.6346426010131836 
model_pd.lambdas: dict_items([('pout', tensor([0.3172])), ('power', tensor([0.0162]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1228	 i:0 	 global-step:24560	 l-p:0.055968210101127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.1777, 26.1772],
        [26.1772, 26.2259, 26.1814],
        [26.1772, 33.7156, 38.0643],
        [26.1772, 29.0722, 29.1436]], grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.05596820265054703 
model_pd.l_d.mean(): -0.6861613988876343 
model_pd.lagr.mean(): -0.6301931738853455 
model_pd.lambdas: dict_items([('pout', tensor([0.3151])), ('power', tensor([0.0161]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1229	 i:0 	 global-step:24580	 l-p:0.05596820265054703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 35.2917, 41.5982],
        [26.1772, 28.8927, 28.8670],
        [26.1772, 26.1772, 26.1772],
        [26.1772, 28.0154, 27.6409]], grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.055968210101127625 
model_pd.l_d.mean(): -0.6817120313644409 
model_pd.lagr.mean(): -0.6257438063621521 
model_pd.lambdas: dict_items([('pout', tensor([0.3130])), ('power', tensor([0.0160]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1230	 i:0 	 global-step:24600	 l-p:0.055968210101127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.2789, 26.1907],
        [26.1772, 33.2392, 37.0371],
        [26.1772, 27.2666, 26.8101],
        [26.1772, 26.2042, 26.1788]], grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.055968210101127625 
model_pd.l_d.mean(): -0.6772626638412476 
model_pd.lagr.mean(): -0.6212944388389587 
model_pd.lambdas: dict_items([('pout', tensor([0.3110])), ('power', tensor([0.0159]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1231	 i:0 	 global-step:24620	 l-p:0.055968210101127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.1806, 26.1773],
        [26.1772, 26.5074, 26.2677],
        [26.1772, 26.1866, 26.1775],
        [26.1772, 28.8928, 28.8671]], grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.055968210101127625 
model_pd.l_d.mean(): -0.6728132963180542 
model_pd.lagr.mean(): -0.6168450713157654 
model_pd.lambdas: dict_items([('pout', tensor([0.3089])), ('power', tensor([0.0157]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1232	 i:0 	 global-step:24640	 l-p:0.055968210101127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 31.5778, 33.6243],
        [26.1772, 26.2042, 26.1788],
        [26.1772, 34.6405, 40.1138],
        [26.1772, 26.5074, 26.2677]], grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 0.055968210101127625 
model_pd.l_d.mean(): -0.6683638691902161 
model_pd.lagr.mean(): -0.6123956441879272 
model_pd.lambdas: dict_items([('pout', tensor([0.3069])), ('power', tensor([0.0156]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1233	 i:0 	 global-step:24660	 l-p:0.055968210101127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.1772, 26.1772],
        [26.1772, 27.9286, 27.5329],
        [26.1772, 29.2651, 29.4493],
        [26.1772, 26.2789, 26.1907]], grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.05596821382641792 
model_pd.l_d.mean(): -0.6639145016670227 
model_pd.lagr.mean(): -0.6079462766647339 
model_pd.lambdas: dict_items([('pout', tensor([0.3048])), ('power', tensor([0.0155]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1234	 i:0 	 global-step:24680	 l-p:0.05596821382641792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.1772, 27.8526, 27.4405],
        [26.1772, 26.9255, 26.5209],
        [26.1772, 30.0395, 30.7528],
        [26.1772, 26.1772, 26.1772]], grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.055968210101127625 
model_pd.l_d.mean(): -0.6594650745391846 
model_pd.lagr.mean(): -0.6034968495368958 
model_pd.lambdas: dict_items([('pout', tensor([0.3027])), ('power', tensor([0.0154]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1235	 i:0 	 global-step:24700	 l-p:0.055968210101127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.5074, 26.2677],
        [26.1772, 33.7161, 38.0652],
        [26.1772, 28.8930, 28.8673],
        [26.1772, 26.3512, 26.2091]], grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.055968210101127625 
model_pd.l_d.mean(): -0.6550157070159912 
model_pd.lagr.mean(): -0.5990474820137024 
model_pd.lambdas: dict_items([('pout', tensor([0.3007])), ('power', tensor([0.0153]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1236	 i:0 	 global-step:24720	 l-p:0.055968210101127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[26.1772, 27.0967, 26.6579],
        [26.1772, 27.8526, 27.4406],
        [26.1772, 26.9255, 26.5209],
        [26.1772, 29.0725, 29.1441]], grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): 0.055968210101127625 
model_pd.l_d.mean(): -0.6505661606788635 
model_pd.lagr.mean(): -0.5945979356765747 
model_pd.lambdas: dict_items([('pout', tensor([0.2986])), ('power', tensor([0.0152]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1237	 i:0 	 global-step:24740	 l-p:0.055968210101127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 29.0726, 29.1441],
        [26.1772, 29.2652, 29.4495],
        [26.1772, 30.5645, 31.6955],
        [26.1772, 26.1806, 26.1773]], grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.05596821382641792 
model_pd.l_d.mean(): -0.6461167931556702 
model_pd.lagr.mean(): -0.5901485681533813 
model_pd.lambdas: dict_items([('pout', tensor([0.2966])), ('power', tensor([0.0151]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1238	 i:0 	 global-step:24760	 l-p:0.05596821382641792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.1786, 26.1772],
        [26.1772, 26.5122, 26.2698],
        [26.1772, 33.7163, 38.0656],
        [26.1772, 27.4026, 26.9426]], grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.641667366027832 
model_pd.lagr.mean(): -0.5856991410255432 
model_pd.lambdas: dict_items([('pout', tensor([0.2945])), ('power', tensor([0.0150]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1239	 i:0 	 global-step:24780	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 27.5891, 27.1385],
        [26.1772, 26.3730, 26.2159],
        [26.1772, 28.7128, 28.5975],
        [26.1772, 28.0158, 27.6412]], grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.05596821382641792 
model_pd.l_d.mean(): -0.6372179388999939 
model_pd.lagr.mean(): -0.5812497138977051 
model_pd.lambdas: dict_items([('pout', tensor([0.2925])), ('power', tensor([0.0149]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1240	 i:0 	 global-step:24800	 l-p:0.05596821382641792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.1772, 26.1772],
        [26.1772, 30.0398, 30.7533],
        [26.1772, 27.9288, 27.5331],
        [26.1772, 26.3512, 26.2091]], grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): 0.05596821382641792 
model_pd.l_d.mean(): -0.6327685117721558 
model_pd.lagr.mean(): -0.5768002867698669 
model_pd.lambdas: dict_items([('pout', tensor([0.2904])), ('power', tensor([0.0148]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1241	 i:0 	 global-step:24820	 l-p:0.05596821382641792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.5004, 26.2645],
        [26.1772, 28.8932, 28.8676],
        [26.1772, 27.0968, 26.6580],
        [26.1772, 26.8051, 26.4354]], grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.6283190846443176 
model_pd.lagr.mean(): -0.5723508596420288 
model_pd.lambdas: dict_items([('pout', tensor([0.2883])), ('power', tensor([0.0147]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1242	 i:0 	 global-step:24840	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[26.1772, 27.9289, 27.5331],
        [26.1772, 28.0159, 27.6413],
        [26.1772, 27.4027, 26.9427],
        [26.1772, 28.9809, 29.0017]], grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.6238696575164795 
model_pd.lagr.mean(): -0.5679014325141907 
model_pd.lambdas: dict_items([('pout', tensor([0.2863])), ('power', tensor([0.0146]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1243	 i:0 	 global-step:24860	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.3730, 26.2159],
        [26.1772, 26.1772, 26.1772],
        [26.1772, 34.6412, 40.1152],
        [26.1772, 27.2668, 26.8103]], grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.05596821382641792 
model_pd.l_d.mean(): -0.6194201707839966 
model_pd.lagr.mean(): -0.5634519457817078 
model_pd.lambdas: dict_items([('pout', tensor([0.2842])), ('power', tensor([0.0145]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1244	 i:0 	 global-step:24880	 l-p:0.05596821382641792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.1773, 26.1772],
        [26.1772, 28.4659, 28.2419],
        [26.1772, 26.5074, 26.2677],
        [26.1772, 26.2042, 26.1788]], grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.6149706840515137 
model_pd.lagr.mean(): -0.5590024590492249 
model_pd.lambdas: dict_items([('pout', tensor([0.2822])), ('power', tensor([0.0144]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1245	 i:0 	 global-step:24900	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 26.1788, 26.1772],
        [26.1772, 26.1772, 26.1772],
        [26.1772, 26.5122, 26.2698],
        [26.1772, 26.1776, 26.1772]], grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.05596821382641792 
model_pd.l_d.mean(): -0.6105213165283203 
model_pd.lagr.mean(): -0.5545530915260315 
model_pd.lambdas: dict_items([('pout', tensor([0.2801])), ('power', tensor([0.0143]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1246	 i:0 	 global-step:24920	 l-p:0.05596821382641792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.1772, 26.5122, 26.2698],
        [26.1772, 33.2402, 37.0390],
        [26.1772, 26.1794, 26.1772],
        [26.1772, 26.1772, 26.1772]], grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.6060717701911926 
model_pd.lagr.mean(): -0.5501035451889038 
model_pd.lambdas: dict_items([('pout', tensor([0.2781])), ('power', tensor([0.0142]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1247	 i:0 	 global-step:24940	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 29.0730, 29.1447],
        [26.1772, 26.2042, 26.1788],
        [26.1772, 27.2669, 26.8103],
        [26.1772, 26.2258, 26.1813]], grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.05596821382641792 
model_pd.l_d.mean(): -0.601622462272644 
model_pd.lagr.mean(): -0.5456542372703552 
model_pd.lambdas: dict_items([('pout', tensor([0.2760])), ('power', tensor([0.0141]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1248	 i:0 	 global-step:24960	 l-p:0.05596821382641792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.1772, 27.4029, 26.9428],
        [26.1772, 33.2404, 37.0392],
        [26.1772, 31.5342, 33.5384],
        [26.1772, 26.3512, 26.2091]], grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.5971729159355164 
model_pd.lagr.mean(): -0.5412046909332275 
model_pd.lambdas: dict_items([('pout', tensor([0.2739])), ('power', tensor([0.0140]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1249	 i:0 	 global-step:24980	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.5105, 26.2690],
        [26.1771, 26.1788, 26.1772],
        [26.1771, 28.9812, 29.0021],
        [26.1771, 26.8052, 26.4354]], grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.5927234292030334 
model_pd.lagr.mean(): -0.5367552042007446 
model_pd.lambdas: dict_items([('pout', tensor([0.2719])), ('power', tensor([0.0139]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1250	 i:0 	 global-step:25000	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.2258, 26.1813],
        [26.1771, 26.2042, 26.1788],
        [26.1771, 31.5343, 33.5386],
        [26.1771, 26.1776, 26.1772]], grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.5882740020751953 
model_pd.lagr.mean(): -0.5323057770729065 
model_pd.lambdas: dict_items([('pout', tensor([0.2698])), ('power', tensor([0.0138]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1251	 i:0 	 global-step:25020	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1772, 26.1771],
        [26.1771, 29.0731, 29.1449],
        [26.1771, 26.1771, 26.1771],
        [26.1771, 26.1866, 26.1775]], grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.5838245153427124 
model_pd.lagr.mean(): -0.5278562903404236 
model_pd.lambdas: dict_items([('pout', tensor([0.2678])), ('power', tensor([0.0137]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1252	 i:0 	 global-step:25040	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1771, 30.5653, 31.6968],
        [26.1771, 27.9292, 27.5334],
        [26.1771, 26.1772, 26.1771],
        [26.1771, 26.1771, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.5793750286102295 
model_pd.lagr.mean(): -0.5234068036079407 
model_pd.lambdas: dict_items([('pout', tensor([0.2657])), ('power', tensor([0.0136]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1253	 i:0 	 global-step:25060	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1866, 26.1775],
        [26.1771, 26.5122, 26.2698],
        [26.1771, 26.2145, 26.1799],
        [26.1771, 28.8937, 28.8682]], grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.5749256014823914 
model_pd.lagr.mean(): -0.5189573764801025 
model_pd.lambdas: dict_items([('pout', tensor([0.2636])), ('power', tensor([0.0135]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1254	 i:0 	 global-step:25080	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 28.9813, 29.0023],
        [26.1771, 31.5345, 33.5389],
        [26.1771, 27.0970, 26.6581],
        [26.1771, 26.5005, 26.2645]], grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.05596821382641792 
model_pd.l_d.mean(): -0.5704761147499084 
model_pd.lagr.mean(): -0.5145078897476196 
model_pd.lambdas: dict_items([('pout', tensor([0.2616])), ('power', tensor([0.0134]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1255	 i:0 	 global-step:25100	 l-p:0.05596821382641792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.5075, 26.2677],
        [26.1771, 26.2145, 26.1799],
        [26.1771, 31.5791, 33.6265],
        [26.1771, 30.3262, 31.2621]], grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.5660265684127808 
model_pd.lagr.mean(): -0.5100583434104919 
model_pd.lambdas: dict_items([('pout', tensor([0.2595])), ('power', tensor([0.0133]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1256	 i:0 	 global-step:25120	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 27.4030, 26.9429],
        [26.1771, 26.5005, 26.2645],
        [26.1771, 26.3730, 26.2159],
        [26.1771, 27.2671, 26.8104]], grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.5615771412849426 
model_pd.lagr.mean(): -0.5056089162826538 
model_pd.lambdas: dict_items([('pout', tensor([0.2575])), ('power', tensor([0.0132]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1257	 i:0 	 global-step:25140	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228]])
 pt:tensor([[26.1771, 29.2661, 29.4507],
        [26.1771, 28.0163, 27.6417],
        [26.1771, 26.8052, 26.4355],
        [26.1771, 33.2409, 37.0402]], grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.5571275949478149 
model_pd.lagr.mean(): -0.5011593699455261 
model_pd.lambdas: dict_items([('pout', tensor([0.2554])), ('power', tensor([0.0131]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1258	 i:0 	 global-step:25160	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 33.2409, 37.0403],
        [26.1771, 33.7174, 38.0679],
        [26.1771, 26.1771, 26.1771],
        [26.1771, 27.9293, 27.5336]], grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.552678108215332 
model_pd.lagr.mean(): -0.4967098832130432 
model_pd.lambdas: dict_items([('pout', tensor([0.2534])), ('power', tensor([0.0130]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1259	 i:0 	 global-step:25180	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]])
 pt:tensor([[26.1771, 28.8939, 28.8685],
        [26.1771, 27.0418, 26.6121],
        [26.1771, 27.5896, 27.1389],
        [26.1771, 34.6421, 40.1171]], grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.5482285618782043 
model_pd.lagr.mean(): -0.4922603368759155 
model_pd.lambdas: dict_items([('pout', tensor([0.2513])), ('power', tensor([0.0129]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1260	 i:0 	 global-step:25200	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.5075, 26.2677],
        [26.1771, 26.1771, 26.1771],
        [26.1771, 30.5657, 31.6974],
        [26.1771, 26.1793, 26.1772]], grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.5437790155410767 
model_pd.lagr.mean(): -0.48781079053878784 
model_pd.lambdas: dict_items([('pout', tensor([0.2492])), ('power', tensor([0.0128]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1261	 i:0 	 global-step:25220	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]])
 pt:tensor([[26.1771, 31.5349, 33.5396],
        [26.1771, 29.2662, 29.4509],
        [26.1771, 30.3265, 31.2625],
        [26.1771, 34.6422, 40.1174]], grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.5393295884132385 
model_pd.lagr.mean(): -0.4833613634109497 
model_pd.lambdas: dict_items([('pout', tensor([0.2472])), ('power', tensor([0.0126]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1262	 i:0 	 global-step:25240	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.9258, 26.5211],
        [26.1771, 26.2789, 26.1906],
        [26.1771, 28.9816, 29.0027],
        [26.1771, 27.9294, 27.5337]], grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.5348800420761108 
model_pd.lagr.mean(): -0.478911817073822 
model_pd.lambdas: dict_items([('pout', tensor([0.2451])), ('power', tensor([0.0125]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1263	 i:0 	 global-step:25260	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]])
 pt:tensor([[26.1771, 29.2663, 29.4510],
        [26.1771, 27.8534, 27.4413],
        [26.1771, 29.0736, 29.1455],
        [26.1771, 33.7177, 38.0685]], grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.5304303765296936 
model_pd.lagr.mean(): -0.4744621515274048 
model_pd.lambdas: dict_items([('pout', tensor([0.2431])), ('power', tensor([0.0124]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1264	 i:0 	 global-step:25280	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 31.5736, 33.6155],
        [26.1771, 27.5897, 27.1390],
        [26.1771, 35.2938, 41.6027],
        [26.1771, 26.3731, 26.2159]], grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.5259808301925659 
model_pd.lagr.mean(): -0.4700126051902771 
model_pd.lambdas: dict_items([('pout', tensor([0.2410])), ('power', tensor([0.0123]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1265	 i:0 	 global-step:25300	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.2041, 26.1787],
        [26.1771, 26.5005, 26.2645],
        [26.1771, 26.2145, 26.1798],
        [26.1771, 28.9818, 29.0029]], grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.5215312838554382 
model_pd.lagr.mean(): -0.4655630588531494 
model_pd.lambdas: dict_items([('pout', tensor([0.2389])), ('power', tensor([0.0122]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1266	 i:0 	 global-step:25320	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 35.2939, 41.6029],
        [26.1771, 26.1771, 26.1771],
        [26.1771, 26.3731, 26.2158],
        [26.1771, 27.2672, 26.8106]], grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.5170817375183105 
model_pd.lagr.mean(): -0.46111351251602173 
model_pd.lambdas: dict_items([('pout', tensor([0.2369])), ('power', tensor([0.0121]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1267	 i:0 	 global-step:25340	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1788, 26.1771],
        [26.1771, 28.4667, 28.2427],
        [26.1771, 30.5661, 31.6979],
        [26.1771, 26.2145, 26.1798]], grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.5126321911811829 
model_pd.lagr.mean(): -0.45666396617889404 
model_pd.lambdas: dict_items([('pout', tensor([0.2348])), ('power', tensor([0.0120]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1268	 i:0 	 global-step:25360	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1805, 26.1772],
        [26.1771, 29.0738, 29.1457],
        [26.1771, 26.1771, 26.1771],
        [26.1771, 26.2789, 26.1906]], grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.5081825852394104 
model_pd.lagr.mean(): -0.4522143602371216 
model_pd.lambdas: dict_items([('pout', tensor([0.2328])), ('power', tensor([0.0119]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1269	 i:0 	 global-step:25380	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.5105, 26.2690],
        [26.1771, 26.1771, 26.1771],
        [26.1771, 27.0420, 26.6122],
        [26.1771, 26.5076, 26.2677]], grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.5037330389022827 
model_pd.lagr.mean(): -0.4477648138999939 
model_pd.lambdas: dict_items([('pout', tensor([0.2307])), ('power', tensor([0.0118]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1270	 i:0 	 global-step:25400	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[26.1771, 26.8054, 26.4355],
        [26.1771, 27.5898, 27.1391],
        [26.1771, 28.7139, 28.5988],
        [26.1771, 28.8943, 28.8690]], grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.49928343296051025 
model_pd.lagr.mean(): -0.44331520795822144 
model_pd.lambdas: dict_items([('pout', tensor([0.2287])), ('power', tensor([0.0117]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1271	 i:0 	 global-step:25420	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 31.1055, 32.7084],
        [26.1771, 26.3512, 26.2091],
        [26.1771, 26.5123, 26.2698],
        [26.1771, 26.1787, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.4948337972164154 
model_pd.lagr.mean(): -0.4388655722141266 
model_pd.lambdas: dict_items([('pout', tensor([0.2266])), ('power', tensor([0.0116]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1272	 i:0 	 global-step:25440	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.3731, 26.2158],
        [26.1771, 27.0420, 26.6122],
        [26.1771, 26.1771, 26.1771],
        [26.1771, 27.4033, 26.9431]], grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.4903842508792877 
model_pd.lagr.mean(): -0.4344160258769989 
model_pd.lambdas: dict_items([('pout', tensor([0.2245])), ('power', tensor([0.0115]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1273	 i:0 	 global-step:25460	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.1771, 26.2041, 26.1787],
        [26.1771, 26.1771, 26.1771],
        [26.1771, 26.1785, 26.1771],
        [26.1771, 26.1771, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.48593467473983765 
model_pd.lagr.mean(): -0.42996644973754883 
model_pd.lambdas: dict_items([('pout', tensor([0.2225])), ('power', tensor([0.0114]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1274	 i:0 	 global-step:25480	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 31.1057, 32.7086],
        [26.1771, 26.5123, 26.2698],
        [26.1771, 33.7183, 38.0696],
        [26.1771, 31.5355, 33.5406]], grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.4814850687980652 
model_pd.lagr.mean(): -0.42551684379577637 
model_pd.lambdas: dict_items([('pout', tensor([0.2204])), ('power', tensor([0.0113]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1275	 i:0 	 global-step:25500	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.9260, 26.5212],
        [26.1771, 27.2674, 26.8107],
        [26.1771, 26.1865, 26.1774],
        [26.1771, 35.2944, 41.6039]], grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.4770354628562927 
model_pd.lagr.mean(): -0.4210672378540039 
model_pd.lambdas: dict_items([('pout', tensor([0.2184])), ('power', tensor([0.0112]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1276	 i:0 	 global-step:25520	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[26.1771, 30.5665, 31.6985],
        [26.1771, 34.6430, 40.1190],
        [26.1771, 27.8537, 27.4416],
        [26.1771, 28.4670, 28.2431]], grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.4725859463214874 
model_pd.lagr.mean(): -0.4166177213191986 
model_pd.lambdas: dict_items([('pout', tensor([0.2163])), ('power', tensor([0.0111]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1277	 i:0 	 global-step:25540	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 27.0973, 26.6583],
        [26.1771, 27.9298, 27.5340],
        [26.1771, 26.5006, 26.2645],
        [26.1771, 26.5106, 26.2690]], grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.46813634037971497 
model_pd.lagr.mean(): -0.41216811537742615 
model_pd.lambdas: dict_items([('pout', tensor([0.2142])), ('power', tensor([0.0110]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1278	 i:0 	 global-step:25560	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1771, 26.1771],
        [26.1771, 26.8055, 26.4356],
        [26.1771, 27.5900, 27.1392],
        [26.1771, 28.7142, 28.5991]], grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.4636867642402649 
model_pd.lagr.mean(): -0.4077185392379761 
model_pd.lambdas: dict_items([('pout', tensor([0.2122])), ('power', tensor([0.0109]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1279	 i:0 	 global-step:25580	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.1771, 27.4035, 26.9433],
        [26.1771, 31.5803, 33.6286],
        [26.1771, 28.4671, 28.2432],
        [26.1771, 26.1771, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.45923712849617004 
model_pd.lagr.mean(): -0.4032689034938812 
model_pd.lambdas: dict_items([('pout', tensor([0.2101])), ('power', tensor([0.0108]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1280	 i:0 	 global-step:25600	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 31.1059, 32.7090],
        [26.1771, 29.0742, 29.1463],
        [26.1771, 30.0417, 30.7560],
        [26.1771, 26.5006, 26.2645]], grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.4547874629497528 
model_pd.lagr.mean(): -0.398819237947464 
model_pd.lambdas: dict_items([('pout', tensor([0.2081])), ('power', tensor([0.0107]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1281	 i:0 	 global-step:25620	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 28.0169, 27.6424],
        [26.1771, 26.1865, 26.1774],
        [26.1771, 27.9299, 27.5341],
        [26.1771, 31.5804, 33.6287]], grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.45033785700798035 
model_pd.lagr.mean(): -0.39436963200569153 
model_pd.lambdas: dict_items([('pout', tensor([0.2060])), ('power', tensor([0.0106]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1282	 i:0 	 global-step:25640	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 33.3848, 37.3481],
        [26.1771, 31.5744, 33.6170],
        [26.1771, 26.1865, 26.1774],
        [26.1771, 26.2145, 26.1798]], grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.05596821755170822 
model_pd.l_d.mean(): -0.4458881616592407 
model_pd.lagr.mean(): -0.3899199366569519 
model_pd.lambdas: dict_items([('pout', tensor([0.2040])), ('power', tensor([0.0105]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1283	 i:0 	 global-step:25660	 l-p:0.05596821755170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 34.6434, 40.1197],
        [26.1771, 26.2145, 26.1798],
        [26.1771, 27.0422, 26.6123],
        [26.1771, 28.9824, 29.0037]], grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.44143858551979065 
model_pd.lagr.mean(): -0.38547036051750183 
model_pd.lambdas: dict_items([('pout', tensor([0.2019])), ('power', tensor([0.0104]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1284	 i:0 	 global-step:25680	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 28.0170, 27.6425],
        [26.1771, 33.3849, 37.3483],
        [26.1771, 28.9824, 29.0037],
        [26.1771, 26.1772, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.4369889497756958 
model_pd.lagr.mean(): -0.381020724773407 
model_pd.lambdas: dict_items([('pout', tensor([0.1998])), ('power', tensor([0.0103]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1285	 i:0 	 global-step:25700	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.2258, 26.1812],
        [26.1771, 26.1865, 26.1774],
        [26.1771, 28.8948, 28.8696],
        [26.1771, 26.5124, 26.2698]], grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.43253928422927856 
model_pd.lagr.mean(): -0.37657105922698975 
model_pd.lambdas: dict_items([('pout', tensor([0.1978])), ('power', tensor([0.0102]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1286	 i:0 	 global-step:25720	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1787, 26.1771],
        [26.1771, 27.0975, 26.6584],
        [26.1771, 28.7145, 28.5994],
        [26.1771, 26.1771, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.42808961868286133 
model_pd.lagr.mean(): -0.3721213936805725 
model_pd.lambdas: dict_items([('pout', tensor([0.1957])), ('power', tensor([0.0101]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1287	 i:0 	 global-step:25740	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.3731, 26.2158],
        [26.1771, 27.0422, 26.6123],
        [26.1771, 33.7190, 38.0708],
        [26.1771, 27.2676, 26.8108]], grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.4236399233341217 
model_pd.lagr.mean(): -0.3676716983318329 
model_pd.lambdas: dict_items([('pout', tensor([0.1937])), ('power', tensor([0.0100]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1288	 i:0 	 global-step:25760	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 31.1063, 32.7096],
        [26.1771, 33.3851, 37.3486],
        [26.1771, 31.5807, 33.6293],
        [26.1771, 26.1776, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.41919031739234924 
model_pd.lagr.mean(): -0.3632220923900604 
model_pd.lambdas: dict_items([('pout', tensor([0.1916])), ('power', tensor([0.0099]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1289	 i:0 	 global-step:25780	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1771, 26.1771],
        [26.1771, 26.5006, 26.2646],
        [26.1771, 26.2790, 26.1906],
        [26.1771, 33.7191, 38.0710]], grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.414740651845932 
model_pd.lagr.mean(): -0.3587724268436432 
model_pd.lambdas: dict_items([('pout', tensor([0.1895])), ('power', tensor([0.0098]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1290	 i:0 	 global-step:25800	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 30.3278, 31.2644],
        [26.1771, 33.7191, 38.0711],
        [26.1771, 30.5670, 31.6994],
        [26.1771, 26.5124, 26.2698]], grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.41029092669487 
model_pd.lagr.mean(): -0.3543227016925812 
model_pd.lambdas: dict_items([('pout', tensor([0.1875])), ('power', tensor([0.0097]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1291	 i:0 	 global-step:25820	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1771, 26.1771],
        [26.1771, 31.1064, 32.7098],
        [26.1771, 26.1771, 26.1771],
        [26.1771, 27.9302, 27.5344]], grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.40584129095077515 
model_pd.lagr.mean(): -0.34987306594848633 
model_pd.lambdas: dict_items([('pout', tensor([0.1854])), ('power', tensor([0.0095]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1292	 i:0 	 global-step:25840	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 28.4674, 28.2436],
        [26.1771, 26.1775, 26.1771],
        [26.1771, 27.0975, 26.6584],
        [26.1771, 26.1787, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.4013916254043579 
model_pd.lagr.mean(): -0.3454234004020691 
model_pd.lambdas: dict_items([('pout', tensor([0.1834])), ('power', tensor([0.0094]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1293	 i:0 	 global-step:25860	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 33.3853, 37.3490],
        [26.1771, 28.9827, 29.0041],
        [26.1771, 30.5672, 31.6996],
        [26.1771, 26.1785, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.3969419598579407 
model_pd.lagr.mean(): -0.34097373485565186 
model_pd.lambdas: dict_items([('pout', tensor([0.1813])), ('power', tensor([0.0093]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1294	 i:0 	 global-step:25880	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]])
 pt:tensor([[26.1771, 31.5750, 33.6178],
        [26.1771, 27.0423, 26.6124],
        [26.1771, 27.8541, 27.4420],
        [26.1771, 34.6439, 40.1208]], grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.39249229431152344 
model_pd.lagr.mean(): -0.3365240693092346 
model_pd.lambdas: dict_items([('pout', tensor([0.1793])), ('power', tensor([0.0092]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1295	 i:0 	 global-step:25900	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 29.2675, 29.4525],
        [26.1771, 26.3513, 26.2091],
        [26.1771, 30.5672, 31.6997],
        [26.1771, 31.5750, 33.6179]], grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.3880425989627838 
model_pd.lagr.mean(): -0.332074373960495 
model_pd.lambdas: dict_items([('pout', tensor([0.1772])), ('power', tensor([0.0091]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1296	 i:0 	 global-step:25920	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.8056, 26.4356],
        [26.1771, 26.3732, 26.2158],
        [26.1771, 31.5365, 33.5423],
        [26.1771, 27.4038, 26.9435]], grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.3835929036140442 
model_pd.lagr.mean(): -0.32762467861175537 
model_pd.lambdas: dict_items([('pout', tensor([0.1751])), ('power', tensor([0.0090]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1297	 i:0 	 global-step:25940	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[26.1771, 27.0424, 26.6124],
        [26.1771, 30.5673, 31.6999],
        [26.1771, 27.2677, 26.8109],
        [26.1771, 29.0748, 29.1470]], grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.37914323806762695 
model_pd.lagr.mean(): -0.32317501306533813 
model_pd.lambdas: dict_items([('pout', tensor([0.1731])), ('power', tensor([0.0089]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1298	 i:0 	 global-step:25960	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1771, 26.1771],
        [26.1771, 27.2678, 26.8109],
        [26.1771, 27.0424, 26.6124],
        [26.1771, 26.1785, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.37469348311424255 
model_pd.lagr.mean(): -0.31872525811195374 
model_pd.lambdas: dict_items([('pout', tensor([0.1710])), ('power', tensor([0.0088]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1299	 i:0 	 global-step:25980	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1785, 26.1771],
        [26.1771, 26.1771, 26.1771],
        [26.1771, 26.1787, 26.1771],
        [26.1771, 30.3281, 31.2649]], grad_fn=<SliceBackward0>)

training epoch:1300, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.3702438473701477 
model_pd.lagr.mean(): -0.3142756223678589 
model_pd.lambdas: dict_items([('pout', tensor([0.1690])), ('power', tensor([0.0087]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1300	 i:0 	 global-step:26000	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1301
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228]])
 pt:tensor([[26.1771, 27.5904, 27.1396],
        [26.1771, 28.4676, 28.2438],
        [26.1771, 29.2676, 29.4527],
        [26.1771, 27.2678, 26.8109]], grad_fn=<SliceBackward0>)

training epoch:1301, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.3657941222190857 
model_pd.lagr.mean(): -0.3098258972167969 
model_pd.lambdas: dict_items([('pout', tensor([0.1669])), ('power', tensor([0.0086]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1301	 i:0 	 global-step:26020	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.2258, 26.1812],
        [26.1771, 30.3281, 31.2650],
        [26.1771, 26.5007, 26.2646],
        [26.1771, 26.1787, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1302, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.3613443970680237 
model_pd.lagr.mean(): -0.30537617206573486 
model_pd.lambdas: dict_items([('pout', tensor([0.1649])), ('power', tensor([0.0085]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1302	 i:0 	 global-step:26040	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 31.5367, 33.5427],
        [26.1771, 29.2677, 29.4528],
        [26.1771, 28.7149, 28.6000],
        [26.1771, 26.1865, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1303, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.35689473152160645 
model_pd.lagr.mean(): -0.3009265065193176 
model_pd.lambdas: dict_items([('pout', tensor([0.1628])), ('power', tensor([0.0084]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1303	 i:0 	 global-step:26060	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1793, 26.1771],
        [26.1771, 26.3513, 26.2091],
        [26.1771, 27.8543, 27.4421],
        [26.1771, 26.1771, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1304, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.35244500637054443 
model_pd.lagr.mean(): -0.2964767813682556 
model_pd.lambdas: dict_items([('pout', tensor([0.1607])), ('power', tensor([0.0083]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1304	 i:0 	 global-step:26080	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 35.2957, 41.6066],
        [26.1771, 26.2041, 26.1787],
        [26.1771, 28.4677, 28.2439],
        [26.1771, 30.0425, 30.7572]], grad_fn=<SliceBackward0>)

training epoch:1305, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.3479953408241272 
model_pd.lagr.mean(): -0.2920271158218384 
model_pd.lambdas: dict_items([('pout', tensor([0.1587])), ('power', tensor([0.0082]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1305	 i:0 	 global-step:26100	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.5007, 26.2646],
        [26.1771, 26.2041, 26.1787],
        [26.1771, 33.2432, 37.0445],
        [26.1771, 28.0175, 27.6429]], grad_fn=<SliceBackward0>)

training epoch:1306, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.3435456156730652 
model_pd.lagr.mean(): -0.28757739067077637 
model_pd.lambdas: dict_items([('pout', tensor([0.1566])), ('power', tensor([0.0081]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1306	 i:0 	 global-step:26120	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.1771, 26.5125, 26.2698],
        [26.1771, 27.0424, 26.6124],
        [26.1771, 26.5107, 26.2690],
        [26.1771, 26.1771, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1307, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.3390958905220032 
model_pd.lagr.mean(): -0.28312766551971436 
model_pd.lambdas: dict_items([('pout', tensor([0.1546])), ('power', tensor([0.0080]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1307	 i:0 	 global-step:26140	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 27.0425, 26.6124],
        [26.1771, 28.4678, 28.2440],
        [26.1771, 26.3732, 26.2158],
        [26.1771, 31.5815, 33.6305]], grad_fn=<SliceBackward0>)

training epoch:1308, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.33464616537094116 
model_pd.lagr.mean(): -0.27867794036865234 
model_pd.lambdas: dict_items([('pout', tensor([0.1525])), ('power', tensor([0.0079]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1308	 i:0 	 global-step:26160	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.1771, 26.1771, 26.1771],
        [26.1771, 31.1070, 32.7108],
        [26.1771, 26.1770, 26.1771],
        [26.1771, 26.2041, 26.1787]], grad_fn=<SliceBackward0>)

training epoch:1309, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.3301964998245239 
model_pd.lagr.mean(): -0.2742282748222351 
model_pd.lambdas: dict_items([('pout', tensor([0.1504])), ('power', tensor([0.0078]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1309	 i:0 	 global-step:26180	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.5107, 26.2690],
        [26.1770, 27.5906, 27.1397],
        [26.1770, 30.3284, 31.2654],
        [26.1770, 26.5125, 26.2698]], grad_fn=<SliceBackward0>)

training epoch:1310, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.3257467448711395 
model_pd.lagr.mean(): -0.2697785198688507 
model_pd.lambdas: dict_items([('pout', tensor([0.1484])), ('power', tensor([0.0077]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1310	 i:0 	 global-step:26200	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1311
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 30.3284, 31.2654],
        [26.1770, 28.9832, 29.0046],
        [26.1770, 26.9263, 26.5214],
        [26.1770, 26.2258, 26.1812]], grad_fn=<SliceBackward0>)

training epoch:1311, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.3212970495223999 
model_pd.lagr.mean(): -0.2653288245201111 
model_pd.lambdas: dict_items([('pout', tensor([0.1463])), ('power', tensor([0.0076]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1311	 i:0 	 global-step:26220	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 27.9305, 27.5347],
        [26.1770, 30.5678, 31.7006],
        [26.1770, 26.1771, 26.1770],
        [26.1770, 28.7151, 28.6002]], grad_fn=<SliceBackward0>)

training epoch:1312, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.3168473243713379 
model_pd.lagr.mean(): -0.2608790993690491 
model_pd.lambdas: dict_items([('pout', tensor([0.1443])), ('power', tensor([0.0075]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1312	 i:0 	 global-step:26240	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1771, 26.1770],
        [26.1770, 26.8058, 26.4357],
        [26.1770, 27.0978, 26.6585],
        [26.1770, 26.3513, 26.2090]], grad_fn=<SliceBackward0>)

training epoch:1313, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.3123975396156311 
model_pd.lagr.mean(): -0.2564293146133423 
model_pd.lambdas: dict_items([('pout', tensor([0.1422])), ('power', tensor([0.0074]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1313	 i:0 	 global-step:26260	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1785, 26.1771],
        [26.1770, 27.0425, 26.6125],
        [26.1770, 26.1771, 26.1771],
        [26.1770, 31.5817, 33.6308]], grad_fn=<SliceBackward0>)

training epoch:1314, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.3079477846622467 
model_pd.lagr.mean(): -0.2519795596599579 
model_pd.lambdas: dict_items([('pout', tensor([0.1402])), ('power', tensor([0.0073]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1314	 i:0 	 global-step:26280	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 28.0176, 27.6431],
        [26.1770, 26.1792, 26.1771],
        [26.1770, 26.2790, 26.1905],
        [26.1770, 26.1785, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1315, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.3034980595111847 
model_pd.lagr.mean(): -0.24752983450889587 
model_pd.lambdas: dict_items([('pout', tensor([0.1381])), ('power', tensor([0.0072]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1315	 i:0 	 global-step:26300	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.3513, 26.2090],
        [26.1770, 26.1772, 26.1770],
        [26.1770, 28.0176, 27.6431],
        [26.1770, 27.5907, 27.1398]], grad_fn=<SliceBackward0>)

training epoch:1316, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.2990482449531555 
model_pd.lagr.mean(): -0.2430800050497055 
model_pd.lambdas: dict_items([('pout', tensor([0.1360])), ('power', tensor([0.0071]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1316	 i:0 	 global-step:26320	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 28.7152, 28.6003],
        [26.1770, 30.0428, 30.7577],
        [26.1770, 27.2680, 26.8111],
        [26.1770, 26.3513, 26.2090]], grad_fn=<SliceBackward0>)

training epoch:1317, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.2945985198020935 
model_pd.lagr.mean(): -0.2386302798986435 
model_pd.lambdas: dict_items([('pout', tensor([0.1340])), ('power', tensor([0.0070]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1317	 i:0 	 global-step:26340	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.5078, 26.2677],
        [26.1770, 26.1772, 26.1770],
        [26.1770, 26.5125, 26.2698],
        [26.1770, 27.0978, 26.6586]], grad_fn=<SliceBackward0>)

training epoch:1318, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.2901487350463867 
model_pd.lagr.mean(): -0.2341804951429367 
model_pd.lambdas: dict_items([('pout', tensor([0.1319])), ('power', tensor([0.0069]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1318	 i:0 	 global-step:26360	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.2041, 26.1787],
        [26.1770, 27.2680, 26.8111],
        [26.1770, 26.5125, 26.2698],
        [26.1770, 35.2962, 41.6077]], grad_fn=<SliceBackward0>)

training epoch:1319, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.2856989800930023 
model_pd.lagr.mean(): -0.2297307550907135 
model_pd.lambdas: dict_items([('pout', tensor([0.1299])), ('power', tensor([0.0068]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1319	 i:0 	 global-step:26380	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.8058, 26.4357],
        [26.1770, 26.1787, 26.1771],
        [26.1770, 33.7202, 38.0732],
        [26.1770, 26.5007, 26.2645]], grad_fn=<SliceBackward0>)

training epoch:1320, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.28124916553497314 
model_pd.lagr.mean(): -0.22528094053268433 
model_pd.lambdas: dict_items([('pout', tensor([0.1278])), ('power', tensor([0.0067]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1320	 i:0 	 global-step:26400	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 33.7202, 38.0733],
        [26.1770, 28.7153, 28.6004],
        [26.1770, 29.2681, 29.4534]], grad_fn=<SliceBackward0>)

training epoch:1321, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.27679941058158875 
model_pd.lagr.mean(): -0.22083118557929993 
model_pd.lambdas: dict_items([('pout', tensor([0.1257])), ('power', tensor([0.0066]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1321	 i:0 	 global-step:26420	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.3732, 26.2158],
        [26.1770, 28.8958, 28.8708],
        [26.1770, 29.2681, 29.4534],
        [26.1770, 31.5819, 33.6312]], grad_fn=<SliceBackward0>)

training epoch:1322, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.27234968543052673 
model_pd.lagr.mean(): -0.21638146042823792 
model_pd.lambdas: dict_items([('pout', tensor([0.1237])), ('power', tensor([0.0064]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1322	 i:0 	 global-step:26440	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 26.1775, 26.1770],
        [26.1770, 33.7203, 38.0734],
        [26.1770, 26.2789, 26.1905]], grad_fn=<SliceBackward0>)

training epoch:1323, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.26789990067481995 
model_pd.lagr.mean(): -0.21193167567253113 
model_pd.lambdas: dict_items([('pout', tensor([0.1216])), ('power', tensor([0.0063]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1323	 i:0 	 global-step:26460	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 30.3288, 31.2659],
        [26.1770, 26.5108, 26.2690],
        [26.1770, 30.0430, 30.7580],
        [26.1770, 34.6449, 40.1229]], grad_fn=<SliceBackward0>)

training epoch:1324, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.26345011591911316 
model_pd.lagr.mean(): -0.20748189091682434 
model_pd.lambdas: dict_items([('pout', tensor([0.1196])), ('power', tensor([0.0062]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1324	 i:0 	 global-step:26480	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 27.0426, 26.6125],
        [26.1770, 31.1075, 32.7116],
        [26.1770, 26.1770, 26.1770],
        [26.1770, 26.5007, 26.2645]], grad_fn=<SliceBackward0>)

training epoch:1325, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.25900033116340637 
model_pd.lagr.mean(): -0.20303210616111755 
model_pd.lambdas: dict_items([('pout', tensor([0.1175])), ('power', tensor([0.0061]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1325	 i:0 	 global-step:26500	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.2145, 26.1798],
        [26.1770, 27.5908, 27.1398],
        [26.1770, 27.9307, 27.5349],
        [26.1770, 28.8958, 28.8709]], grad_fn=<SliceBackward0>)

training epoch:1326, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.2545505464076996 
model_pd.lagr.mean(): -0.19858230650424957 
model_pd.lambdas: dict_items([('pout', tensor([0.1155])), ('power', tensor([0.0060]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1326	 i:0 	 global-step:26520	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1775, 26.1770],
        [26.1770, 26.1770, 26.1770],
        [26.1770, 31.5375, 33.5439],
        [26.1770, 26.1804, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1327, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.2501007616519928 
model_pd.lagr.mean(): -0.19413253664970398 
model_pd.lambdas: dict_items([('pout', tensor([0.1134])), ('power', tensor([0.0059]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1327	 i:0 	 global-step:26540	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 33.7204, 38.0737],
        [26.1770, 26.3513, 26.2090],
        [26.1770, 29.2683, 29.4536],
        [26.1770, 27.0979, 26.6586]], grad_fn=<SliceBackward0>)

training epoch:1328, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.24565096199512482 
model_pd.lagr.mean(): -0.1896827220916748 
model_pd.lambdas: dict_items([('pout', tensor([0.1113])), ('power', tensor([0.0058]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1328	 i:0 	 global-step:26560	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1787, 26.1770],
        [26.1770, 29.2683, 29.4536],
        [26.1770, 26.8058, 26.4357],
        [26.1770, 29.0755, 29.1479]], grad_fn=<SliceBackward0>)

training epoch:1329, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.24120117723941803 
model_pd.lagr.mean(): -0.18523293733596802 
model_pd.lambdas: dict_items([('pout', tensor([0.1093])), ('power', tensor([0.0057]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1329	 i:0 	 global-step:26580	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.1770, 33.3865, 37.3514],
        [26.1770, 27.9308, 27.5350],
        [26.1770, 26.2257, 26.1812],
        [26.1770, 26.1770, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1330, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.23675140738487244 
model_pd.lagr.mean(): -0.18078316748142242 
model_pd.lambdas: dict_items([('pout', tensor([0.1072])), ('power', tensor([0.0056]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1330	 i:0 	 global-step:26600	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 31.1076, 32.7118],
        [26.1770, 26.5108, 26.2690],
        [26.1770, 29.2683, 29.4536],
        [26.1770, 26.1771, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1331, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.23230157792568207 
model_pd.lagr.mean(): -0.17633333802223206 
model_pd.lambdas: dict_items([('pout', tensor([0.1052])), ('power', tensor([0.0055]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1331	 i:0 	 global-step:26620	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1332
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.1770, 34.6451, 40.1233],
        [26.1770, 28.7155, 28.6007],
        [26.1770, 35.2966, 41.6085],
        [26.1770, 33.3866, 37.3515]], grad_fn=<SliceBackward0>)

training epoch:1332, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.22785180807113647 
model_pd.lagr.mean(): -0.17188356816768646 
model_pd.lambdas: dict_items([('pout', tensor([0.1031])), ('power', tensor([0.0054]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1332	 i:0 	 global-step:26640	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 33.3866, 37.3516],
        [26.1770, 31.5376, 33.5442],
        [26.1770, 26.1771, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1333, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.2234020233154297 
model_pd.lagr.mean(): -0.16743376851081848 
model_pd.lambdas: dict_items([('pout', tensor([0.1010])), ('power', tensor([0.0053]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1333	 i:0 	 global-step:26660	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 28.7155, 28.6007],
        [26.1770, 26.3513, 26.2090],
        [26.1770, 31.1077, 32.7119],
        [26.1770, 26.9265, 26.5214]], grad_fn=<SliceBackward0>)

training epoch:1334, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.21895219385623932 
model_pd.lagr.mean(): -0.1629839539527893 
model_pd.lambdas: dict_items([('pout', tensor([0.0990])), ('power', tensor([0.0052]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1334	 i:0 	 global-step:26680	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1770, 27.0426, 26.6125],
        [26.1770, 28.8960, 28.8710],
        [26.1770, 26.5007, 26.2645],
        [26.1770, 26.1770, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1335, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.21450240910053253 
model_pd.lagr.mean(): -0.15853416919708252 
model_pd.lambdas: dict_items([('pout', tensor([0.0969])), ('power', tensor([0.0051]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1335	 i:0 	 global-step:26700	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1336
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 27.0979, 26.6586],
        [26.1770, 26.5007, 26.2645],
        [26.1770, 31.5377, 33.5443],
        [26.1770, 26.3732, 26.2158]], grad_fn=<SliceBackward0>)

training epoch:1336, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.21005259454250336 
model_pd.lagr.mean(): -0.15408435463905334 
model_pd.lambdas: dict_items([('pout', tensor([0.0949])), ('power', tensor([0.0050]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1336	 i:0 	 global-step:26720	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.9265, 26.5214],
        [26.1770, 27.9309, 27.5350],
        [26.1770, 26.1770, 26.1770],
        [26.1770, 26.1784, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1337, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.20560280978679657 
model_pd.lagr.mean(): -0.14963456988334656 
model_pd.lambdas: dict_items([('pout', tensor([0.0928])), ('power', tensor([0.0049]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1337	 i:0 	 global-step:26740	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 26.1770, 26.1770],
        [26.1770, 26.2789, 26.1905],
        [26.1770, 26.1792, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1338, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.2011529952287674 
model_pd.lagr.mean(): -0.14518475532531738 
model_pd.lambdas: dict_items([('pout', tensor([0.0908])), ('power', tensor([0.0048]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1338	 i:0 	 global-step:26760	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.5125, 26.2698],
        [26.1770, 26.9265, 26.5214],
        [26.1770, 26.5078, 26.2677],
        [26.1770, 28.9837, 29.0053]], grad_fn=<SliceBackward0>)

training epoch:1339, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.19670318067073822 
model_pd.lagr.mean(): -0.1407349407672882 
model_pd.lambdas: dict_items([('pout', tensor([0.0887])), ('power', tensor([0.0047]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1339	 i:0 	 global-step:26780	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.5078, 26.2677],
        [26.1770, 31.1078, 32.7121],
        [26.1770, 26.1864, 26.1773],
        [26.1770, 26.1784, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1340, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.19225336611270905 
model_pd.lagr.mean(): -0.13628512620925903 
model_pd.lambdas: dict_items([('pout', tensor([0.0866])), ('power', tensor([0.0046]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1340	 i:0 	 global-step:26800	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1804, 26.1770],
        [26.1770, 26.5007, 26.2645],
        [26.1770, 27.4042, 26.9438],
        [26.1770, 31.5378, 33.5445]], grad_fn=<SliceBackward0>)

training epoch:1341, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.18780353665351868 
model_pd.lagr.mean(): -0.13183528184890747 
model_pd.lambdas: dict_items([('pout', tensor([0.0846])), ('power', tensor([0.0045]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1341	 i:0 	 global-step:26820	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 26.3732, 26.2158],
        [26.1770, 27.5909, 27.1400],
        [26.1770, 26.1770, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1342, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.1833537369966507 
model_pd.lagr.mean(): -0.12738549709320068 
model_pd.lambdas: dict_items([('pout', tensor([0.0825])), ('power', tensor([0.0044]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1342	 i:0 	 global-step:26840	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.3732, 26.2158],
        [26.1770, 30.0434, 30.7586],
        [26.1770, 29.0757, 29.1482],
        [26.1770, 27.5909, 27.1400]], grad_fn=<SliceBackward0>)

training epoch:1343, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.17890390753746033 
model_pd.lagr.mean(): -0.12293566018342972 
model_pd.lambdas: dict_items([('pout', tensor([0.0805])), ('power', tensor([0.0043]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1343	 i:0 	 global-step:26860	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1344
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.5078, 26.2676],
        [26.1770, 30.5685, 31.7017],
        [26.1770, 27.0979, 26.6586],
        [26.1770, 28.9837, 29.0054]], grad_fn=<SliceBackward0>)

training epoch:1344, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.17445409297943115 
model_pd.lagr.mean(): -0.11848584562540054 
model_pd.lambdas: dict_items([('pout', tensor([0.0784])), ('power', tensor([0.0042]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1344	 i:0 	 global-step:26880	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.2144, 26.1797],
        [26.1770, 27.0979, 26.6586],
        [26.1770, 31.5824, 33.6321],
        [26.1770, 26.3732, 26.2158]], grad_fn=<SliceBackward0>)

training epoch:1345, step:0 
model_pd.l_p.mean(): 0.05596825107932091 
model_pd.l_d.mean(): -0.17000427842140198 
model_pd.lagr.mean(): -0.11403602361679077 
model_pd.lambdas: dict_items([('pout', tensor([0.0763])), ('power', tensor([0.0041]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1345	 i:0 	 global-step:26900	 l-p:0.05596825107932091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1346
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[26.1770, 26.8059, 26.4357],
        [26.1770, 27.9310, 27.5351],
        [26.1770, 28.0180, 27.6434],
        [26.1770, 35.2969, 41.6092]], grad_fn=<SliceBackward0>)

training epoch:1346, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.1655544638633728 
model_pd.lagr.mean(): -0.1095862165093422 
model_pd.lambdas: dict_items([('pout', tensor([0.0743])), ('power', tensor([0.0040]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1346	 i:0 	 global-step:26920	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.3513, 26.2090],
        [26.1770, 26.1804, 26.1770],
        [26.1770, 33.3869, 37.3522],
        [26.1770, 27.0980, 26.6586]], grad_fn=<SliceBackward0>)

training epoch:1347, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.16110464930534363 
model_pd.lagr.mean(): -0.10513640195131302 
model_pd.lambdas: dict_items([('pout', tensor([0.0722])), ('power', tensor([0.0039]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1347	 i:0 	 global-step:26940	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 29.0758, 29.1483],
        [26.1770, 34.6455, 40.1241],
        [26.1770, 26.1786, 26.1770],
        [26.1770, 26.1784, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1348, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.15665483474731445 
model_pd.lagr.mean(): -0.10068659484386444 
model_pd.lambdas: dict_items([('pout', tensor([0.0702])), ('power', tensor([0.0038]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1348	 i:0 	 global-step:26960	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1770, 28.0180, 27.6435],
        [26.1770, 26.5108, 26.2690],
        [26.1770, 26.1769, 26.1770],
        [26.1770, 26.1770, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1349, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.1522049903869629 
model_pd.lagr.mean(): -0.09623674303293228 
model_pd.lambdas: dict_items([('pout', tensor([0.0681])), ('power', tensor([0.0037]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1349	 i:0 	 global-step:26980	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 29.0758, 29.1483],
        [26.1770, 26.1791, 26.1770],
        [26.1770, 27.8548, 27.4427],
        [26.1770, 29.2686, 29.4540]], grad_fn=<SliceBackward0>)

training epoch:1350, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.14775514602661133 
model_pd.lagr.mean(): -0.09178689867258072 
model_pd.lambdas: dict_items([('pout', tensor([0.0661])), ('power', tensor([0.0036]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1350	 i:0 	 global-step:27000	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 26.5108, 26.2690],
        [26.1770, 26.1774, 26.1770],
        [26.1770, 26.1770, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1351, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.14330533146858215 
model_pd.lagr.mean(): -0.08733708411455154 
model_pd.lambdas: dict_items([('pout', tensor([0.0640])), ('power', tensor([0.0035]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1351	 i:0 	 global-step:27020	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 28.0181, 27.6435],
        [26.1770, 26.1864, 26.1773],
        [26.1770, 26.5108, 26.2690]], grad_fn=<SliceBackward0>)

training epoch:1352, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.13885550200939178 
model_pd.lagr.mean(): -0.08288725465536118 
model_pd.lambdas: dict_items([('pout', tensor([0.0619])), ('power', tensor([0.0033]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1352	 i:0 	 global-step:27040	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 27.0980, 26.6587],
        [26.1770, 31.1081, 32.7125],
        [26.1770, 26.2257, 26.1811],
        [26.1770, 33.7210, 38.0748]], grad_fn=<SliceBackward0>)

training epoch:1353, step:0 
model_pd.l_p.mean(): 0.05596825107932091 
model_pd.l_d.mean(): -0.13440565764904022 
model_pd.lagr.mean(): -0.07843740284442902 
model_pd.lambdas: dict_items([('pout', tensor([0.0599])), ('power', tensor([0.0032]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1353	 i:0 	 global-step:27060	 l-p:0.05596825107932091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1354
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[26.1770, 33.3871, 37.3524],
        [26.1770, 31.5826, 33.6324],
        [26.1770, 30.5687, 31.7020],
        [26.1770, 31.1081, 32.7126]], grad_fn=<SliceBackward0>)

training epoch:1354, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.12995584309101105 
model_pd.lagr.mean(): -0.07398759573698044 
model_pd.lambdas: dict_items([('pout', tensor([0.0578])), ('power', tensor([0.0031]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1354	 i:0 	 global-step:27080	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1804, 26.1770],
        [26.1770, 28.0181, 27.6435],
        [26.1770, 26.1770, 26.1770],
        [26.1770, 27.5910, 27.1400]], grad_fn=<SliceBackward0>)

training epoch:1355, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.12550601363182068 
model_pd.lagr.mean(): -0.06953776627779007 
model_pd.lambdas: dict_items([('pout', tensor([0.0558])), ('power', tensor([0.0030]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1355	 i:0 	 global-step:27100	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1356
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[26.1770, 34.6457, 40.1244],
        [26.1770, 27.0980, 26.6587],
        [26.1770, 29.2687, 29.4542],
        [26.1770, 28.9839, 29.0056]], grad_fn=<SliceBackward0>)

training epoch:1356, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.12105615437030792 
model_pd.lagr.mean(): -0.06508791446685791 
model_pd.lambdas: dict_items([('pout', tensor([0.0537])), ('power', tensor([0.0029]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1356	 i:0 	 global-step:27120	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 28.7158, 28.6011],
        [26.1770, 26.2144, 26.1797],
        [26.1770, 34.6457, 40.1244]], grad_fn=<SliceBackward0>)

training epoch:1357, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.11660631000995636 
model_pd.lagr.mean(): -0.06063806265592575 
model_pd.lambdas: dict_items([('pout', tensor([0.0516])), ('power', tensor([0.0028]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0667]))])
epoch：1357	 i:0 	 global-step:27140	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 28.4685, 28.2448],
        [26.1770, 26.5126, 26.2698],
        [26.1770, 33.2445, 37.0471],
        [26.1770, 29.0759, 29.1485]], grad_fn=<SliceBackward0>)

training epoch:1358, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.11215648055076599 
model_pd.lagr.mean(): -0.05618823319673538 
model_pd.lambdas: dict_items([('pout', tensor([0.0496])), ('power', tensor([0.0027]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1358	 i:0 	 global-step:27160	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.2257, 26.1811],
        [26.1770, 28.7158, 28.6011],
        [26.1770, 26.1770, 26.1770],
        [26.1770, 26.5078, 26.2677]], grad_fn=<SliceBackward0>)

training epoch:1359, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.10770662128925323 
model_pd.lagr.mean(): -0.05173838138580322 
model_pd.lambdas: dict_items([('pout', tensor([0.0475])), ('power', tensor([0.0026]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1359	 i:0 	 global-step:27180	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1784, 26.1770],
        [26.1770, 27.4044, 26.9439],
        [26.1770, 31.5827, 33.6326],
        [26.1770, 26.5078, 26.2677]], grad_fn=<SliceBackward0>)

training epoch:1360, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.10325679928064346 
model_pd.lagr.mean(): -0.04728855937719345 
model_pd.lambdas: dict_items([('pout', tensor([0.0455])), ('power', tensor([0.0025]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1360	 i:0 	 global-step:27200	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 31.1082, 32.7128],
        [26.1770, 30.0437, 30.7590],
        [26.1770, 26.5126, 26.2698]], grad_fn=<SliceBackward0>)

training epoch:1361, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.0988069474697113 
model_pd.lagr.mean(): -0.04283870756626129 
model_pd.lambdas: dict_items([('pout', tensor([0.0434])), ('power', tensor([0.0024]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1361	 i:0 	 global-step:27220	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.5108, 26.2690],
        [26.1770, 26.1770, 26.1770],
        [26.1770, 26.2040, 26.1786],
        [26.1770, 26.1774, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1362, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.09435710310935974 
model_pd.lagr.mean(): -0.03838885575532913 
model_pd.lambdas: dict_items([('pout', tensor([0.0414])), ('power', tensor([0.0023]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1362	 i:0 	 global-step:27240	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1770, 33.7212, 38.0751],
        [26.1770, 28.0182, 27.6436],
        [26.1770, 33.3872, 37.3527],
        [26.1770, 26.1770, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1363, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.08990725129842758 
model_pd.lagr.mean(): -0.03393901139497757 
model_pd.lambdas: dict_items([('pout', tensor([0.0393])), ('power', tensor([0.0022]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1363	 i:0 	 global-step:27260	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.1770, 28.8964, 28.8715],
        [26.1770, 31.5382, 33.5452],
        [26.1770, 33.3872, 37.3527],
        [26.1770, 26.1770, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1364, step:0 
model_pd.l_p.mean(): 0.05596823990345001 
model_pd.l_d.mean(): -0.08545739948749542 
model_pd.lagr.mean(): -0.02948915958404541 
model_pd.lambdas: dict_items([('pout', tensor([0.0372])), ('power', tensor([0.0021]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1364	 i:0 	 global-step:27280	 l-p:0.05596823990345001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1792, 26.1770],
        [26.1770, 33.3872, 37.3528],
        [26.1770, 30.0437, 30.7591],
        [26.1770, 27.5911, 27.1401]], grad_fn=<SliceBackward0>)

training epoch:1365, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.08100755512714386 
model_pd.lagr.mean(): -0.025039318948984146 
model_pd.lambdas: dict_items([('pout', tensor([0.0352])), ('power', tensor([0.0020]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1365	 i:0 	 global-step:27300	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1784, 26.1770],
        [26.1770, 27.8550, 27.4428],
        [26.1770, 31.1083, 32.7129],
        [26.1770, 26.2144, 26.1797]], grad_fn=<SliceBackward0>)

training epoch:1366, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.0765576958656311 
model_pd.lagr.mean(): -0.02058945968747139 
model_pd.lambdas: dict_items([('pout', tensor([0.0331])), ('power', tensor([0.0019]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1366	 i:0 	 global-step:27320	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1367
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.1770, 33.2447, 37.0474],
        [26.1770, 31.5382, 33.5452],
        [26.1770, 26.3513, 26.2090],
        [26.1770, 26.1770, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1367, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.07210784405469894 
model_pd.lagr.mean(): -0.01613960787653923 
model_pd.lambdas: dict_items([('pout', tensor([0.0311])), ('power', tensor([0.0018]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1367	 i:0 	 global-step:27340	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.3513, 26.2090],
        [26.1770, 35.2973, 41.6100],
        [26.1770, 28.4686, 28.2450],
        [26.1770, 26.1864, 26.1773]], grad_fn=<SliceBackward0>)

training epoch:1368, step:0 
model_pd.l_p.mean(): 0.05596824735403061 
model_pd.l_d.mean(): -0.06765799224376678 
model_pd.lagr.mean(): -0.011689744889736176 
model_pd.lambdas: dict_items([('pout', tensor([0.0290])), ('power', tensor([0.0017]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1368	 i:0 	 global-step:27360	 l-p:0.05596824735403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 33.2447, 37.0474],
        [26.1770, 35.2973, 41.6100],
        [26.1770, 26.1786, 26.1770],
        [26.1770, 26.8060, 26.4358]], grad_fn=<SliceBackward0>)

training epoch:1369, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.06320813298225403 
model_pd.lagr.mean(): -0.007239896804094315 
model_pd.lambdas: dict_items([('pout', tensor([0.0269])), ('power', tensor([0.0016]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1369	 i:0 	 global-step:27380	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.5109, 26.2690],
        [26.1770, 31.5768, 33.6210],
        [26.1770, 26.3513, 26.2090],
        [26.1770, 33.3873, 37.3529]], grad_fn=<SliceBackward0>)

training epoch:1370, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.058758266270160675 
model_pd.lagr.mean(): -0.0027900300920009613 
model_pd.lambdas: dict_items([('pout', tensor([0.0249])), ('power', tensor([0.0015]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1370	 i:0 	 global-step:27400	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 28.8964, 28.8716],
        [26.1770, 26.5008, 26.2646],
        [26.1770, 26.9267, 26.5215],
        [26.1770, 35.2973, 41.6101]], grad_fn=<SliceBackward0>)

training epoch:1371, step:0 
model_pd.l_p.mean(): 0.055968236178159714 
model_pd.l_d.mean(): -0.054308414459228516 
model_pd.lagr.mean(): 0.0016598217189311981 
model_pd.lambdas: dict_items([('pout', tensor([0.0228])), ('power', tensor([0.0014]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1371	 i:0 	 global-step:27420	 l-p:0.055968236178159714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 26.1770, 26.1770],
        [26.1770, 26.1792, 26.1770],
        [26.1770, 26.9267, 26.5215]], grad_fn=<SliceBackward0>)

training epoch:1372, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.04985855147242546 
model_pd.lagr.mean(): 0.0061096809804439545 
model_pd.lambdas: dict_items([('pout', tensor([0.0208])), ('power', tensor([0.0013]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1372	 i:0 	 global-step:27440	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1804, 26.1771],
        [26.1770, 26.3514, 26.2090],
        [26.1770, 26.1786, 26.1770],
        [26.1770, 26.5109, 26.2690]], grad_fn=<SliceBackward0>)

training epoch:1373, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.0454086996614933 
model_pd.lagr.mean(): 0.010559532791376114 
model_pd.lambdas: dict_items([('pout', tensor([0.0187])), ('power', tensor([0.0012]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1373	 i:0 	 global-step:27460	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.5079, 26.2677],
        [26.1770, 27.8550, 27.4429],
        [26.1770, 26.5126, 26.2698],
        [26.1770, 27.0981, 26.6588]], grad_fn=<SliceBackward0>)

training epoch:1374, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.040958844125270844 
model_pd.lagr.mean(): 0.015009388327598572 
model_pd.lambdas: dict_items([('pout', tensor([0.0167])), ('power', tensor([0.0011]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1374	 i:0 	 global-step:27480	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 28.8965, 28.8716],
        [26.1770, 30.3296, 31.2672],
        [26.1770, 26.9267, 26.5215],
        [26.1770, 26.1784, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1375, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.03650898486375809 
model_pd.lagr.mean(): 0.019459247589111328 
model_pd.lambdas: dict_items([('pout', tensor([0.0146])), ('power', tensor([0.0010]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1375	 i:0 	 global-step:27500	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 27.8551, 27.4429],
        [26.1770, 26.2145, 26.1797],
        [26.1770, 33.3874, 37.3530],
        [26.1770, 26.1792, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1376, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.03205912187695503 
model_pd.lagr.mean(): 0.023909106850624084 
model_pd.lambdas: dict_items([('pout', tensor([0.0125])), ('power', tensor([0.0009]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1376	 i:0 	 global-step:27520	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1377
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[26.1770, 31.5384, 33.5454],
        [26.1770, 27.0981, 26.6588],
        [26.1770, 27.4045, 26.9440],
        [26.1770, 30.0439, 30.7592]], grad_fn=<SliceBackward0>)

training epoch:1377, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.027609264478087425 
model_pd.lagr.mean(): 0.02835896424949169 
model_pd.lambdas: dict_items([('pout', tensor([0.0105])), ('power', tensor([0.0008]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1377	 i:0 	 global-step:27540	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1792, 26.1770],
        [26.1770, 27.0982, 26.6588],
        [26.1770, 28.7160, 28.6013],
        [26.1770, 26.5109, 26.2690]], grad_fn=<SliceBackward0>)

training epoch:1378, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.02315939962863922 
model_pd.lagr.mean(): 0.032808829098939896 
model_pd.lambdas: dict_items([('pout', tensor([0.0084])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1378	 i:0 	 global-step:27560	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 34.6460, 40.1250],
        [26.1770, 26.8060, 26.4358],
        [26.1770, 28.4687, 28.2451],
        [26.1770, 26.1775, 26.1770]], grad_fn=<SliceBackward0>)

training epoch:1379, step:0 
model_pd.l_p.mean(): 0.055968232452869415 
model_pd.l_d.mean(): -0.018709536641836166 
model_pd.lagr.mean(): 0.03725869581103325 
model_pd.lambdas: dict_items([('pout', tensor([0.0064])), ('power', tensor([0.0006]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1379	 i:0 	 global-step:27580	 l-p:0.055968232452869415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1787, 26.1771],
        [26.1770, 28.0183, 27.6437],
        [26.1770, 33.3874, 37.3531],
        [26.1770, 28.7160, 28.6013]], grad_fn=<SliceBackward0>)

training epoch:1380, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.014259671792387962 
model_pd.lagr.mean(): 0.041708558797836304 
model_pd.lambdas: dict_items([('pout', tensor([0.0043])), ('power', tensor([0.0005]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1380	 i:0 	 global-step:27600	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 26.1772, 26.1770],
        [26.1770, 34.6460, 40.1251],
        [26.1770, 27.5912, 27.1402]], grad_fn=<SliceBackward0>)

training epoch:1381, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.009809811599552631 
model_pd.lagr.mean(): 0.04615841805934906 
model_pd.lambdas: dict_items([('pout', tensor([0.0023])), ('power', tensor([0.0004]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1381	 i:0 	 global-step:27620	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.1770, 26.1770],
        [26.1770, 26.2258, 26.1812],
        [26.1770, 33.3875, 37.3531],
        [26.1770, 26.1787, 26.1771]], grad_fn=<SliceBackward0>)

training epoch:1382, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.005359948147088289 
model_pd.lagr.mean(): 0.05060827359557152 
model_pd.lambdas: dict_items([('pout', tensor([0.0002])), ('power', tensor([0.0002]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1382	 i:0 	 global-step:27640	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 33.3875, 37.3531],
        [26.1770, 29.2690, 29.4545],
        [26.1770, 28.0183, 27.6438],
        [26.1770, 26.1865, 26.1774]], grad_fn=<SliceBackward0>)

training epoch:1383, step:0 
model_pd.l_p.mean(): 0.05596822872757912 
model_pd.l_d.mean(): -0.0009100829483941197 
model_pd.lagr.mean(): 0.05505814403295517 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0001]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1383	 i:0 	 global-step:27660	 l-p:0.05596822872757912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.1770, 26.2258, 26.1812],
        [26.1770, 26.5009, 26.2646],
        [26.1770, 26.2041, 26.1787],
        [26.1770, 31.5384, 33.5455]], grad_fn=<SliceBackward0>)

training epoch:1384, step:0 
model_pd.l_p.mean(): 0.05596822127699852 
model_pd.l_d.mean(): -0.00029957512742839754 
model_pd.lagr.mean(): 0.05566864460706711 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([4.1634e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0582])), ('power', tensor([-2.0666]))])
epoch：1384	 i:0 	 global-step:27680	 l-p:0.05596822127699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1385
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[26.1783, 29.2705, 29.4560],
        [26.1783, 27.5926, 27.1416],
        [26.1783, 26.9281, 26.5229],
        [26.1783, 30.3312, 31.2689]], grad_fn=<SliceBackward0>)

training epoch:1385, step:0 
model_pd.l_p.mean(): 0.05596791207790375 
model_pd.l_d.mean(): -8.598613931098953e-05 
model_pd.lagr.mean(): 0.05588192492723465 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0583])), ('power', tensor([-2.0653]))])
epoch：1385	 i:0 	 global-step:27700	 l-p:0.05596791207790375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.1823, 26.1839, 26.1823],
        [26.1823, 34.6531, 40.1334],
        [26.1823, 33.7283, 38.0834],
        [26.1823, 30.0500, 30.7656]], grad_fn=<SliceBackward0>)

training epoch:1386, step:0 
model_pd.l_p.mean(): 0.05596698448061943 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05596698448061943 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0585])), ('power', tensor([-2.0613]))])
epoch：1386	 i:0 	 global-step:27720	 l-p:0.05596698448061943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.1892, 26.1908, 26.1892],
        [26.1892, 26.2163, 26.1908],
        [26.1892, 28.9978, 29.0196],
        [26.1892, 26.1892, 26.1892]], grad_fn=<SliceBackward0>)

training epoch:1387, step:0 
model_pd.l_p.mean(): 0.05596533790230751 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05596533790230751 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0588])), ('power', tensor([-2.0545]))])
epoch：1387	 i:0 	 global-step:27740	 l-p:0.05596533790230751
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.1987, 35.3272, 41.6457],
        [26.1987, 26.5229, 26.2864],
        [26.1987, 29.0085, 29.0303],
        [26.1987, 26.1988, 26.1987]], grad_fn=<SliceBackward0>)

training epoch:1388, step:0 
model_pd.l_p.mean(): 0.055963072925806046 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055963072925806046 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0593])), ('power', tensor([-2.0450]))])
epoch：1388	 i:0 	 global-step:27760	 l-p:0.055963072925806046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.2107, 33.2883, 37.0964],
        [26.2107, 30.6088, 31.7440],
        [26.2107, 26.3128, 26.2242],
        [26.2107, 27.4400, 26.9788]], grad_fn=<SliceBackward0>)

training epoch:1389, step:0 
model_pd.l_p.mean(): 0.055960241705179214 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055960241705179214 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0599])), ('power', tensor([-2.0330]))])
epoch：1389	 i:0 	 global-step:27780	 l-p:0.055960241705179214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.2248, 30.3857, 31.3253],
        [26.2248, 26.5594, 26.3171],
        [26.2248, 27.4548, 26.9935],
        [26.2248, 29.3230, 29.5089]], grad_fn=<SliceBackward0>)

training epoch:1390, step:0 
model_pd.l_p.mean(): 0.0559568926692009 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0559568926692009 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0606])), ('power', tensor([-2.0190]))])
epoch：1390	 i:0 	 global-step:27800	 l-p:0.0559568926692009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.2409, 28.9677, 28.9429],
        [26.2409, 26.4158, 26.2730],
        [26.2409, 30.4046, 31.3448],
        [26.2409, 26.2504, 26.2413]], grad_fn=<SliceBackward0>)

training epoch:1391, step:0 
model_pd.l_p.mean(): 0.05595308914780617 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05595308914780617 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0614])), ('power', tensor([-2.0030]))])
epoch：1391	 i:0 	 global-step:27820	 l-p:0.05595308914780617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.2588, 26.2964, 26.2615],
        [26.2588, 26.2859, 26.2604],
        [26.2588, 26.4557, 26.2977],
        [26.2588, 31.2068, 32.8171]], grad_fn=<SliceBackward0>)

training epoch:1392, step:0 
model_pd.l_p.mean(): 0.05594887584447861 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05594887584447861 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0623])), ('power', tensor([-1.9852]))])
epoch：1392	 i:0 	 global-step:27840	 l-p:0.05594887584447861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.2782, 26.9100, 26.5382],
        [26.2782, 28.8280, 28.7129],
        [26.2782, 27.5110, 27.0486],
        [26.2782, 26.3272, 26.2824]], grad_fn=<SliceBackward0>)

training epoch:1393, step:0 
model_pd.l_p.mean(): 0.0559442900121212 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0559442900121212 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0633])), ('power', tensor([-1.9658]))])
epoch：1393	 i:0 	 global-step:27860	 l-p:0.0559442900121212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.2991, 26.2991, 26.2991],
        [26.2991, 30.1856, 30.9048],
        [26.2991, 30.4728, 31.4154],
        [26.2991, 29.1206, 29.1426]], grad_fn=<SliceBackward0>)

training epoch:1394, step:0 
model_pd.l_p.mean(): 0.05593937262892723 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05593937262892723 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0643])), ('power', tensor([-1.9451]))])
epoch：1394	 i:0 	 global-step:27880	 l-p:0.05593937262892723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.3213, 26.3227, 26.3213],
        [26.3213, 28.1738, 27.7971],
        [26.3213, 27.4194, 26.9596],
        [26.3213, 26.3704, 26.3255]], grad_fn=<SliceBackward0>)

training epoch:1395, step:0 
model_pd.l_p.mean(): 0.055934157222509384 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055934157222509384 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0654])), ('power', tensor([-1.9230]))])
epoch：1395	 i:0 	 global-step:27900	 l-p:0.055934157222509384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.3447, 26.3469, 26.3447],
        [26.3447, 26.5203, 26.3769],
        [26.3447, 33.6044, 37.5977],
        [26.3447, 26.9782, 26.6053]], grad_fn=<SliceBackward0>)

training epoch:1396, step:0 
model_pd.l_p.mean(): 0.05592867732048035 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05592867732048035 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0666])), ('power', tensor([-1.8997]))])
epoch：1396	 i:0 	 global-step:27920	 l-p:0.05592867732048035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.3691, 29.2914, 29.3649],
        [26.3691, 26.3691, 26.3691],
        [26.3691, 31.8116, 33.8724],
        [26.3691, 26.7075, 26.4627]], grad_fn=<SliceBackward0>)

training epoch:1397, step:0 
model_pd.l_p.mean(): 0.055922962725162506 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055922962725162506 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0678])), ('power', tensor([-1.8754]))])
epoch：1397	 i:0 	 global-step:27940	 l-p:0.055922962725162506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.3945, 27.4959, 27.0347],
        [26.3945, 26.3967, 26.3945],
        [26.3945, 28.9565, 28.8411],
        [26.3945, 26.3961, 26.3945]], grad_fn=<SliceBackward0>)

training epoch:1398, step:0 
model_pd.l_p.mean(): 0.05591703578829765 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05591703578829765 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0691])), ('power', tensor([-1.8501]))])
epoch：1398	 i:0 	 global-step:27960	 l-p:0.05591703578829765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.4207, 28.7358, 28.5101],
        [26.4207, 26.4481, 26.4224],
        [26.4207, 26.5237, 26.4344],
        [26.4207, 27.1782, 26.7689]], grad_fn=<SliceBackward0>)

training epoch:1399, step:0 
model_pd.l_p.mean(): 0.05591090768575668 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05591090768575668 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0704])), ('power', tensor([-1.8240]))])
epoch：1399	 i:0 	 global-step:27980	 l-p:0.05591090768575668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.4477, 27.3235, 26.8885],
        [26.4477, 26.4478, 26.4477],
        [26.4477, 26.4971, 26.4520],
        [26.4477, 26.4856, 26.4505]], grad_fn=<SliceBackward0>)

training epoch:1400, step:0 
model_pd.l_p.mean(): 0.05590462684631348 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05590462684631348 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0717])), ('power', tensor([-1.7971]))])
epoch：1400	 i:0 	 global-step:28000	 l-p:0.05590462684631348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.4755, 26.4759, 26.4755],
        [26.4755, 29.3175, 29.3400],
        [26.4755, 33.6294, 37.4793],
        [26.4755, 35.0472, 40.5937]], grad_fn=<SliceBackward0>)

training epoch:1401, step:0 
model_pd.l_p.mean(): 0.05589817836880684 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05589817836880684 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0731])), ('power', tensor([-1.7695]))])
epoch：1401	 i:0 	 global-step:28020	 l-p:0.05589817836880684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.5038, 26.5039, 26.5039],
        [26.5038, 30.7126, 31.6635],
        [26.5038, 29.2604, 29.2357],
        [26.5038, 26.5043, 26.5039]], grad_fn=<SliceBackward0>)

training epoch:1402, step:0 
model_pd.l_p.mean(): 0.055891603231430054 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055891603231430054 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0745])), ('power', tensor([-1.7413]))])
epoch：1402	 i:0 	 global-step:28040	 l-p:0.055891603231430054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.5328, 26.5328, 26.5328],
        [26.5328, 27.7787, 27.3115],
        [26.5328, 27.9682, 27.5107],
        [26.5328, 35.7849, 42.1900]], grad_fn=<SliceBackward0>)

training epoch:1403, step:0 
model_pd.l_p.mean(): 0.05588490888476372 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05588490888476372 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0759])), ('power', tensor([-1.7124]))])
epoch：1403	 i:0 	 global-step:28060	 l-p:0.05588490888476372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.5623, 29.5079, 29.5823],
        [26.5623, 26.9034, 26.6567],
        [26.5623, 30.7811, 31.7344],
        [26.5623, 26.6119, 26.5665]], grad_fn=<SliceBackward0>)

training epoch:1404, step:0 
model_pd.l_p.mean(): 0.05587811395525932 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05587811395525932 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0773])), ('power', tensor([-1.6831]))])
epoch：1404	 i:0 	 global-step:28080	 l-p:0.05587811395525932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.5922, 33.9248, 37.9588],
        [26.5922, 27.5296, 27.0826],
        [26.5922, 28.9237, 28.6967],
        [26.5922, 26.6304, 26.5950]], grad_fn=<SliceBackward0>)

training epoch:1405, step:0 
model_pd.l_p.mean(): 0.05587122589349747 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05587122589349747 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0788])), ('power', tensor([-1.6533]))])
epoch：1405	 i:0 	 global-step:28100	 l-p:0.05587122589349747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.6226, 31.0954, 32.2507],
        [26.6226, 26.6261, 26.6227],
        [26.6226, 29.3927, 29.3680],
        [26.6226, 26.9629, 26.7164]], grad_fn=<SliceBackward0>)

training epoch:1406, step:0 
model_pd.l_p.mean(): 0.05586424469947815 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05586424469947815 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0803])), ('power', tensor([-1.6230]))])
epoch：1406	 i:0 	 global-step:28120	 l-p:0.05586424469947815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1407
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[26.6534, 28.9908, 28.7632],
        [26.6534, 32.1590, 34.2443],
        [26.6534, 34.0040, 38.0480],
        [26.6534, 27.9056, 27.4361]], grad_fn=<SliceBackward0>)

training epoch:1407, step:0 
model_pd.l_p.mean(): 0.05585719645023346 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05585719645023346 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0818])), ('power', tensor([-1.5924]))])
epoch：1407	 i:0 	 global-step:28140	 l-p:0.05585719645023346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.6845, 26.6880, 26.6846],
        [26.6845, 27.5690, 27.1297],
        [26.6845, 30.9243, 31.8826],
        [26.6845, 27.6255, 27.1768]], grad_fn=<SliceBackward0>)

training epoch:1408, step:0 
model_pd.l_p.mean(): 0.055850084871053696 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055850084871053696 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0834])), ('power', tensor([-1.5614]))])
epoch：1408	 i:0 	 global-step:28160	 l-p:0.055850084871053696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.7160, 26.7177, 26.7160],
        [26.7160, 26.7160, 26.7160],
        [26.7160, 28.5099, 28.1056],
        [26.7160, 34.0850, 38.1393]], grad_fn=<SliceBackward0>)

training epoch:1409, step:0 
model_pd.l_p.mean(): 0.055842913687229156 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055842913687229156 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0849])), ('power', tensor([-1.5301]))])
epoch：1409	 i:0 	 global-step:28180	 l-p:0.055842913687229156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.7477, 29.5319, 29.5073],
        [26.7477, 27.5156, 27.1007],
        [26.7477, 26.7512, 26.7478],
        [26.7477, 26.9264, 26.7805]], grad_fn=<SliceBackward0>)

training epoch:1410, step:0 
model_pd.l_p.mean(): 0.05583569407463074 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05583569407463074 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0864])), ('power', tensor([-1.4985]))])
epoch：1410	 i:0 	 global-step:28200	 l-p:0.05583569407463074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1411
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[26.7797, 36.1232, 42.5924],
        [26.7797, 34.1675, 38.2323],
        [26.7797, 29.6574, 29.6805],
        [26.7797, 28.2298, 27.7677]], grad_fn=<SliceBackward0>)

training epoch:1411, step:0 
model_pd.l_p.mean(): 0.05582844093441963 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05582844093441963 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0880])), ('power', tensor([-1.4666]))])
epoch：1411	 i:0 	 global-step:28220	 l-p:0.05582844093441963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.8120, 26.8142, 26.8120],
        [26.8120, 26.8505, 26.8148],
        [26.8120, 26.8120, 26.8120],
        [26.8120, 27.7011, 27.2595]], grad_fn=<SliceBackward0>)

training epoch:1412, step:0 
model_pd.l_p.mean(): 0.055821146816015244 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055821146816015244 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0896])), ('power', tensor([-1.4345]))])
epoch：1412	 i:0 	 global-step:28240	 l-p:0.055821146816015244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.8444, 26.8444, 26.8444],
        [26.8444, 26.8444, 26.8444],
        [26.8444, 26.8446, 26.8444],
        [26.8444, 32.3924, 34.4942]], grad_fn=<SliceBackward0>)

training epoch:1413, step:0 
model_pd.l_p.mean(): 0.055813826620578766 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055813826620578766 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0912])), ('power', tensor([-1.4022]))])
epoch：1413	 i:0 	 global-step:28260	 l-p:0.055813826620578766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.8771, 29.7661, 29.7894],
        [26.8771, 26.8868, 26.8774],
        [26.8771, 27.6490, 27.2320],
        [26.8771, 28.6829, 28.2760]], grad_fn=<SliceBackward0>)

training epoch:1414, step:0 
model_pd.l_p.mean(): 0.0558064728975296 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0558064728975296 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0928])), ('power', tensor([-1.3697]))])
epoch：1414	 i:0 	 global-step:28280	 l-p:0.0558064728975296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.9099, 27.1123, 26.9500],
        [26.9099, 27.2543, 27.0049],
        [26.9099, 28.7181, 28.3107],
        [26.9099, 31.4348, 32.6040]], grad_fn=<SliceBackward0>)

training epoch:1415, step:0 
model_pd.l_p.mean(): 0.055799104273319244 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055799104273319244 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0944])), ('power', tensor([-1.3370]))])
epoch：1415	 i:0 	 global-step:28300	 l-p:0.055799104273319244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1416
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[26.9429, 32.5190, 34.6353],
        [26.9429, 27.8369, 27.3929],
        [26.9429, 28.0696, 27.5981],
        [26.9429, 27.8939, 27.4406]], grad_fn=<SliceBackward0>)

training epoch:1416, step:0 
model_pd.l_p.mean(): 0.055791713297367096 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055791713297367096 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0960])), ('power', tensor([-1.3042]))])
epoch：1416	 i:0 	 global-step:28320	 l-p:0.055791713297367096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.9761, 26.9761, 26.9761],
        [26.9761, 32.5135, 34.5883],
        [26.9761, 27.9284, 27.4744],
        [26.9761, 31.5130, 32.6854]], grad_fn=<SliceBackward0>)

training epoch:1417, step:0 
model_pd.l_p.mean(): 0.055784307420253754 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055784307420253754 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0976])), ('power', tensor([-1.2712]))])
epoch：1417	 i:0 	 global-step:28340	 l-p:0.055784307420253754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.0094, 28.8249, 28.4159],
        [27.0094, 27.0374, 27.0111],
        [27.0094, 27.0099, 27.0094],
        [27.0094, 27.0482, 27.0122]], grad_fn=<SliceBackward0>)

training epoch:1418, step:0 
model_pd.l_p.mean(): 0.055776890367269516 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055776890367269516 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0992])), ('power', tensor([-1.2380]))])
epoch：1418	 i:0 	 global-step:28360	 l-p:0.055776890367269516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[27.0428, 27.0433, 27.0428],
        [27.0428, 28.1741, 27.7007],
        [27.0428, 31.0483, 31.7908],
        [27.0428, 27.0428, 27.0428]], grad_fn=<SliceBackward0>)

training epoch:1419, step:0 
model_pd.l_p.mean(): 0.05576946586370468 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05576946586370468 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1008])), ('power', tensor([-1.2047]))])
epoch：1419	 i:0 	 global-step:28380	 l-p:0.05576946586370468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[27.0763, 30.2839, 30.4777],
        [27.0763, 27.4199, 27.1706],
        [27.0763, 27.0763, 27.0763],
        [27.0763, 28.9871, 28.5991]], grad_fn=<SliceBackward0>)

training epoch:1420, step:0 
model_pd.l_p.mean(): 0.05576203390955925 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05576203390955925 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1024])), ('power', tensor([-1.1714]))])
epoch：1420	 i:0 	 global-step:28400	 l-p:0.05576203390955925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[27.1100, 27.4540, 27.2043],
        [27.1100, 29.0233, 28.6349],
        [27.1100, 28.3858, 27.9076],
        [27.1100, 31.6711, 32.8501]], grad_fn=<SliceBackward0>)

training epoch:1421, step:0 
model_pd.l_p.mean(): 0.05575460568070412 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05575460568070412 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1040])), ('power', tensor([-1.1379]))])
epoch：1421	 i:0 	 global-step:28420	 l-p:0.05575460568070412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[27.1437, 27.1535, 27.1440],
        [27.1437, 30.3599, 30.5543],
        [27.1437, 29.9728, 29.9482],
        [27.1437, 27.3253, 27.1771]], grad_fn=<SliceBackward0>)

training epoch:1422, step:0 
model_pd.l_p.mean(): 0.05574715510010719 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05574715510010719 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1057])), ('power', tensor([-1.1043]))])
epoch：1422	 i:0 	 global-step:28440	 l-p:0.05574715510010719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[27.1775, 36.6681, 43.2405],
        [27.1775, 31.5019, 32.4802],
        [27.1775, 27.5225, 27.2721],
        [27.1775, 27.1776, 27.1775]], grad_fn=<SliceBackward0>)

training epoch:1423, step:0 
model_pd.l_p.mean(): 0.055739715695381165 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055739715695381165 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1073])), ('power', tensor([-1.0706]))])
epoch：1423	 i:0 	 global-step:28460	 l-p:0.055739715695381165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[27.2114, 27.2213, 27.2117],
        [27.2114, 28.3505, 27.8739],
        [27.2114, 27.2150, 27.2115],
        [27.2114, 31.5416, 32.5213]], grad_fn=<SliceBackward0>)

training epoch:1424, step:0 
model_pd.l_p.mean(): 0.05573228746652603 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05573228746652603 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1089])), ('power', tensor([-1.0369]))])
epoch：1424	 i:0 	 global-step:28480	 l-p:0.05573228746652603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[27.2454, 27.5963, 27.3425],
        [27.2454, 27.4506, 27.2860],
        [27.2454, 27.2477, 27.2454],
        [27.2454, 29.1692, 28.7787]], grad_fn=<SliceBackward0>)

training epoch:1425, step:0 
model_pd.l_p.mean(): 0.0557248555123806 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0557248555123806 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1106])), ('power', tensor([-1.0031]))])
epoch：1425	 i:0 	 global-step:28500	 l-p:0.0557248555123806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[27.2794, 27.6308, 27.3767],
        [27.2794, 27.2817, 27.2794],
        [27.2794, 35.1628, 39.7156],
        [27.2794, 31.3227, 32.0728]], grad_fn=<SliceBackward0>)

training epoch:1426, step:0 
model_pd.l_p.mean(): 0.05571742355823517 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05571742355823517 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1122])), ('power', tensor([-0.9692]))])
epoch：1426	 i:0 	 global-step:28520	 l-p:0.05571742355823517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[27.3135, 27.6604, 27.4087],
        [27.3135, 28.0993, 27.6749],
        [27.3135, 28.2211, 27.7705],
        [27.3135, 27.3150, 27.3135]], grad_fn=<SliceBackward0>)

training epoch:1427, step:0 
model_pd.l_p.mean(): 0.05571000650525093 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05571000650525093 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1138])), ('power', tensor([-0.9352]))])
epoch：1427	 i:0 	 global-step:28540	 l-p:0.05571000650525093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1428
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[27.3477, 31.7013, 32.6865],
        [27.3477, 27.5308, 27.3813],
        [27.3477, 31.9519, 33.1425],
        [27.3477, 28.1346, 27.7096]], grad_fn=<SliceBackward0>)

training epoch:1428, step:0 
model_pd.l_p.mean(): 0.05570258945226669 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05570258945226669 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1155])), ('power', tensor([-0.9012]))])
epoch：1428	 i:0 	 global-step:28560	 l-p:0.05570258945226669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[27.3819, 27.5883, 27.4227],
        [27.3819, 28.8678, 28.3947],
        [27.3819, 27.3834, 27.3819],
        [27.3819, 29.3163, 28.9238]], grad_fn=<SliceBackward0>)

training epoch:1429, step:0 
model_pd.l_p.mean(): 0.05569517984986305 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05569517984986305 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1171])), ('power', tensor([-0.8671]))])
epoch：1429	 i:0 	 global-step:28580	 l-p:0.05569517984986305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[27.4161, 27.4161, 27.4162],
        [27.4161, 27.4166, 27.4162],
        [27.4161, 27.4556, 27.4190],
        [27.4161, 27.4185, 27.4162]], grad_fn=<SliceBackward0>)

training epoch:1430, step:0 
model_pd.l_p.mean(): 0.05568777397274971 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05568777397274971 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1187])), ('power', tensor([-0.8330]))])
epoch：1430	 i:0 	 global-step:28600	 l-p:0.05568777397274971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[27.4505, 27.4506, 27.4505],
        [27.4505, 27.5580, 27.4647],
        [27.4505, 28.4213, 27.9586],
        [27.4505, 30.4065, 30.4312]], grad_fn=<SliceBackward0>)

training epoch:1431, step:0 
model_pd.l_p.mean(): 0.05568039044737816 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05568039044737816 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1204])), ('power', tensor([-0.7988]))])
epoch：1431	 i:0 	 global-step:28620	 l-p:0.05568039044737816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[27.4848, 37.0892, 43.7414],
        [27.4848, 27.8392, 27.5829],
        [27.4848, 30.1624, 30.0431],
        [27.4848, 33.1343, 35.2522]], grad_fn=<SliceBackward0>)

training epoch:1432, step:0 
model_pd.l_p.mean(): 0.05567300692200661 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05567300692200661 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1220])), ('power', tensor([-0.7646]))])
epoch：1432	 i:0 	 global-step:28640	 l-p:0.05567300692200661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[27.5192, 29.9395, 29.7048],
        [27.5192, 27.8741, 27.6174],
        [27.5192, 29.0134, 28.5376],
        [27.5192, 27.8722, 27.6166]], grad_fn=<SliceBackward0>)

training epoch:1433, step:0 
model_pd.l_p.mean(): 0.05566564202308655 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05566564202308655 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1236])), ('power', tensor([-0.7304]))])
epoch：1433	 i:0 	 global-step:28660	 l-p:0.05566564202308655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1434
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[27.5536, 27.9090, 27.6520],
        [27.5536, 31.6409, 32.3996],
        [27.5536, 28.8524, 28.3659],
        [27.5536, 35.1691, 39.3613]], grad_fn=<SliceBackward0>)

training epoch:1434, step:0 
model_pd.l_p.mean(): 0.05565827339887619 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05565827339887619 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1253])), ('power', tensor([-0.6961]))])
epoch：1434	 i:0 	 global-step:28680	 l-p:0.05565827339887619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[27.5881, 37.2307, 43.9097],
        [27.5881, 27.7730, 27.6221],
        [27.5881, 32.2360, 33.4383],
        [27.5881, 33.2604, 35.3870]], grad_fn=<SliceBackward0>)

training epoch:1435, step:0 
model_pd.l_p.mean(): 0.05565092712640762 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05565092712640762 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1269])), ('power', tensor([-0.6618]))])
epoch：1435	 i:0 	 global-step:28700	 l-p:0.05565092712640762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[27.6226, 27.6228, 27.6226],
        [27.6226, 29.4834, 29.0646],
        [27.6226, 29.5756, 29.1795],
        [27.6226, 27.6226, 27.6226]], grad_fn=<SliceBackward0>)

training epoch:1436, step:0 
model_pd.l_p.mean(): 0.05564359202980995 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05564359202980995 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1285])), ('power', tensor([-0.6274]))])
epoch：1436	 i:0 	 global-step:28720	 l-p:0.05564359202980995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[27.6572, 32.0639, 33.0617],
        [27.6572, 27.6572, 27.6572],
        [27.6572, 30.3530, 30.2331],
        [27.6572, 30.7350, 30.8142]], grad_fn=<SliceBackward0>)

training epoch:1437, step:0 
model_pd.l_p.mean(): 0.055636268109083176 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055636268109083176 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1302])), ('power', tensor([-0.5930]))])
epoch：1437	 i:0 	 global-step:28740	 l-p:0.055636268109083176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[27.6917, 27.6917, 27.6917],
        [27.6917, 30.6760, 30.7012],
        [27.6917, 36.6822, 42.5033],
        [27.6917, 27.6941, 27.6918]], grad_fn=<SliceBackward0>)

training epoch:1438, step:0 
model_pd.l_p.mean(): 0.05562895908951759 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05562895908951759 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1318])), ('power', tensor([-0.5586]))])
epoch：1438	 i:0 	 global-step:28760	 l-p:0.05562895908951759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1439
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[27.7263, 29.5947, 29.1743],
        [27.7263, 36.7287, 42.5576],
        [27.7263, 32.1449, 33.1456],
        [27.7263, 30.6215, 30.5971]], grad_fn=<SliceBackward0>)

training epoch:1439, step:0 
model_pd.l_p.mean(): 0.05562165379524231 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05562165379524231 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1334])), ('power', tensor([-0.5241]))])
epoch：1439	 i:0 	 global-step:28780	 l-p:0.05562165379524231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[27.7610, 27.8010, 27.7639],
        [27.7610, 30.8513, 30.9310],
        [27.7610, 29.7246, 29.3265],
        [27.7610, 30.4678, 30.3475]], grad_fn=<SliceBackward0>)

training epoch:1440, step:0 
model_pd.l_p.mean(): 0.05561436712741852 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05561436712741852 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1351])), ('power', tensor([-0.4897]))])
epoch：1440	 i:0 	 global-step:28800	 l-p:0.05561436712741852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[27.7956, 35.4823, 39.7143],
        [27.7956, 33.0557, 34.7708],
        [27.7956, 30.7921, 30.8175],
        [27.7956, 28.0055, 27.8371]], grad_fn=<SliceBackward0>)

training epoch:1441, step:0 
model_pd.l_p.mean(): 0.05560709536075592 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05560709536075592 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1367])), ('power', tensor([-0.4551]))])
epoch：1441	 i:0 	 global-step:28820	 l-p:0.05560709536075592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[27.8303, 28.9979, 28.5096],
        [27.8303, 30.7372, 30.7129],
        [27.8303, 29.3430, 28.8615],
        [27.8303, 27.8326, 27.8303]], grad_fn=<SliceBackward0>)

training epoch:1442, step:0 
model_pd.l_p.mean(): 0.05559983849525452 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05559983849525452 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1383])), ('power', tensor([-0.4206]))])
epoch：1442	 i:0 	 global-step:28840	 l-p:0.05559983849525452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[27.8650, 33.6395, 35.8293],
        [27.8650, 30.8696, 30.8952],
        [27.8650, 27.8650, 27.8650],
        [27.8650, 30.5829, 30.4622]], grad_fn=<SliceBackward0>)

training epoch:1443, step:0 
model_pd.l_p.mean(): 0.05559259280562401 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05559259280562401 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1400])), ('power', tensor([-0.3861]))])
epoch：1443	 i:0 	 global-step:28860	 l-p:0.05559259280562401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[27.8997, 35.6171, 39.8662],
        [27.8997, 31.0068, 31.0871],
        [27.8997, 27.8997, 27.8997],
        [27.8997, 27.8997, 27.8997]], grad_fn=<SliceBackward0>)

training epoch:1444, step:0 
model_pd.l_p.mean(): 0.05558536574244499 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05558536574244499 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1416])), ('power', tensor([-0.3515]))])
epoch：1444	 i:0 	 global-step:28880	 l-p:0.05558536574244499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[27.9345, 28.8648, 28.4031],
        [27.9345, 27.9382, 27.9345],
        [27.9345, 30.3946, 30.1564],
        [27.9345, 29.8182, 29.3945]], grad_fn=<SliceBackward0>)

training epoch:1445, step:0 
model_pd.l_p.mean(): 0.05557813495397568 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05557813495397568 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1432])), ('power', tensor([-0.3169]))])
epoch：1445	 i:0 	 global-step:28900	 l-p:0.05557813495397568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1446
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.9692, 28.9009, 28.4385],
        [27.9692, 32.4296, 33.4401],
        [27.9692, 28.0217, 27.9737],
        [27.9692, 28.0096, 27.9722]], grad_fn=<SliceBackward0>)

training epoch:1446, step:0 
model_pd.l_p.mean(): 0.05557093769311905 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05557093769311905 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1448])), ('power', tensor([-0.2822]))])
epoch：1446	 i:0 	 global-step:28920	 l-p:0.05557093769311905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[28.0040, 31.0249, 31.0508],
        [28.0040, 32.4703, 33.4823],
        [28.0040, 28.3640, 28.1033],
        [28.0040, 32.1634, 32.9362]], grad_fn=<SliceBackward0>)

training epoch:1447, step:0 
model_pd.l_p.mean(): 0.055563751608133316 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055563751608133316 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1465])), ('power', tensor([-0.2476]))])
epoch：1447	 i:0 	 global-step:28940	 l-p:0.055563751608133316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[28.0388, 32.7685, 33.9928],
        [28.0388, 28.0412, 28.0389],
        [28.0388, 28.4012, 28.1391],
        [28.0388, 31.3702, 31.5729]], grad_fn=<SliceBackward0>)

training epoch:1448, step:0 
model_pd.l_p.mean(): 0.05555657297372818 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555657297372818 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1481])), ('power', tensor([-0.2129]))])
epoch：1448	 i:0 	 global-step:28960	 l-p:0.05555657297372818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[28.0737, 28.0839, 28.0740],
        [28.0737, 28.0754, 28.0737],
        [28.0737, 30.0615, 29.6588],
        [28.0737, 35.8422, 40.1200]], grad_fn=<SliceBackward0>)

training epoch:1449, step:0 
model_pd.l_p.mean(): 0.055549412965774536 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055549412965774536 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1497])), ('power', tensor([-0.1782]))])
epoch：1449	 i:0 	 global-step:28980	 l-p:0.055549412965774536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1450
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[28.1085, 35.8873, 40.1708],
        [28.1085, 33.4322, 35.1687],
        [28.1085, 29.9228, 29.4786],
        [28.1085, 32.5928, 33.6090]], grad_fn=<SliceBackward0>)

training epoch:1450, step:0 
model_pd.l_p.mean(): 0.05554226413369179 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554226413369179 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1513])), ('power', tensor([-0.1435]))])
epoch：1450	 i:0 	 global-step:29000	 l-p:0.05554226413369179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1451
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[28.1434, 29.3255, 28.8313],
        [28.1434, 33.4741, 35.2130],
        [28.1434, 33.9862, 36.2064],
        [28.1434, 30.8908, 30.7692]], grad_fn=<SliceBackward0>)

training epoch:1451, step:0 
model_pd.l_p.mean(): 0.055535126477479935 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055535126477479935 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1530])), ('power', tensor([-0.1088]))])
epoch：1451	 i:0 	 global-step:29020	 l-p:0.055535126477479935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[28.1783, 28.9916, 28.5525],
        [28.1783, 31.2195, 31.2459],
        [28.1783, 28.5374, 28.2768],
        [28.1783, 28.1783, 28.1783]], grad_fn=<SliceBackward0>)

training epoch:1452, step:0 
model_pd.l_p.mean(): 0.05552802234888077 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05552802234888077 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1546])), ('power', tensor([-0.0741]))])
epoch：1452	 i:0 	 global-step:29040	 l-p:0.05552802234888077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[28.2132, 28.2137, 28.2132],
        [28.2132, 28.5729, 28.3119],
        [28.2132, 28.2235, 28.2135],
        [28.2132, 29.3985, 28.9030]], grad_fn=<SliceBackward0>)

training epoch:1453, step:0 
model_pd.l_p.mean(): 0.055520910769701004 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055520910769701004 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1562])), ('power', tensor([-0.0393]))])
epoch：1453	 i:0 	 global-step:29060	 l-p:0.055520910769701004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1454
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228]])
 pt:tensor([[28.2481, 29.5829, 29.0832],
        [28.2481, 29.2501, 28.7728],
        [28.2481, 30.0723, 29.6258],
        [28.2481, 30.2495, 29.8441]], grad_fn=<SliceBackward0>)

training epoch:1454, step:0 
model_pd.l_p.mean(): 0.05551382526755333 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05551382526755333 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1578])), ('power', tensor([-0.0045]))])
epoch：1454	 i:0 	 global-step:29080	 l-p:0.05551382526755333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[28.2830, 33.6422, 35.3906],
        [28.2830, 28.2836, 28.2830],
        [28.2830, 32.7973, 33.8206],
        [28.2830, 35.9587, 40.0942]], grad_fn=<SliceBackward0>)

training epoch:1455, step:0 
model_pd.l_p.mean(): 0.05550675094127655 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05550675094127655 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([1.5145e-06]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1594])), ('power', tensor([0.0303]))])
epoch：1455	 i:0 	 global-step:29100	 l-p:0.05550675094127655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[28.3180, 28.3182, 28.3180],
        [28.3180, 31.0840, 30.9617],
        [28.3180, 28.3180, 28.3180],
        [28.3180, 28.3180, 28.3180]], grad_fn=<SliceBackward0>)

training epoch:1456, step:0 
model_pd.l_p.mean(): 0.055499691516160965 
model_pd.l_d.mean(): 9.86078134701529e-08 
model_pd.lagr.mean(): 0.055499788373708725 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([4.7700e-06]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1611])), ('power', tensor([0.0651]))])
epoch：1456	 i:0 	 global-step:29120	 l-p:0.055499691516160965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[28.3530, 33.1396, 34.3793],
        [28.3530, 28.7146, 28.4522],
        [28.3530, 36.0488, 40.1953],
        [28.3530, 31.3192, 31.2951]], grad_fn=<SliceBackward0>)

training epoch:1457, step:0 
model_pd.l_p.mean(): 0.05549265071749687 
model_pd.l_d.mean(): 4.7663903046668565e-07 
model_pd.lagr.mean(): 0.055493127554655075 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([9.7662e-06]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1627])), ('power', tensor([0.0999]))])
epoch：1457	 i:0 	 global-step:29140	 l-p:0.05549265071749687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[28.3878, 28.3916, 28.3879],
        [28.3878, 31.1612, 31.0387],
        [28.3878, 28.7423, 28.4838],
        [28.3878, 31.4536, 31.4805]], grad_fn=<SliceBackward0>)

training epoch:1458, step:0 
model_pd.l_p.mean(): 0.055485643446445465 
model_pd.l_d.mean(): 1.3151966413715854e-06 
model_pd.lagr.mean(): 0.05548695847392082 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([1.6500e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1643])), ('power', tensor([0.1347]))])
epoch：1458	 i:0 	 global-step:29160	 l-p:0.055485643446445465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[28.4226, 36.2939, 40.6291],
        [28.4226, 30.9296, 30.6874],
        [28.4226, 28.4231, 28.4226],
        [28.4226, 28.4637, 28.4256]], grad_fn=<SliceBackward0>)

training epoch:1459, step:0 
model_pd.l_p.mean(): 0.05547867715358734 
model_pd.l_d.mean(): 2.793042085613706e-06 
model_pd.lagr.mean(): 0.05548147112131119 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([2.4964e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1659])), ('power', tensor([0.1693]))])
epoch：1459	 i:0 	 global-step:29180	 l-p:0.05547867715358734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[28.4571, 28.4571, 28.4571],
        [28.4571, 36.7028, 41.4681],
        [28.4571, 30.3794, 29.9474],
        [28.4571, 28.8254, 28.5591]], grad_fn=<SliceBackward0>)

training epoch:1460, step:0 
model_pd.l_p.mean(): 0.05547178164124489 
model_pd.l_d.mean(): 5.0839244067901745e-06 
model_pd.lagr.mean(): 0.05547686666250229 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([3.5146e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1675])), ('power', tensor([0.2037]))])
epoch：1460	 i:0 	 global-step:29200	 l-p:0.05547178164124489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[28.4912, 28.4928, 28.4913],
        [28.4912, 32.7287, 33.5169],
        [28.4912, 28.4913, 28.4912],
        [28.4912, 29.5027, 29.0210]], grad_fn=<SliceBackward0>)

training epoch:1461, step:0 
model_pd.l_p.mean(): 0.055464982986450195 
model_pd.l_d.mean(): 8.352681106771342e-06 
model_pd.lagr.mean(): 0.05547333508729935 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([4.7029e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1690])), ('power', tensor([0.2377]))])
epoch：1461	 i:0 	 global-step:29220	 l-p:0.055464982986450195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[28.5249, 31.6067, 31.6339],
        [28.5249, 28.5249, 28.5249],
        [28.5249, 31.3128, 31.1898],
        [28.5249, 29.4769, 29.0046]], grad_fn=<SliceBackward0>)

training epoch:1462, step:0 
model_pd.l_p.mean(): 0.05545828863978386 
model_pd.l_d.mean(): 1.275245267606806e-05 
model_pd.lagr.mean(): 0.055471040308475494 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([6.0587e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1706])), ('power', tensor([0.2712]))])
epoch：1462	 i:0 	 global-step:29240	 l-p:0.05545828863978386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[28.5579, 28.9224, 28.6579],
        [28.5579, 28.5594, 28.5579],
        [28.5579, 28.5877, 28.5597],
        [28.5579, 29.3833, 28.9377]], grad_fn=<SliceBackward0>)

training epoch:1463, step:0 
model_pd.l_p.mean(): 0.05545175075531006 
model_pd.l_d.mean(): 1.841881930886302e-05 
model_pd.lagr.mean(): 0.055470168590545654 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([7.5787e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1721])), ('power', tensor([0.3040]))])
epoch：1463	 i:0 	 global-step:29260	 l-p:0.05545175075531006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[28.5900, 28.5900, 28.5900],
        [28.5900, 30.1480, 29.6526],
        [28.5900, 37.8899, 43.9140],
        [28.5900, 30.6178, 30.2074]], grad_fn=<SliceBackward0>)

training epoch:1464, step:0 
model_pd.l_p.mean(): 0.05544538050889969 
model_pd.l_d.mean(): 2.5466448278166354e-05 
model_pd.lagr.mean(): 0.055470846593379974 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([9.2589e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1736])), ('power', tensor([0.3360]))])
epoch：1464	 i:0 	 global-step:29280	 l-p:0.05544538050889969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[28.6211, 32.8794, 33.6717],
        [28.6211, 34.5701, 36.8317],
        [28.6211, 28.8138, 28.6565],
        [28.6211, 34.5635, 36.8187]], grad_fn=<SliceBackward0>)

training epoch:1465, step:0 
model_pd.l_p.mean(): 0.055439237505197525 
model_pd.l_d.mean(): 3.398218541406095e-05 
model_pd.lagr.mean(): 0.055473219603300095 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0001]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1750])), ('power', tensor([0.3670]))])
epoch：1465	 i:0 	 global-step:29300	 l-p:0.055439237505197525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[28.6510, 28.7050, 28.6556],
        [28.6510, 36.4329, 40.6266],
        [28.6510, 34.6067, 36.8709],
        [28.6510, 28.6925, 28.6541]], grad_fn=<SliceBackward0>)

training epoch:1466, step:0 
model_pd.l_p.mean(): 0.05543334782123566 
model_pd.l_d.mean(): 4.4021973735652864e-05 
model_pd.lagr.mean(): 0.05547736957669258 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0001]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1763])), ('power', tensor([0.3968]))])
epoch：1466	 i:0 	 global-step:29320	 l-p:0.05543334782123566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[28.6795, 28.8968, 28.7225],
        [28.6795, 34.5924, 36.8117],
        [28.6795, 31.2111, 30.9668],
        [28.6795, 32.0934, 32.3020]], grad_fn=<SliceBackward0>)

training epoch:1467, step:0 
model_pd.l_p.mean(): 0.05542774498462677 
model_pd.l_d.mean(): 5.560453064390458e-05 
model_pd.lagr.mean(): 0.05548334866762161 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0002]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1776])), ('power', tensor([0.4252]))])
epoch：1467	 i:0 	 global-step:29340	 l-p:0.05542774498462677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[28.7063, 29.5365, 29.0884],
        [28.7063, 28.7168, 28.7067],
        [28.7063, 38.0463, 44.0967],
        [28.7063, 28.7101, 28.7064]], grad_fn=<SliceBackward0>)

training epoch:1468, step:0 
model_pd.l_p.mean(): 0.05542248114943504 
model_pd.l_d.mean(): 6.870835204608738e-05 
model_pd.lagr.mean(): 0.055491190403699875 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0002]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1789])), ('power', tensor([0.4519]))])
epoch：1468	 i:0 	 global-step:29360	 l-p:0.05542248114943504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[28.7313, 28.9248, 28.7669],
        [28.7313, 32.1519, 32.3610],
        [28.7313, 28.7329, 28.7313],
        [28.7313, 29.4287, 29.0186]], grad_fn=<SliceBackward0>)

training epoch:1469, step:0 
model_pd.l_p.mean(): 0.05541758984327316 
model_pd.l_d.mean(): 8.326544048031792e-05 
model_pd.lagr.mean(): 0.055500853806734085 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0002]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1800])), ('power', tensor([0.4768]))])
epoch：1469	 i:0 	 global-step:29380	 l-p:0.05541758984327316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[28.7542, 34.2092, 35.9898],
        [28.7542, 31.8630, 31.8906],
        [28.7542, 28.7544, 28.7542],
        [28.7542, 29.5860, 29.1370]], grad_fn=<SliceBackward0>)

training epoch:1470, step:0 
model_pd.l_p.mean(): 0.05541310831904411 
model_pd.l_d.mean(): 9.916076669469476e-05 
model_pd.lagr.mean(): 0.05551226809620857 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0002]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1810])), ('power', tensor([0.4996]))])
epoch：1470	 i:0 	 global-step:29400	 l-p:0.05541310831904411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[28.7748, 28.7786, 28.7749],
        [28.7748, 36.5925, 40.8058],
        [28.7748, 28.7750, 28.7748],
        [28.7748, 30.6362, 30.1810]], grad_fn=<SliceBackward0>)

training epoch:1471, step:0 
model_pd.l_p.mean(): 0.055409085005521774 
model_pd.l_d.mean(): 0.00011622707825154066 
model_pd.lagr.mean(): 0.05552531033754349 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0002]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1820])), ('power', tensor([0.5201]))])
epoch：1471	 i:0 	 global-step:29420	 l-p:0.055409085005521774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[28.7929, 29.4919, 29.0809],
        [28.7929, 29.1530, 28.8904],
        [28.7929, 31.9062, 31.9339],
        [28.7929, 28.8346, 28.7960]], grad_fn=<SliceBackward0>)

training epoch:1472, step:0 
model_pd.l_p.mean(): 0.05540556460618973 
model_pd.l_d.mean(): 0.00013424589997157454 
model_pd.lagr.mean(): 0.0555398091673851 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0003]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1828])), ('power', tensor([0.5381]))])
epoch：1472	 i:0 	 global-step:29440	 l-p:0.05540556460618973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[28.8082, 28.9217, 28.8233],
        [28.8082, 28.8082, 28.8082],
        [28.8082, 32.0252, 32.1095],
        [28.8082, 29.7707, 29.2932]], grad_fn=<SliceBackward0>)

training epoch:1473, step:0 
model_pd.l_p.mean(): 0.05540258437395096 
model_pd.l_d.mean(): 0.00015294527111109346 
model_pd.lagr.mean(): 0.05555552989244461 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0003]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1835])), ('power', tensor([0.5534]))])
epoch：1473	 i:0 	 global-step:29460	 l-p:0.05540258437395096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[28.8206, 28.8206, 28.8206],
        [28.8206, 36.8091, 41.2100],
        [28.8206, 29.1889, 28.9217],
        [28.8206, 28.8208, 28.8206]], grad_fn=<SliceBackward0>)

training epoch:1474, step:0 
model_pd.l_p.mean(): 0.055400170385837555 
model_pd.l_d.mean(): 0.0001720035361358896 
model_pd.lagr.mean(): 0.05557217448949814 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0003]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1841])), ('power', tensor([0.5657]))])
epoch：1474	 i:0 	 global-step:29480	 l-p:0.055400170385837555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[28.8298, 33.1215, 33.9204],
        [28.8298, 28.8298, 28.8298],
        [28.8298, 34.8252, 37.1049],
        [28.8298, 28.9434, 28.8449]], grad_fn=<SliceBackward0>)

training epoch:1475, step:0 
model_pd.l_p.mean(): 0.05539838224649429 
model_pd.l_d.mean(): 0.0001910510763991624 
model_pd.lagr.mean(): 0.05558943375945091 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0004]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1845])), ('power', tensor([0.5749]))])
epoch：1475	 i:0 	 global-step:29500	 l-p:0.05539838224649429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[28.8356, 32.2696, 32.4797],
        [28.8356, 28.8356, 28.8356],
        [28.8356, 28.8356, 28.8356],
        [28.8356, 28.8361, 28.8356]], grad_fn=<SliceBackward0>)

training epoch:1476, step:0 
model_pd.l_p.mean(): 0.05539725348353386 
model_pd.l_d.mean(): 0.00020967028103768826 
model_pd.lagr.mean(): 0.05560692399740219 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0004]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1847])), ('power', tensor([0.5807]))])
epoch：1476	 i:0 	 global-step:29520	 l-p:0.05539725348353386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[28.8379, 34.3098, 36.0962],
        [28.8379, 29.2064, 28.9391],
        [28.8379, 28.8384, 28.8379],
        [28.8379, 31.9564, 31.9843]], grad_fn=<SliceBackward0>)

training epoch:1477, step:0 
model_pd.l_p.mean(): 0.05539681017398834 
model_pd.l_d.mean(): 0.00022741418797522783 
model_pd.lagr.mean(): 0.05562422424554825 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0004]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1848])), ('power', tensor([0.5830]))])
epoch：1477	 i:0 	 global-step:29540	 l-p:0.05539681017398834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[28.8364, 28.8364, 28.8365],
        [28.8364, 34.3081, 36.0943],
        [28.8364, 28.8908, 28.8411],
        [28.8364, 30.7022, 30.2459]], grad_fn=<SliceBackward0>)

training epoch:1478, step:0 
model_pd.l_p.mean(): 0.05539708957076073 
model_pd.l_d.mean(): 0.00024380227841902524 
model_pd.lagr.mean(): 0.05564089119434357 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0004]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1848])), ('power', tensor([0.5815]))])
epoch：1478	 i:0 	 global-step:29560	 l-p:0.05539708957076073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[28.8311, 29.0253, 28.8668],
        [28.8311, 29.5312, 29.1196],
        [28.8311, 30.4035, 29.9037],
        [28.8311, 33.7046, 34.9677]], grad_fn=<SliceBackward0>)

training epoch:1479, step:0 
model_pd.l_p.mean(): 0.0553981252014637 
model_pd.l_d.mean(): 0.0002583335735835135 
model_pd.lagr.mean(): 0.05565645918250084 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0005]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1845])), ('power', tensor([0.5762]))])
epoch：1479	 i:0 	 global-step:29580	 l-p:0.0553981252014637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[28.8218, 28.9354, 28.8369],
        [28.8218, 28.8218, 28.8218],
        [28.8218, 28.8234, 28.8218],
        [28.8218, 31.8413, 31.8173]], grad_fn=<SliceBackward0>)

training epoch:1480, step:0 
model_pd.l_p.mean(): 0.05539993196725845 
model_pd.l_d.mean(): 0.0002704999642446637 
model_pd.lagr.mean(): 0.05567043274641037 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0005]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1841])), ('power', tensor([0.5669]))])
epoch：1480	 i:0 	 global-step:29600	 l-p:0.05539993196725845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[28.8083, 34.7990, 37.0768],
        [28.8083, 30.7565, 30.3190],
        [28.8083, 28.8099, 28.8084],
        [28.8083, 36.6357, 40.8542]], grad_fn=<SliceBackward0>)

training epoch:1481, step:0 
model_pd.l_p.mean(): 0.05540255457162857 
model_pd.l_d.mean(): 0.0002797951747197658 
model_pd.lagr.mean(): 0.05568234995007515 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0005]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1835])), ('power', tensor([0.5535]))])
epoch：1481	 i:0 	 global-step:29620	 l-p:0.05540255457162857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[28.7907, 29.1507, 28.8882],
        [28.7907, 34.2530, 36.0361],
        [28.7907, 36.7704, 41.1663],
        [28.7907, 28.7907, 28.7907]], grad_fn=<SliceBackward0>)

training epoch:1482, step:0 
model_pd.l_p.mean(): 0.05540599673986435 
model_pd.l_d.mean(): 0.0002857330837287009 
model_pd.lagr.mean(): 0.05569173023104668 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0006]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1827])), ('power', tensor([0.5359]))])
epoch：1482	 i:0 	 global-step:29640	 l-p:0.05540599673986435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[28.7688, 38.8485, 45.8341],
        [28.7688, 33.0507, 33.8476],
        [28.7688, 28.7712, 28.7688],
        [28.7688, 36.5847, 40.7970]], grad_fn=<SliceBackward0>)

training epoch:1483, step:0 
model_pd.l_p.mean(): 0.055410269647836685 
model_pd.l_d.mean(): 0.0002878630184568465 
model_pd.lagr.mean(): 0.05569813400506973 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0006]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1817])), ('power', tensor([0.5141]))])
epoch：1483	 i:0 	 global-step:29660	 l-p:0.055410269647836685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1484
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[28.7425, 31.7530, 31.7290],
        [28.7425, 29.1097, 28.8433],
        [28.7425, 28.7425, 28.7425],
        [28.7425, 31.5535, 31.4298]], grad_fn=<SliceBackward0>)

training epoch:1484, step:0 
model_pd.l_p.mean(): 0.055415403097867966 
model_pd.l_d.mean(): 0.00028576585464179516 
model_pd.lagr.mean(): 0.05570117011666298 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0006]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1805])), ('power', tensor([0.4879]))])
epoch：1484	 i:0 	 global-step:29680	 l-p:0.055415403097867966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1485
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[28.7120, 31.2467, 31.0021],
        [28.7120, 33.5638, 34.8210],
        [28.7120, 32.1300, 32.3390],
        [28.7120, 31.9172, 32.0011]], grad_fn=<SliceBackward0>)

training epoch:1485, step:0 
model_pd.l_p.mean(): 0.055421382188797 
model_pd.l_d.mean(): 0.0002791017177514732 
model_pd.lagr.mean(): 0.05570048466324806 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0006]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1791])), ('power', tensor([0.4575]))])
epoch：1485	 i:0 	 global-step:29700	 l-p:0.055421382188797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1486
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[28.6771, 29.5064, 29.0588],
        [28.6771, 29.8839, 29.3795],
        [28.6771, 32.0907, 32.2993],
        [28.6771, 31.4812, 31.3576]], grad_fn=<SliceBackward0>)

training epoch:1486, step:0 
model_pd.l_p.mean(): 0.055428218096494675 
model_pd.l_d.mean(): 0.0002675970899872482 
model_pd.lagr.mean(): 0.05569581687450409 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1775])), ('power', tensor([0.4228]))])
epoch：1486	 i:0 	 global-step:29720	 l-p:0.055428218096494675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[28.6380, 28.6380, 28.6380],
        [28.6380, 36.9394, 41.7374],
        [28.6380, 30.4897, 30.0368],
        [28.6380, 29.9931, 29.4859]], grad_fn=<SliceBackward0>)

training epoch:1487, step:0 
model_pd.l_p.mean(): 0.055435910820961 
model_pd.l_d.mean(): 0.00025106751127168536 
model_pd.lagr.mean(): 0.05568697676062584 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1757])), ('power', tensor([0.3839]))])
epoch：1487	 i:0 	 global-step:29740	 l-p:0.055435910820961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[28.5948, 28.5963, 28.5948],
        [28.5948, 28.9631, 28.6964],
        [28.5948, 37.8963, 43.9214],
        [28.5948, 29.9476, 29.4413]], grad_fn=<SliceBackward0>)

training epoch:1488, step:0 
model_pd.l_p.mean(): 0.055444445461034775 
model_pd.l_d.mean(): 0.0002294358127983287 
model_pd.lagr.mean(): 0.055673882365226746 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1738])), ('power', tensor([0.3408]))])
epoch：1488	 i:0 	 global-step:29760	 l-p:0.055444445461034775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[28.5475, 28.7637, 28.5903],
        [28.5475, 28.5475, 28.5475],
        [28.5475, 28.5475, 28.5475],
        [28.5475, 33.3694, 34.6187]], grad_fn=<SliceBackward0>)

training epoch:1489, step:0 
model_pd.l_p.mean(): 0.05545379966497421 
model_pd.l_d.mean(): 0.0002027376467594877 
model_pd.lagr.mean(): 0.05565653741359711 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1716])), ('power', tensor([0.2937]))])
epoch：1489	 i:0 	 global-step:29780	 l-p:0.05545379966497421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[28.4963, 32.7346, 33.5230],
        [28.4963, 38.4752, 45.3900],
        [28.4963, 28.4964, 28.4963],
        [28.4963, 34.4110, 36.6554]], grad_fn=<SliceBackward0>)

training epoch:1490, step:0 
model_pd.l_p.mean(): 0.05546396225690842 
model_pd.l_d.mean(): 0.0001711275544948876 
model_pd.lagr.mean(): 0.05563509091734886 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1693])), ('power', tensor([0.2427]))])
epoch：1490	 i:0 	 global-step:29800	 l-p:0.05546396225690842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[28.4415, 36.6824, 41.4449],
        [28.4415, 32.9829, 34.0127],
        [28.4415, 28.4420, 28.4415],
        [28.4415, 37.6902, 43.6807]], grad_fn=<SliceBackward0>)

training epoch:1491, step:0 
model_pd.l_p.mean(): 0.0554748997092247 
model_pd.l_d.mean(): 0.00013488986587617546 
model_pd.lagr.mean(): 0.05560978874564171 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1667])), ('power', tensor([0.1881]))])
epoch：1491	 i:0 	 global-step:29820	 l-p:0.0554748997092247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[28.3831, 32.6033, 33.3881],
        [28.3831, 28.3855, 28.3832],
        [28.3831, 28.4128, 28.3849],
        [28.3831, 34.2792, 36.5202]], grad_fn=<SliceBackward0>)

training epoch:1492, step:0 
model_pd.l_p.mean(): 0.055486589670181274 
model_pd.l_d.mean(): 9.443642920814455e-05 
model_pd.lagr.mean(): 0.055581025779247284 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1641])), ('power', tensor([0.1300]))])
epoch：1492	 i:0 	 global-step:29840	 l-p:0.055486589670181274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[28.3216, 29.0079, 28.6043],
        [28.3216, 28.3319, 28.3219],
        [28.3216, 28.6751, 28.4173],
        [28.3216, 29.5119, 29.0143]], grad_fn=<SliceBackward0>)

training epoch:1493, step:0 
model_pd.l_p.mean(): 0.05549897253513336 
model_pd.l_d.mean(): 5.031832552049309e-05 
model_pd.lagr.mean(): 0.05554929003119469 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1612])), ('power', tensor([0.0686]))])
epoch：1493	 i:0 	 global-step:29860	 l-p:0.05549897253513336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[28.2570, 32.7668, 33.7890],
        [28.2570, 28.6173, 28.3559],
        [28.2570, 33.0262, 34.2612],
        [28.2570, 28.2570, 28.2570]], grad_fn=<SliceBackward0>)

training epoch:1494, step:0 
model_pd.l_p.mean(): 0.05551202595233917 
model_pd.l_d.mean(): 3.208564976375783e-06 
model_pd.lagr.mean(): 0.05551523342728615 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1582])), ('power', tensor([0.0044]))])
epoch：1494	 i:0 	 global-step:29880	 l-p:0.05551202595233917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[28.1898, 28.5415, 28.2850],
        [28.1898, 28.8726, 28.4710],
        [28.1898, 34.0364, 36.2542],
        [28.1898, 28.1898, 28.1898]], grad_fn=<SliceBackward0>)

training epoch:1495, step:0 
model_pd.l_p.mean(): 0.05552567541599274 
model_pd.l_d.mean(): -4.6111941628623754e-05 
model_pd.lagr.mean(): 0.055479563772678375 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1551])), ('power', tensor([-0.0626]))])
epoch：1495	 i:0 	 global-step:29900	 l-p:0.05552567541599274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[28.1202, 28.4710, 28.2152],
        [28.1202, 28.1207, 28.1202],
        [28.1202, 30.1117, 29.7082],
        [28.1202, 31.1547, 31.1809]], grad_fn=<SliceBackward0>)

training epoch:1496, step:0 
model_pd.l_p.mean(): 0.05553986504673958 
model_pd.l_d.mean(): -9.672746091382578e-05 
model_pd.lagr.mean(): 0.055443137884140015 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1519])), ('power', tensor([-0.1319]))])
epoch：1496	 i:0 	 global-step:29920	 l-p:0.05553986504673958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[28.0487, 28.0502, 28.0487],
        [28.0487, 28.4093, 28.1482],
        [28.0487, 37.1621, 43.0638],
        [28.0487, 35.6567, 39.7551]], grad_fn=<SliceBackward0>)

training epoch:1497, step:0 
model_pd.l_p.mean(): 0.05555454269051552 
model_pd.l_d.mean(): -0.00014763951185159385 
model_pd.lagr.mean(): 0.055406901985406876 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1486])), ('power', tensor([-0.2031]))])
epoch：1497	 i:0 	 global-step:29940	 l-p:0.05555454269051552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[27.9756, 28.9669, 28.4946],
        [27.9756, 28.0160, 27.9785],
        [27.9756, 29.2963, 28.8017],
        [27.9756, 35.7152, 39.9769]], grad_fn=<SliceBackward0>)

training epoch:1498, step:0 
model_pd.l_p.mean(): 0.055569618940353394 
model_pd.l_d.mean(): -0.0001977778592845425 
model_pd.lagr.mean(): 0.05537183955311775 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1451])), ('power', tensor([-0.2759]))])
epoch：1498	 i:0 	 global-step:29960	 l-p:0.055569618940353394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[27.9013, 28.2565, 27.9987],
        [27.9013, 27.9303, 27.9030],
        [27.9013, 27.9013, 27.9013],
        [27.9013, 29.4182, 28.9354]], grad_fn=<SliceBackward0>)

training epoch:1499, step:0 
model_pd.l_p.mean(): 0.05558503046631813 
model_pd.l_d.mean(): -0.0002460035029798746 
model_pd.lagr.mean(): 0.0553390271961689 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-2.1417])), ('power', tensor([-0.3499]))])
epoch：1499	 i:0 	 global-step:29980	 l-p:0.05558503046631813
