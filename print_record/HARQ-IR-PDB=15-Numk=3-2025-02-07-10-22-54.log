
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.12033862620592117 
model_pd.l_d.mean(): -24.47382926940918 
model_pd.lagr.mean(): -24.353490829467773 
model_pd.lambdas: dict_items([('pout', tensor([1.0001], device='cuda:0')), ('power', tensor([0.9988], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0805], device='cuda:0')), ('power', tensor([-24.5543], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.12033862620592117
epoch£º0	 i:1 	 global-step:1	 l-p:0.15623173117637634
epoch£º0	 i:2 	 global-step:2	 l-p:0.15599572658538818
epoch£º0	 i:3 	 global-step:3	 l-p:0.13996995985507965
epoch£º0	 i:4 	 global-step:4	 l-p:-1.0029131174087524
epoch£º0	 i:5 	 global-step:5	 l-p:0.11748262494802475
epoch£º0	 i:6 	 global-step:6	 l-p:0.13973014056682587
epoch£º0	 i:7 	 global-step:7	 l-p:0.11967911571264267
epoch£º0	 i:8 	 global-step:8	 l-p:0.03531193360686302
epoch£º0	 i:9 	 global-step:9	 l-p:0.11698617786169052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2296, 3.2300, 3.2296],
        [3.2296, 3.4206, 3.3823],
        [3.2296, 3.2843, 3.2494],
        [3.2296, 4.1190, 4.7376]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.1074749156832695 
model_pd.l_d.mean(): -23.637460708618164 
model_pd.lagr.mean(): -23.529985427856445 
model_pd.lambdas: dict_items([('pout', tensor([0.9984], device='cuda:0')), ('power', tensor([0.9868], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2763], device='cuda:0')), ('power', tensor([-23.6466], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.1074749156832695
epoch£º1	 i:1 	 global-step:21	 l-p:0.11298112571239471
epoch£º1	 i:2 	 global-step:22	 l-p:0.10698827356100082
epoch£º1	 i:3 	 global-step:23	 l-p:0.09772323071956635
epoch£º1	 i:4 	 global-step:24	 l-p:0.12798264622688293
epoch£º1	 i:5 	 global-step:25	 l-p:0.11354013532400131
epoch£º1	 i:6 	 global-step:26	 l-p:0.12517113983631134
epoch£º1	 i:7 	 global-step:27	 l-p:0.1289316564798355
epoch£º1	 i:8 	 global-step:28	 l-p:0.12185292690992355
epoch£º1	 i:9 	 global-step:29	 l-p:0.09246747940778732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9441, 3.5246, 3.8162],
        [2.9441, 2.9988, 2.9654],
        [2.9441, 3.3860, 3.5335],
        [2.9441, 2.9900, 2.9601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.13487528264522552 
model_pd.l_d.mean(): -23.885231018066406 
model_pd.lagr.mean(): -23.750356674194336 
model_pd.lambdas: dict_items([('pout', tensor([0.9957], device='cuda:0')), ('power', tensor([0.9748], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2001], device='cuda:0')), ('power', tensor([-24.2691], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.13487528264522552
epoch£º2	 i:1 	 global-step:41	 l-p:0.15497861802577972
epoch£º2	 i:2 	 global-step:42	 l-p:0.1366599202156067
epoch£º2	 i:3 	 global-step:43	 l-p:0.12539826333522797
epoch£º2	 i:4 	 global-step:44	 l-p:0.13382168114185333
epoch£º2	 i:5 	 global-step:45	 l-p:0.17353428900241852
epoch£º2	 i:6 	 global-step:46	 l-p:0.11660190671682358
epoch£º2	 i:7 	 global-step:47	 l-p:0.17922121286392212
epoch£º2	 i:8 	 global-step:48	 l-p:0.13090595602989197
epoch£º2	 i:9 	 global-step:49	 l-p:0.1258503496646881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9074, 3.4273, 3.6595],
        [2.9074, 2.9327, 2.9135],
        [2.9074, 2.9597, 2.9275],
        [2.9074, 3.6501, 4.1493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.12943986058235168 
model_pd.l_d.mean(): -22.828493118286133 
model_pd.lagr.mean(): -22.699052810668945 
model_pd.lambdas: dict_items([('pout', tensor([0.9943], device='cuda:0')), ('power', tensor([0.9627], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0844], device='cuda:0')), ('power', tensor([-23.5960], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.12943986058235168
epoch£º3	 i:1 	 global-step:61	 l-p:0.15020222961902618
epoch£º3	 i:2 	 global-step:62	 l-p:0.10976523160934448
epoch£º3	 i:3 	 global-step:63	 l-p:0.12252172082662582
epoch£º3	 i:4 	 global-step:64	 l-p:0.13838240504264832
epoch£º3	 i:5 	 global-step:65	 l-p:0.11287195980548859
epoch£º3	 i:6 	 global-step:66	 l-p:0.11583522707223892
epoch£º3	 i:7 	 global-step:67	 l-p:0.12656213343143463
epoch£º3	 i:8 	 global-step:68	 l-p:0.12532168626785278
epoch£º3	 i:9 	 global-step:69	 l-p:0.12984393537044525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9926, 3.3842, 3.4838],
        [2.9926, 3.4693, 3.6473],
        [2.9926, 3.8150, 4.4036],
        [2.9926, 3.0412, 3.0101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.11725188046693802 
model_pd.l_d.mean(): -22.72794532775879 
model_pd.lagr.mean(): -22.610692977905273 
model_pd.lambdas: dict_items([('pout', tensor([0.9921], device='cuda:0')), ('power', tensor([0.9507], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1631], device='cuda:0')), ('power', tensor([-23.7065], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.11725188046693802
epoch£º4	 i:1 	 global-step:81	 l-p:0.12108179181814194
epoch£º4	 i:2 	 global-step:82	 l-p:0.12206286936998367
epoch£º4	 i:3 	 global-step:83	 l-p:0.1183309555053711
epoch£º4	 i:4 	 global-step:84	 l-p:0.1377755105495453
epoch£º4	 i:5 	 global-step:85	 l-p:0.12631668150424957
epoch£º4	 i:6 	 global-step:86	 l-p:0.13798578083515167
epoch£º4	 i:7 	 global-step:87	 l-p:0.12111519277095795
epoch£º4	 i:8 	 global-step:88	 l-p:0.10971179604530334
epoch£º4	 i:9 	 global-step:89	 l-p:0.13885226845741272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9777, 2.9778, 2.9777],
        [2.9777, 2.9813, 2.9780],
        [2.9777, 3.0322, 2.9991],
        [2.9777, 3.0558, 3.0164]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.14621081948280334 
model_pd.l_d.mean(): -22.924341201782227 
model_pd.lagr.mean(): -22.77812957763672 
model_pd.lambdas: dict_items([('pout', tensor([0.9900], device='cuda:0')), ('power', tensor([0.9387], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1898], device='cuda:0')), ('power', tensor([-24.1911], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.14621081948280334
epoch£º5	 i:1 	 global-step:101	 l-p:0.11178810894489288
epoch£º5	 i:2 	 global-step:102	 l-p:0.11507999897003174
epoch£º5	 i:3 	 global-step:103	 l-p:0.12310147285461426
epoch£º5	 i:4 	 global-step:104	 l-p:0.13932909071445465
epoch£º5	 i:5 	 global-step:105	 l-p:0.12649528682231903
epoch£º5	 i:6 	 global-step:106	 l-p:0.1141982227563858
epoch£º5	 i:7 	 global-step:107	 l-p:0.12012176215648651
epoch£º5	 i:8 	 global-step:108	 l-p:0.1202128604054451
epoch£º5	 i:9 	 global-step:109	 l-p:0.1154056116938591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0154, 3.0309, 3.0182],
        [3.0154, 3.0154, 3.0154],
        [3.0154, 3.7883, 4.3092],
        [3.0154, 3.0181, 3.0156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.12803731858730316 
model_pd.l_d.mean(): -22.896106719970703 
model_pd.lagr.mean(): -22.768070220947266 
model_pd.lambdas: dict_items([('pout', tensor([0.9878], device='cuda:0')), ('power', tensor([0.9266], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2635], device='cuda:0')), ('power', tensor([-24.3959], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.12803731858730316
epoch£º6	 i:1 	 global-step:121	 l-p:0.11651264131069183
epoch£º6	 i:2 	 global-step:122	 l-p:0.1385548859834671
epoch£º6	 i:3 	 global-step:123	 l-p:0.10701581835746765
epoch£º6	 i:4 	 global-step:124	 l-p:0.14730072021484375
epoch£º6	 i:5 	 global-step:125	 l-p:0.11932460218667984
epoch£º6	 i:6 	 global-step:126	 l-p:0.12256114929914474
epoch£º6	 i:7 	 global-step:127	 l-p:0.13623791933059692
epoch£º6	 i:8 	 global-step:128	 l-p:0.11463510245084763
epoch£º6	 i:9 	 global-step:129	 l-p:0.12663674354553223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0820, 3.4474, 3.5216],
        [3.0820, 3.1172, 3.0923],
        [3.0820, 3.0820, 3.0819],
        [3.0820, 3.0939, 3.0837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.12424714863300323 
model_pd.l_d.mean(): -22.5183162689209 
model_pd.lagr.mean(): -22.39406967163086 
model_pd.lambdas: dict_items([('pout', tensor([0.9857], device='cuda:0')), ('power', tensor([0.9146], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2758], device='cuda:0')), ('power', tensor([-24.2911], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.12424714863300323
epoch£º7	 i:1 	 global-step:141	 l-p:0.1334528774023056
epoch£º7	 i:2 	 global-step:142	 l-p:0.1159702017903328
epoch£º7	 i:3 	 global-step:143	 l-p:0.11717589944601059
epoch£º7	 i:4 	 global-step:144	 l-p:0.120798259973526
epoch£º7	 i:5 	 global-step:145	 l-p:0.11995456367731094
epoch£º7	 i:6 	 global-step:146	 l-p:0.1445290595293045
epoch£º7	 i:7 	 global-step:147	 l-p:0.13444140553474426
epoch£º7	 i:8 	 global-step:148	 l-p:0.11110875755548477
epoch£º7	 i:9 	 global-step:149	 l-p:0.1096622571349144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0015, 3.0015, 3.0015],
        [3.0015, 3.0015, 3.0015],
        [3.0015, 3.4517, 3.6098],
        [3.0015, 3.0823, 3.0430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.13040801882743835 
model_pd.l_d.mean(): -21.480379104614258 
model_pd.lagr.mean(): -21.349971771240234 
model_pd.lambdas: dict_items([('pout', tensor([0.9836], device='cuda:0')), ('power', tensor([0.9026], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1566], device='cuda:0')), ('power', tensor([-23.5962], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.13040801882743835
epoch£º8	 i:1 	 global-step:161	 l-p:0.1220020055770874
epoch£º8	 i:2 	 global-step:162	 l-p:0.1236276626586914
epoch£º8	 i:3 	 global-step:163	 l-p:0.1245100200176239
epoch£º8	 i:4 	 global-step:164	 l-p:0.08850257843732834
epoch£º8	 i:5 	 global-step:165	 l-p:0.12348346412181854
epoch£º8	 i:6 	 global-step:166	 l-p:0.10661297291517258
epoch£º8	 i:7 	 global-step:167	 l-p:0.16381584107875824
epoch£º8	 i:8 	 global-step:168	 l-p:0.12355437874794006
epoch£º8	 i:9 	 global-step:169	 l-p:0.12299035489559174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0657, 3.0668, 3.0658],
        [3.0657, 3.0668, 3.0658],
        [3.0657, 3.1474, 3.1076],
        [3.0657, 3.0772, 3.0674]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.1390194147825241 
model_pd.l_d.mean(): -20.742372512817383 
model_pd.lagr.mean(): -20.60335350036621 
model_pd.lambdas: dict_items([('pout', tensor([0.9814], device='cuda:0')), ('power', tensor([0.8906], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1542], device='cuda:0')), ('power', tensor([-23.0897], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.1390194147825241
epoch£º9	 i:1 	 global-step:181	 l-p:0.10857845842838287
epoch£º9	 i:2 	 global-step:182	 l-p:0.10005073249340057
epoch£º9	 i:3 	 global-step:183	 l-p:0.12473857402801514
epoch£º9	 i:4 	 global-step:184	 l-p:0.1058967188000679
epoch£º9	 i:5 	 global-step:185	 l-p:0.10861002653837204
epoch£º9	 i:6 	 global-step:186	 l-p:0.1165299043059349
epoch£º9	 i:7 	 global-step:187	 l-p:0.12133273482322693
epoch£º9	 i:8 	 global-step:188	 l-p:0.11740335077047348
epoch£º9	 i:9 	 global-step:189	 l-p:0.11648525297641754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0491, 3.7279, 4.1311],
        [3.0491, 3.5116, 3.6789],
        [3.0491, 3.0518, 3.0493],
        [3.0491, 3.0491, 3.0491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.12944874167442322 
model_pd.l_d.mean(): -21.52324867248535 
model_pd.lagr.mean(): -21.393800735473633 
model_pd.lambdas: dict_items([('pout', tensor([0.9787], device='cuda:0')), ('power', tensor([0.8786], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2343], device='cuda:0')), ('power', tensor([-24.2033], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.12944874167442322
epoch£º10	 i:1 	 global-step:201	 l-p:0.10352456569671631
epoch£º10	 i:2 	 global-step:202	 l-p:0.13268981873989105
epoch£º10	 i:3 	 global-step:203	 l-p:0.11908096075057983
epoch£º10	 i:4 	 global-step:204	 l-p:0.12659309804439545
epoch£º10	 i:5 	 global-step:205	 l-p:0.12700457870960236
epoch£º10	 i:6 	 global-step:206	 l-p:0.14037658274173737
epoch£º10	 i:7 	 global-step:207	 l-p:0.1228363960981369
epoch£º10	 i:8 	 global-step:208	 l-p:0.11993633955717087
epoch£º10	 i:9 	 global-step:209	 l-p:0.12579801678657532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0042, 3.2850, 3.3094],
        [3.0042, 3.7145, 4.1668],
        [3.0042, 3.3893, 3.4927],
        [3.0042, 3.0166, 3.0062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.1155148297548294 
model_pd.l_d.mean(): -21.185291290283203 
model_pd.lagr.mean(): -21.06977653503418 
model_pd.lambdas: dict_items([('pout', tensor([0.9766], device='cuda:0')), ('power', tensor([0.8666], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2081], device='cuda:0')), ('power', tensor([-24.1793], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.1155148297548294
epoch£º11	 i:1 	 global-step:221	 l-p:0.12320148199796677
epoch£º11	 i:2 	 global-step:222	 l-p:0.1260664016008377
epoch£º11	 i:3 	 global-step:223	 l-p:0.1354869306087494
epoch£º11	 i:4 	 global-step:224	 l-p:0.11924359947443008
epoch£º11	 i:5 	 global-step:225	 l-p:0.11781495809555054
epoch£º11	 i:6 	 global-step:226	 l-p:0.11910665035247803
epoch£º11	 i:7 	 global-step:227	 l-p:0.12094645202159882
epoch£º11	 i:8 	 global-step:228	 l-p:0.14855331182479858
epoch£º11	 i:9 	 global-step:229	 l-p:0.11919198930263519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0602, 3.2747, 3.2602],
        [3.0602, 3.0796, 3.0643],
        [3.0602, 3.0767, 3.0633],
        [3.0602, 3.1505, 3.1105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12487474828958511 
model_pd.l_d.mean(): -20.557138442993164 
model_pd.lagr.mean(): -20.43226432800293 
model_pd.lambdas: dict_items([('pout', tensor([0.9746], device='cuda:0')), ('power', tensor([0.8545], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1722], device='cuda:0')), ('power', tensor([-23.8266], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.12487474828958511
epoch£º12	 i:1 	 global-step:241	 l-p:0.11938492208719254
epoch£º12	 i:2 	 global-step:242	 l-p:0.11724426597356796
epoch£º12	 i:3 	 global-step:243	 l-p:0.11421409249305725
epoch£º12	 i:4 	 global-step:244	 l-p:0.12417478114366531
epoch£º12	 i:5 	 global-step:245	 l-p:0.12217307835817337
epoch£º12	 i:6 	 global-step:246	 l-p:0.11684783548116684
epoch£º12	 i:7 	 global-step:247	 l-p:0.08759550005197525
epoch£º12	 i:8 	 global-step:248	 l-p:0.1252407282590866
epoch£º12	 i:9 	 global-step:249	 l-p:0.14377807080745697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0746, 3.6996, 4.0398],
        [3.0746, 3.0746, 3.0746],
        [3.0746, 3.0763, 3.0746],
        [3.0746, 3.2880, 3.2731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.12878172099590302 
model_pd.l_d.mean(): -20.040212631225586 
model_pd.lagr.mean(): -19.91143035888672 
model_pd.lambdas: dict_items([('pout', tensor([0.9721], device='cuda:0')), ('power', tensor([0.8426], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1884], device='cuda:0')), ('power', tensor([-23.5349], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.12878172099590302
epoch£º13	 i:1 	 global-step:261	 l-p:0.12569226324558258
epoch£º13	 i:2 	 global-step:262	 l-p:0.10402591526508331
epoch£º13	 i:3 	 global-step:263	 l-p:0.11441163718700409
epoch£º13	 i:4 	 global-step:264	 l-p:0.11785484850406647
epoch£º13	 i:5 	 global-step:265	 l-p:0.09762946516275406
epoch£º13	 i:6 	 global-step:266	 l-p:0.12012701481580734
epoch£º13	 i:7 	 global-step:267	 l-p:0.12917061150074005
epoch£º13	 i:8 	 global-step:268	 l-p:0.11516530811786652
epoch£º13	 i:9 	 global-step:269	 l-p:0.11808925122022629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1275, 3.1275, 3.1275],
        [3.1275, 3.6281, 3.8286],
        [3.1275, 3.1460, 3.1313],
        [3.1275, 3.5908, 3.7552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.11246131360530853 
model_pd.l_d.mean(): -19.740673065185547 
model_pd.lagr.mean(): -19.628211975097656 
model_pd.lambdas: dict_items([('pout', tensor([0.9696], device='cuda:0')), ('power', tensor([0.8306], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2236], device='cuda:0')), ('power', tensor([-23.4738], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.11246131360530853
epoch£º14	 i:1 	 global-step:281	 l-p:0.0845368281006813
epoch£º14	 i:2 	 global-step:282	 l-p:0.10979034006595612
epoch£º14	 i:3 	 global-step:283	 l-p:0.11671283096075058
epoch£º14	 i:4 	 global-step:284	 l-p:0.11922872066497803
epoch£º14	 i:5 	 global-step:285	 l-p:0.07456750422716141
epoch£º14	 i:6 	 global-step:286	 l-p:0.11426994204521179
epoch£º14	 i:7 	 global-step:287	 l-p:0.12134121358394623
epoch£º14	 i:8 	 global-step:288	 l-p:0.12872913479804993
epoch£º14	 i:9 	 global-step:289	 l-p:0.11415458470582962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2173, 3.2349, 3.2207],
        [3.2173, 3.2194, 3.2174],
        [3.2173, 3.3158, 3.2737],
        [3.2173, 3.2233, 3.2179]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.11836843937635422 
model_pd.l_d.mean(): -20.187753677368164 
model_pd.lagr.mean(): -20.069385528564453 
model_pd.lambdas: dict_items([('pout', tensor([0.9666], device='cuda:0')), ('power', tensor([0.8185], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3370], device='cuda:0')), ('power', tensor([-24.2292], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.11836843937635422
epoch£º15	 i:1 	 global-step:301	 l-p:0.09723711758852005
epoch£º15	 i:2 	 global-step:302	 l-p:0.11936547607183456
epoch£º15	 i:3 	 global-step:303	 l-p:0.11031120270490646
epoch£º15	 i:4 	 global-step:304	 l-p:0.11964380741119385
epoch£º15	 i:5 	 global-step:305	 l-p:0.11365054547786713
epoch£º15	 i:6 	 global-step:306	 l-p:0.1419365406036377
epoch£º15	 i:7 	 global-step:307	 l-p:0.1346612572669983
epoch£º15	 i:8 	 global-step:308	 l-p:0.0851333811879158
epoch£º15	 i:9 	 global-step:309	 l-p:0.11611364036798477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0993, 3.1002, 3.0993],
        [3.0993, 3.1130, 3.1016],
        [3.0993, 3.0994, 3.0993],
        [3.0993, 3.2906, 3.2676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.10862176865339279 
model_pd.l_d.mean(): -19.343507766723633 
model_pd.lagr.mean(): -19.234886169433594 
model_pd.lambdas: dict_items([('pout', tensor([0.9640], device='cuda:0')), ('power', tensor([0.8066], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2379], device='cuda:0')), ('power', tensor([-23.6631], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.10862176865339279
epoch£º16	 i:1 	 global-step:321	 l-p:0.1267368346452713
epoch£º16	 i:2 	 global-step:322	 l-p:0.10813649743795395
epoch£º16	 i:3 	 global-step:323	 l-p:0.11919870972633362
epoch£º16	 i:4 	 global-step:324	 l-p:0.12744484841823578
epoch£º16	 i:5 	 global-step:325	 l-p:0.1277657151222229
epoch£º16	 i:6 	 global-step:326	 l-p:0.1025659516453743
epoch£º16	 i:7 	 global-step:327	 l-p:0.10291709750890732
epoch£º16	 i:8 	 global-step:328	 l-p:0.11652253568172455
epoch£º16	 i:9 	 global-step:329	 l-p:0.12366991490125656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1792, 3.6943, 3.9063],
        [3.1792, 3.3501, 3.3179],
        [3.1792, 3.1792, 3.1792],
        [3.1792, 3.6874, 3.8925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.12951399385929108 
model_pd.l_d.mean(): -19.462120056152344 
model_pd.lagr.mean(): -19.332605361938477 
model_pd.lambdas: dict_items([('pout', tensor([0.9614], device='cuda:0')), ('power', tensor([0.7946], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2912], device='cuda:0')), ('power', tensor([-24.1055], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.12951399385929108
epoch£º17	 i:1 	 global-step:341	 l-p:0.11872643977403641
epoch£º17	 i:2 	 global-step:342	 l-p:0.10791394859552383
epoch£º17	 i:3 	 global-step:343	 l-p:0.13852085173130035
epoch£º17	 i:4 	 global-step:344	 l-p:0.1159445196390152
epoch£º17	 i:5 	 global-step:345	 l-p:0.1128087192773819
epoch£º17	 i:6 	 global-step:346	 l-p:0.12334729731082916
epoch£º17	 i:7 	 global-step:347	 l-p:0.07542865723371506
epoch£º17	 i:8 	 global-step:348	 l-p:0.11360033601522446
epoch£º17	 i:9 	 global-step:349	 l-p:0.10622170567512512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1531, 3.1793, 3.1599],
        [3.1531, 3.1533, 3.1531],
        [3.1531, 3.2142, 3.1799],
        [3.1531, 3.1598, 3.1539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.12791228294372559 
model_pd.l_d.mean(): -19.11005210876465 
model_pd.lagr.mean(): -18.982139587402344 
model_pd.lambdas: dict_items([('pout', tensor([0.9587], device='cuda:0')), ('power', tensor([0.7826], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2608], device='cuda:0')), ('power', tensor([-24.0632], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.12791228294372559
epoch£º18	 i:1 	 global-step:361	 l-p:0.10264761745929718
epoch£º18	 i:2 	 global-step:362	 l-p:0.11982841044664383
epoch£º18	 i:3 	 global-step:363	 l-p:0.11917787045240402
epoch£º18	 i:4 	 global-step:364	 l-p:0.11686408519744873
epoch£º18	 i:5 	 global-step:365	 l-p:0.11745498329401016
epoch£º18	 i:6 	 global-step:366	 l-p:0.11811693012714386
epoch£º18	 i:7 	 global-step:367	 l-p:0.11065042018890381
epoch£º18	 i:8 	 global-step:368	 l-p:0.1310017704963684
epoch£º18	 i:9 	 global-step:369	 l-p:0.08894149214029312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1300, 3.2231, 3.1836],
        [3.1300, 3.1456, 3.1329],
        [3.1300, 3.1300, 3.1300],
        [3.1300, 3.2409, 3.2011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.10905154049396515 
model_pd.l_d.mean(): -19.045330047607422 
model_pd.lagr.mean(): -18.936279296875 
model_pd.lambdas: dict_items([('pout', tensor([0.9560], device='cuda:0')), ('power', tensor([0.7706], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3086], device='cuda:0')), ('power', tensor([-24.2950], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.10905154049396515
epoch£º19	 i:1 	 global-step:381	 l-p:0.1088389977812767
epoch£º19	 i:2 	 global-step:382	 l-p:0.11389824748039246
epoch£º19	 i:3 	 global-step:383	 l-p:0.10083847492933273
epoch£º19	 i:4 	 global-step:384	 l-p:0.12547965347766876
epoch£º19	 i:5 	 global-step:385	 l-p:0.1174580454826355
epoch£º19	 i:6 	 global-step:386	 l-p:0.12079509347677231
epoch£º19	 i:7 	 global-step:387	 l-p:0.11892877519130707
epoch£º19	 i:8 	 global-step:388	 l-p:0.02603347785770893
epoch£º19	 i:9 	 global-step:389	 l-p:0.11240829527378082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2693, 3.2707, 3.2693],
        [3.2693, 3.5050, 3.4955],
        [3.2693, 3.2921, 3.2746],
        [3.2693, 3.3024, 3.2789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.11841467767953873 
model_pd.l_d.mean(): -18.7164306640625 
model_pd.lagr.mean(): -18.5980167388916 
model_pd.lambdas: dict_items([('pout', tensor([0.9531], device='cuda:0')), ('power', tensor([0.7586], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3541], device='cuda:0')), ('power', tensor([-24.1893], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.11841467767953873
epoch£º20	 i:1 	 global-step:401	 l-p:2.5639095306396484
epoch£º20	 i:2 	 global-step:402	 l-p:0.11700545251369476
epoch£º20	 i:3 	 global-step:403	 l-p:0.12149392813444138
epoch£º20	 i:4 	 global-step:404	 l-p:0.10927467793226242
epoch£º20	 i:5 	 global-step:405	 l-p:0.0806373879313469
epoch£º20	 i:6 	 global-step:406	 l-p:0.10996993631124496
epoch£º20	 i:7 	 global-step:407	 l-p:0.11524733155965805
epoch£º20	 i:8 	 global-step:408	 l-p:-0.3814622163772583
epoch£º20	 i:9 	 global-step:409	 l-p:0.11294150352478027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4133, 4.2753, 4.8484],
        [3.4133, 3.4487, 3.4237],
        [3.4133, 3.4133, 3.4133],
        [3.4133, 3.4711, 3.4365]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.11633718013763428 
model_pd.l_d.mean(): -18.337034225463867 
model_pd.lagr.mean(): -18.2206974029541 
model_pd.lambdas: dict_items([('pout', tensor([0.9497], device='cuda:0')), ('power', tensor([0.7467], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3933], device='cuda:0')), ('power', tensor([-24.0200], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.11633718013763428
epoch£º21	 i:1 	 global-step:421	 l-p:0.11415815353393555
epoch£º21	 i:2 	 global-step:422	 l-p:0.1131243109703064
epoch£º21	 i:3 	 global-step:423	 l-p:-0.03437914326786995
epoch£º21	 i:4 	 global-step:424	 l-p:0.10108804702758789
epoch£º21	 i:5 	 global-step:425	 l-p:0.11515045166015625
epoch£º21	 i:6 	 global-step:426	 l-p:0.17410537600517273
epoch£º21	 i:7 	 global-step:427	 l-p:0.11992470175027847
epoch£º21	 i:8 	 global-step:428	 l-p:0.09286721795797348
epoch£º21	 i:9 	 global-step:429	 l-p:0.10049765557050705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3568, 3.4141, 3.3801],
        [3.3568, 3.5266, 3.4903],
        [3.3568, 3.3581, 3.3569],
        [3.3568, 3.3746, 3.3603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.11405397206544876 
model_pd.l_d.mean(): -18.11585807800293 
model_pd.lagr.mean(): -18.00180435180664 
model_pd.lambdas: dict_items([('pout', tensor([0.9462], device='cuda:0')), ('power', tensor([0.7347], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3888], device='cuda:0')), ('power', tensor([-24.1166], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.11405397206544876
epoch£º22	 i:1 	 global-step:441	 l-p:0.0792955532670021
epoch£º22	 i:2 	 global-step:442	 l-p:0.0976550355553627
epoch£º22	 i:3 	 global-step:443	 l-p:0.10613320767879486
epoch£º22	 i:4 	 global-step:444	 l-p:0.09436481446027756
epoch£º22	 i:5 	 global-step:445	 l-p:0.11337961256504059
epoch£º22	 i:6 	 global-step:446	 l-p:0.1361893266439438
epoch£º22	 i:7 	 global-step:447	 l-p:0.11344051361083984
epoch£º22	 i:8 	 global-step:448	 l-p:0.11706268787384033
epoch£º22	 i:9 	 global-step:449	 l-p:0.21850769221782684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4268, 4.2885, 4.8599],
        [3.4268, 3.7520, 3.7852],
        [3.4268, 3.4271, 3.4268],
        [3.4268, 3.9805, 4.2041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.12054135650396347 
model_pd.l_d.mean(): -17.641958236694336 
model_pd.lagr.mean(): -17.52141761779785 
model_pd.lambdas: dict_items([('pout', tensor([0.9425], device='cuda:0')), ('power', tensor([0.7228], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3736], device='cuda:0')), ('power', tensor([-23.8807], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.12054135650396347
epoch£º23	 i:1 	 global-step:461	 l-p:0.09711633622646332
epoch£º23	 i:2 	 global-step:462	 l-p:0.1100185364484787
epoch£º23	 i:3 	 global-step:463	 l-p:0.10739045590162277
epoch£º23	 i:4 	 global-step:464	 l-p:0.09650697559118271
epoch£º23	 i:5 	 global-step:465	 l-p:0.11375342309474945
epoch£º23	 i:6 	 global-step:466	 l-p:0.11880204826593399
epoch£º23	 i:7 	 global-step:467	 l-p:0.11827713996171951
epoch£º23	 i:8 	 global-step:468	 l-p:0.024928919970989227
epoch£º23	 i:9 	 global-step:469	 l-p:0.10613755881786346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3270, 3.5887, 3.5908],
        [3.3270, 3.4909, 3.4547],
        [3.3270, 3.3724, 3.3431],
        [3.3270, 3.3270, 3.3270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.11151254177093506 
model_pd.l_d.mean(): -17.72956657409668 
model_pd.lagr.mean(): -17.618053436279297 
model_pd.lambdas: dict_items([('pout', tensor([0.9388], device='cuda:0')), ('power', tensor([0.7109], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4260], device='cuda:0')), ('power', tensor([-24.3364], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.11151254177093506
epoch£º24	 i:1 	 global-step:481	 l-p:0.08014731854200363
epoch£º24	 i:2 	 global-step:482	 l-p:0.10336179286241531
epoch£º24	 i:3 	 global-step:483	 l-p:0.17105968296527863
epoch£º24	 i:4 	 global-step:484	 l-p:0.11816392838954926
epoch£º24	 i:5 	 global-step:485	 l-p:0.20731395483016968
epoch£º24	 i:6 	 global-step:486	 l-p:0.11000577360391617
epoch£º24	 i:7 	 global-step:487	 l-p:0.1071503683924675
epoch£º24	 i:8 	 global-step:488	 l-p:0.10942927747964859
epoch£º24	 i:9 	 global-step:489	 l-p:0.1096469983458519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5020, 3.5020, 3.5020],
        [3.5020, 3.5020, 3.5020],
        [3.5020, 3.5020, 3.5020],
        [3.5020, 4.2172, 4.6014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.07888105511665344 
model_pd.l_d.mean(): -16.52437973022461 
model_pd.lagr.mean(): -16.445499420166016 
model_pd.lambdas: dict_items([('pout', tensor([0.9352], device='cuda:0')), ('power', tensor([0.6990], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2993], device='cuda:0')), ('power', tensor([-23.2006], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.07888105511665344
epoch£º25	 i:1 	 global-step:501	 l-p:0.1362917423248291
epoch£º25	 i:2 	 global-step:502	 l-p:0.10872229933738708
epoch£º25	 i:3 	 global-step:503	 l-p:0.11276202648878098
epoch£º25	 i:4 	 global-step:504	 l-p:0.10514785349369049
epoch£º25	 i:5 	 global-step:505	 l-p:0.10788185149431229
epoch£º25	 i:6 	 global-step:506	 l-p:0.05071548745036125
epoch£º25	 i:7 	 global-step:507	 l-p:0.11532621085643768
epoch£º25	 i:8 	 global-step:508	 l-p:0.11864553391933441
epoch£º25	 i:9 	 global-step:509	 l-p:0.11464143544435501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3343, 3.4437, 3.4014],
        [3.3343, 3.6874, 3.7479],
        [3.3343, 3.3678, 3.3442],
        [3.3343, 3.6376, 3.6642]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.11440203338861465 
model_pd.l_d.mean(): -16.995059967041016 
model_pd.lagr.mean(): -16.880657196044922 
model_pd.lambdas: dict_items([('pout', tensor([0.9313], device='cuda:0')), ('power', tensor([0.6871], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3827], device='cuda:0')), ('power', tensor([-24.1743], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.11440203338861465
epoch£º26	 i:1 	 global-step:521	 l-p:0.09970428049564362
epoch£º26	 i:2 	 global-step:522	 l-p:0.12654314935207367
epoch£º26	 i:3 	 global-step:523	 l-p:0.11820760369300842
epoch£º26	 i:4 	 global-step:524	 l-p:0.05314461886882782
epoch£º26	 i:5 	 global-step:525	 l-p:0.0913107693195343
epoch£º26	 i:6 	 global-step:526	 l-p:0.11226371675729752
epoch£º26	 i:7 	 global-step:527	 l-p:0.12483783811330795
epoch£º26	 i:8 	 global-step:528	 l-p:0.10244601219892502
epoch£º26	 i:9 	 global-step:529	 l-p:0.11864068359136581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2697, 3.3337, 3.2985],
        [3.2697, 3.4556, 3.4280],
        [3.2697, 3.2697, 3.2697],
        [3.2697, 3.2711, 3.2698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.11771263927221298 
model_pd.l_d.mean(): -15.981199264526367 
model_pd.lagr.mean(): -15.863486289978027 
model_pd.lambdas: dict_items([('pout', tensor([0.9283], device='cuda:0')), ('power', tensor([0.6751], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2531], device='cuda:0')), ('power', tensor([-23.2826], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.11771263927221298
epoch£º27	 i:1 	 global-step:541	 l-p:0.09898295998573303
epoch£º27	 i:2 	 global-step:542	 l-p:0.11461285501718521
epoch£º27	 i:3 	 global-step:543	 l-p:0.11176968365907669
epoch£º27	 i:4 	 global-step:544	 l-p:0.09004656225442886
epoch£º27	 i:5 	 global-step:545	 l-p:0.16509269177913666
epoch£º27	 i:6 	 global-step:546	 l-p:0.11344938725233078
epoch£º27	 i:7 	 global-step:547	 l-p:0.1118130087852478
epoch£º27	 i:8 	 global-step:548	 l-p:0.11224928498268127
epoch£º27	 i:9 	 global-step:549	 l-p:0.21383608877658844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4326, 3.5517, 3.5079],
        [3.4326, 3.7978, 3.8599],
        [3.4326, 3.4609, 3.4400],
        [3.4326, 4.2517, 4.7718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.09758323431015015 
model_pd.l_d.mean(): -16.17576789855957 
model_pd.lagr.mean(): -16.078184127807617 
model_pd.lambdas: dict_items([('pout', tensor([0.9247], device='cuda:0')), ('power', tensor([0.6632], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3652], device='cuda:0')), ('power', tensor([-23.8390], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.09758323431015015
epoch£º28	 i:1 	 global-step:561	 l-p:0.1098504364490509
epoch£º28	 i:2 	 global-step:562	 l-p:0.1102261170744896
epoch£º28	 i:3 	 global-step:563	 l-p:0.11516902595758438
epoch£º28	 i:4 	 global-step:564	 l-p:0.13078932464122772
epoch£º28	 i:5 	 global-step:565	 l-p:-0.0041734217666089535
epoch£º28	 i:6 	 global-step:566	 l-p:0.10229159891605377
epoch£º28	 i:7 	 global-step:567	 l-p:0.13366073369979858
epoch£º28	 i:8 	 global-step:568	 l-p:0.10897541046142578
epoch£º28	 i:9 	 global-step:569	 l-p:0.10680998861789703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5721, 3.5721, 3.5721],
        [3.5721, 3.6969, 3.6507],
        [3.5721, 3.9768, 4.0569],
        [3.5721, 3.5747, 3.5723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.1047268807888031 
model_pd.l_d.mean(): -15.830520629882812 
model_pd.lagr.mean(): -15.725793838500977 
model_pd.lambdas: dict_items([('pout', tensor([0.9207], device='cuda:0')), ('power', tensor([0.6513], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4222], device='cuda:0')), ('power', tensor([-23.6660], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.1047268807888031
epoch£º29	 i:1 	 global-step:581	 l-p:0.11371815204620361
epoch£º29	 i:2 	 global-step:582	 l-p:0.10730293393135071
epoch£º29	 i:3 	 global-step:583	 l-p:0.10743162035942078
epoch£º29	 i:4 	 global-step:584	 l-p:0.1501687914133072
epoch£º29	 i:5 	 global-step:585	 l-p:0.1055268868803978
epoch£º29	 i:6 	 global-step:586	 l-p:0.09514114260673523
epoch£º29	 i:7 	 global-step:587	 l-p:0.12631221115589142
epoch£º29	 i:8 	 global-step:588	 l-p:0.09555219113826752
epoch£º29	 i:9 	 global-step:589	 l-p:-0.07905568927526474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4963, 3.4973, 3.4963],
        [3.4963, 4.3261, 4.8478],
        [3.4963, 3.6273, 3.5829],
        [3.4963, 4.2478, 4.6768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.12449797242879868 
model_pd.l_d.mean(): -15.29745864868164 
model_pd.lagr.mean(): -15.17296028137207 
model_pd.lambdas: dict_items([('pout', tensor([0.9168], device='cuda:0')), ('power', tensor([0.6394], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3253], device='cuda:0')), ('power', tensor([-23.4143], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.12449797242879868
epoch£º30	 i:1 	 global-step:601	 l-p:0.18992766737937927
epoch£º30	 i:2 	 global-step:602	 l-p:0.10541589558124542
epoch£º30	 i:3 	 global-step:603	 l-p:0.10611741989850998
epoch£º30	 i:4 	 global-step:604	 l-p:-0.17314639687538147
epoch£º30	 i:5 	 global-step:605	 l-p:0.10512224584817886
epoch£º30	 i:6 	 global-step:606	 l-p:0.11257833242416382
epoch£º30	 i:7 	 global-step:607	 l-p:0.09762448072433472
epoch£º30	 i:8 	 global-step:608	 l-p:0.10497718304395676
epoch£º30	 i:9 	 global-step:609	 l-p:0.19919152557849884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7504, 3.7504, 3.7504],
        [3.7504, 3.7559, 3.7509],
        [3.7504, 4.7140, 5.3513],
        [3.7504, 3.7878, 3.7612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.10329993814229965 
model_pd.l_d.mean(): -14.789328575134277 
model_pd.lagr.mean(): -14.686028480529785 
model_pd.lambdas: dict_items([('pout', tensor([0.9121], device='cuda:0')), ('power', tensor([0.6276], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3785], device='cuda:0')), ('power', tensor([-22.9720], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.10329993814229965
epoch£º31	 i:1 	 global-step:621	 l-p:0.14126227796077728
epoch£º31	 i:2 	 global-step:622	 l-p:0.14179180562496185
epoch£º31	 i:3 	 global-step:623	 l-p:0.09237039089202881
epoch£º31	 i:4 	 global-step:624	 l-p:0.10879121720790863
epoch£º31	 i:5 	 global-step:625	 l-p:0.10034584999084473
epoch£º31	 i:6 	 global-step:626	 l-p:0.10261501371860504
epoch£º31	 i:7 	 global-step:627	 l-p:0.11480114609003067
epoch£º31	 i:8 	 global-step:628	 l-p:0.10195405781269073
epoch£º31	 i:9 	 global-step:629	 l-p:0.10482622683048248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6677, 4.6625, 5.3571],
        [3.6677, 4.3073, 4.5890],
        [3.6677, 3.7049, 3.6785],
        [3.6677, 3.6686, 3.6677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.10176364332437515 
model_pd.l_d.mean(): -15.059975624084473 
model_pd.lagr.mean(): -14.958211898803711 
model_pd.lambdas: dict_items([('pout', tensor([0.9068], device='cuda:0')), ('power', tensor([0.6158], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4814], device='cuda:0')), ('power', tensor([-23.7006], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.10176364332437515
epoch£º32	 i:1 	 global-step:641	 l-p:0.14365172386169434
epoch£º32	 i:2 	 global-step:642	 l-p:0.11192206293344498
epoch£º32	 i:3 	 global-step:643	 l-p:0.1518394500017166
epoch£º32	 i:4 	 global-step:644	 l-p:0.10644969344139099
epoch£º32	 i:5 	 global-step:645	 l-p:0.0892743244767189
epoch£º32	 i:6 	 global-step:646	 l-p:0.09256923198699951
epoch£º32	 i:7 	 global-step:647	 l-p:0.114417664706707
epoch£º32	 i:8 	 global-step:648	 l-p:0.11783085763454437
epoch£º32	 i:9 	 global-step:649	 l-p:0.11220073699951172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3664, 3.3665, 3.3664],
        [3.3664, 3.5082, 3.4680],
        [3.3664, 3.3668, 3.3664],
        [3.3664, 3.5083, 3.4682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.11624568700790405 
model_pd.l_d.mean(): -14.904220581054688 
model_pd.lagr.mean(): -14.787975311279297 
model_pd.lambdas: dict_items([('pout', tensor([0.9028], device='cuda:0')), ('power', tensor([0.6039], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3748], device='cuda:0')), ('power', tensor([-24.0712], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.11624568700790405
epoch£º33	 i:1 	 global-step:661	 l-p:0.1097121387720108
epoch£º33	 i:2 	 global-step:662	 l-p:0.09133957326412201
epoch£º33	 i:3 	 global-step:663	 l-p:0.12894557416439056
epoch£º33	 i:4 	 global-step:664	 l-p:0.11937236785888672
epoch£º33	 i:5 	 global-step:665	 l-p:0.08399937301874161
epoch£º33	 i:6 	 global-step:666	 l-p:0.11510340124368668
epoch£º33	 i:7 	 global-step:667	 l-p:0.26146307587623596
epoch£º33	 i:8 	 global-step:668	 l-p:0.11049807071685791
epoch£º33	 i:9 	 global-step:669	 l-p:0.10093707591295242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4204, 3.7842, 3.8477],
        [3.4204, 3.4205, 3.4204],
        [3.4204, 3.5877, 3.5508],
        [3.4204, 3.4529, 3.4297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.11670895665884018 
model_pd.l_d.mean(): -14.62511920928955 
model_pd.lagr.mean(): -14.508410453796387 
model_pd.lambdas: dict_items([('pout', tensor([0.8995], device='cuda:0')), ('power', tensor([0.5920], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3996], device='cuda:0')), ('power', tensor([-24.0501], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.11670895665884018
epoch£º34	 i:1 	 global-step:681	 l-p:0.1455279141664505
epoch£º34	 i:2 	 global-step:682	 l-p:0.3461485207080841
epoch£º34	 i:3 	 global-step:683	 l-p:0.10933291912078857
epoch£º34	 i:4 	 global-step:684	 l-p:0.10811662673950195
epoch£º34	 i:5 	 global-step:685	 l-p:0.1037592962384224
epoch£º34	 i:6 	 global-step:686	 l-p:0.10574308037757874
epoch£º34	 i:7 	 global-step:687	 l-p:0.09917621314525604
epoch£º34	 i:8 	 global-step:688	 l-p:0.10285943001508713
epoch£º34	 i:9 	 global-step:689	 l-p:0.17873241007328033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7664, 4.0781, 4.0842],
        [3.7664, 3.7664, 3.7664],
        [3.7664, 3.8553, 3.8102],
        [3.7664, 4.7155, 5.3328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.11294828355312347 
model_pd.l_d.mean(): -14.159513473510742 
model_pd.lagr.mean(): -14.046565055847168 
model_pd.lambdas: dict_items([('pout', tensor([0.8949], device='cuda:0')), ('power', tensor([0.5801], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4836], device='cuda:0')), ('power', tensor([-23.6128], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.11294828355312347
epoch£º35	 i:1 	 global-step:701	 l-p:0.11345336586236954
epoch£º35	 i:2 	 global-step:702	 l-p:0.1142066940665245
epoch£º35	 i:3 	 global-step:703	 l-p:0.1030840054154396
epoch£º35	 i:4 	 global-step:704	 l-p:0.09883826225996017
epoch£º35	 i:5 	 global-step:705	 l-p:0.0999334305524826
epoch£º35	 i:6 	 global-step:706	 l-p:0.10150212049484253
epoch£º35	 i:7 	 global-step:707	 l-p:0.10682283341884613
epoch£º35	 i:8 	 global-step:708	 l-p:-0.013841390609741211
epoch£º35	 i:9 	 global-step:709	 l-p:0.10011067986488342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5898, 3.5899, 3.5898],
        [3.5898, 3.5898, 3.5898],
        [3.5898, 4.0056, 4.0941],
        [3.5898, 3.6188, 3.5973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.12144093960523605 
model_pd.l_d.mean(): -13.689319610595703 
model_pd.lagr.mean(): -13.567878723144531 
model_pd.lambdas: dict_items([('pout', tensor([0.8901], device='cuda:0')), ('power', tensor([0.5683], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3984], device='cuda:0')), ('power', tensor([-23.4142], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.12144093960523605
epoch£º36	 i:1 	 global-step:721	 l-p:0.10273875296115875
epoch£º36	 i:2 	 global-step:722	 l-p:0.10669147223234177
epoch£º36	 i:3 	 global-step:723	 l-p:0.10419260710477829
epoch£º36	 i:4 	 global-step:724	 l-p:0.11374103277921677
epoch£º36	 i:5 	 global-step:725	 l-p:0.1075340062379837
epoch£º36	 i:6 	 global-step:726	 l-p:0.1112830713391304
epoch£º36	 i:7 	 global-step:727	 l-p:0.1095290333032608
epoch£º36	 i:8 	 global-step:728	 l-p:0.10773135721683502
epoch£º36	 i:9 	 global-step:729	 l-p:0.08146041631698608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5527, 3.5565, 3.5530],
        [3.5527, 4.4056, 4.9468],
        [3.5527, 3.8059, 3.7943],
        [3.5527, 3.8716, 3.8956]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.08523782342672348 
model_pd.l_d.mean(): -13.265089988708496 
model_pd.lagr.mean(): -13.179852485656738 
model_pd.lambdas: dict_items([('pout', tensor([0.8858], device='cuda:0')), ('power', tensor([0.5565], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3419], device='cuda:0')), ('power', tensor([-23.2440], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.08523782342672348
epoch£º37	 i:1 	 global-step:741	 l-p:0.1013169065117836
epoch£º37	 i:2 	 global-step:742	 l-p:0.10624758154153824
epoch£º37	 i:3 	 global-step:743	 l-p:0.1066541075706482
epoch£º37	 i:4 	 global-step:744	 l-p:0.1093815416097641
epoch£º37	 i:5 	 global-step:745	 l-p:0.11422856152057648
epoch£º37	 i:6 	 global-step:746	 l-p:0.16430625319480896
epoch£º37	 i:7 	 global-step:747	 l-p:0.06404183059930801
epoch£º37	 i:8 	 global-step:748	 l-p:0.10315615683794022
epoch£º37	 i:9 	 global-step:749	 l-p:0.1061934381723404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6908, 3.7633, 3.7229],
        [3.6908, 3.7337, 3.7045],
        [3.6908, 3.7073, 3.6937],
        [3.6908, 3.7317, 3.7035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.3755072355270386 
model_pd.l_d.mean(): -13.313830375671387 
model_pd.lagr.mean(): -12.938323020935059 
model_pd.lambdas: dict_items([('pout', tensor([0.8813], device='cuda:0')), ('power', tensor([0.5446], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4575], device='cuda:0')), ('power', tensor([-23.6542], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.3755072355270386
epoch£º38	 i:1 	 global-step:761	 l-p:0.09677565097808838
epoch£º38	 i:2 	 global-step:762	 l-p:0.10117631405591965
epoch£º38	 i:3 	 global-step:763	 l-p:0.11743983626365662
epoch£º38	 i:4 	 global-step:764	 l-p:0.10818925499916077
epoch£º38	 i:5 	 global-step:765	 l-p:0.10356573015451431
epoch£º38	 i:6 	 global-step:766	 l-p:0.1048160120844841
epoch£º38	 i:7 	 global-step:767	 l-p:0.10260287672281265
epoch£º38	 i:8 	 global-step:768	 l-p:0.14227716624736786
epoch£º38	 i:9 	 global-step:769	 l-p:0.10056322067975998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8005, 3.8005, 3.8005],
        [3.8005, 4.5728, 4.9780],
        [3.8005, 3.8005, 3.8005],
        [3.8005, 3.8019, 3.8006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.10239449888467789 
model_pd.l_d.mean(): -13.265384674072266 
model_pd.lagr.mean(): -13.16299057006836 
model_pd.lambdas: dict_items([('pout', tensor([0.8761], device='cuda:0')), ('power', tensor([0.5328], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5775], device='cuda:0')), ('power', tensor([-23.8927], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.10239449888467789
epoch£º39	 i:1 	 global-step:781	 l-p:0.1087847650051117
epoch£º39	 i:2 	 global-step:782	 l-p:0.09954984486103058
epoch£º39	 i:3 	 global-step:783	 l-p:0.10489784926176071
epoch£º39	 i:4 	 global-step:784	 l-p:0.10697965323925018
epoch£º39	 i:5 	 global-step:785	 l-p:0.10423608869314194
epoch£º39	 i:6 	 global-step:786	 l-p:0.1384606510400772
epoch£º39	 i:7 	 global-step:787	 l-p:0.04508737474679947
epoch£º39	 i:8 	 global-step:788	 l-p:0.1120925098657608
epoch£º39	 i:9 	 global-step:789	 l-p:-0.03726300224661827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6831, 3.6832, 3.6831],
        [3.6831, 3.6950, 3.6848],
        [3.6831, 3.7564, 3.7159],
        [3.6831, 3.6878, 3.6835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.10698642581701279 
model_pd.l_d.mean(): -12.950886726379395 
model_pd.lagr.mean(): -12.843900680541992 
model_pd.lambdas: dict_items([('pout', tensor([0.8713], device='cuda:0')), ('power', tensor([0.5210], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5151], device='cuda:0')), ('power', tensor([-23.9405], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.10698642581701279
epoch£º40	 i:1 	 global-step:801	 l-p:0.10773130506277084
epoch£º40	 i:2 	 global-step:802	 l-p:0.11806600540876389
epoch£º40	 i:3 	 global-step:803	 l-p:0.10394659638404846
epoch£º40	 i:4 	 global-step:804	 l-p:0.10820846259593964
epoch£º40	 i:5 	 global-step:805	 l-p:0.1861332207918167
epoch£º40	 i:6 	 global-step:806	 l-p:0.14368897676467896
epoch£º40	 i:7 	 global-step:807	 l-p:0.10175628215074539
epoch£º40	 i:8 	 global-step:808	 l-p:0.09903210401535034
epoch£º40	 i:9 	 global-step:809	 l-p:0.09739604592323303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9005, 3.9023, 3.9006],
        [3.9005, 3.9007, 3.9005],
        [3.9005, 4.0324, 3.9811],
        [3.9005, 3.9825, 3.9379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.10476696491241455 
model_pd.l_d.mean(): -12.617080688476562 
model_pd.lagr.mean(): -12.512313842773438 
model_pd.lambdas: dict_items([('pout', tensor([0.8662], device='cuda:0')), ('power', tensor([0.5092], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5878], device='cuda:0')), ('power', tensor([-23.7211], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.10476696491241455
epoch£º41	 i:1 	 global-step:821	 l-p:0.11957871913909912
epoch£º41	 i:2 	 global-step:822	 l-p:0.09736724942922592
epoch£º41	 i:3 	 global-step:823	 l-p:0.10675838589668274
epoch£º41	 i:4 	 global-step:824	 l-p:0.10630295425653458
epoch£º41	 i:5 	 global-step:825	 l-p:0.09927298128604889
epoch£º41	 i:6 	 global-step:826	 l-p:0.11997660994529724
epoch£º41	 i:7 	 global-step:827	 l-p:0.10750928521156311
epoch£º41	 i:8 	 global-step:828	 l-p:0.09384704381227493
epoch£º41	 i:9 	 global-step:829	 l-p:0.1018090546131134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8071, 4.0073, 3.9667],
        [3.8071, 3.8071, 3.8071],
        [3.8071, 4.8066, 5.4806],
        [3.8071, 3.8181, 3.8086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.11137370765209198 
model_pd.l_d.mean(): -12.007976531982422 
model_pd.lagr.mean(): -11.896602630615234 
model_pd.lambdas: dict_items([('pout', tensor([0.8609], device='cuda:0')), ('power', tensor([0.4975], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4792], device='cuda:0')), ('power', tensor([-23.2526], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.11137370765209198
epoch£º42	 i:1 	 global-step:841	 l-p:0.10755842924118042
epoch£º42	 i:2 	 global-step:842	 l-p:0.1699841469526291
epoch£º42	 i:3 	 global-step:843	 l-p:0.11215660721063614
epoch£º42	 i:4 	 global-step:844	 l-p:0.11101216822862625
epoch£º42	 i:5 	 global-step:845	 l-p:0.10648533701896667
epoch£º42	 i:6 	 global-step:846	 l-p:0.15465550124645233
epoch£º42	 i:7 	 global-step:847	 l-p:0.09375429153442383
epoch£º42	 i:8 	 global-step:848	 l-p:0.10408025234937668
epoch£º42	 i:9 	 global-step:849	 l-p:0.10077779740095139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[3.8016, 4.6980, 5.2469],
        [3.8016, 4.2010, 4.2611],
        [3.8016, 4.0203, 3.9852],
        [3.8016, 4.6529, 5.1486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.1135762557387352 
model_pd.l_d.mean(): -11.907692909240723 
model_pd.lagr.mean(): -11.794116973876953 
model_pd.lambdas: dict_items([('pout', tensor([0.8559], device='cuda:0')), ('power', tensor([0.4857], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5001], device='cuda:0')), ('power', tensor([-23.5780], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.1135762557387352
epoch£º43	 i:1 	 global-step:861	 l-p:0.10184262692928314
epoch£º43	 i:2 	 global-step:862	 l-p:0.10567586869001389
epoch£º43	 i:3 	 global-step:863	 l-p:0.10688706487417221
epoch£º43	 i:4 	 global-step:864	 l-p:0.09881486743688583
epoch£º43	 i:5 	 global-step:865	 l-p:0.09810973703861237
epoch£º43	 i:6 	 global-step:866	 l-p:0.1930340975522995
epoch£º43	 i:7 	 global-step:867	 l-p:0.11156217008829117
epoch£º43	 i:8 	 global-step:868	 l-p:0.1045469418168068
epoch£º43	 i:9 	 global-step:869	 l-p:0.15540711581707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7752, 3.8628, 3.8182],
        [3.7752, 3.7800, 3.7756],
        [3.7752, 3.7768, 3.7753],
        [3.7752, 3.7752, 3.7752]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.11386429518461227 
model_pd.l_d.mean(): -11.766541481018066 
model_pd.lagr.mean(): -11.652677536010742 
model_pd.lambdas: dict_items([('pout', tensor([0.8508], device='cuda:0')), ('power', tensor([0.4739], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5449], device='cuda:0')), ('power', tensor([-23.7910], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.11386429518461227
epoch£º44	 i:1 	 global-step:881	 l-p:0.1006041094660759
epoch£º44	 i:2 	 global-step:882	 l-p:0.09906474500894547
epoch£º44	 i:3 	 global-step:883	 l-p:0.1088157594203949
epoch£º44	 i:4 	 global-step:884	 l-p:0.10568968206644058
epoch£º44	 i:5 	 global-step:885	 l-p:0.10150641947984695
epoch£º44	 i:6 	 global-step:886	 l-p:0.1641106754541397
epoch£º44	 i:7 	 global-step:887	 l-p:0.0980621799826622
epoch£º44	 i:8 	 global-step:888	 l-p:0.10123038291931152
epoch£º44	 i:9 	 global-step:889	 l-p:0.10411981493234634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8710, 3.8710, 3.8710],
        [3.8710, 3.8717, 3.8710],
        [3.8710, 4.0410, 3.9929],
        [3.8710, 3.8710, 3.8710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.10154499858617783 
model_pd.l_d.mean(): -11.579343795776367 
model_pd.lagr.mean(): -11.477798461914062 
model_pd.lambdas: dict_items([('pout', tensor([0.8455], device='cuda:0')), ('power', tensor([0.4621], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6078], device='cuda:0')), ('power', tensor([-23.8828], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.10154499858617783
epoch£º45	 i:1 	 global-step:901	 l-p:0.09954198449850082
epoch£º45	 i:2 	 global-step:902	 l-p:0.12196998298168182
epoch£º45	 i:3 	 global-step:903	 l-p:0.09460771083831787
epoch£º45	 i:4 	 global-step:904	 l-p:0.10054002702236176
epoch£º45	 i:5 	 global-step:905	 l-p:0.13007646799087524
epoch£º45	 i:6 	 global-step:906	 l-p:0.10952544957399368
epoch£º45	 i:7 	 global-step:907	 l-p:0.10260562598705292
epoch£º45	 i:8 	 global-step:908	 l-p:0.10568594932556152
epoch£º45	 i:9 	 global-step:909	 l-p:0.10210610181093216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8995, 3.9025, 3.8996],
        [3.8995, 3.9187, 3.9030],
        [3.8995, 3.8994, 3.8995],
        [3.8995, 3.9182, 3.9029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.09936727583408356 
model_pd.l_d.mean(): -11.03105354309082 
model_pd.lagr.mean(): -10.931686401367188 
model_pd.lambdas: dict_items([('pout', tensor([0.8402], device='cuda:0')), ('power', tensor([0.4504], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5291], device='cuda:0')), ('power', tensor([-23.4440], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.09936727583408356
epoch£º46	 i:1 	 global-step:921	 l-p:0.09844920039176941
epoch£º46	 i:2 	 global-step:922	 l-p:0.09880328923463821
epoch£º46	 i:3 	 global-step:923	 l-p:0.09949970245361328
epoch£º46	 i:4 	 global-step:924	 l-p:0.1410495638847351
epoch£º46	 i:5 	 global-step:925	 l-p:0.10467788577079773
epoch£º46	 i:6 	 global-step:926	 l-p:0.10059768706560135
epoch£º46	 i:7 	 global-step:927	 l-p:0.09702813625335693
epoch£º46	 i:8 	 global-step:928	 l-p:0.1101643368601799
epoch£º46	 i:9 	 global-step:929	 l-p:0.10620788484811783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9026, 3.9441, 3.9151],
        [3.9026, 3.9820, 3.9383],
        [3.9026, 3.9039, 3.9027],
        [3.9026, 4.1598, 4.1346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.09527038037776947 
model_pd.l_d.mean(): -10.546723365783691 
model_pd.lagr.mean(): -10.45145320892334 
model_pd.lambdas: dict_items([('pout', tensor([0.8348], device='cuda:0')), ('power', tensor([0.4387], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5058], device='cuda:0')), ('power', tensor([-23.0197], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.09527038037776947
epoch£º47	 i:1 	 global-step:941	 l-p:0.11045049130916595
epoch£º47	 i:2 	 global-step:942	 l-p:0.10022065043449402
epoch£º47	 i:3 	 global-step:943	 l-p:0.09896555542945862
epoch£º47	 i:4 	 global-step:944	 l-p:0.0997386947274208
epoch£º47	 i:5 	 global-step:945	 l-p:0.11492188274860382
epoch£º47	 i:6 	 global-step:946	 l-p:0.10117962956428528
epoch£º47	 i:7 	 global-step:947	 l-p:0.11734768003225327
epoch£º47	 i:8 	 global-step:948	 l-p:0.10348708182573318
epoch£º47	 i:9 	 global-step:949	 l-p:0.12620018422603607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8763, 4.1323, 4.1079],
        [3.8763, 4.0123, 3.9618],
        [3.8763, 3.8763, 3.8763],
        [3.8763, 4.0499, 4.0024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.10060641914606094 
model_pd.l_d.mean(): -10.719915390014648 
model_pd.lagr.mean(): -10.619309425354004 
model_pd.lambdas: dict_items([('pout', tensor([0.8293], device='cuda:0')), ('power', tensor([0.4269], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6066], device='cuda:0')), ('power', tensor([-23.8673], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.10060641914606094
epoch£º48	 i:1 	 global-step:961	 l-p:0.12103517353534698
epoch£º48	 i:2 	 global-step:962	 l-p:0.09813744574785233
epoch£º48	 i:3 	 global-step:963	 l-p:0.10134745389223099
epoch£º48	 i:4 	 global-step:964	 l-p:0.11234281212091446
epoch£º48	 i:5 	 global-step:965	 l-p:0.09812287986278534
epoch£º48	 i:6 	 global-step:966	 l-p:0.09947577863931656
epoch£º48	 i:7 	 global-step:967	 l-p:0.09750707447528839
epoch£º48	 i:8 	 global-step:968	 l-p:0.10418296605348587
epoch£º48	 i:9 	 global-step:969	 l-p:0.10385555028915405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0229, 4.0491, 4.0287],
        [4.0229, 4.0239, 4.0229],
        [4.0229, 4.1442, 4.0919],
        [4.0229, 4.0276, 4.0233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.0983642116189003 
model_pd.l_d.mean(): -10.438732147216797 
model_pd.lagr.mean(): -10.340368270874023 
model_pd.lambdas: dict_items([('pout', tensor([0.8236], device='cuda:0')), ('power', tensor([0.4151], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6615], device='cuda:0')), ('power', tensor([-23.7634], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.0983642116189003
epoch£º49	 i:1 	 global-step:981	 l-p:0.1026512086391449
epoch£º49	 i:2 	 global-step:982	 l-p:0.09735044091939926
epoch£º49	 i:3 	 global-step:983	 l-p:0.11240697652101517
epoch£º49	 i:4 	 global-step:984	 l-p:0.10290191322565079
epoch£º49	 i:5 	 global-step:985	 l-p:0.10355432331562042
epoch£º49	 i:6 	 global-step:986	 l-p:0.12166530638933182
epoch£º49	 i:7 	 global-step:987	 l-p:0.10191325843334198
epoch£º49	 i:8 	 global-step:988	 l-p:0.09299036860466003
epoch£º49	 i:9 	 global-step:989	 l-p:0.09835205227136612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9356, 4.5791, 4.8342],
        [3.9356, 3.9357, 3.9356],
        [3.9356, 4.1797, 4.1486],
        [3.9356, 3.9370, 3.9357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.10806608945131302 
model_pd.l_d.mean(): -9.988486289978027 
model_pd.lagr.mean(): -9.880419731140137 
model_pd.lambdas: dict_items([('pout', tensor([0.8180], device='cuda:0')), ('power', tensor([0.4034], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5590], device='cuda:0')), ('power', tensor([-23.5555], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.10806608945131302
epoch£º50	 i:1 	 global-step:1001	 l-p:0.11315688490867615
epoch£º50	 i:2 	 global-step:1002	 l-p:0.11795821040868759
epoch£º50	 i:3 	 global-step:1003	 l-p:0.10038787871599197
epoch£º50	 i:4 	 global-step:1004	 l-p:0.09091968089342117
epoch£º50	 i:5 	 global-step:1005	 l-p:0.0984291210770607
epoch£º50	 i:6 	 global-step:1006	 l-p:0.09970472007989883
epoch£º50	 i:7 	 global-step:1007	 l-p:0.09961231797933578
epoch£º50	 i:8 	 global-step:1008	 l-p:0.10632380098104477
epoch£º50	 i:9 	 global-step:1009	 l-p:0.09459438174962997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0125, 4.0125, 4.0125],
        [4.0125, 4.4747, 4.5644],
        [4.0125, 5.0321, 5.6925],
        [4.0125, 4.0125, 4.0125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.09990516304969788 
model_pd.l_d.mean(): -9.467798233032227 
model_pd.lagr.mean(): -9.36789321899414 
model_pd.lambdas: dict_items([('pout', tensor([0.8123], device='cuda:0')), ('power', tensor([0.3917], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5275], device='cuda:0')), ('power', tensor([-23.0060], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.09990516304969788
epoch£º51	 i:1 	 global-step:1021	 l-p:0.11972356587648392
epoch£º51	 i:2 	 global-step:1022	 l-p:0.10104940086603165
epoch£º51	 i:3 	 global-step:1023	 l-p:0.09928617626428604
epoch£º51	 i:4 	 global-step:1024	 l-p:0.10237053036689758
epoch£º51	 i:5 	 global-step:1025	 l-p:0.08687776327133179
epoch£º51	 i:6 	 global-step:1026	 l-p:0.10296821594238281
epoch£º51	 i:7 	 global-step:1027	 l-p:0.09961359947919846
epoch£º51	 i:8 	 global-step:1028	 l-p:0.09892765432596207
epoch£º51	 i:9 	 global-step:1029	 l-p:0.09522777050733566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0406, 4.5207, 4.6218],
        [4.0406, 4.3854, 4.3958],
        [4.0406, 4.0806, 4.0520],
        [4.0406, 4.0406, 4.0406]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.0869486853480339 
model_pd.l_d.mean(): -9.537843704223633 
model_pd.lagr.mean(): -9.450895309448242 
model_pd.lambdas: dict_items([('pout', tensor([0.8062], device='cuda:0')), ('power', tensor([0.3800], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6498], device='cuda:0')), ('power', tensor([-23.6445], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.0869486853480339
epoch£º52	 i:1 	 global-step:1041	 l-p:0.09924191981554031
epoch£º52	 i:2 	 global-step:1042	 l-p:0.11380155384540558
epoch£º52	 i:3 	 global-step:1043	 l-p:0.10083403438329697
epoch£º52	 i:4 	 global-step:1044	 l-p:0.09947513043880463
epoch£º52	 i:5 	 global-step:1045	 l-p:0.10133761912584305
epoch£º52	 i:6 	 global-step:1046	 l-p:0.10042601078748703
epoch£º52	 i:7 	 global-step:1047	 l-p:0.10530778765678406
epoch£º52	 i:8 	 global-step:1048	 l-p:0.10153781622648239
epoch£º52	 i:9 	 global-step:1049	 l-p:0.10908372700214386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9892, 3.9958, 3.9899],
        [3.9892, 3.9893, 3.9892],
        [3.9892, 3.9897, 3.9892],
        [3.9892, 3.9896, 3.9892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.09781095385551453 
model_pd.l_d.mean(): -9.009169578552246 
model_pd.lagr.mean(): -8.911358833312988 
model_pd.lambdas: dict_items([('pout', tensor([0.8005], device='cuda:0')), ('power', tensor([0.3683], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5564], device='cuda:0')), ('power', tensor([-23.1757], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.09781095385551453
epoch£º53	 i:1 	 global-step:1061	 l-p:0.0883537009358406
epoch£º53	 i:2 	 global-step:1062	 l-p:0.10007357597351074
epoch£º53	 i:3 	 global-step:1063	 l-p:0.10457943379878998
epoch£º53	 i:4 	 global-step:1064	 l-p:0.10261474549770355
epoch£º53	 i:5 	 global-step:1065	 l-p:0.09664054960012436
epoch£º53	 i:6 	 global-step:1066	 l-p:0.09577176719903946
epoch£º53	 i:7 	 global-step:1067	 l-p:0.10260593146085739
epoch£º53	 i:8 	 global-step:1068	 l-p:0.10594730079174042
epoch£º53	 i:9 	 global-step:1069	 l-p:0.10420448333024979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1284, 4.1284, 4.1284],
        [4.1284, 4.6919, 4.8526],
        [4.1284, 4.7330, 4.9296],
        [4.1284, 4.2121, 4.1656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.09758052974939346 
model_pd.l_d.mean(): -8.97256088256836 
model_pd.lagr.mean(): -8.874979972839355 
model_pd.lambdas: dict_items([('pout', tensor([0.7944], device='cuda:0')), ('power', tensor([0.3566], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6730], device='cuda:0')), ('power', tensor([-23.5802], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.09758052974939346
epoch£º54	 i:1 	 global-step:1081	 l-p:0.0936954990029335
epoch£º54	 i:2 	 global-step:1082	 l-p:0.09382441639900208
epoch£º54	 i:3 	 global-step:1083	 l-p:0.0997207835316658
epoch£º54	 i:4 	 global-step:1084	 l-p:0.10015416890382767
epoch£º54	 i:5 	 global-step:1085	 l-p:0.104546919465065
epoch£º54	 i:6 	 global-step:1086	 l-p:0.09873656928539276
epoch£º54	 i:7 	 global-step:1087	 l-p:0.09231371432542801
epoch£º54	 i:8 	 global-step:1088	 l-p:0.09862089157104492
epoch£º54	 i:9 	 global-step:1089	 l-p:0.09519389271736145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[4.2173, 4.8396, 5.0424],
        [4.2173, 5.0945, 5.5533],
        [4.2173, 4.5842, 4.5964],
        [4.2173, 4.8473, 5.0569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.10273294895887375 
model_pd.l_d.mean(): -8.574067115783691 
model_pd.lagr.mean(): -8.471334457397461 
model_pd.lambdas: dict_items([('pout', tensor([0.7881], device='cuda:0')), ('power', tensor([0.3450], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6475], device='cuda:0')), ('power', tensor([-23.2925], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.10273294895887375
epoch£º55	 i:1 	 global-step:1101	 l-p:0.09880195558071136
epoch£º55	 i:2 	 global-step:1102	 l-p:0.09682302922010422
epoch£º55	 i:3 	 global-step:1103	 l-p:0.09889668226242065
epoch£º55	 i:4 	 global-step:1104	 l-p:0.08237730711698532
epoch£º55	 i:5 	 global-step:1105	 l-p:0.09475868195295334
epoch£º55	 i:6 	 global-step:1106	 l-p:0.09735578298568726
epoch£º55	 i:7 	 global-step:1107	 l-p:0.09656139463186264
epoch£º55	 i:8 	 global-step:1108	 l-p:0.09155601263046265
epoch£º55	 i:9 	 global-step:1109	 l-p:0.09817329794168472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2567, 5.3606, 6.0781],
        [4.2567, 4.2567, 4.2567],
        [4.2567, 4.4579, 4.4051],
        [4.2567, 4.3214, 4.2806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.09900615364313126 
model_pd.l_d.mean(): -8.291902542114258 
model_pd.lagr.mean(): -8.192896842956543 
model_pd.lambdas: dict_items([('pout', tensor([0.7815], device='cuda:0')), ('power', tensor([0.3334], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6514], device='cuda:0')), ('power', tensor([-23.2616], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.09900615364313126
epoch£º56	 i:1 	 global-step:1121	 l-p:0.09900444000959396
epoch£º56	 i:2 	 global-step:1122	 l-p:0.10104738920927048
epoch£º56	 i:3 	 global-step:1123	 l-p:0.09277695417404175
epoch£º56	 i:4 	 global-step:1124	 l-p:0.10009695589542389
epoch£º56	 i:5 	 global-step:1125	 l-p:0.09856259822845459
epoch£º56	 i:6 	 global-step:1126	 l-p:0.09102723747491837
epoch£º56	 i:7 	 global-step:1127	 l-p:0.0955251008272171
epoch£º56	 i:8 	 global-step:1128	 l-p:0.07514843344688416
epoch£º56	 i:9 	 global-step:1129	 l-p:0.09407801181077957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3082, 5.1716, 5.6017],
        [4.3082, 5.2725, 5.8145],
        [4.3082, 4.3082, 4.3082],
        [4.3082, 4.3667, 4.3283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.09545530378818512 
model_pd.l_d.mean(): -8.064590454101562 
model_pd.lagr.mean(): -7.969135284423828 
model_pd.lambdas: dict_items([('pout', tensor([0.7748], device='cuda:0')), ('power', tensor([0.3218], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6954], device='cuda:0')), ('power', tensor([-23.3013], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.09545530378818512
epoch£º57	 i:1 	 global-step:1141	 l-p:0.09318015724420547
epoch£º57	 i:2 	 global-step:1142	 l-p:0.09084708988666534
epoch£º57	 i:3 	 global-step:1143	 l-p:0.09075488895177841
epoch£º57	 i:4 	 global-step:1144	 l-p:0.07635433226823807
epoch£º57	 i:5 	 global-step:1145	 l-p:0.09163759648799896
epoch£º57	 i:6 	 global-step:1146	 l-p:0.09387155622243881
epoch£º57	 i:7 	 global-step:1147	 l-p:0.09282476454973221
epoch£º57	 i:8 	 global-step:1148	 l-p:0.09884992241859436
epoch£º57	 i:9 	 global-step:1149	 l-p:0.09517177194356918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4888, 4.7739, 4.7351],
        [4.4888, 4.6887, 4.6296],
        [4.4888, 5.6258, 6.3390],
        [4.4888, 4.4888, 4.4888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.09287095814943314 
model_pd.l_d.mean(): -7.854381561279297 
model_pd.lagr.mean(): -7.761510372161865 
model_pd.lambdas: dict_items([('pout', tensor([0.7678], device='cuda:0')), ('power', tensor([0.3102], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7804], device='cuda:0')), ('power', tensor([-23.2971], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.09287095814943314
epoch£º58	 i:1 	 global-step:1161	 l-p:0.09798135608434677
epoch£º58	 i:2 	 global-step:1162	 l-p:0.08830203860998154
epoch£º58	 i:3 	 global-step:1163	 l-p:0.26446375250816345
epoch£º58	 i:4 	 global-step:1164	 l-p:0.09238576889038086
epoch£º58	 i:5 	 global-step:1165	 l-p:0.09430038928985596
epoch£º58	 i:6 	 global-step:1166	 l-p:0.09299694001674652
epoch£º58	 i:7 	 global-step:1167	 l-p:0.09072308987379074
epoch£º58	 i:8 	 global-step:1168	 l-p:0.08998996764421463
epoch£º58	 i:9 	 global-step:1169	 l-p:0.0881255641579628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6547, 4.9916, 4.9653],
        [4.6547, 4.6569, 4.6548],
        [4.6547, 4.6547, 4.6547],
        [4.6547, 4.8174, 4.7531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.09098205715417862 
model_pd.l_d.mean(): -7.356847763061523 
model_pd.lagr.mean(): -7.265865802764893 
model_pd.lambdas: dict_items([('pout', tensor([0.7603], device='cuda:0')), ('power', tensor([0.2988], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7509], device='cuda:0')), ('power', tensor([-22.6249], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.09098205715417862
epoch£º59	 i:1 	 global-step:1181	 l-p:0.09046344459056854
epoch£º59	 i:2 	 global-step:1182	 l-p:0.08996555209159851
epoch£º59	 i:3 	 global-step:1183	 l-p:0.09194263070821762
epoch£º59	 i:4 	 global-step:1184	 l-p:0.11556758731603622
epoch£º59	 i:5 	 global-step:1185	 l-p:0.09092157334089279
epoch£º59	 i:6 	 global-step:1186	 l-p:0.08823040872812271
epoch£º59	 i:7 	 global-step:1187	 l-p:0.09035176783800125
epoch£º59	 i:8 	 global-step:1188	 l-p:0.09187538176774979
epoch£º59	 i:9 	 global-step:1189	 l-p:0.08607052266597748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6925, 4.7970, 4.7404],
        [4.6925, 4.6926, 4.6925],
        [4.6925, 4.6945, 4.6926],
        [4.6925, 5.0245, 4.9946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.11075545847415924 
model_pd.l_d.mean(): -6.958502769470215 
model_pd.lagr.mean(): -6.847747325897217 
model_pd.lambdas: dict_items([('pout', tensor([0.7525], device='cuda:0')), ('power', tensor([0.2874], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6764], device='cuda:0')), ('power', tensor([-22.3562], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.11075545847415924
epoch£º60	 i:1 	 global-step:1201	 l-p:0.08932019770145416
epoch£º60	 i:2 	 global-step:1202	 l-p:0.08887753635644913
epoch£º60	 i:3 	 global-step:1203	 l-p:0.09313693642616272
epoch£º60	 i:4 	 global-step:1204	 l-p:0.08971069008111954
epoch£º60	 i:5 	 global-step:1205	 l-p:0.08425273001194
epoch£º60	 i:6 	 global-step:1206	 l-p:0.08943357318639755
epoch£º60	 i:7 	 global-step:1207	 l-p:0.08656259626150131
epoch£º60	 i:8 	 global-step:1208	 l-p:0.09289226680994034
epoch£º60	 i:9 	 global-step:1209	 l-p:0.09177601337432861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7069, 4.7069, 4.7069],
        [4.7069, 5.3309, 5.4847],
        [4.7069, 4.7136, 4.7075],
        [4.7069, 5.4175, 5.6451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.08801959455013275 
model_pd.l_d.mean(): -6.857765197753906 
model_pd.lagr.mean(): -6.769745826721191 
model_pd.lambdas: dict_items([('pout', tensor([0.7444], device='cuda:0')), ('power', tensor([0.2759], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7756], device='cuda:0')), ('power', tensor([-22.6668], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.08801959455013275
epoch£º61	 i:1 	 global-step:1221	 l-p:0.08678582310676575
epoch£º61	 i:2 	 global-step:1222	 l-p:0.0903615728020668
epoch£º61	 i:3 	 global-step:1223	 l-p:0.09058042615652084
epoch£º61	 i:4 	 global-step:1224	 l-p:0.09084866940975189
epoch£º61	 i:5 	 global-step:1225	 l-p:0.09544111788272858
epoch£º61	 i:6 	 global-step:1226	 l-p:0.08880598098039627
epoch£º61	 i:7 	 global-step:1227	 l-p:0.09513619542121887
epoch£º61	 i:8 	 global-step:1228	 l-p:0.0879787877202034
epoch£º61	 i:9 	 global-step:1229	 l-p:0.11939802020788193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6594, 4.6913, 4.6664],
        [4.6594, 5.5748, 6.0085],
        [4.6594, 4.7910, 4.7294],
        [4.6594, 4.8731, 4.8116]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.09443430602550507 
model_pd.l_d.mean(): -6.651792526245117 
model_pd.lagr.mean(): -6.557358264923096 
model_pd.lambdas: dict_items([('pout', tensor([0.7366], device='cuda:0')), ('power', tensor([0.2645], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7660], device='cuda:0')), ('power', tensor([-22.9173], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.09443430602550507
epoch£º62	 i:1 	 global-step:1241	 l-p:0.0920730009675026
epoch£º62	 i:2 	 global-step:1242	 l-p:0.08520759642124176
epoch£º62	 i:3 	 global-step:1243	 l-p:0.08673617988824844
epoch£º62	 i:4 	 global-step:1244	 l-p:0.09061538428068161
epoch£º62	 i:5 	 global-step:1245	 l-p:0.09105920791625977
epoch£º62	 i:6 	 global-step:1246	 l-p:0.08986794203519821
epoch£º62	 i:7 	 global-step:1247	 l-p:0.08417905867099762
epoch£º62	 i:8 	 global-step:1248	 l-p:0.10049153864383698
epoch£º62	 i:9 	 global-step:1249	 l-p:0.0893457755446434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8633, 4.8660, 4.8634],
        [4.8633, 6.2257, 7.1445],
        [4.8633, 6.2304, 7.1552],
        [4.8633, 4.8633, 4.8633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.09133330732584 
model_pd.l_d.mean(): -6.401712417602539 
model_pd.lagr.mean(): -6.3103790283203125 
model_pd.lambdas: dict_items([('pout', tensor([0.7284], device='cuda:0')), ('power', tensor([0.2531], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8419], device='cuda:0')), ('power', tensor([-22.7682], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.09133330732584
epoch£º63	 i:1 	 global-step:1261	 l-p:0.08829350024461746
epoch£º63	 i:2 	 global-step:1262	 l-p:0.08799341320991516
epoch£º63	 i:3 	 global-step:1263	 l-p:0.09006660431623459
epoch£º63	 i:4 	 global-step:1264	 l-p:0.10220237821340561
epoch£º63	 i:5 	 global-step:1265	 l-p:0.0884794369339943
epoch£º63	 i:6 	 global-step:1266	 l-p:0.0881451666355133
epoch£º63	 i:7 	 global-step:1267	 l-p:0.08831385523080826
epoch£º63	 i:8 	 global-step:1268	 l-p:0.08357022702693939
epoch£º63	 i:9 	 global-step:1269	 l-p:0.07786352932453156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9173, 4.9192, 4.9174],
        [4.9173, 4.9173, 4.9173],
        [4.9173, 4.9478, 4.9236],
        [4.9173, 4.9173, 4.9173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.09869640320539474 
model_pd.l_d.mean(): -6.117070198059082 
model_pd.lagr.mean(): -6.018373966217041 
model_pd.lambdas: dict_items([('pout', tensor([0.7200], device='cuda:0')), ('power', tensor([0.2417], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8399], device='cuda:0')), ('power', tensor([-22.6954], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.09869640320539474
epoch£º64	 i:1 	 global-step:1281	 l-p:0.08857090771198273
epoch£º64	 i:2 	 global-step:1282	 l-p:0.08461346477270126
epoch£º64	 i:3 	 global-step:1283	 l-p:0.08249977231025696
epoch£º64	 i:4 	 global-step:1284	 l-p:0.08749139308929443
epoch£º64	 i:5 	 global-step:1285	 l-p:0.08805812895298004
epoch£º64	 i:6 	 global-step:1286	 l-p:0.08642800897359848
epoch£º64	 i:7 	 global-step:1287	 l-p:0.08891868591308594
epoch£º64	 i:8 	 global-step:1288	 l-p:0.08646280318498611
epoch£º64	 i:9 	 global-step:1289	 l-p:0.08454422652721405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0192, 5.0197, 5.0192],
        [5.0192, 5.0192, 5.0192],
        [5.0192, 6.1544, 6.7723],
        [5.0192, 5.1505, 5.0848]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.08957410603761673 
model_pd.l_d.mean(): -5.864619255065918 
model_pd.lagr.mean(): -5.775044918060303 
model_pd.lambdas: dict_items([('pout', tensor([0.7112], device='cuda:0')), ('power', tensor([0.2304], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8756], device='cuda:0')), ('power', tensor([-22.6360], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.08957410603761673
epoch£º65	 i:1 	 global-step:1301	 l-p:0.08689442276954651
epoch£º65	 i:2 	 global-step:1302	 l-p:0.08698974549770355
epoch£º65	 i:3 	 global-step:1303	 l-p:0.08208268880844116
epoch£º65	 i:4 	 global-step:1304	 l-p:0.0958578884601593
epoch£º65	 i:5 	 global-step:1305	 l-p:0.08556219935417175
epoch£º65	 i:6 	 global-step:1306	 l-p:0.08810040354728699
epoch£º65	 i:7 	 global-step:1307	 l-p:0.08806385844945908
epoch£º65	 i:8 	 global-step:1308	 l-p:0.07864735275506973
epoch£º65	 i:9 	 global-step:1309	 l-p:0.08466779440641403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0778, 5.0778, 5.0778],
        [5.0778, 6.2149, 6.8253],
        [5.0778, 5.6951, 5.8100],
        [5.0778, 5.1067, 5.0834]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.08701299130916595 
model_pd.l_d.mean(): -5.652856349945068 
model_pd.lagr.mean(): -5.56584358215332 
model_pd.lambdas: dict_items([('pout', tensor([0.7024], device='cuda:0')), ('power', tensor([0.2191], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9290], device='cuda:0')), ('power', tensor([-22.6992], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.08701299130916595
epoch£º66	 i:1 	 global-step:1321	 l-p:0.08733464777469635
epoch£º66	 i:2 	 global-step:1322	 l-p:0.08898837864398956
epoch£º66	 i:3 	 global-step:1323	 l-p:0.08871275931596756
epoch£º66	 i:4 	 global-step:1324	 l-p:0.08545009791851044
epoch£º66	 i:5 	 global-step:1325	 l-p:0.08857551217079163
epoch£º66	 i:6 	 global-step:1326	 l-p:0.08263030648231506
epoch£º66	 i:7 	 global-step:1327	 l-p:0.08293639868497849
epoch£º66	 i:8 	 global-step:1328	 l-p:0.07714959979057312
epoch£º66	 i:9 	 global-step:1329	 l-p:0.08597379177808762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2078, 5.2078, 5.2078],
        [5.2078, 5.2078, 5.2078],
        [5.2078, 6.3119, 6.8655],
        [5.2078, 6.0833, 6.4046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.08703279495239258 
model_pd.l_d.mean(): -5.326459884643555 
model_pd.lagr.mean(): -5.239427089691162 
model_pd.lambdas: dict_items([('pout', tensor([0.6933], device='cuda:0')), ('power', tensor([0.2079], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9231], device='cuda:0')), ('power', tensor([-22.4191], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.08703279495239258
epoch£º67	 i:1 	 global-step:1341	 l-p:0.08509397506713867
epoch£º67	 i:2 	 global-step:1342	 l-p:0.08481060713529587
epoch£º67	 i:3 	 global-step:1343	 l-p:0.0739322230219841
epoch£º67	 i:4 	 global-step:1344	 l-p:0.08473189175128937
epoch£º67	 i:5 	 global-step:1345	 l-p:0.08989237248897552
epoch£º67	 i:6 	 global-step:1346	 l-p:0.08568841964006424
epoch£º67	 i:7 	 global-step:1347	 l-p:0.08039161562919617
epoch£º67	 i:8 	 global-step:1348	 l-p:0.08059337735176086
epoch£º67	 i:9 	 global-step:1349	 l-p:0.08536861091852188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3666, 6.8504, 7.8193],
        [5.3666, 6.9153, 7.9661],
        [5.3666, 5.3789, 5.3679],
        [5.3666, 6.6029, 7.2774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.07301416248083115 
model_pd.l_d.mean(): -4.986263275146484 
model_pd.lagr.mean(): -4.9132490158081055 
model_pd.lambdas: dict_items([('pout', tensor([0.6838], device='cuda:0')), ('power', tensor([0.1967], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9032], device='cuda:0')), ('power', tensor([-22.0802], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.07301416248083115
epoch£º68	 i:1 	 global-step:1361	 l-p:0.08467280864715576
epoch£º68	 i:2 	 global-step:1362	 l-p:0.08374183624982834
epoch£º68	 i:3 	 global-step:1363	 l-p:0.0848650187253952
epoch£º68	 i:4 	 global-step:1364	 l-p:0.07412506639957428
epoch£º68	 i:5 	 global-step:1365	 l-p:0.0842226892709732
epoch£º68	 i:6 	 global-step:1366	 l-p:0.08045585453510284
epoch£º68	 i:7 	 global-step:1367	 l-p:0.0826708972454071
epoch£º68	 i:8 	 global-step:1368	 l-p:0.08318090438842773
epoch£º68	 i:9 	 global-step:1369	 l-p:0.08664979040622711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5584, 5.7742, 5.6932],
        [5.5584, 6.8985, 7.6592],
        [5.5584, 5.8481, 5.7741],
        [5.5584, 5.5584, 5.5584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.08036405593156815 
model_pd.l_d.mean(): -4.6983442306518555 
model_pd.lagr.mean(): -4.617980003356934 
model_pd.lambdas: dict_items([('pout', tensor([0.6739], device='cuda:0')), ('power', tensor([0.1856], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9354], device='cuda:0')), ('power', tensor([-21.7831], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.08036405593156815
epoch£º69	 i:1 	 global-step:1381	 l-p:0.08128675073385239
epoch£º69	 i:2 	 global-step:1382	 l-p:0.08213470131158829
epoch£º69	 i:3 	 global-step:1383	 l-p:0.0419933944940567
epoch£º69	 i:4 	 global-step:1384	 l-p:0.08617904782295227
epoch£º69	 i:5 	 global-step:1385	 l-p:0.08233512938022614
epoch£º69	 i:6 	 global-step:1386	 l-p:0.08147884160280228
epoch£º69	 i:7 	 global-step:1387	 l-p:0.08160452544689178
epoch£º69	 i:8 	 global-step:1388	 l-p:0.08060459047555923
epoch£º69	 i:9 	 global-step:1389	 l-p:0.08058910071849823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8915, 7.6960, 8.9632],
        [5.8915, 5.9306, 5.8995],
        [5.8915, 5.8915, 5.8915],
        [5.8915, 7.5829, 8.7053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.11468949913978577 
model_pd.l_d.mean(): -4.410126209259033 
model_pd.lagr.mean(): -4.295436859130859 
model_pd.lambdas: dict_items([('pout', tensor([0.6634], device='cuda:0')), ('power', tensor([0.1746], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9794], device='cuda:0')), ('power', tensor([-21.3959], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.11468949913978577
epoch£º70	 i:1 	 global-step:1401	 l-p:0.08047107607126236
epoch£º70	 i:2 	 global-step:1402	 l-p:0.0618993379175663
epoch£º70	 i:3 	 global-step:1403	 l-p:0.08185768127441406
epoch£º70	 i:4 	 global-step:1404	 l-p:0.07899168878793716
epoch£º70	 i:5 	 global-step:1405	 l-p:0.07906471192836761
epoch£º70	 i:6 	 global-step:1406	 l-p:0.07936103641986847
epoch£º70	 i:7 	 global-step:1407	 l-p:0.07848048210144043
epoch£º70	 i:8 	 global-step:1408	 l-p:0.07856528460979462
epoch£º70	 i:9 	 global-step:1409	 l-p:0.07773537188768387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3179, 6.6174, 6.5248],
        [6.3179, 6.3230, 6.3182],
        [6.3179, 6.3179, 6.3179],
        [6.3179, 7.2292, 7.4602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.07829739153385162 
model_pd.l_d.mean(): -4.335575103759766 
model_pd.lagr.mean(): -4.257277488708496 
model_pd.lambdas: dict_items([('pout', tensor([0.6518], device='cuda:0')), ('power', tensor([0.1638], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1996], device='cuda:0')), ('power', tensor([-21.5421], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.07829739153385162
epoch£º71	 i:1 	 global-step:1421	 l-p:0.07842203974723816
epoch£º71	 i:2 	 global-step:1422	 l-p:0.0753173828125
epoch£º71	 i:3 	 global-step:1423	 l-p:-1.5795317888259888
epoch£º71	 i:4 	 global-step:1424	 l-p:0.07773782312870026
epoch£º71	 i:5 	 global-step:1425	 l-p:0.07884155213832855
epoch£º71	 i:6 	 global-step:1426	 l-p:0.07763069868087769
epoch£º71	 i:7 	 global-step:1427	 l-p:0.07723771035671234
epoch£º71	 i:8 	 global-step:1428	 l-p:0.07639343291521072
epoch£º71	 i:9 	 global-step:1429	 l-p:0.07662550359964371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5975, 6.5975, 6.5975],
        [6.5975, 6.5976, 6.5975],
        [6.5975, 6.6052, 6.5981],
        [6.5975, 6.5976, 6.5975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.07211388647556305 
model_pd.l_d.mean(): -4.042508602142334 
model_pd.lagr.mean(): -3.9703946113586426 
model_pd.lambdas: dict_items([('pout', tensor([0.6397], device='cuda:0')), ('power', tensor([0.1532], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2341], device='cuda:0')), ('power', tensor([-21.0830], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.07211388647556305
epoch£º72	 i:1 	 global-step:1441	 l-p:0.07700759917497635
epoch£º72	 i:2 	 global-step:1442	 l-p:0.09548328071832657
epoch£º72	 i:3 	 global-step:1443	 l-p:0.07750716805458069
epoch£º72	 i:4 	 global-step:1444	 l-p:0.07792715728282928
epoch£º72	 i:5 	 global-step:1445	 l-p:0.07697108387947083
epoch£º72	 i:6 	 global-step:1446	 l-p:0.08047197014093399
epoch£º72	 i:7 	 global-step:1447	 l-p:0.07578713446855545
epoch£º72	 i:8 	 global-step:1448	 l-p:0.06484627723693848
epoch£º72	 i:9 	 global-step:1449	 l-p:0.07557320594787598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7589, 6.7589, 6.7589],
        [6.7589, 7.0255, 6.9221],
        [6.7589, 8.2966, 9.0811],
        [6.7589, 7.1300, 7.0360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.07077112048864365 
model_pd.l_d.mean(): -3.8126606941223145 
model_pd.lagr.mean(): -3.741889476776123 
model_pd.lambdas: dict_items([('pout', tensor([0.6271], device='cuda:0')), ('power', tensor([0.1426], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2767], device='cuda:0')), ('power', tensor([-20.9543], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.07077112048864365
epoch£º73	 i:1 	 global-step:1461	 l-p:0.07584496587514877
epoch£º73	 i:2 	 global-step:1462	 l-p:0.07612559199333191
epoch£º73	 i:3 	 global-step:1463	 l-p:0.09297988563776016
epoch£º73	 i:4 	 global-step:1464	 l-p:0.07556039094924927
epoch£º73	 i:5 	 global-step:1465	 l-p:0.07634279131889343
epoch£º73	 i:6 	 global-step:1466	 l-p:0.07594850659370422
epoch£º73	 i:7 	 global-step:1467	 l-p:0.07612117379903793
epoch£º73	 i:8 	 global-step:1468	 l-p:0.0771758034825325
epoch£º73	 i:9 	 global-step:1469	 l-p:0.060992028564214706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8475, 6.8795, 6.8527],
        [6.8475, 7.4664, 7.4581],
        [6.8475, 8.0366, 8.4540],
        [6.8475, 7.2233, 7.1276]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.0766259953379631 
model_pd.l_d.mean(): -3.564145088195801 
model_pd.lagr.mean(): -3.4875190258026123 
model_pd.lambdas: dict_items([('pout', tensor([0.6143], device='cuda:0')), ('power', tensor([0.1321], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2600], device='cuda:0')), ('power', tensor([-20.9412], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.0766259953379631
epoch£º74	 i:1 	 global-step:1481	 l-p:0.07703602313995361
epoch£º74	 i:2 	 global-step:1482	 l-p:0.07032492756843567
epoch£º74	 i:3 	 global-step:1483	 l-p:0.0591169111430645
epoch£º74	 i:4 	 global-step:1484	 l-p:0.0753585547208786
epoch£º74	 i:5 	 global-step:1485	 l-p:0.07619411498308182
epoch£º74	 i:6 	 global-step:1486	 l-p:0.07497502118349075
epoch£º74	 i:7 	 global-step:1487	 l-p:0.0751657485961914
epoch£º74	 i:8 	 global-step:1488	 l-p:0.08537483215332031
epoch£º74	 i:9 	 global-step:1489	 l-p:0.07494670152664185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.0327, 7.6152, 7.5794],
        [7.0327, 7.4059, 7.3045],
        [7.0327, 7.0327, 7.0327],
        [7.0327, 8.3526, 8.8717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.07531029731035233 
model_pd.l_d.mean(): -3.3580024242401123 
model_pd.lagr.mean(): -3.2826921939849854 
model_pd.lambdas: dict_items([('pout', tensor([0.6012], device='cuda:0')), ('power', tensor([0.1217], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3326], device='cuda:0')), ('power', tensor([-20.8220], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.07531029731035233
epoch£º75	 i:1 	 global-step:1501	 l-p:0.07462871074676514
epoch£º75	 i:2 	 global-step:1502	 l-p:0.07590591907501221
epoch£º75	 i:3 	 global-step:1503	 l-p:0.07477907836437225
epoch£º75	 i:4 	 global-step:1504	 l-p:0.07406063377857208
epoch£º75	 i:5 	 global-step:1505	 l-p:0.03181584179401398
epoch£º75	 i:6 	 global-step:1506	 l-p:0.07410865277051926
epoch£º75	 i:7 	 global-step:1507	 l-p:0.07457809150218964
epoch£º75	 i:8 	 global-step:1508	 l-p:0.07389649003744125
epoch£º75	 i:9 	 global-step:1509	 l-p:0.07330728322267532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.3795, 7.3806, 7.3795],
        [7.3795, 7.3795, 7.3795],
        [7.3795, 8.7200, 9.2151],
        [7.3795, 7.8162, 7.7167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.07601583003997803 
model_pd.l_d.mean(): -3.084080457687378 
model_pd.lagr.mean(): -3.0080647468566895 
model_pd.lambdas: dict_items([('pout', tensor([0.5878], device='cuda:0')), ('power', tensor([0.1113], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3518], device='cuda:0')), ('power', tensor([-20.3600], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.07601583003997803
epoch£º76	 i:1 	 global-step:1521	 l-p:0.09575576335191727
epoch£º76	 i:2 	 global-step:1522	 l-p:0.07340173423290253
epoch£º76	 i:3 	 global-step:1523	 l-p:0.07246984541416168
epoch£º76	 i:4 	 global-step:1524	 l-p:0.07330528646707535
epoch£º76	 i:5 	 global-step:1525	 l-p:0.072650246322155
epoch£º76	 i:6 	 global-step:1526	 l-p:0.07528220117092133
epoch£º76	 i:7 	 global-step:1527	 l-p:0.07205427438020706
epoch£º76	 i:8 	 global-step:1528	 l-p:0.0716867595911026
epoch£º76	 i:9 	 global-step:1529	 l-p:0.0716768279671669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.9989, 8.0361, 8.0048],
        [7.9989, 7.9997, 7.9989],
        [7.9989, 8.0889, 8.0235],
        [7.9989, 8.2123, 8.0993]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.07139897346496582 
model_pd.l_d.mean(): -2.9237985610961914 
model_pd.lagr.mean(): -2.8523995876312256 
model_pd.lambdas: dict_items([('pout', tensor([0.5732], device='cuda:0')), ('power', tensor([0.1013], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5240], device='cuda:0')), ('power', tensor([-20.0249], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.07139897346496582
epoch£º77	 i:1 	 global-step:1541	 l-p:0.07229451835155487
epoch£º77	 i:2 	 global-step:1542	 l-p:0.02434207685291767
epoch£º77	 i:3 	 global-step:1543	 l-p:0.0712864026427269
epoch£º77	 i:4 	 global-step:1544	 l-p:0.0705033615231514
epoch£º77	 i:5 	 global-step:1545	 l-p:0.07070428878068924
epoch£º77	 i:6 	 global-step:1546	 l-p:0.07043168693780899
epoch£º77	 i:7 	 global-step:1547	 l-p:0.0738360658288002
epoch£º77	 i:8 	 global-step:1548	 l-p:0.07027603685855865
epoch£º77	 i:9 	 global-step:1549	 l-p:0.07357114553451538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 8.5465,  9.8582, 10.1970],
        [ 8.5465, 10.4303, 11.3142],
        [ 8.5465,  8.7854,  8.6614],
        [ 8.5465,  8.6697,  8.5855]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.07020331919193268 
model_pd.l_d.mean(): -2.6658263206481934 
model_pd.lagr.mean(): -2.595623016357422 
model_pd.lambdas: dict_items([('pout', tensor([0.5578], device='cuda:0')), ('power', tensor([0.0915], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5676], device='cuda:0')), ('power', tensor([-19.3526], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.07020331919193268
epoch£º78	 i:1 	 global-step:1561	 l-p:0.06985589116811752
epoch£º78	 i:2 	 global-step:1562	 l-p:0.06939499825239182
epoch£º78	 i:3 	 global-step:1563	 l-p:0.06972363591194153
epoch£º78	 i:4 	 global-step:1564	 l-p:0.07462678849697113
epoch£º78	 i:5 	 global-step:1565	 l-p:0.06996793299913406
epoch£º78	 i:6 	 global-step:1566	 l-p:0.07096166163682938
epoch£º78	 i:7 	 global-step:1567	 l-p:0.06926075369119644
epoch£º78	 i:8 	 global-step:1568	 l-p:0.06908868998289108
epoch£º78	 i:9 	 global-step:1569	 l-p:0.07848332077264786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 8.8474, 10.3243, 10.7729],
        [ 8.8474,  8.8474,  8.8474],
        [ 8.8474,  8.8516,  8.8476],
        [ 8.8474,  8.8585,  8.8482]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.06907934695482254 
model_pd.l_d.mean(): -2.4867405891418457 
model_pd.lagr.mean(): -2.417661190032959 
model_pd.lambdas: dict_items([('pout', tensor([0.5417], device='cuda:0')), ('power', tensor([0.0819], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6496], device='cuda:0')), ('power', tensor([-19.2038], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.06907934695482254
epoch£º79	 i:1 	 global-step:1581	 l-p:0.07196634262800217
epoch£º79	 i:2 	 global-step:1582	 l-p:0.06889747083187103
epoch£º79	 i:3 	 global-step:1583	 l-p:0.06887125223875046
epoch£º79	 i:4 	 global-step:1584	 l-p:0.06884826719760895
epoch£º79	 i:5 	 global-step:1585	 l-p:0.06918784230947495
epoch£º79	 i:6 	 global-step:1586	 l-p:0.07736608386039734
epoch£º79	 i:7 	 global-step:1587	 l-p:0.06988319754600525
epoch£º79	 i:8 	 global-step:1588	 l-p:0.06849963963031769
epoch£º79	 i:9 	 global-step:1589	 l-p:0.06873679906129837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 9.1373, 12.2036, 14.3972],
        [ 9.1373,  9.1810,  9.1442],
        [ 9.1373,  9.1373,  9.1373],
        [ 9.1373, 11.3297, 12.4565]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.07431048899888992 
model_pd.l_d.mean(): -2.237078905105591 
model_pd.lagr.mean(): -2.1627683639526367 
model_pd.lambdas: dict_items([('pout', tensor([0.5253], device='cuda:0')), ('power', tensor([0.0724], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6468], device='cuda:0')), ('power', tensor([-18.6739], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.07431048899888992
epoch£º80	 i:1 	 global-step:1601	 l-p:0.06857365369796753
epoch£º80	 i:2 	 global-step:1602	 l-p:0.06816820055246353
epoch£º80	 i:3 	 global-step:1603	 l-p:0.06837286800146103
epoch£º80	 i:4 	 global-step:1604	 l-p:0.07058504968881607
epoch£º80	 i:5 	 global-step:1605	 l-p:0.06829103827476501
epoch£º80	 i:6 	 global-step:1606	 l-p:0.0679992288351059
epoch£º80	 i:7 	 global-step:1607	 l-p:0.06978363543748856
epoch£º80	 i:8 	 global-step:1608	 l-p:0.06782153248786926
epoch£º80	 i:9 	 global-step:1609	 l-p:0.06792045384645462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 9.5128,  9.5291,  9.5142],
        [ 9.5128,  9.5128,  9.5128],
        [ 9.5128, 11.8162, 13.0054],
        [ 9.5128,  9.7012,  9.5849]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.06770782172679901 
model_pd.l_d.mean(): -2.0673210620880127 
model_pd.lagr.mean(): -1.9996132850646973 
model_pd.lambdas: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([0.0631], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7303], device='cuda:0')), ('power', tensor([-18.5140], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.06770782172679901
epoch£º81	 i:1 	 global-step:1621	 l-p:0.06737775355577469
epoch£º81	 i:2 	 global-step:1622	 l-p:0.06760071963071823
epoch£º81	 i:3 	 global-step:1623	 l-p:0.06822295486927032
epoch£º81	 i:4 	 global-step:1624	 l-p:0.06845695525407791
epoch£º81	 i:5 	 global-step:1625	 l-p:0.07047180831432343
epoch£º81	 i:6 	 global-step:1626	 l-p:0.06694351136684418
epoch£º81	 i:7 	 global-step:1627	 l-p:0.06708014756441116
epoch£º81	 i:8 	 global-step:1628	 l-p:0.0690440759062767
epoch£º81	 i:9 	 global-step:1629	 l-p:0.06679494678974152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 9.9995, 10.1993, 10.0760],
        [ 9.9995, 11.7564, 12.3259],
        [ 9.9995, 10.5835, 10.4341],
        [ 9.9995, 12.7217, 14.3073]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.06670542061328888 
model_pd.l_d.mean(): -1.8759052753448486 
model_pd.lagr.mean(): -1.8091998100280762 
model_pd.lambdas: dict_items([('pout', tensor([0.4908], device='cuda:0')), ('power', tensor([0.0540], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8002], device='cuda:0')), ('power', tensor([-18.0302], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.06670542061328888
epoch£º82	 i:1 	 global-step:1641	 l-p:0.0666964054107666
epoch£º82	 i:2 	 global-step:1642	 l-p:0.0662413015961647
epoch£º82	 i:3 	 global-step:1643	 l-p:0.0662691667675972
epoch£º82	 i:4 	 global-step:1644	 l-p:0.06629669666290283
epoch£º82	 i:5 	 global-step:1645	 l-p:0.06705263257026672
epoch£º82	 i:6 	 global-step:1646	 l-p:0.06745031476020813
epoch£º82	 i:7 	 global-step:1647	 l-p:0.06827186793088913
epoch£º82	 i:8 	 global-step:1648	 l-p:0.06572972238063812
epoch£º82	 i:9 	 global-step:1649	 l-p:0.06664563715457916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[10.6426, 13.1263, 14.3346],
        [10.6426, 11.3290, 11.1814],
        [10.6426, 10.6447, 10.6427],
        [10.6426, 12.0606, 12.2887]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.06641027331352234 
model_pd.l_d.mean(): -1.6638160943984985 
model_pd.lagr.mean(): -1.5974057912826538 
model_pd.lambdas: dict_items([('pout', tensor([0.4726], device='cuda:0')), ('power', tensor([0.0451], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8311], device='cuda:0')), ('power', tensor([-17.2883], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.06641027331352234
epoch£º83	 i:1 	 global-step:1661	 l-p:0.06545893847942352
epoch£º83	 i:2 	 global-step:1662	 l-p:0.06512432545423508
epoch£º83	 i:3 	 global-step:1663	 l-p:0.06707759201526642
epoch£º83	 i:4 	 global-step:1664	 l-p:0.06494710594415665
epoch£º83	 i:5 	 global-step:1665	 l-p:0.06500427424907684
epoch£º83	 i:6 	 global-step:1666	 l-p:0.0648280680179596
epoch£º83	 i:7 	 global-step:1667	 l-p:0.06526196002960205
epoch£º83	 i:8 	 global-step:1668	 l-p:0.06498663127422333
epoch£º83	 i:9 	 global-step:1669	 l-p:0.06540348380804062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[11.4712, 11.4719, 11.4712],
        [11.4712, 11.5009, 11.4744],
        [11.4712, 11.7202, 11.5705],
        [11.4712, 12.2199, 12.0597]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.06434500217437744 
model_pd.l_d.mean(): -1.5206992626190186 
model_pd.lagr.mean(): -1.4563542604446411 
model_pd.lambdas: dict_items([('pout', tensor([0.4533], device='cuda:0')), ('power', tensor([0.0366], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9713], device='cuda:0')), ('power', tensor([-16.6273], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.06434500217437744
epoch£º84	 i:1 	 global-step:1681	 l-p:0.066411592066288
epoch£º84	 i:2 	 global-step:1682	 l-p:0.06403347849845886
epoch£º84	 i:3 	 global-step:1683	 l-p:0.06395058333873749
epoch£º84	 i:4 	 global-step:1684	 l-p:0.06504720449447632
epoch£º84	 i:5 	 global-step:1685	 l-p:0.06362460553646088
epoch£º84	 i:6 	 global-step:1686	 l-p:0.06343162059783936
epoch£º84	 i:7 	 global-step:1687	 l-p:0.06370306760072708
epoch£º84	 i:8 	 global-step:1688	 l-p:0.06322306394577026
epoch£º84	 i:9 	 global-step:1689	 l-p:0.0632263645529747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[12.5101, 14.9929, 15.9387],
        [12.5101, 14.1405, 14.3658],
        [12.5101, 15.3220, 16.5997],
        [12.5101, 13.6761, 13.6250]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.06311869621276855 
model_pd.l_d.mean(): -1.3575505018234253 
model_pd.lagr.mean(): -1.2944318056106567 
model_pd.lambdas: dict_items([('pout', tensor([0.4331], device='cuda:0')), ('power', tensor([0.0286], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0702], device='cuda:0')), ('power', tensor([-15.5279], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.06311869621276855
epoch£º85	 i:1 	 global-step:1701	 l-p:0.06311516463756561
epoch£º85	 i:2 	 global-step:1702	 l-p:0.0628131777048111
epoch£º85	 i:3 	 global-step:1703	 l-p:0.06261240690946579
epoch£º85	 i:4 	 global-step:1704	 l-p:0.06252722442150116
epoch£º85	 i:5 	 global-step:1705	 l-p:0.06265977025032043
epoch£º85	 i:6 	 global-step:1706	 l-p:0.062235813587903976
epoch£º85	 i:7 	 global-step:1707	 l-p:0.06245165690779686
epoch£º85	 i:8 	 global-step:1708	 l-p:0.06192167475819588
epoch£º85	 i:9 	 global-step:1709	 l-p:0.06323561817407608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[13.7906, 16.9375, 18.3789],
        [13.7906, 13.8326, 13.7955],
        [13.7906, 13.7907, 13.7906],
        [13.7906, 15.0182, 14.9290]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.06271196901798248 
model_pd.l_d.mean(): -1.1443722248077393 
model_pd.lagr.mean(): -1.081660270690918 
model_pd.lambdas: dict_items([('pout', tensor([0.4118], device='cuda:0')), ('power', tensor([0.0212], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0275], device='cuda:0')), ('power', tensor([-13.9446], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.06271196901798248
epoch£º86	 i:1 	 global-step:1721	 l-p:0.0617339201271534
epoch£º86	 i:2 	 global-step:1722	 l-p:0.061429958790540695
epoch£º86	 i:3 	 global-step:1723	 l-p:0.06200321391224861
epoch£º86	 i:4 	 global-step:1724	 l-p:0.06117802485823631
epoch£º86	 i:5 	 global-step:1725	 l-p:0.06116427853703499
epoch£º86	 i:6 	 global-step:1726	 l-p:0.06095975637435913
epoch£º86	 i:7 	 global-step:1727	 l-p:0.06076394021511078
epoch£º86	 i:8 	 global-step:1728	 l-p:0.060592081397771835
epoch£º86	 i:9 	 global-step:1729	 l-p:0.060805730521678925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.3828, 15.8296, 15.5924],
        [15.3828, 15.4047, 15.3844],
        [15.3828, 15.3851, 15.3829],
        [15.3828, 17.6130, 18.0317]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.06054672971367836 
model_pd.l_d.mean(): -1.0943799018859863 
model_pd.lagr.mean(): -1.0338331460952759 
model_pd.lambdas: dict_items([('pout', tensor([0.3888], device='cuda:0')), ('power', tensor([0.0145], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3101], device='cuda:0')), ('power', tensor([-12.6532], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.06054672971367836
epoch£º87	 i:1 	 global-step:1741	 l-p:0.060677383095026016
epoch£º87	 i:2 	 global-step:1742	 l-p:0.06029825657606125
epoch£º87	 i:3 	 global-step:1743	 l-p:0.059982869774103165
epoch£º87	 i:4 	 global-step:1744	 l-p:0.059858281165361404
epoch£º87	 i:5 	 global-step:1745	 l-p:0.060005467385053635
epoch£º87	 i:6 	 global-step:1746	 l-p:0.059819210320711136
epoch£º87	 i:7 	 global-step:1747	 l-p:0.05962447077035904
epoch£º87	 i:8 	 global-step:1748	 l-p:0.059854209423065186
epoch£º87	 i:9 	 global-step:1749	 l-p:0.05923723056912422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.3520, 17.3521, 17.3520],
        [17.3520, 18.8648, 18.7232],
        [17.3520, 17.9005, 17.6215],
        [17.3520, 17.3826, 17.3545]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.059138622134923935 
model_pd.l_d.mean(): -1.0362709760665894 
model_pd.lagr.mean(): -0.9771323800086975 
model_pd.lambdas: dict_items([('pout', tensor([0.3644], device='cuda:0')), ('power', tensor([0.0086], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5528], device='cuda:0')), ('power', tensor([-10.9081], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.059138622134923935
epoch£º88	 i:1 	 global-step:1761	 l-p:0.059038836508989334
epoch£º88	 i:2 	 global-step:1762	 l-p:0.059077948331832886
epoch£º88	 i:3 	 global-step:1763	 l-p:0.058834969997406006
epoch£º88	 i:4 	 global-step:1764	 l-p:0.05869363993406296
epoch£º88	 i:5 	 global-step:1765	 l-p:0.05882454663515091
epoch£º88	 i:6 	 global-step:1766	 l-p:0.05840740725398064
epoch£º88	 i:7 	 global-step:1767	 l-p:0.058278195559978485
epoch£º88	 i:8 	 global-step:1768	 l-p:0.058417171239852905
epoch£º88	 i:9 	 global-step:1769	 l-p:0.058481793850660324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.7801, 25.1978, 28.1857],
        [19.7801, 19.9443, 19.8145],
        [19.7801, 20.0005, 19.8357],
        [19.7801, 20.7392, 20.3915]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.05809704586863518 
model_pd.l_d.mean(): -0.9244000315666199 
model_pd.lagr.mean(): -0.8663029670715332 
model_pd.lambdas: dict_items([('pout', tensor([0.3385], device='cuda:0')), ('power', tensor([0.0038], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6064], device='cuda:0')), ('power', tensor([-8.3344], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.05809704586863518
epoch£º89	 i:1 	 global-step:1781	 l-p:0.05785777047276497
epoch£º89	 i:2 	 global-step:1782	 l-p:0.05785064399242401
epoch£º89	 i:3 	 global-step:1783	 l-p:0.05769777670502663
epoch£º89	 i:4 	 global-step:1784	 l-p:0.05765169858932495
epoch£º89	 i:5 	 global-step:1785	 l-p:0.05778108537197113
epoch£º89	 i:6 	 global-step:1786	 l-p:0.05737730860710144
epoch£º89	 i:7 	 global-step:1787	 l-p:0.05720642954111099
epoch£º89	 i:8 	 global-step:1788	 l-p:0.057109635323286057
epoch£º89	 i:9 	 global-step:1789	 l-p:0.056951940059661865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.7437, 29.5523, 33.6420],
        [22.7437, 22.7437, 22.7436],
        [22.7437, 22.7440, 22.7437],
        [22.7437, 23.0240, 22.8184]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.05701838433742523 
model_pd.l_d.mean(): -0.895889163017273 
model_pd.lagr.mean(): -0.8388707637786865 
model_pd.lambdas: dict_items([('pout', tensor([0.3107], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8458], device='cuda:0')), ('power', tensor([-5.4578], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.05701838433742523
epoch£º90	 i:1 	 global-step:1801	 l-p:0.057034920901060104
epoch£º90	 i:2 	 global-step:1802	 l-p:0.05674593895673752
epoch£º90	 i:3 	 global-step:1803	 l-p:0.056618064641952515
epoch£º90	 i:4 	 global-step:1804	 l-p:0.05651455745100975
epoch£º90	 i:5 	 global-step:1805	 l-p:0.056394755840301514
epoch£º90	 i:6 	 global-step:1806	 l-p:0.05629391968250275
epoch£º90	 i:7 	 global-step:1807	 l-p:0.05621396750211716
epoch£º90	 i:8 	 global-step:1808	 l-p:0.05610281974077225
epoch£º90	 i:9 	 global-step:1809	 l-p:0.05612602457404137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.2499, 26.2499, 26.2499],
        [26.2499, 26.2536, 26.2500],
        [26.2499, 26.3069, 26.2551],
        [26.2499, 32.1167, 34.5853]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.05603644624352455 
model_pd.l_d.mean(): -0.8662495613098145 
model_pd.lagr.mean(): -0.8102130889892578 
model_pd.lambdas: dict_items([('pout', tensor([0.2810], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0496], device='cuda:0')), ('power', tensor([-2.0234], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.05603644624352455
epoch£º91	 i:1 	 global-step:1821	 l-p:0.05584755912423134
epoch£º91	 i:2 	 global-step:1822	 l-p:0.05579786002635956
epoch£º91	 i:3 	 global-step:1823	 l-p:0.05583709478378296
epoch£º91	 i:4 	 global-step:1824	 l-p:0.05559324100613594
epoch£º91	 i:5 	 global-step:1825	 l-p:0.05557689443230629
epoch£º91	 i:6 	 global-step:1826	 l-p:0.055460695177316666
epoch£º91	 i:7 	 global-step:1827	 l-p:0.05537879839539528
epoch£º91	 i:8 	 global-step:1828	 l-p:0.055288903415203094
epoch£º91	 i:9 	 global-step:1829	 l-p:0.055299803614616394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[30.0466, 30.0467, 30.0466],
        [30.0466, 38.8365, 43.9261],
        [30.0466, 31.4787, 30.9353],
        [30.0466, 30.6490, 30.2629]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.05519489571452141 
model_pd.l_d.mean(): -0.8009952306747437 
model_pd.lagr.mean(): -0.7458003163337708 
model_pd.lambdas: dict_items([('pout', tensor([0.2496], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1701], device='cuda:0')), ('power', tensor([1.8623], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.05519489571452141
epoch£º92	 i:1 	 global-step:1841	 l-p:0.055073026567697525
epoch£º92	 i:2 	 global-step:1842	 l-p:0.05506962537765503
epoch£º92	 i:3 	 global-step:1843	 l-p:0.05500961095094681
epoch£º92	 i:4 	 global-step:1844	 l-p:0.05490824952721596
epoch£º92	 i:5 	 global-step:1845	 l-p:0.05489432066679001
epoch£º92	 i:6 	 global-step:1846	 l-p:0.05477304384112358
epoch£º92	 i:7 	 global-step:1847	 l-p:0.05481145158410072
epoch£º92	 i:8 	 global-step:1848	 l-p:0.054729051887989044
epoch£º92	 i:9 	 global-step:1849	 l-p:0.054627370089292526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[33.7871, 33.7870, 33.7871],
        [33.7871, 37.6148, 37.7043],
        [33.7871, 37.9403, 38.2233],
        [33.7871, 36.7174, 36.3763]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.054603975266218185 
model_pd.l_d.mean(): -0.7302238345146179 
model_pd.lagr.mean(): -0.6756198406219482 
model_pd.lambdas: dict_items([('pout', tensor([0.2164], device='cuda:0')), ('power', tensor([0.0022], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.3702], device='cuda:0')), ('power', tensor([5.5521], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.054603975266218185
epoch£º93	 i:1 	 global-step:1861	 l-p:0.054519906640052795
epoch£º93	 i:2 	 global-step:1862	 l-p:0.054504942148923874
epoch£º93	 i:3 	 global-step:1863	 l-p:0.0544537715613842
epoch£º93	 i:4 	 global-step:1864	 l-p:0.054457101970911026
epoch£º93	 i:5 	 global-step:1865	 l-p:0.05434491112828255
epoch£º93	 i:6 	 global-step:1866	 l-p:0.054329317063093185
epoch£º93	 i:7 	 global-step:1867	 l-p:0.05433050915598869
epoch£º93	 i:8 	 global-step:1868	 l-p:0.05426059290766716
epoch£º93	 i:9 	 global-step:1869	 l-p:0.054231610149145126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[36.8181, 42.7611, 44.0735],
        [36.8181, 36.8181, 36.8181],
        [36.8181, 37.3768, 36.9849],
        [36.8181, 37.0050, 36.8462]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.05417760834097862 
model_pd.l_d.mean(): -0.612224280834198 
model_pd.lagr.mean(): -0.5580466985702515 
model_pd.lambdas: dict_items([('pout', tensor([0.1819], device='cuda:0')), ('power', tensor([0.0058], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.5477], device='cuda:0')), ('power', tensor([8.4964], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.05417760834097862
epoch£º94	 i:1 	 global-step:1881	 l-p:0.05421428382396698
epoch£º94	 i:2 	 global-step:1882	 l-p:0.05415238440036774
epoch£º94	 i:3 	 global-step:1883	 l-p:0.05412431061267853
epoch£º94	 i:4 	 global-step:1884	 l-p:0.054129332304000854
epoch£º94	 i:5 	 global-step:1885	 l-p:0.05408487841486931
epoch£º94	 i:6 	 global-step:1886	 l-p:0.05406976491212845
epoch£º94	 i:7 	 global-step:1887	 l-p:0.054051678627729416
epoch£º94	 i:8 	 global-step:1888	 l-p:0.054104480892419815
epoch£º94	 i:9 	 global-step:1889	 l-p:0.05406041070818901
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[38.1509, 38.1519, 38.1509],
        [38.1509, 38.1509, 38.1509],
        [38.1509, 38.1710, 38.1517],
        [38.1509, 38.3650, 38.1852]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.05407177284359932 
model_pd.l_d.mean(): -0.42552638053894043 
model_pd.lagr.mean(): -0.3714545965194702 
model_pd.lambdas: dict_items([('pout', tensor([0.1468], device='cuda:0')), ('power', tensor([0.0106], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.4947], device='cuda:0')), ('power', tensor([9.9252], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.05407177284359932
epoch£º95	 i:1 	 global-step:1901	 l-p:0.05403659865260124
epoch£º95	 i:2 	 global-step:1902	 l-p:0.05404277518391609
epoch£º95	 i:3 	 global-step:1903	 l-p:0.0540846586227417
epoch£º95	 i:4 	 global-step:1904	 l-p:0.05407916381955147
epoch£º95	 i:5 	 global-step:1905	 l-p:0.054060377180576324
epoch£º95	 i:6 	 global-step:1906	 l-p:0.054081812500953674
epoch£º95	 i:7 	 global-step:1907	 l-p:0.054116301238536835
epoch£º95	 i:8 	 global-step:1908	 l-p:0.054182641208171844
epoch£º95	 i:9 	 global-step:1909	 l-p:0.05413251742720604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[37.0030, 49.1987, 57.0943],
        [37.0030, 37.0120, 37.0033],
        [37.0030, 45.3888, 48.9160],
        [37.0030, 37.0030, 37.0030]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.05422699451446533 
model_pd.l_d.mean(): -0.25546133518218994 
model_pd.lagr.mean(): -0.2012343406677246 
model_pd.lambdas: dict_items([('pout', tensor([0.1117], device='cuda:0')), ('power', tensor([0.0153], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.3648], device='cuda:0')), ('power', tensor([8.8761], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.05422699451446533
epoch£º96	 i:1 	 global-step:1921	 l-p:0.05421324446797371
epoch£º96	 i:2 	 global-step:1922	 l-p:0.05422346293926239
epoch£º96	 i:3 	 global-step:1923	 l-p:0.05427034571766853
epoch£º96	 i:4 	 global-step:1924	 l-p:0.05429622158408165
epoch£º96	 i:5 	 global-step:1925	 l-p:0.05441739037632942
epoch£º96	 i:6 	 global-step:1926	 l-p:0.05438346788287163
epoch£º96	 i:7 	 global-step:1927	 l-p:0.054462626576423645
epoch£º96	 i:8 	 global-step:1928	 l-p:0.05448782816529274
epoch£º96	 i:9 	 global-step:1929	 l-p:0.0545756034553051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[33.6243, 38.4149, 39.1393],
        [33.6243, 41.7382, 45.4726],
        [33.6243, 33.6246, 33.6243],
        [33.6243, 45.5439, 53.8219]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.05460022762417793 
model_pd.l_d.mean(): -0.17497733235359192 
model_pd.lagr.mean(): -0.12037710845470428 
model_pd.lambdas: dict_items([('pout', tensor([0.0772], device='cuda:0')), ('power', tensor([0.0188], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.3946], device='cuda:0')), ('power', tensor([5.3281], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.05460022762417793
epoch£º97	 i:1 	 global-step:1941	 l-p:0.05465945973992348
epoch£º97	 i:2 	 global-step:1942	 l-p:0.054750531911849976
epoch£º97	 i:3 	 global-step:1943	 l-p:0.054837796837091446
epoch£º97	 i:4 	 global-step:1944	 l-p:0.054878994822502136
epoch£º97	 i:5 	 global-step:1945	 l-p:0.054922983050346375
epoch£º97	 i:6 	 global-step:1946	 l-p:0.0550074502825737
epoch£º97	 i:7 	 global-step:1947	 l-p:0.0550709031522274
epoch£º97	 i:8 	 global-step:1948	 l-p:0.05518586188554764
epoch£º97	 i:9 	 global-step:1949	 l-p:0.05536026880145073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[29.2096, 32.7077, 32.9086],
        [29.2096, 29.2097, 29.2096],
        [29.2096, 29.6430, 29.3382],
        [29.2096, 34.6710, 36.3767]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.05531832203269005 
model_pd.l_d.mean(): -0.1329759955406189 
model_pd.lagr.mean(): -0.07765766978263855 
model_pd.lambdas: dict_items([('pout', tensor([0.0445], device='cuda:0')), ('power', tensor([0.0203], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2039], device='cuda:0')), ('power', tensor([0.9802], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.05531832203269005
epoch£º98	 i:1 	 global-step:1961	 l-p:0.055462293326854706
epoch£º98	 i:2 	 global-step:1962	 l-p:0.05557699501514435
epoch£º98	 i:3 	 global-step:1963	 l-p:0.05568569153547287
epoch£º98	 i:4 	 global-step:1964	 l-p:0.055652227252721786
epoch£º98	 i:5 	 global-step:1965	 l-p:0.05575520917773247
epoch£º98	 i:6 	 global-step:1966	 l-p:0.05591091513633728
epoch£º98	 i:7 	 global-step:1967	 l-p:0.056000128388404846
epoch£º98	 i:8 	 global-step:1968	 l-p:0.056152649223804474
epoch£º98	 i:9 	 global-step:1969	 l-p:0.056151993572711945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.9196, 24.9196, 24.9196],
        [24.9196, 24.9207, 24.9196],
        [24.9196, 28.4090, 28.9306],
        [24.9196, 24.9202, 24.9196]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.056381240487098694 
model_pd.l_d.mean(): -0.11511710286140442 
model_pd.lagr.mean(): -0.058735862374305725 
model_pd.lambdas: dict_items([('pout', tensor([0.0138], device='cuda:0')), ('power', tensor([0.0196], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9920], device='cuda:0')), ('power', tensor([-3.2973], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.056381240487098694
epoch£º99	 i:1 	 global-step:1981	 l-p:0.05638311430811882
epoch£º99	 i:2 	 global-step:1982	 l-p:0.05650448054075241
epoch£º99	 i:3 	 global-step:1983	 l-p:0.0565849207341671
epoch£º99	 i:4 	 global-step:1984	 l-p:0.05674386024475098
epoch£º99	 i:5 	 global-step:1985	 l-p:0.05689321458339691
epoch£º99	 i:6 	 global-step:1986	 l-p:0.05705205351114273
epoch£º99	 i:7 	 global-step:1987	 l-p:0.05714081600308418
epoch£º99	 i:8 	 global-step:1988	 l-p:0.057172078639268875
epoch£º99	 i:9 	 global-step:1989	 l-p:0.05727474018931389
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.3395, 21.3444, 21.3396],
        [21.3395, 24.5598, 25.1951],
        [21.3395, 21.9797, 21.6408],
        [21.3395, 21.3395, 21.3395]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.057341068983078 
model_pd.l_d.mean(): -0.11943850666284561 
model_pd.lagr.mean(): -0.06209743767976761 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0169], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8365], device='cuda:0')), ('power', tensor([-6.9321], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.057341068983078
epoch£º100	 i:1 	 global-step:2001	 l-p:0.057480745017528534
epoch£º100	 i:2 	 global-step:2002	 l-p:0.057767998427152634
epoch£º100	 i:3 	 global-step:2003	 l-p:0.05778837949037552
epoch£º100	 i:4 	 global-step:2004	 l-p:0.057835351675748825
epoch£º100	 i:5 	 global-step:2005	 l-p:0.05803392454981804
epoch£º100	 i:6 	 global-step:2006	 l-p:0.05826577916741371
epoch£º100	 i:7 	 global-step:2007	 l-p:0.05814875289797783
epoch£º100	 i:8 	 global-step:2008	 l-p:0.058382757008075714
epoch£º100	 i:9 	 global-step:2009	 l-p:0.058609675616025925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.6579, 19.0460, 18.8036],
        [18.6579, 18.6582, 18.6579],
        [18.6579, 18.6723, 18.6586],
        [18.6579, 18.6579, 18.6579]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.0585416778922081 
model_pd.l_d.mean(): -0.1254718005657196 
model_pd.lagr.mean(): -0.0669301226735115 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0127], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6189], device='cuda:0')), ('power', tensor([-9.5316], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.0585416778922081
epoch£º101	 i:1 	 global-step:2021	 l-p:0.05864189937710762
epoch£º101	 i:2 	 global-step:2022	 l-p:0.05887496471405029
epoch£º101	 i:3 	 global-step:2023	 l-p:0.058789074420928955
epoch£º101	 i:4 	 global-step:2024	 l-p:0.059299468994140625
epoch£º101	 i:5 	 global-step:2025	 l-p:0.059194087982177734
epoch£º101	 i:6 	 global-step:2026	 l-p:0.05915380269289017
epoch£º101	 i:7 	 global-step:2027	 l-p:0.05922872945666313
epoch£º101	 i:8 	 global-step:2028	 l-p:0.05927569419145584
epoch£º101	 i:9 	 global-step:2029	 l-p:0.05952446907758713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[16.8521, 20.2423, 21.5104],
        [16.8521, 20.2630, 21.5510],
        [16.8521, 22.3798, 26.0675],
        [16.8521, 17.7584, 17.4724]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.05950227752327919 
model_pd.l_d.mean(): -0.09013041853904724 
model_pd.lagr.mean(): -0.03062814101576805 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0074], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4622], device='cuda:0')), ('power', tensor([-11.3078], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.05950227752327919
epoch£º102	 i:1 	 global-step:2041	 l-p:0.05959562957286835
epoch£º102	 i:2 	 global-step:2042	 l-p:0.06007733196020126
epoch£º102	 i:3 	 global-step:2043	 l-p:0.060037076473236084
epoch£º102	 i:4 	 global-step:2044	 l-p:0.05973090976476669
epoch£º102	 i:5 	 global-step:2045	 l-p:0.05982726067304611
epoch£º102	 i:6 	 global-step:2046	 l-p:0.05983133986592293
epoch£º102	 i:7 	 global-step:2047	 l-p:0.06001934036612511
epoch£º102	 i:8 	 global-step:2048	 l-p:0.06046576052904129
epoch£º102	 i:9 	 global-step:2049	 l-p:0.060155849903821945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.7948, 16.3124, 16.0564],
        [15.7948, 17.4875, 17.5186],
        [15.7948, 16.2787, 16.0292],
        [15.7948, 15.8560, 15.8030]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.06022314354777336 
model_pd.l_d.mean(): -0.025339391082525253 
model_pd.lagr.mean(): 0.03488375246524811 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0014], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3615], device='cuda:0')), ('power', tensor([-12.3128], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.06022314354777336
epoch£º103	 i:1 	 global-step:2061	 l-p:0.060154903680086136
epoch£º103	 i:2 	 global-step:2062	 l-p:0.06062458083033562
epoch£º103	 i:3 	 global-step:2063	 l-p:0.060861002653837204
epoch£º103	 i:4 	 global-step:2064	 l-p:0.06056120619177818
epoch£º103	 i:5 	 global-step:2065	 l-p:0.06031666696071625
epoch£º103	 i:6 	 global-step:2066	 l-p:0.06041189655661583
epoch£º103	 i:7 	 global-step:2067	 l-p:0.06038583442568779
epoch£º103	 i:8 	 global-step:2068	 l-p:0.06052270159125328
epoch£º103	 i:9 	 global-step:2069	 l-p:0.06044817343354225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.3601, 15.3701, 15.3606],
        [15.3601, 16.0791, 15.8146],
        [15.3601, 15.6579, 15.4683],
        [15.3601, 19.2717, 21.2983]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.06038974970579147 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06038974970579147 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4057], device='cuda:0')), ('power', tensor([-12.8961], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.06038974970579147
epoch£º104	 i:1 	 global-step:2081	 l-p:0.06049805507063866
epoch£º104	 i:2 	 global-step:2082	 l-p:0.0604025162756443
epoch£º104	 i:3 	 global-step:2083	 l-p:0.06132298335433006
epoch£º104	 i:4 	 global-step:2084	 l-p:0.060767076909542084
epoch£º104	 i:5 	 global-step:2085	 l-p:0.060643117874860764
epoch£º104	 i:6 	 global-step:2086	 l-p:0.06047144904732704
epoch£º104	 i:7 	 global-step:2087	 l-p:0.06072116643190384
epoch£º104	 i:8 	 global-step:2088	 l-p:0.06046983599662781
epoch£º104	 i:9 	 global-step:2089	 l-p:0.06076918914914131
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.2547, 15.2547, 15.2547],
        [15.2547, 15.2547, 15.2547],
        [15.2547, 16.2624, 16.0386],
        [15.2547, 17.6427, 18.1971]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.06053687259554863 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06053687259554863 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3623], device='cuda:0')), ('power', tensor([-12.9265], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.06053687259554863
epoch£º105	 i:1 	 global-step:2101	 l-p:0.06047176197171211
epoch£º105	 i:2 	 global-step:2102	 l-p:0.06104745715856552
epoch£º105	 i:3 	 global-step:2103	 l-p:0.06042727828025818
epoch£º105	 i:4 	 global-step:2104	 l-p:0.06064616143703461
epoch£º105	 i:5 	 global-step:2105	 l-p:0.060574185103178024
epoch£º105	 i:6 	 global-step:2106	 l-p:0.06055011227726936
epoch£º105	 i:7 	 global-step:2107	 l-p:0.06080935522913933
epoch£º105	 i:8 	 global-step:2108	 l-p:0.06095881387591362
epoch£º105	 i:9 	 global-step:2109	 l-p:0.06076354905962944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.2649, 15.2811, 15.2659],
        [15.2649, 18.3561, 19.5381],
        [15.2649, 15.3656, 15.2835],
        [15.2649, 19.1626, 21.1894]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.061023686081171036 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.061023686081171036 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2544], device='cuda:0')), ('power', tensor([-12.6879], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.061023686081171036
epoch£º106	 i:1 	 global-step:2121	 l-p:0.060468077659606934
epoch£º106	 i:2 	 global-step:2122	 l-p:0.06044859439134598
epoch£º106	 i:3 	 global-step:2123	 l-p:0.06045381352305412
epoch£º106	 i:4 	 global-step:2124	 l-p:0.06051023304462433
epoch£º106	 i:5 	 global-step:2125	 l-p:0.061171650886535645
epoch£º106	 i:6 	 global-step:2126	 l-p:0.060519635677337646
epoch£º106	 i:7 	 global-step:2127	 l-p:0.060453757643699646
epoch£º106	 i:8 	 global-step:2128	 l-p:0.06060989201068878
epoch£º106	 i:9 	 global-step:2129	 l-p:0.06087896227836609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.3165, 15.3312, 15.3174],
        [15.3165, 15.3455, 15.3190],
        [15.3165, 19.6759, 22.2263],
        [15.3165, 18.9634, 20.7073]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.060741718858480453 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.060741718858480453 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3149], device='cuda:0')), ('power', tensor([-12.7715], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.060741718858480453
epoch£º107	 i:1 	 global-step:2141	 l-p:0.06051625683903694
epoch£º107	 i:2 	 global-step:2142	 l-p:0.060587458312511444
epoch£º107	 i:3 	 global-step:2143	 l-p:0.060465697199106216
epoch£º107	 i:4 	 global-step:2144	 l-p:0.06085227429866791
epoch£º107	 i:5 	 global-step:2145	 l-p:0.06039008870720863
epoch£º107	 i:6 	 global-step:2146	 l-p:0.06037603318691254
epoch£º107	 i:7 	 global-step:2147	 l-p:0.06101857125759125
epoch£º107	 i:8 	 global-step:2148	 l-p:0.060718558728694916
epoch£º107	 i:9 	 global-step:2149	 l-p:0.06042075157165527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.3828, 15.4422, 15.3907],
        [15.3828, 15.3835, 15.3828],
        [15.3828, 15.8934, 15.6434],
        [15.3828, 15.3829, 15.3828]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.06095171719789505 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06095171719789505 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3072], device='cuda:0')), ('power', tensor([-12.6473], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.06095171719789505
epoch£º108	 i:1 	 global-step:2161	 l-p:0.06035975366830826
epoch£º108	 i:2 	 global-step:2162	 l-p:0.060345347970724106
epoch£º108	 i:3 	 global-step:2163	 l-p:0.06084509566426277
epoch£º108	 i:4 	 global-step:2164	 l-p:0.06061258167028427
epoch£º108	 i:5 	 global-step:2165	 l-p:0.06054072827100754
epoch£º108	 i:6 	 global-step:2166	 l-p:0.06047021597623825
epoch£º108	 i:7 	 global-step:2167	 l-p:0.06031724438071251
epoch£º108	 i:8 	 global-step:2168	 l-p:0.06052675098180771
epoch£º108	 i:9 	 global-step:2169	 l-p:0.060598935931921005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.4547, 15.8959, 15.6594],
        [15.4547, 15.4637, 15.4551],
        [15.4547, 20.7197, 24.3738],
        [15.4547, 15.4547, 15.4547]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.06045085936784744 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06045085936784744 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3515], device='cuda:0')), ('power', tensor([-12.6854], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.06045085936784744
epoch£º109	 i:1 	 global-step:2181	 l-p:0.06100357696413994
epoch£º109	 i:2 	 global-step:2182	 l-p:0.06036665663123131
epoch£º109	 i:3 	 global-step:2183	 l-p:0.06029720976948738
epoch£º109	 i:4 	 global-step:2184	 l-p:0.06042007356882095
epoch£º109	 i:5 	 global-step:2185	 l-p:0.06083276495337486
epoch£º109	 i:6 	 global-step:2186	 l-p:0.06040910631418228
epoch£º109	 i:7 	 global-step:2187	 l-p:0.06056954711675644
epoch£º109	 i:8 	 global-step:2188	 l-p:0.06038591265678406
epoch£º109	 i:9 	 global-step:2189	 l-p:0.0602896474301815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.5286, 15.5313, 15.5287],
        [15.5286, 15.5286, 15.5286],
        [15.5286, 15.5344, 15.5288],
        [15.5286, 15.6441, 15.5515]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.06024589389562607 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06024589389562607 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4327], device='cuda:0')), ('power', tensor([-12.7091], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.06024589389562607
epoch£º110	 i:1 	 global-step:2201	 l-p:0.06025822460651398
epoch£º110	 i:2 	 global-step:2202	 l-p:0.06080327928066254
epoch£º110	 i:3 	 global-step:2203	 l-p:0.060296621173620224
epoch£º110	 i:4 	 global-step:2204	 l-p:0.06034522503614426
epoch£º110	 i:5 	 global-step:2205	 l-p:0.060607537627220154
epoch£º110	 i:6 	 global-step:2206	 l-p:0.060715749859809875
epoch£º110	 i:7 	 global-step:2207	 l-p:0.06054609641432762
epoch£º110	 i:8 	 global-step:2208	 l-p:0.06029357761144638
epoch£º110	 i:9 	 global-step:2209	 l-p:0.060363080352544785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.6031, 15.6295, 15.6053],
        [15.6031, 18.7398, 19.9224],
        [15.6031, 18.0509, 18.6197],
        [15.6031, 19.4954, 21.4613]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.06032358855009079 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06032358855009079 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3658], device='cuda:0')), ('power', tensor([-12.5258], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.06032358855009079
epoch£º111	 i:1 	 global-step:2221	 l-p:0.06033923104405403
epoch£º111	 i:2 	 global-step:2222	 l-p:0.0604635588824749
epoch£º111	 i:3 	 global-step:2223	 l-p:0.06047860160470009
epoch£º111	 i:4 	 global-step:2224	 l-p:0.060214895755052567
epoch£º111	 i:5 	 global-step:2225	 l-p:0.060241084545850754
epoch£º111	 i:6 	 global-step:2226	 l-p:0.060624897480010986
epoch£º111	 i:7 	 global-step:2227	 l-p:0.060190293937921524
epoch£º111	 i:8 	 global-step:2228	 l-p:0.0602957047522068
epoch£º111	 i:9 	 global-step:2229	 l-p:0.06075610965490341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.6782, 16.0307, 15.8187],
        [15.6782, 16.0203, 15.8120],
        [15.6782, 15.6783, 15.6782],
        [15.6782, 15.8050, 15.7047]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.06019255891442299 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06019255891442299 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4080], device='cuda:0')), ('power', tensor([-12.5589], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.06019255891442299
epoch£º112	 i:1 	 global-step:2241	 l-p:0.060607366263866425
epoch£º112	 i:2 	 global-step:2242	 l-p:0.0602823831140995
epoch£º112	 i:3 	 global-step:2243	 l-p:0.06019950658082962
epoch£º112	 i:4 	 global-step:2244	 l-p:0.060389697551727295
epoch£º112	 i:5 	 global-step:2245	 l-p:0.06041725352406502
epoch£º112	 i:6 	 global-step:2246	 l-p:0.060302648693323135
epoch£º112	 i:7 	 global-step:2247	 l-p:0.060685139149427414
epoch£º112	 i:8 	 global-step:2248	 l-p:0.060109708458185196
epoch£º112	 i:9 	 global-step:2249	 l-p:0.0602005235850811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.7541, 15.7543, 15.7541],
        [15.7541, 18.9535, 20.1776],
        [15.7541, 16.1085, 15.8953],
        [15.7541, 15.7543, 15.7541]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.06038777157664299 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06038777157664299 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3614], device='cuda:0')), ('power', tensor([-12.3468], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.06038777157664299
epoch£º113	 i:1 	 global-step:2261	 l-p:0.060358189046382904
epoch£º113	 i:2 	 global-step:2262	 l-p:0.06012453883886337
epoch£º113	 i:3 	 global-step:2263	 l-p:0.060128916054964066
epoch£º113	 i:4 	 global-step:2264	 l-p:0.060259632766246796
epoch£º113	 i:5 	 global-step:2265	 l-p:0.060182102024555206
epoch£º113	 i:6 	 global-step:2266	 l-p:0.060169193893671036
epoch£º113	 i:7 	 global-step:2267	 l-p:0.060927294194698334
epoch£º113	 i:8 	 global-step:2268	 l-p:0.06025228276848793
epoch£º113	 i:9 	 global-step:2269	 l-p:0.06005339324474335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.8300, 15.8301, 15.8300],
        [15.8300, 15.8400, 15.8305],
        [15.8300, 15.8354, 15.8302],
        [15.8300, 19.8848, 21.9942]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.06025652587413788 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06025652587413788 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3428], device='cuda:0')), ('power', tensor([-12.2466], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.06025652587413788
epoch£º114	 i:1 	 global-step:2281	 l-p:0.060081612318754196
epoch£º114	 i:2 	 global-step:2282	 l-p:0.06017095595598221
epoch£º114	 i:3 	 global-step:2283	 l-p:0.06076310947537422
epoch£º114	 i:4 	 global-step:2284	 l-p:0.060058534145355225
epoch£º114	 i:5 	 global-step:2285	 l-p:0.06003160774707794
epoch£º114	 i:6 	 global-step:2286	 l-p:0.060335539281368256
epoch£º114	 i:7 	 global-step:2287	 l-p:0.06009092554450035
epoch£º114	 i:8 	 global-step:2288	 l-p:0.06046399846673012
epoch£º114	 i:9 	 global-step:2289	 l-p:0.060053419321775436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.9064, 15.9064, 15.9064],
        [15.9064, 15.9065, 15.9064],
        [15.9064, 15.9065, 15.9064],
        [15.9064, 16.9602, 16.7257]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.06008566543459892 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06008566543459892 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4069], device='cuda:0')), ('power', tensor([-12.2641], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.06008566543459892
epoch£º115	 i:1 	 global-step:2301	 l-p:0.06013799086213112
epoch£º115	 i:2 	 global-step:2302	 l-p:0.06050128489732742
epoch£º115	 i:3 	 global-step:2303	 l-p:0.06005360558629036
epoch£º115	 i:4 	 global-step:2304	 l-p:0.06044536456465721
epoch£º115	 i:5 	 global-step:2305	 l-p:0.06019692122936249
epoch£º115	 i:6 	 global-step:2306	 l-p:0.060050103813409805
epoch£º115	 i:7 	 global-step:2307	 l-p:0.060008905827999115
epoch£º115	 i:8 	 global-step:2308	 l-p:0.06032639369368553
epoch£º115	 i:9 	 global-step:2309	 l-p:0.05996588617563248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.9832, 16.4401, 16.1950],
        [15.9832, 16.2021, 16.0468],
        [15.9832, 15.9834, 15.9832],
        [15.9832, 17.0673, 16.8376]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.06014261022210121 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06014261022210121 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3738], device='cuda:0')), ('power', tensor([-12.1461], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.06014261022210121
epoch£º116	 i:1 	 global-step:2321	 l-p:0.05997998267412186
epoch£º116	 i:2 	 global-step:2322	 l-p:0.06027531623840332
epoch£º116	 i:3 	 global-step:2323	 l-p:0.06026292219758034
epoch£º116	 i:4 	 global-step:2324	 l-p:0.05997084453701973
epoch£º116	 i:5 	 global-step:2325	 l-p:0.05994753539562225
epoch£º116	 i:6 	 global-step:2326	 l-p:0.059985943138599396
epoch£º116	 i:7 	 global-step:2327	 l-p:0.059980038553476334
epoch£º116	 i:8 	 global-step:2328	 l-p:0.06049944832921028
epoch£º116	 i:9 	 global-step:2329	 l-p:0.060193855315446854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.0599, 16.0659, 16.0601],
        [16.0599, 16.4104, 16.1967],
        [16.0599, 16.0600, 16.0599],
        [16.0599, 16.0599, 16.0599]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.05990754812955856 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05990754812955856 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4656], device='cuda:0')), ('power', tensor([-12.2470], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.05990754812955856
epoch£º117	 i:1 	 global-step:2341	 l-p:0.059989020228385925
epoch£º117	 i:2 	 global-step:2342	 l-p:0.05994860455393791
epoch£º117	 i:3 	 global-step:2343	 l-p:0.05998222157359123
epoch£º117	 i:4 	 global-step:2344	 l-p:0.06006578356027603
epoch£º117	 i:5 	 global-step:2345	 l-p:0.05989516153931618
epoch£º117	 i:6 	 global-step:2346	 l-p:0.060499247163534164
epoch£º117	 i:7 	 global-step:2347	 l-p:0.06032488867640495
epoch£º117	 i:8 	 global-step:2348	 l-p:0.06019167602062225
epoch£º117	 i:9 	 global-step:2349	 l-p:0.05990728363394737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.1370, 21.6200, 25.4065],
        [16.1370, 17.7930, 17.7835],
        [16.1370, 16.2680, 16.1644],
        [16.1370, 16.1370, 16.1370]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.060031212866306305 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.060031212866306305 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3829], device='cuda:0')), ('power', tensor([-11.9807], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.060031212866306305
epoch£º118	 i:1 	 global-step:2361	 l-p:0.059919681400060654
epoch£º118	 i:2 	 global-step:2362	 l-p:0.06058526784181595
epoch£º118	 i:3 	 global-step:2363	 l-p:0.05986723676323891
epoch£º118	 i:4 	 global-step:2364	 l-p:0.05988119915127754
epoch£º118	 i:5 	 global-step:2365	 l-p:0.06004015728831291
epoch£º118	 i:6 	 global-step:2366	 l-p:0.05994611606001854
epoch£º118	 i:7 	 global-step:2367	 l-p:0.05992953106760979
epoch£º118	 i:8 	 global-step:2368	 l-p:0.06008625775575638
epoch£º118	 i:9 	 global-step:2369	 l-p:0.059904277324676514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.2149, 17.2930, 17.0541],
        [16.2149, 16.2667, 16.2210],
        [16.2149, 18.8959, 19.5956],
        [16.2149, 19.4857, 20.7200]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.0598011277616024 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0598011277616024 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4739], device='cuda:0')), ('power', tensor([-12.0136], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.0598011277616024
epoch£º119	 i:1 	 global-step:2381	 l-p:0.05990658700466156
epoch£º119	 i:2 	 global-step:2382	 l-p:0.06028648093342781
epoch£º119	 i:3 	 global-step:2383	 l-p:0.05987917259335518
epoch£º119	 i:4 	 global-step:2384	 l-p:0.05983465909957886
epoch£º119	 i:5 	 global-step:2385	 l-p:0.06001023203134537
epoch£º119	 i:6 	 global-step:2386	 l-p:0.059898097068071365
epoch£º119	 i:7 	 global-step:2387	 l-p:0.05975808575749397
epoch£º119	 i:8 	 global-step:2388	 l-p:0.06024327501654625
epoch£º119	 i:9 	 global-step:2389	 l-p:0.06004882603883743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.2926, 16.2932, 16.2926],
        [16.2926, 16.2936, 16.2926],
        [16.2926, 16.2987, 16.2928],
        [16.2926, 18.4048, 18.6622]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.059815797954797745 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.059815797954797745 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4397], device='cuda:0')), ('power', tensor([-11.8743], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.059815797954797745
epoch£º120	 i:1 	 global-step:2401	 l-p:0.060196712613105774
epoch£º120	 i:2 	 global-step:2402	 l-p:0.05977397784590721
epoch£º120	 i:3 	 global-step:2403	 l-p:0.059802617877721786
epoch£º120	 i:4 	 global-step:2404	 l-p:0.059791114181280136
epoch£º120	 i:5 	 global-step:2405	 l-p:0.06010672450065613
epoch£º120	 i:6 	 global-step:2406	 l-p:0.05983877182006836
epoch£º120	 i:7 	 global-step:2407	 l-p:0.06020057573914528
epoch£º120	 i:8 	 global-step:2408	 l-p:0.059697892516851425
epoch£º120	 i:9 	 global-step:2409	 l-p:0.05992705747485161
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.3708, 17.4586, 17.2168],
        [16.3708, 16.5496, 16.4158],
        [16.3708, 16.3709, 16.3708],
        [16.3708, 16.3748, 16.3709]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.060093700885772705 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.060093700885772705 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4552], device='cuda:0')), ('power', tensor([-11.7622], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.060093700885772705
epoch£º121	 i:1 	 global-step:2421	 l-p:0.059731852263212204
epoch£º121	 i:2 	 global-step:2422	 l-p:0.05976502224802971
epoch£º121	 i:3 	 global-step:2423	 l-p:0.0598873607814312
epoch£º121	 i:4 	 global-step:2424	 l-p:0.05990266054868698
epoch£º121	 i:5 	 global-step:2425	 l-p:0.059742193669080734
epoch£º121	 i:6 	 global-step:2426	 l-p:0.05998540297150612
epoch£º121	 i:7 	 global-step:2427	 l-p:0.05977486073970795
epoch£º121	 i:8 	 global-step:2428	 l-p:0.05993513762950897
epoch£º121	 i:9 	 global-step:2429	 l-p:0.05981903895735741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.4494, 16.6201, 16.4910],
        [16.4494, 16.5135, 16.4579],
        [16.4494, 16.4495, 16.4494],
        [16.4494, 16.4947, 16.4543]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.06019287928938866 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.06019287928938866 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3870], device='cuda:0')), ('power', tensor([-11.5651], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.06019287928938866
epoch£º122	 i:1 	 global-step:2441	 l-p:0.06000562012195587
epoch£º122	 i:2 	 global-step:2442	 l-p:0.05972663685679436
epoch£º122	 i:3 	 global-step:2443	 l-p:0.05968383699655533
epoch£º122	 i:4 	 global-step:2444	 l-p:0.059625089168548584
epoch£º122	 i:5 	 global-step:2445	 l-p:0.059680353850126266
epoch£º122	 i:6 	 global-step:2446	 l-p:0.05960167199373245
epoch£º122	 i:7 	 global-step:2447	 l-p:0.05975919961929321
epoch£º122	 i:8 	 global-step:2448	 l-p:0.05974918231368065
epoch£º122	 i:9 	 global-step:2449	 l-p:0.06010204181075096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.5283, 16.5317, 16.5284],
        [16.5283, 16.8745, 16.6598],
        [16.5283, 16.5702, 16.5326],
        [16.5283, 16.5283, 16.5283]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.05963832885026932 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05963832885026932 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4762], device='cuda:0')), ('power', tensor([-11.6705], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.05963832885026932
epoch£º123	 i:1 	 global-step:2461	 l-p:0.05962098762392998
epoch£º123	 i:2 	 global-step:2462	 l-p:0.05967264994978905
epoch£º123	 i:3 	 global-step:2463	 l-p:0.05959088355302811
epoch£º123	 i:4 	 global-step:2464	 l-p:0.06011315807700157
epoch£º123	 i:5 	 global-step:2465	 l-p:0.05964542552828789
epoch£º123	 i:6 	 global-step:2466	 l-p:0.0600520484149456
epoch£º123	 i:7 	 global-step:2467	 l-p:0.059779323637485504
epoch£º123	 i:8 	 global-step:2468	 l-p:0.059604376554489136
epoch£º123	 i:9 	 global-step:2469	 l-p:0.059899963438510895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.6071, 16.6096, 16.6072],
        [16.6071, 20.3292, 21.9616],
        [16.6071, 16.8400, 16.6757],
        [16.6071, 16.9451, 16.7331]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.05967584624886513 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05967584624886513 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4587], device='cuda:0')), ('power', tensor([-11.5851], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.05967584624886513
epoch£º124	 i:1 	 global-step:2481	 l-p:0.05978759750723839
epoch£º124	 i:2 	 global-step:2482	 l-p:0.05964254215359688
epoch£º124	 i:3 	 global-step:2483	 l-p:0.05997799336910248
epoch£º124	 i:4 	 global-step:2484	 l-p:0.059814173728227615
epoch£º124	 i:5 	 global-step:2485	 l-p:0.0599316768348217
epoch£º124	 i:6 	 global-step:2486	 l-p:0.059527646750211716
epoch£º124	 i:7 	 global-step:2487	 l-p:0.05962837114930153
epoch£º124	 i:8 	 global-step:2488	 l-p:0.05955862998962402
epoch£º124	 i:9 	 global-step:2489	 l-p:0.05957278236746788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.6869, 16.7764, 16.7014],
        [16.6869, 22.6481, 26.9409],
        [16.6869, 16.7993, 16.7078],
        [16.6869, 18.8649, 19.1362]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.059950701892375946 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.059950701892375946 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4365], device='cuda:0')), ('power', tensor([-11.3864], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.059950701892375946
epoch£º125	 i:1 	 global-step:2501	 l-p:0.059859149158000946
epoch£º125	 i:2 	 global-step:2502	 l-p:0.05980308726429939
epoch£º125	 i:3 	 global-step:2503	 l-p:0.059531643986701965
epoch£º125	 i:4 	 global-step:2504	 l-p:0.059768084436655045
epoch£º125	 i:5 	 global-step:2505	 l-p:0.05948770418763161
epoch£º125	 i:6 	 global-step:2506	 l-p:0.05961865186691284
epoch£º125	 i:7 	 global-step:2507	 l-p:0.05959707126021385
epoch£º125	 i:8 	 global-step:2508	 l-p:0.05953378602862358
epoch£º125	 i:9 	 global-step:2509	 l-p:0.05946646258234978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.7670, 20.3882, 21.8954],
        [16.7670, 19.2007, 19.6497],
        [16.7670, 16.7671, 16.7670],
        [16.7670, 16.8070, 16.7710]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.05948786064982414 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05948786064982414 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5183], device='cuda:0')), ('power', tensor([-11.4963], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.05948786064982414
epoch£º126	 i:1 	 global-step:2521	 l-p:0.05986659973859787
epoch£º126	 i:2 	 global-step:2522	 l-p:0.059636715799570084
epoch£º126	 i:3 	 global-step:2523	 l-p:0.05963897705078125
epoch£º126	 i:4 	 global-step:2524	 l-p:0.05949953943490982
epoch£º126	 i:5 	 global-step:2525	 l-p:0.05948816239833832
epoch£º126	 i:6 	 global-step:2526	 l-p:0.05971849337220192
epoch£º126	 i:7 	 global-step:2527	 l-p:0.059483837336301804
epoch£º126	 i:8 	 global-step:2528	 l-p:0.05952316150069237
epoch£º126	 i:9 	 global-step:2529	 l-p:0.05977142974734306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[16.8468, 17.5965, 17.3038],
        [16.8468, 17.3775, 17.1075],
        [16.8468, 18.0466, 17.8171],
        [16.8468, 21.0049, 23.0648]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.05962848663330078 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05962848663330078 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4716], device='cuda:0')), ('power', tensor([-11.2773], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.05962848663330078
epoch£º127	 i:1 	 global-step:2541	 l-p:0.05944744125008583
epoch£º127	 i:2 	 global-step:2542	 l-p:0.05953771620988846
epoch£º127	 i:3 	 global-step:2543	 l-p:0.059594057500362396
epoch£º127	 i:4 	 global-step:2544	 l-p:0.05989256873726845
epoch£º127	 i:5 	 global-step:2545	 l-p:0.05951238423585892
epoch£º127	 i:6 	 global-step:2546	 l-p:0.059387292712926865
epoch£º127	 i:7 	 global-step:2547	 l-p:0.059456147253513336
epoch£º127	 i:8 	 global-step:2548	 l-p:0.059456001967191696
epoch£º127	 i:9 	 global-step:2549	 l-p:0.05970977619290352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.9270, 19.3020, 19.6934],
        [16.9270, 17.9562, 17.6851],
        [16.9270, 16.9461, 16.9283],
        [16.9270, 16.9318, 16.9272]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.05953030288219452 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05953030288219452 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4399], device='cuda:0')), ('power', tensor([-11.2020], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.05953030288219452
epoch£º128	 i:1 	 global-step:2561	 l-p:0.059483062475919724
epoch£º128	 i:2 	 global-step:2562	 l-p:0.05942289158701897
epoch£º128	 i:3 	 global-step:2563	 l-p:0.0597950741648674
epoch£º128	 i:4 	 global-step:2564	 l-p:0.059346526861190796
epoch£º128	 i:5 	 global-step:2565	 l-p:0.059370722621679306
epoch£º128	 i:6 	 global-step:2566	 l-p:0.05959438160061836
epoch£º128	 i:7 	 global-step:2567	 l-p:0.05955992266535759
epoch£º128	 i:8 	 global-step:2568	 l-p:0.05968816205859184
epoch£º128	 i:9 	 global-step:2569	 l-p:0.059340111911296844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.0077, 17.0141, 17.0079],
        [17.0077, 17.1355, 17.0331],
        [17.0077, 17.0078, 17.0077],
        [17.0077, 17.8486, 17.5542]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.059424903243780136 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.059424903243780136 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4561], device='cuda:0')), ('power', tensor([-11.1181], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.059424903243780136
epoch£º129	 i:1 	 global-step:2581	 l-p:0.059784889221191406
epoch£º129	 i:2 	 global-step:2582	 l-p:0.05949534848332405
epoch£º129	 i:3 	 global-step:2583	 l-p:0.059349510818719864
epoch£º129	 i:4 	 global-step:2584	 l-p:0.05952274054288864
epoch£º129	 i:5 	 global-step:2585	 l-p:0.05934000387787819
epoch£º129	 i:6 	 global-step:2586	 l-p:0.05982629954814911
epoch£º129	 i:7 	 global-step:2587	 l-p:0.05928252264857292
epoch£º129	 i:8 	 global-step:2588	 l-p:0.059314899146556854
epoch£º129	 i:9 	 global-step:2589	 l-p:0.05930384621024132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.0889, 17.9684, 17.6746],
        [17.0889, 17.0891, 17.0889],
        [17.0889, 17.0891, 17.0889],
        [17.0889, 17.0971, 17.0893]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.059734657406806946 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.059734657406806946 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4314], device='cuda:0')), ('power', tensor([-10.9878], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.059734657406806946
epoch£º130	 i:1 	 global-step:2601	 l-p:0.059596069157123566
epoch£º130	 i:2 	 global-step:2602	 l-p:0.05930052325129509
epoch£º130	 i:3 	 global-step:2603	 l-p:0.05926492437720299
epoch£º130	 i:4 	 global-step:2604	 l-p:0.05961396172642708
epoch£º130	 i:5 	 global-step:2605	 l-p:0.059383708983659744
epoch£º130	 i:6 	 global-step:2606	 l-p:0.059534862637519836
epoch£º130	 i:7 	 global-step:2607	 l-p:0.05925776809453964
epoch£º130	 i:8 	 global-step:2608	 l-p:0.059233978390693665
epoch£º130	 i:9 	 global-step:2609	 l-p:0.05923973768949509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.1705, 17.1705, 17.1705],
        [17.1705, 17.6751, 17.4075],
        [17.1705, 17.1714, 17.1705],
        [17.1705, 17.1706, 17.1705]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.05939124897122383 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05939124897122383 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4733], device='cuda:0')), ('power', tensor([-10.9514], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.05939124897122383
epoch£º131	 i:1 	 global-step:2621	 l-p:0.05927431583404541
epoch£º131	 i:2 	 global-step:2622	 l-p:0.059286050498485565
epoch£º131	 i:3 	 global-step:2623	 l-p:0.05939380079507828
epoch£º131	 i:4 	 global-step:2624	 l-p:0.059435322880744934
epoch£º131	 i:5 	 global-step:2625	 l-p:0.05960260331630707
epoch£º131	 i:6 	 global-step:2626	 l-p:0.05925928056240082
epoch£º131	 i:7 	 global-step:2627	 l-p:0.05923207104206085
epoch£º131	 i:8 	 global-step:2628	 l-p:0.059522245079278946
epoch£º131	 i:9 	 global-step:2629	 l-p:0.059278134256601334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.2519, 17.2521, 17.2519],
        [17.2519, 22.6457, 26.0811],
        [17.2519, 17.2519, 17.2518],
        [17.2519, 19.6433, 20.0190]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.05946747586131096 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05946747586131096 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4415], device='cuda:0')), ('power', tensor([-10.8428], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.05946747586131096
epoch£º132	 i:1 	 global-step:2641	 l-p:0.059576988220214844
epoch£º132	 i:2 	 global-step:2642	 l-p:0.05919935926795006
epoch£º132	 i:3 	 global-step:2643	 l-p:0.05913342162966728
epoch£º132	 i:4 	 global-step:2644	 l-p:0.05923398211598396
epoch£º132	 i:5 	 global-step:2645	 l-p:0.05951010435819626
epoch£º132	 i:6 	 global-step:2646	 l-p:0.05920732766389847
epoch£º132	 i:7 	 global-step:2647	 l-p:0.05920139700174332
epoch£º132	 i:8 	 global-step:2648	 l-p:0.05944202467799187
epoch£º132	 i:9 	 global-step:2649	 l-p:0.05922667309641838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.3339, 17.3339, 17.3339],
        [17.3339, 17.3339, 17.3339],
        [17.3339, 19.1315, 19.1257],
        [17.3339, 18.0611, 17.7606]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.05958814173936844 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05958814173936844 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4624], device='cuda:0')), ('power', tensor([-10.6669], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.05958814173936844
epoch£º133	 i:1 	 global-step:2661	 l-p:0.05926838889718056
epoch£º133	 i:2 	 global-step:2662	 l-p:0.059135209769010544
epoch£º133	 i:3 	 global-step:2663	 l-p:0.0591224767267704
epoch£º133	 i:4 	 global-step:2664	 l-p:0.059542011469602585
epoch£º133	 i:5 	 global-step:2665	 l-p:0.059289779514074326
epoch£º133	 i:6 	 global-step:2666	 l-p:0.05920775234699249
epoch£º133	 i:7 	 global-step:2667	 l-p:0.05917883664369583
epoch£º133	 i:8 	 global-step:2668	 l-p:0.05916110426187515
epoch£º133	 i:9 	 global-step:2669	 l-p:0.059229832142591476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.4163, 19.1553, 19.1145],
        [17.4163, 17.9554, 17.6777],
        [17.4163, 17.5341, 17.4382],
        [17.4163, 17.4360, 17.4175]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.05915401503443718 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05915401503443718 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5322], device='cuda:0')), ('power', tensor([-10.8164], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.05915401503443718
epoch£º134	 i:1 	 global-step:2681	 l-p:0.05975694581866264
epoch£º134	 i:2 	 global-step:2682	 l-p:0.0591510534286499
epoch£º134	 i:3 	 global-step:2683	 l-p:0.059314992278814316
epoch£º134	 i:4 	 global-step:2684	 l-p:0.05908043310046196
epoch£º134	 i:5 	 global-step:2685	 l-p:0.059111498296260834
epoch£º134	 i:6 	 global-step:2686	 l-p:0.05940059572458267
epoch£º134	 i:7 	 global-step:2687	 l-p:0.05907350778579712
epoch£º134	 i:8 	 global-step:2688	 l-p:0.05912184715270996
epoch£º134	 i:9 	 global-step:2689	 l-p:0.05908631160855293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.4990, 17.4990, 17.4990],
        [17.4990, 23.1011, 26.7468],
        [17.4990, 18.3294, 18.0246],
        [17.4990, 22.2629, 24.8952]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.0594087690114975 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0594087690114975 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4421], device='cuda:0')), ('power', tensor([-10.5137], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.0594087690114975
epoch£º135	 i:1 	 global-step:2701	 l-p:0.05918029323220253
epoch£º135	 i:2 	 global-step:2702	 l-p:0.059118468314409256
epoch£º135	 i:3 	 global-step:2703	 l-p:0.059028260409832
epoch£º135	 i:4 	 global-step:2704	 l-p:0.059140872210264206
epoch£º135	 i:5 	 global-step:2705	 l-p:0.05907813087105751
epoch£º135	 i:6 	 global-step:2706	 l-p:0.059048041701316833
epoch£º135	 i:7 	 global-step:2707	 l-p:0.05924079939723015
epoch£º135	 i:8 	 global-step:2708	 l-p:0.05921645835042
epoch£º135	 i:9 	 global-step:2709	 l-p:0.0593186616897583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.5816, 17.5824, 17.5816],
        [17.5816, 18.5358, 18.2369],
        [17.5816, 17.5876, 17.5818],
        [17.5816, 18.7072, 18.4351]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.05897694453597069 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05897694453597069 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6042], device='cuda:0')), ('power', tensor([-10.7211], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.05897694453597069
epoch£º136	 i:1 	 global-step:2721	 l-p:0.05906367301940918
epoch£º136	 i:2 	 global-step:2722	 l-p:0.05924660712480545
epoch£º136	 i:3 	 global-step:2723	 l-p:0.059110358357429504
epoch£º136	 i:4 	 global-step:2724	 l-p:0.05902013182640076
epoch£º136	 i:5 	 global-step:2725	 l-p:0.05938469246029854
epoch£º136	 i:6 	 global-step:2726	 l-p:0.05901378393173218
epoch£º136	 i:7 	 global-step:2727	 l-p:0.059311799705028534
epoch£º136	 i:8 	 global-step:2728	 l-p:0.05895023047924042
epoch£º136	 i:9 	 global-step:2729	 l-p:0.059235237538814545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.6647, 17.6647, 17.6647],
        [17.6647, 17.7944, 17.6900],
        [17.6647, 17.6647, 17.6647],
        [17.6647, 22.8634, 25.9785]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.05896594747900963 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05896594747900963 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5751], device='cuda:0')), ('power', tensor([-10.5855], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.05896594747900963
epoch£º137	 i:1 	 global-step:2741	 l-p:0.05899294093251228
epoch£º137	 i:2 	 global-step:2742	 l-p:0.0590163990855217
epoch£º137	 i:3 	 global-step:2743	 l-p:0.059332627803087234
epoch£º137	 i:4 	 global-step:2744	 l-p:0.05929679423570633
epoch£º137	 i:5 	 global-step:2745	 l-p:0.05913746356964111
epoch£º137	 i:6 	 global-step:2746	 l-p:0.0590127632021904
epoch£º137	 i:7 	 global-step:2747	 l-p:0.059107135981321335
epoch£º137	 i:8 	 global-step:2748	 l-p:0.05901839956641197
epoch£º137	 i:9 	 global-step:2749	 l-p:0.05897115170955658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.7485, 18.2615, 17.9865],
        [17.7485, 17.7485, 17.7485],
        [17.7485, 17.7485, 17.7485],
        [17.7485, 17.7485, 17.7485]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.05892338976264 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05892338976264 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5888], device='cuda:0')), ('power', tensor([-10.4990], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.05892338976264
epoch£º138	 i:1 	 global-step:2761	 l-p:0.059009987860918045
epoch£º138	 i:2 	 global-step:2762	 l-p:0.05933646112680435
epoch£º138	 i:3 	 global-step:2763	 l-p:0.058950118720531464
epoch£º138	 i:4 	 global-step:2764	 l-p:0.05924342945218086
epoch£º138	 i:5 	 global-step:2765	 l-p:0.05919994041323662
epoch£º138	 i:6 	 global-step:2766	 l-p:0.05887238681316376
epoch£º138	 i:7 	 global-step:2767	 l-p:0.05898385867476463
epoch£º138	 i:8 	 global-step:2768	 l-p:0.058901794254779816
epoch£º138	 i:9 	 global-step:2769	 l-p:0.058969710022211075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.8326, 23.0844, 26.2316],
        [17.8326, 23.9376, 28.1575],
        [17.8326, 17.8326, 17.8326],
        [17.8326, 17.8511, 17.8337]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.059276435524225235 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.059276435524225235 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4865], device='cuda:0')), ('power', tensor([-10.2135], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.059276435524225235
epoch£º139	 i:1 	 global-step:2781	 l-p:0.05884142220020294
epoch£º139	 i:2 	 global-step:2782	 l-p:0.05887196958065033
epoch£º139	 i:3 	 global-step:2783	 l-p:0.05900944396853447
epoch£º139	 i:4 	 global-step:2784	 l-p:0.05894194543361664
epoch£º139	 i:5 	 global-step:2785	 l-p:0.0589754655957222
epoch£º139	 i:6 	 global-step:2786	 l-p:0.05893713980913162
epoch£º139	 i:7 	 global-step:2787	 l-p:0.059152811765670776
epoch£º139	 i:8 	 global-step:2788	 l-p:0.05903705582022667
epoch£º139	 i:9 	 global-step:2789	 l-p:0.05888833850622177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.9167, 23.4619, 26.9511],
        [17.9167, 18.1906, 18.0015],
        [17.9167, 21.4099, 22.6431],
        [17.9167, 23.5504, 27.1498]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.05890468508005142 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05890468508005142 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5466], device='cuda:0')), ('power', tensor([-10.2277], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.05890468508005142
epoch£º140	 i:1 	 global-step:2801	 l-p:0.058866363018751144
epoch£º140	 i:2 	 global-step:2802	 l-p:0.059085726737976074
epoch£º140	 i:3 	 global-step:2803	 l-p:0.05883484706282616
epoch£º140	 i:4 	 global-step:2804	 l-p:0.058800891041755676
epoch£º140	 i:5 	 global-step:2805	 l-p:0.05895046517252922
epoch£º140	 i:6 	 global-step:2806	 l-p:0.059005554765462875
epoch£º140	 i:7 	 global-step:2807	 l-p:0.05914111062884331
epoch£º140	 i:8 	 global-step:2808	 l-p:0.05886610224843025
epoch£º140	 i:9 	 global-step:2809	 l-p:0.059020690619945526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.0011, 18.0022, 18.0011],
        [18.0011, 18.0021, 18.0011],
        [18.0011, 18.0011, 18.0011],
        [18.0011, 18.8928, 18.5796]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.058857936412096024 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058857936412096024 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5622], device='cuda:0')), ('power', tensor([-10.1891], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.058857936412096024
epoch£º141	 i:1 	 global-step:2821	 l-p:0.05885866656899452
epoch£º141	 i:2 	 global-step:2822	 l-p:0.05878207087516785
epoch£º141	 i:3 	 global-step:2823	 l-p:0.05884278193116188
epoch£º141	 i:4 	 global-step:2824	 l-p:0.05877622589468956
epoch£º141	 i:5 	 global-step:2825	 l-p:0.05890894681215286
epoch£º141	 i:6 	 global-step:2826	 l-p:0.0588848777115345
epoch£º141	 i:7 	 global-step:2827	 l-p:0.05886422097682953
epoch£º141	 i:8 	 global-step:2828	 l-p:0.058744896203279495
epoch£º141	 i:9 	 global-step:2829	 l-p:0.05950284004211426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.0856, 18.8472, 18.5326],
        [18.0856, 19.3046, 19.0371],
        [18.0856, 19.1932, 18.9025],
        [18.0856, 18.0906, 18.0857]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.058805253356695175 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058805253356695175 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5718], device='cuda:0')), ('power', tensor([-10.1116], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.058805253356695175
epoch£º142	 i:1 	 global-step:2841	 l-p:0.05896909534931183
epoch£º142	 i:2 	 global-step:2842	 l-p:0.058773014694452286
epoch£º142	 i:3 	 global-step:2843	 l-p:0.05874987319111824
epoch£º142	 i:4 	 global-step:2844	 l-p:0.0590045265853405
epoch£º142	 i:5 	 global-step:2845	 l-p:0.058778293430805206
epoch£º142	 i:6 	 global-step:2846	 l-p:0.05911849066615105
epoch£º142	 i:7 	 global-step:2847	 l-p:0.058896951377391815
epoch£º142	 i:8 	 global-step:2848	 l-p:0.05876872316002846
epoch£º142	 i:9 	 global-step:2849	 l-p:0.05871446058154106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.1712, 18.1712, 18.1712],
        [18.1712, 18.1712, 18.1712],
        [18.1712, 18.1713, 18.1712],
        [18.1712, 22.2800, 24.0883]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.05901232734322548 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05901232734322548 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5178], device='cuda:0')), ('power', tensor([-9.9256], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.05901232734322548
epoch£º143	 i:1 	 global-step:2861	 l-p:0.05875701829791069
epoch£º143	 i:2 	 global-step:2862	 l-p:0.0587087981402874
epoch£º143	 i:3 	 global-step:2863	 l-p:0.058673057705163956
epoch£º143	 i:4 	 global-step:2864	 l-p:0.058815501630306244
epoch£º143	 i:5 	 global-step:2865	 l-p:0.05880117416381836
epoch£º143	 i:6 	 global-step:2866	 l-p:0.05870503932237625
epoch£º143	 i:7 	 global-step:2867	 l-p:0.05864132568240166
epoch£º143	 i:8 	 global-step:2868	 l-p:0.05905956029891968
epoch£º143	 i:9 	 global-step:2869	 l-p:0.05895591899752617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.2567, 24.7997, 29.5017],
        [18.2567, 21.7369, 22.9169],
        [18.2567, 19.0032, 18.6867],
        [18.2567, 18.3105, 18.2627]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.058893024921417236 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058893024921417236 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5690], device='cuda:0')), ('power', tensor([-9.9403], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.058893024921417236
epoch£º144	 i:1 	 global-step:2881	 l-p:0.05864524841308594
epoch£º144	 i:2 	 global-step:2882	 l-p:0.05898892879486084
epoch£º144	 i:3 	 global-step:2883	 l-p:0.058681830763816833
epoch£º144	 i:4 	 global-step:2884	 l-p:0.058660268783569336
epoch£º144	 i:5 	 global-step:2885	 l-p:0.05863431841135025
epoch£º144	 i:6 	 global-step:2886	 l-p:0.05868622288107872
epoch£º144	 i:7 	 global-step:2887	 l-p:0.05907129496335983
epoch£º144	 i:8 	 global-step:2888	 l-p:0.058687303215265274
epoch£º144	 i:9 	 global-step:2889	 l-p:0.05873928964138031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.3428, 22.3842, 24.0992],
        [18.3428, 22.8355, 25.0249],
        [18.3428, 18.4204, 18.3536],
        [18.3428, 24.1036, 27.7748]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.0586305670440197 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0586305670440197 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6109], device='cuda:0')), ('power', tensor([-9.8690], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.0586305670440197
epoch£º145	 i:1 	 global-step:2901	 l-p:0.05896865576505661
epoch£º145	 i:2 	 global-step:2902	 l-p:0.05875033140182495
epoch£º145	 i:3 	 global-step:2903	 l-p:0.058892201632261276
epoch£º145	 i:4 	 global-step:2904	 l-p:0.05866134911775589
epoch£º145	 i:5 	 global-step:2905	 l-p:0.058640021830797195
epoch£º145	 i:6 	 global-step:2906	 l-p:0.05867481231689453
epoch£º145	 i:7 	 global-step:2907	 l-p:0.05888956040143967
epoch£º145	 i:8 	 global-step:2908	 l-p:0.05858595296740532
epoch£º145	 i:9 	 global-step:2909	 l-p:0.058553826063871384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.4293, 18.4293, 18.4293],
        [18.4293, 18.7056, 18.5136],
        [18.4293, 20.1819, 20.0919],
        [18.4293, 18.9222, 18.6466]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.05874039605259895 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05874039605259895 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5740], device='cuda:0')), ('power', tensor([-9.7260], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.05874039605259895
epoch£º146	 i:1 	 global-step:2921	 l-p:0.05862720310688019
epoch£º146	 i:2 	 global-step:2922	 l-p:0.05866182968020439
epoch£º146	 i:3 	 global-step:2923	 l-p:0.05860036611557007
epoch£º146	 i:4 	 global-step:2924	 l-p:0.05878977105021477
epoch£º146	 i:5 	 global-step:2925	 l-p:0.058948952704668045
epoch£º146	 i:6 	 global-step:2926	 l-p:0.05853525176644325
epoch£º146	 i:7 	 global-step:2927	 l-p:0.05874411389231682
epoch£º146	 i:8 	 global-step:2928	 l-p:0.05856078863143921
epoch£º146	 i:9 	 global-step:2929	 l-p:0.058599039912223816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.5158, 19.8069, 19.5427],
        [18.5158, 18.5167, 18.5159],
        [18.5158, 18.9084, 18.6651],
        [18.5158, 20.3134, 20.2402]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.05852268263697624 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05852268263697624 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6516], device='cuda:0')), ('power', tensor([-9.7616], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.05852268263697624
epoch£º147	 i:1 	 global-step:2941	 l-p:0.05889219045639038
epoch£º147	 i:2 	 global-step:2942	 l-p:0.05858482047915459
epoch£º147	 i:3 	 global-step:2943	 l-p:0.05854397639632225
epoch£º147	 i:4 	 global-step:2944	 l-p:0.058626316487789154
epoch£º147	 i:5 	 global-step:2945	 l-p:0.05862876772880554
epoch£º147	 i:6 	 global-step:2946	 l-p:0.05862610787153244
epoch£º147	 i:7 	 global-step:2947	 l-p:0.05850429832935333
epoch£º147	 i:8 	 global-step:2948	 l-p:0.05850982666015625
epoch£º147	 i:9 	 global-step:2949	 l-p:0.05893264710903168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.6026, 18.7879, 18.6462],
        [18.6026, 18.6919, 18.6159],
        [18.6026, 24.5842, 28.4791],
        [18.6026, 18.6026, 18.6025]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.05850934982299805 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05850934982299805 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6349], device='cuda:0')), ('power', tensor([-9.6484], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.05850934982299805
epoch£º148	 i:1 	 global-step:2961	 l-p:0.05854092910885811
epoch£º148	 i:2 	 global-step:2962	 l-p:0.058499012142419815
epoch£º148	 i:3 	 global-step:2963	 l-p:0.05860224738717079
epoch£º148	 i:4 	 global-step:2964	 l-p:0.058490607887506485
epoch£º148	 i:5 	 global-step:2965	 l-p:0.05864523723721504
epoch£º148	 i:6 	 global-step:2966	 l-p:0.05873466655611992
epoch£º148	 i:7 	 global-step:2967	 l-p:0.0587310828268528
epoch£º148	 i:8 	 global-step:2968	 l-p:0.05847657844424248
epoch£º148	 i:9 	 global-step:2969	 l-p:0.05870935320854187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.6896, 19.2277, 18.9379],
        [18.6896, 18.9645, 18.7724],
        [18.6896, 24.6310, 28.4572],
        [18.6896, 21.6699, 22.3674]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.058725107461214066 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058725107461214066 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5265], device='cuda:0')), ('power', tensor([-9.3480], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.058725107461214066
epoch£º149	 i:1 	 global-step:2981	 l-p:0.05844096839427948
epoch£º149	 i:2 	 global-step:2982	 l-p:0.05843294784426689
epoch£º149	 i:3 	 global-step:2983	 l-p:0.058453626930713654
epoch£º149	 i:4 	 global-step:2984	 l-p:0.05855104327201843
epoch£º149	 i:5 	 global-step:2985	 l-p:0.058564357459545135
epoch£º149	 i:6 	 global-step:2986	 l-p:0.05846669152379036
epoch£º149	 i:7 	 global-step:2987	 l-p:0.05850651487708092
epoch£º149	 i:8 	 global-step:2988	 l-p:0.058455727994441986
epoch£º149	 i:9 	 global-step:2989	 l-p:0.05891282856464386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.7771, 20.8590, 20.9205],
        [18.7771, 21.5827, 22.1337],
        [18.7771, 20.8214, 20.8618],
        [18.7771, 18.7977, 18.7784]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.058768004179000854 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058768004179000854 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5562], device='cuda:0')), ('power', tensor([-9.2803], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.058768004179000854
epoch£º150	 i:1 	 global-step:3001	 l-p:0.05857929214835167
epoch£º150	 i:2 	 global-step:3002	 l-p:0.05839135870337486
epoch£º150	 i:3 	 global-step:3003	 l-p:0.058625783771276474
epoch£º150	 i:4 	 global-step:3004	 l-p:0.05840098112821579
epoch£º150	 i:5 	 global-step:3005	 l-p:0.058539435267448425
epoch£º150	 i:6 	 global-step:3006	 l-p:0.058482758700847626
epoch£º150	 i:7 	 global-step:3007	 l-p:0.05846681445837021
epoch£º150	 i:8 	 global-step:3008	 l-p:0.05840311571955681
epoch£º150	 i:9 	 global-step:3009	 l-p:0.058428023010492325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.8655, 21.2698, 21.5250],
        [18.8655, 18.8655, 18.8655],
        [18.8655, 23.4958, 25.7532],
        [18.8655, 21.9345, 22.6872]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.058354929089546204 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058354929089546204 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6748], device='cuda:0')), ('power', tensor([-9.3913], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.058354929089546204
epoch£º151	 i:1 	 global-step:3021	 l-p:0.05836585909128189
epoch£º151	 i:2 	 global-step:3022	 l-p:0.05835021287202835
epoch£º151	 i:3 	 global-step:3023	 l-p:0.05846315249800682
epoch£º151	 i:4 	 global-step:3024	 l-p:0.058375414460897446
epoch£º151	 i:5 	 global-step:3025	 l-p:0.05852443724870682
epoch£º151	 i:6 	 global-step:3026	 l-p:0.05868136137723923
epoch£º151	 i:7 	 global-step:3027	 l-p:0.05851522460579872
epoch£º151	 i:8 	 global-step:3028	 l-p:0.0586840845644474
epoch£º151	 i:9 	 global-step:3029	 l-p:0.05834208428859711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[18.9534, 20.0251, 19.7051],
        [18.9534, 24.4386, 27.6549],
        [18.9534, 20.2934, 20.0267],
        [18.9534, 20.2601, 19.9848]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.05841244384646416 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05841244384646416 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5843], device='cuda:0')), ('power', tensor([-9.1832], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.05841244384646416
epoch£º152	 i:1 	 global-step:3041	 l-p:0.05835200473666191
epoch£º152	 i:2 	 global-step:3042	 l-p:0.058376867324113846
epoch£º152	 i:3 	 global-step:3043	 l-p:0.05863348767161369
epoch£º152	 i:4 	 global-step:3044	 l-p:0.05830782279372215
epoch£º152	 i:5 	 global-step:3045	 l-p:0.05864463746547699
epoch£º152	 i:6 	 global-step:3046	 l-p:0.05829034373164177
epoch£º152	 i:7 	 global-step:3047	 l-p:0.05840682238340378
epoch£º152	 i:8 	 global-step:3048	 l-p:0.05853767320513725
epoch£º152	 i:9 	 global-step:3049	 l-p:0.05827523022890091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.0422, 19.0823, 19.0459],
        [19.0422, 19.0433, 19.0422],
        [19.0422, 19.1458, 19.0589],
        [19.0422, 19.0422, 19.0422]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.05854399502277374 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05854399502277374 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6327], device='cuda:0')), ('power', tensor([-9.1051], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.05854399502277374
epoch£º153	 i:1 	 global-step:3061	 l-p:0.058483678847551346
epoch£º153	 i:2 	 global-step:3062	 l-p:0.058293215930461884
epoch£º153	 i:3 	 global-step:3063	 l-p:0.05831503868103027
epoch£º153	 i:4 	 global-step:3064	 l-p:0.05829036608338356
epoch£º153	 i:5 	 global-step:3065	 l-p:0.05831102654337883
epoch£º153	 i:6 	 global-step:3066	 l-p:0.05831114575266838
epoch£º153	 i:7 	 global-step:3067	 l-p:0.058468710631132126
epoch£º153	 i:8 	 global-step:3068	 l-p:0.05842766538262367
epoch£º153	 i:9 	 global-step:3069	 l-p:0.05837271735072136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.1312, 23.0425, 24.5227],
        [19.1312, 20.3105, 20.0021],
        [19.1312, 19.3663, 19.1944],
        [19.1312, 19.1793, 19.1361]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.05853290110826492 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05853290110826492 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5794], device='cuda:0')), ('power', tensor([-8.9179], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.05853290110826492
epoch£º154	 i:1 	 global-step:3081	 l-p:0.058344658464193344
epoch£º154	 i:2 	 global-step:3082	 l-p:0.058289144188165665
epoch£º154	 i:3 	 global-step:3083	 l-p:0.05849374458193779
epoch£º154	 i:4 	 global-step:3084	 l-p:0.058477241545915604
epoch£º154	 i:5 	 global-step:3085	 l-p:0.05825464054942131
epoch£º154	 i:6 	 global-step:3086	 l-p:0.05829120799899101
epoch£º154	 i:7 	 global-step:3087	 l-p:0.058269720524549484
epoch£º154	 i:8 	 global-step:3088	 l-p:0.058233439922332764
epoch£º154	 i:9 	 global-step:3089	 l-p:0.05821509286761284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.2208, 23.1814, 24.6979],
        [19.2208, 22.7222, 23.8090],
        [19.2208, 19.6024, 19.3597],
        [19.2208, 21.4565, 21.5791]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.05829478055238724 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05829478055238724 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6353], device='cuda:0')), ('power', tensor([-8.9793], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.05829478055238724
epoch£º155	 i:1 	 global-step:3101	 l-p:0.058257296681404114
epoch£º155	 i:2 	 global-step:3102	 l-p:0.05831744149327278
epoch£º155	 i:3 	 global-step:3103	 l-p:0.058186061680316925
epoch£º155	 i:4 	 global-step:3104	 l-p:0.05838564410805702
epoch£º155	 i:5 	 global-step:3105	 l-p:0.05816967785358429
epoch£º155	 i:6 	 global-step:3106	 l-p:0.05867573246359825
epoch£º155	 i:7 	 global-step:3107	 l-p:0.05817839130759239
epoch£º155	 i:8 	 global-step:3108	 l-p:0.05820341035723686
epoch£º155	 i:9 	 global-step:3109	 l-p:0.05831655487418175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[19.3102, 25.7964, 30.1846],
        [19.3102, 25.9104, 30.4466],
        [19.3102, 20.5517, 20.2498],
        [19.3102, 25.7800, 30.1471]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.058247294276952744 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058247294276952744 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6301], device='cuda:0')), ('power', tensor([-8.8928], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.058247294276952744
epoch£º156	 i:1 	 global-step:3121	 l-p:0.05844905599951744
epoch£º156	 i:2 	 global-step:3122	 l-p:0.05860243737697601
epoch£º156	 i:3 	 global-step:3123	 l-p:0.0581805519759655
epoch£º156	 i:4 	 global-step:3124	 l-p:0.058183453977108
epoch£º156	 i:5 	 global-step:3125	 l-p:0.058241985738277435
epoch£º156	 i:6 	 global-step:3126	 l-p:0.05827292427420616
epoch£º156	 i:7 	 global-step:3127	 l-p:0.058106325566768646
epoch£º156	 i:8 	 global-step:3128	 l-p:0.05814291909337044
epoch£º156	 i:9 	 global-step:3129	 l-p:0.05814901366829872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[19.4005, 19.9775, 19.6718],
        [19.4005, 22.7188, 23.6256],
        [19.4005, 24.7738, 27.7774],
        [19.4005, 21.1670, 21.0333]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.05816855654120445 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05816855654120445 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6535], device='cuda:0')), ('power', tensor([-8.7511], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.05816855654120445
epoch£º157	 i:1 	 global-step:3141	 l-p:0.05813521146774292
epoch£º157	 i:2 	 global-step:3142	 l-p:0.05826246738433838
epoch£º157	 i:3 	 global-step:3143	 l-p:0.05841768905520439
epoch£º157	 i:4 	 global-step:3144	 l-p:0.05808825418353081
epoch£º157	 i:5 	 global-step:3145	 l-p:0.05829255282878876
epoch£º157	 i:6 	 global-step:3146	 l-p:0.058186691254377365
epoch£º157	 i:7 	 global-step:3147	 l-p:0.058168161660432816
epoch£º157	 i:8 	 global-step:3148	 l-p:0.05834926292300224
epoch£º157	 i:9 	 global-step:3149	 l-p:0.0580952912569046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.4906, 19.4910, 19.4906],
        [19.4906, 19.5815, 19.5040],
        [19.4906, 19.4908, 19.4906],
        [19.4906, 22.3555, 22.8871]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.05826924741268158 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05826924741268158 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6609], device='cuda:0')), ('power', tensor([-8.6772], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.05826924741268158
epoch£º158	 i:1 	 global-step:3161	 l-p:0.05807819217443466
epoch£º158	 i:2 	 global-step:3162	 l-p:0.058165643364191055
epoch£º158	 i:3 	 global-step:3163	 l-p:0.05830101668834686
epoch£º158	 i:4 	 global-step:3164	 l-p:0.05810211971402168
epoch£º158	 i:5 	 global-step:3165	 l-p:0.0581955723464489
epoch£º158	 i:6 	 global-step:3166	 l-p:0.05841433256864548
epoch£º158	 i:7 	 global-step:3167	 l-p:0.058043334633111954
epoch£º158	 i:8 	 global-step:3168	 l-p:0.05802619457244873
epoch£º158	 i:9 	 global-step:3169	 l-p:0.05816313251852989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.5813, 20.7908, 20.4746],
        [19.5813, 20.1744, 19.8633],
        [19.5813, 20.5275, 20.1836],
        [19.5813, 19.9307, 19.7000]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.05806354433298111 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05806354433298111 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6879], device='cuda:0')), ('power', tensor([-8.6494], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.05806354433298111
epoch£º159	 i:1 	 global-step:3181	 l-p:0.05804996192455292
epoch£º159	 i:2 	 global-step:3182	 l-p:0.05812669172883034
epoch£º159	 i:3 	 global-step:3183	 l-p:0.058066610246896744
epoch£º159	 i:4 	 global-step:3184	 l-p:0.0583404116332531
epoch£º159	 i:5 	 global-step:3185	 l-p:0.05800800025463104
epoch£º159	 i:6 	 global-step:3186	 l-p:0.058166246861219406
epoch£º159	 i:7 	 global-step:3187	 l-p:0.05820430442690849
epoch£º159	 i:8 	 global-step:3188	 l-p:0.05806461349129677
epoch£º159	 i:9 	 global-step:3189	 l-p:0.05826272815465927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.6721, 20.2315, 19.9275],
        [19.6721, 24.2747, 26.3801],
        [19.6721, 19.6720, 19.6721],
        [19.6721, 19.6794, 19.6723]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.057985514402389526 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057985514402389526 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7427], device='cuda:0')), ('power', tensor([-8.6265], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.057985514402389526
epoch£º160	 i:1 	 global-step:3201	 l-p:0.0580192469060421
epoch£º160	 i:2 	 global-step:3202	 l-p:0.05829621106386185
epoch£º160	 i:3 	 global-step:3203	 l-p:0.05819588527083397
epoch£º160	 i:4 	 global-step:3204	 l-p:0.05800783634185791
epoch£º160	 i:5 	 global-step:3205	 l-p:0.05825980007648468
epoch£º160	 i:6 	 global-step:3206	 l-p:0.05807546526193619
epoch£º160	 i:7 	 global-step:3207	 l-p:0.058009762316942215
epoch£º160	 i:8 	 global-step:3208	 l-p:0.05808795243501663
epoch£º160	 i:9 	 global-step:3209	 l-p:0.05801587551832199
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[19.7636, 20.9445, 20.6184],
        [19.7636, 20.3885, 20.0687],
        [19.7636, 23.6952, 25.1149],
        [19.7636, 24.1007, 25.9186]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.05794917792081833 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05794917792081833 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7427], device='cuda:0')), ('power', tensor([-8.5596], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.05794917792081833
epoch£º161	 i:1 	 global-step:3221	 l-p:0.058020953088998795
epoch£º161	 i:2 	 global-step:3222	 l-p:0.05841023847460747
epoch£º161	 i:3 	 global-step:3223	 l-p:0.05794825404882431
epoch£º161	 i:4 	 global-step:3224	 l-p:0.05808461084961891
epoch£º161	 i:5 	 global-step:3225	 l-p:0.05802152678370476
epoch£º161	 i:6 	 global-step:3226	 l-p:0.0579819530248642
epoch£º161	 i:7 	 global-step:3227	 l-p:0.05795080587267876
epoch£º161	 i:8 	 global-step:3228	 l-p:0.05823664367198944
epoch£º161	 i:9 	 global-step:3229	 l-p:0.057949550449848175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.8554, 23.4959, 24.6355],
        [19.8554, 19.8630, 19.8557],
        [19.8554, 20.6988, 20.3508],
        [19.8554, 24.1969, 26.0067]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.05796516314148903 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05796516314148903 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6898], device='cuda:0')), ('power', tensor([-8.3480], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.05796516314148903
epoch£º162	 i:1 	 global-step:3241	 l-p:0.05829816311597824
epoch£º162	 i:2 	 global-step:3242	 l-p:0.05792047455906868
epoch£º162	 i:3 	 global-step:3243	 l-p:0.057892683893442154
epoch£º162	 i:4 	 global-step:3244	 l-p:0.057941146194934845
epoch£º162	 i:5 	 global-step:3245	 l-p:0.058048900216817856
epoch£º162	 i:6 	 global-step:3246	 l-p:0.05801142379641533
epoch£º162	 i:7 	 global-step:3247	 l-p:0.05798042193055153
epoch£º162	 i:8 	 global-step:3248	 l-p:0.05804211646318436
epoch£º162	 i:9 	 global-step:3249	 l-p:0.05805594474077225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.9474, 25.4827, 28.5778],
        [19.9474, 19.9475, 19.9474],
        [19.9474, 22.3887, 22.5876],
        [19.9474, 19.9476, 19.9474]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.058202922344207764 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058202922344207764 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6022], device='cuda:0')), ('power', tensor([-8.0891], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.058202922344207764
epoch£º163	 i:1 	 global-step:3261	 l-p:0.057976286858320236
epoch£º163	 i:2 	 global-step:3262	 l-p:0.057889752089977264
epoch£º163	 i:3 	 global-step:3263	 l-p:0.05790981277823448
epoch£º163	 i:4 	 global-step:3264	 l-p:0.05791448429226875
epoch£º163	 i:5 	 global-step:3265	 l-p:0.057877663522958755
epoch£º163	 i:6 	 global-step:3266	 l-p:0.05784602090716362
epoch£º163	 i:7 	 global-step:3267	 l-p:0.05802489444613457
epoch£º163	 i:8 	 global-step:3268	 l-p:0.05794761702418327
epoch£º163	 i:9 	 global-step:3269	 l-p:0.058172404766082764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.0398, 21.8765, 21.7416],
        [20.0398, 20.0913, 20.0451],
        [20.0398, 20.0513, 20.0403],
        [20.0398, 20.2977, 20.1109]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.05790875479578972 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05790875479578972 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7024], device='cuda:0')), ('power', tensor([-8.1782], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.05790875479578972
epoch£º164	 i:1 	 global-step:3281	 l-p:0.05815102532505989
epoch£º164	 i:2 	 global-step:3282	 l-p:0.05808483064174652
epoch£º164	 i:3 	 global-step:3283	 l-p:0.05813213810324669
epoch£º164	 i:4 	 global-step:3284	 l-p:0.05786151438951492
epoch£º164	 i:5 	 global-step:3285	 l-p:0.057889897376298904
epoch£º164	 i:6 	 global-step:3286	 l-p:0.057798080146312714
epoch£º164	 i:7 	 global-step:3287	 l-p:0.05784665793180466
epoch£º164	 i:8 	 global-step:3288	 l-p:0.057844627648591995
epoch£º164	 i:9 	 global-step:3289	 l-p:0.057854365557432175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.1330, 20.1330, 20.1330],
        [20.1330, 25.1689, 27.6691],
        [20.1330, 23.9214, 25.1629],
        [20.1330, 21.7587, 21.5354]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.05786774680018425 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05786774680018425 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7074], device='cuda:0')), ('power', tensor([-8.0984], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.05786774680018425
epoch£º165	 i:1 	 global-step:3301	 l-p:0.05822174996137619
epoch£º165	 i:2 	 global-step:3302	 l-p:0.05780494958162308
epoch£º165	 i:3 	 global-step:3303	 l-p:0.057836901396512985
epoch£º165	 i:4 	 global-step:3304	 l-p:0.05790787562727928
epoch£º165	 i:5 	 global-step:3305	 l-p:0.057784222066402435
epoch£º165	 i:6 	 global-step:3306	 l-p:0.05787971243262291
epoch£º165	 i:7 	 global-step:3307	 l-p:0.057857196778059006
epoch£º165	 i:8 	 global-step:3308	 l-p:0.057779986411333084
epoch£º165	 i:9 	 global-step:3309	 l-p:0.05803930014371872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.2260, 20.2493, 20.2275],
        [20.2260, 20.9483, 20.6064],
        [20.2260, 21.0320, 20.6806],
        [20.2260, 20.2263, 20.2260]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.05801008641719818 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05801008641719818 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6586], device='cuda:0')), ('power', tensor([-7.8744], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.05801008641719818
epoch£º166	 i:1 	 global-step:3321	 l-p:0.05797435715794563
epoch£º166	 i:2 	 global-step:3322	 l-p:0.05774438753724098
epoch£º166	 i:3 	 global-step:3323	 l-p:0.05793505534529686
epoch£º166	 i:4 	 global-step:3324	 l-p:0.05780334025621414
epoch£º166	 i:5 	 global-step:3325	 l-p:0.05781123414635658
epoch£º166	 i:6 	 global-step:3326	 l-p:0.05772111192345619
epoch£º166	 i:7 	 global-step:3327	 l-p:0.05797486752271652
epoch£º166	 i:8 	 global-step:3328	 l-p:0.05784027278423309
epoch£º166	 i:9 	 global-step:3329	 l-p:0.057778578251600266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.3196, 20.3196, 20.3196],
        [20.3196, 21.6968, 21.3927],
        [20.3196, 27.6214, 32.8532],
        [20.3196, 20.4664, 20.3478]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.0578995980322361 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0578995980322361 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6874], device='cuda:0')), ('power', tensor([-7.8468], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.0578995980322361
epoch£º167	 i:1 	 global-step:3341	 l-p:0.058084964752197266
epoch£º167	 i:2 	 global-step:3342	 l-p:0.05780799314379692
epoch£º167	 i:3 	 global-step:3343	 l-p:0.057749561965465546
epoch£º167	 i:4 	 global-step:3344	 l-p:0.05776454880833626
epoch£º167	 i:5 	 global-step:3345	 l-p:0.05775528773665428
epoch£º167	 i:6 	 global-step:3346	 l-p:0.057726867496967316
epoch£º167	 i:7 	 global-step:3347	 l-p:0.057697612792253494
epoch£º167	 i:8 	 global-step:3348	 l-p:0.057734906673431396
epoch£º167	 i:9 	 global-step:3349	 l-p:0.05798651650547981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.4134, 20.4230, 20.4138],
        [20.4134, 27.1801, 31.6900],
        [20.4134, 20.4136, 20.4134],
        [20.4134, 20.4136, 20.4134]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.057989828288555145 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057989828288555145 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6564], device='cuda:0')), ('power', tensor([-7.6504], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.057989828288555145
epoch£º168	 i:1 	 global-step:3361	 l-p:0.057732343673706055
epoch£º168	 i:2 	 global-step:3362	 l-p:0.0577213279902935
epoch£º168	 i:3 	 global-step:3363	 l-p:0.05778977647423744
epoch£º168	 i:4 	 global-step:3364	 l-p:0.057729288935661316
epoch£º168	 i:5 	 global-step:3365	 l-p:0.05773959681391716
epoch£º168	 i:6 	 global-step:3366	 l-p:0.05789561942219734
epoch£º168	 i:7 	 global-step:3367	 l-p:0.057705555111169815
epoch£º168	 i:8 	 global-step:3368	 l-p:0.05769222974777222
epoch£º168	 i:9 	 global-step:3369	 l-p:0.05782961845397949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.5077, 20.5113, 20.5077],
        [20.5077, 20.7529, 20.5722],
        [20.5077, 21.8100, 21.4837],
        [20.5077, 21.0978, 20.7786]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.05766478553414345 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05766478553414345 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7791], device='cuda:0')), ('power', tensor([-7.7912], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.05766478553414345
epoch£º169	 i:1 	 global-step:3381	 l-p:0.05780993402004242
epoch£º169	 i:2 	 global-step:3382	 l-p:0.05786370113492012
epoch£º169	 i:3 	 global-step:3383	 l-p:0.05773166939616203
epoch£º169	 i:4 	 global-step:3384	 l-p:0.0578329935669899
epoch£º169	 i:5 	 global-step:3385	 l-p:0.057705268263816833
epoch£º169	 i:6 	 global-step:3386	 l-p:0.057656943798065186
epoch£º169	 i:7 	 global-step:3387	 l-p:0.05770953372120857
epoch£º169	 i:8 	 global-step:3388	 l-p:0.0578378289937973
epoch£º169	 i:9 	 global-step:3389	 l-p:0.05763215944170952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.6023, 22.3895, 22.2066],
        [20.6023, 20.6161, 20.6029],
        [20.6023, 20.9030, 20.6920],
        [20.6023, 20.6126, 20.6027]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.05789047107100487 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05789047107100487 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6806], device='cuda:0')), ('power', tensor([-7.4378], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.05789047107100487
epoch£º170	 i:1 	 global-step:3401	 l-p:0.057898588478565216
epoch£º170	 i:2 	 global-step:3402	 l-p:0.057602424174547195
epoch£º170	 i:3 	 global-step:3403	 l-p:0.057648565620183945
epoch£º170	 i:4 	 global-step:3404	 l-p:0.057617511600255966
epoch£º170	 i:5 	 global-step:3405	 l-p:0.057600412517786026
epoch£º170	 i:6 	 global-step:3406	 l-p:0.05761457234621048
epoch£º170	 i:7 	 global-step:3407	 l-p:0.057681187987327576
epoch£º170	 i:8 	 global-step:3408	 l-p:0.05769283324480057
epoch£º170	 i:9 	 global-step:3409	 l-p:0.0578201487660408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.6972, 24.4306, 25.5578],
        [20.6972, 20.7192, 20.6986],
        [20.6972, 21.0180, 20.7966],
        [20.6972, 20.6976, 20.6972]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.05784263461828232 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05784263461828232 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6658], device='cuda:0')), ('power', tensor([-7.3619], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.05784263461828232
epoch£º171	 i:1 	 global-step:3421	 l-p:0.05770556628704071
epoch£º171	 i:2 	 global-step:3422	 l-p:0.057583607733249664
epoch£º171	 i:3 	 global-step:3423	 l-p:0.05762173980474472
epoch£º171	 i:4 	 global-step:3424	 l-p:0.05781101435422897
epoch£º171	 i:5 	 global-step:3425	 l-p:0.05764729902148247
epoch£º171	 i:6 	 global-step:3426	 l-p:0.05758451297879219
epoch£º171	 i:7 	 global-step:3427	 l-p:0.057673171162605286
epoch£º171	 i:8 	 global-step:3428	 l-p:0.057573795318603516
epoch£º171	 i:9 	 global-step:3429	 l-p:0.05764787644147873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.7927, 20.8028, 20.7931],
        [20.7927, 20.8843, 20.8056],
        [20.7927, 23.1170, 23.1874],
        [20.7927, 20.7927, 20.7927]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.05754866451025009 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05754866451025009 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7985], device='cuda:0')), ('power', tensor([-7.5140], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.05754866451025009
epoch£º172	 i:1 	 global-step:3441	 l-p:0.0575912743806839
epoch£º172	 i:2 	 global-step:3442	 l-p:0.057569149881601334
epoch£º172	 i:3 	 global-step:3443	 l-p:0.05755306035280228
epoch£º172	 i:4 	 global-step:3444	 l-p:0.05783244967460632
epoch£º172	 i:5 	 global-step:3445	 l-p:0.05774770677089691
epoch£º172	 i:6 	 global-step:3446	 l-p:0.05773260071873665
epoch£º172	 i:7 	 global-step:3447	 l-p:0.05754962936043739
epoch£º172	 i:8 	 global-step:3448	 l-p:0.05756575986742973
epoch£º172	 i:9 	 global-step:3449	 l-p:0.05762607604265213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[20.8882, 27.9180, 32.6637],
        [20.8882, 21.8834, 21.5143],
        [20.8882, 25.6590, 27.7627],
        [20.8882, 21.9831, 21.6185]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.05763818696141243 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05763818696141243 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7064], device='cuda:0')), ('power', tensor([-7.2265], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.05763818696141243
epoch£º173	 i:1 	 global-step:3461	 l-p:0.05770673602819443
epoch£º173	 i:2 	 global-step:3462	 l-p:0.057534441351890564
epoch£º173	 i:3 	 global-step:3463	 l-p:0.05750175192952156
epoch£º173	 i:4 	 global-step:3464	 l-p:0.05748907849192619
epoch£º173	 i:5 	 global-step:3465	 l-p:0.057551316916942596
epoch£º173	 i:6 	 global-step:3466	 l-p:0.05752506107091904
epoch£º173	 i:7 	 global-step:3467	 l-p:0.057655028998851776
epoch£º173	 i:8 	 global-step:3468	 l-p:0.057617224752902985
epoch£º173	 i:9 	 global-step:3469	 l-p:0.05772623419761658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.9841, 26.8265, 30.0949],
        [20.9841, 21.1956, 21.0340],
        [20.9841, 21.8800, 21.5107],
        [20.9841, 22.4124, 22.0983]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.05752968788146973 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05752968788146973 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7648], device='cuda:0')), ('power', tensor([-7.2444], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.05752968788146973
epoch£º174	 i:1 	 global-step:3481	 l-p:0.05757560580968857
epoch£º174	 i:2 	 global-step:3482	 l-p:0.05751130357384682
epoch£º174	 i:3 	 global-step:3483	 l-p:0.057508766651153564
epoch£º174	 i:4 	 global-step:3484	 l-p:0.057694561779499054
epoch£º174	 i:5 	 global-step:3485	 l-p:0.05754927545785904
epoch£º174	 i:6 	 global-step:3486	 l-p:0.05744939297437668
epoch£º174	 i:7 	 global-step:3487	 l-p:0.05764704570174217
epoch£º174	 i:8 	 global-step:3488	 l-p:0.05748876556754112
epoch£º174	 i:9 	 global-step:3489	 l-p:0.05762212723493576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.0805, 21.1202, 21.0839],
        [21.0805, 21.2348, 21.1103],
        [21.0805, 21.6837, 21.3561],
        [21.0805, 21.9536, 21.5840]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.05745718255639076 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05745718255639076 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8166], device='cuda:0')), ('power', tensor([-7.2482], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.05745718255639076
epoch£º175	 i:1 	 global-step:3501	 l-p:0.057470016181468964
epoch£º175	 i:2 	 global-step:3502	 l-p:0.057568587362766266
epoch£º175	 i:3 	 global-step:3503	 l-p:0.05743971839547157
epoch£º175	 i:4 	 global-step:3504	 l-p:0.05761956423521042
epoch£º175	 i:5 	 global-step:3505	 l-p:0.05764321982860565
epoch£º175	 i:6 	 global-step:3506	 l-p:0.05747143551707268
epoch£º175	 i:7 	 global-step:3507	 l-p:0.057573866099119186
epoch£º175	 i:8 	 global-step:3508	 l-p:0.057519394904375076
epoch£º175	 i:9 	 global-step:3509	 l-p:0.057446710765361786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.1774, 21.1774, 21.1774],
        [21.1774, 21.4513, 21.2530],
        [21.1774, 25.3587, 26.8398],
        [21.1774, 23.1934, 23.0809]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.057585638016462326 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057585638016462326 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7441], device='cuda:0')), ('power', tensor([-6.9548], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.057585638016462326
epoch£º176	 i:1 	 global-step:3521	 l-p:0.05739704146981239
epoch£º176	 i:2 	 global-step:3522	 l-p:0.05745350196957588
epoch£º176	 i:3 	 global-step:3523	 l-p:0.05738891288638115
epoch£º176	 i:4 	 global-step:3524	 l-p:0.057476967573165894
epoch£º176	 i:5 	 global-step:3525	 l-p:0.05739321932196617
epoch£º176	 i:6 	 global-step:3526	 l-p:0.05776494741439819
epoch£º176	 i:7 	 global-step:3527	 l-p:0.05752507969737053
epoch£º176	 i:8 	 global-step:3528	 l-p:0.05743122845888138
epoch£º176	 i:9 	 global-step:3529	 l-p:0.05742805451154709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.2746, 28.8059, 34.1207],
        [21.2746, 21.2875, 21.2752],
        [21.2746, 22.4489, 22.0824],
        [21.2746, 21.2806, 21.2748]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.05746934935450554 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05746934935450554 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7573], device='cuda:0')), ('power', tensor([-6.9394], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.05746934935450554
epoch£º177	 i:1 	 global-step:3541	 l-p:0.05762606859207153
epoch£º177	 i:2 	 global-step:3542	 l-p:0.05737609416246414
epoch£º177	 i:3 	 global-step:3543	 l-p:0.057548873126506805
epoch£º177	 i:4 	 global-step:3544	 l-p:0.057398948818445206
epoch£º177	 i:5 	 global-step:3545	 l-p:0.057388827204704285
epoch£º177	 i:6 	 global-step:3546	 l-p:0.05742921680212021
epoch£º177	 i:7 	 global-step:3547	 l-p:0.057341378182172775
epoch£º177	 i:8 	 global-step:3548	 l-p:0.05749515816569328
epoch£º177	 i:9 	 global-step:3549	 l-p:0.05740852653980255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.3724, 21.3762, 21.3724],
        [21.3724, 21.3727, 21.3724],
        [21.3724, 21.3724, 21.3723],
        [21.3724, 22.2280, 21.8551]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.05765990912914276 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05765990912914276 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7204], device='cuda:0')), ('power', tensor([-6.6889], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.05765990912914276
epoch£º178	 i:1 	 global-step:3561	 l-p:0.057369306683540344
epoch£º178	 i:2 	 global-step:3562	 l-p:0.057310886681079865
epoch£º178	 i:3 	 global-step:3563	 l-p:0.05745802819728851
epoch£º178	 i:4 	 global-step:3564	 l-p:0.05746804177761078
epoch£º178	 i:5 	 global-step:3565	 l-p:0.0573405958712101
epoch£º178	 i:6 	 global-step:3566	 l-p:0.057494860142469406
epoch£º178	 i:7 	 global-step:3567	 l-p:0.05737491324543953
epoch£º178	 i:8 	 global-step:3568	 l-p:0.05732918158173561
epoch£º178	 i:9 	 global-step:3569	 l-p:0.05731615796685219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.4705, 21.4932, 21.4719],
        [21.4705, 21.8139, 21.5788],
        [21.4705, 21.7970, 21.5702],
        [21.4705, 21.4800, 21.4708]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.05731801688671112 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05731801688671112 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8267], device='cuda:0')), ('power', tensor([-6.7881], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.05731801688671112
epoch£º179	 i:1 	 global-step:3581	 l-p:0.0574265755712986
epoch£º179	 i:2 	 global-step:3582	 l-p:0.057356398552656174
epoch£º179	 i:3 	 global-step:3583	 l-p:0.05735881254076958
epoch£º179	 i:4 	 global-step:3584	 l-p:0.05738447979092598
epoch£º179	 i:5 	 global-step:3585	 l-p:0.05727846175432205
epoch£º179	 i:6 	 global-step:3586	 l-p:0.057559918612241745
epoch£º179	 i:7 	 global-step:3587	 l-p:0.05734484642744064
epoch£º179	 i:8 	 global-step:3588	 l-p:0.05727771669626236
epoch£º179	 i:9 	 global-step:3589	 l-p:0.05745643377304077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.5685, 21.5850, 21.5693],
        [21.5685, 21.5801, 21.5690],
        [21.5685, 23.5570, 23.4119],
        [21.5685, 21.5685, 21.5685]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.05725380778312683 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05725380778312683 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8641], device='cuda:0')), ('power', tensor([-6.7404], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.05725380778312683
epoch£º180	 i:1 	 global-step:3601	 l-p:0.057266563177108765
epoch£º180	 i:2 	 global-step:3602	 l-p:0.05748536437749863
epoch£º180	 i:3 	 global-step:3603	 l-p:0.05727697163820267
epoch£º180	 i:4 	 global-step:3604	 l-p:0.05755164101719856
epoch£º180	 i:5 	 global-step:3605	 l-p:0.05727420747280121
epoch£º180	 i:6 	 global-step:3606	 l-p:0.05744520202279091
epoch£º180	 i:7 	 global-step:3607	 l-p:0.057248782366514206
epoch£º180	 i:8 	 global-step:3608	 l-p:0.05733688175678253
epoch£º180	 i:9 	 global-step:3609	 l-p:0.05726710334420204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.6672, 21.7888, 21.6871],
        [21.6672, 22.6000, 22.2176],
        [21.6672, 25.1354, 25.9359],
        [21.6672, 26.7952, 29.1587]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.057254426181316376 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057254426181316376 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8213], device='cuda:0')), ('power', tensor([-6.6054], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.057254426181316376
epoch£º181	 i:1 	 global-step:3621	 l-p:0.05724471062421799
epoch£º181	 i:2 	 global-step:3622	 l-p:0.057404521852731705
epoch£º181	 i:3 	 global-step:3623	 l-p:0.05727110803127289
epoch£º181	 i:4 	 global-step:3624	 l-p:0.05737948417663574
epoch£º181	 i:5 	 global-step:3625	 l-p:0.05726167932152748
epoch£º181	 i:6 	 global-step:3626	 l-p:0.057245973497629166
epoch£º181	 i:7 	 global-step:3627	 l-p:0.05733166262507439
epoch£º181	 i:8 	 global-step:3628	 l-p:0.05742994695901871
epoch£º181	 i:9 	 global-step:3629	 l-p:0.05722823366522789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.7662, 21.9151, 21.7937],
        [21.7662, 22.4602, 22.1053],
        [21.7662, 23.1857, 22.8444],
        [21.7662, 21.7662, 21.7662]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.057254523038864136 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057254523038864136 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8344], device='cuda:0')), ('power', tensor([-6.5205], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.057254523038864136
epoch£º182	 i:1 	 global-step:3641	 l-p:0.057404279708862305
epoch£º182	 i:2 	 global-step:3642	 l-p:0.05719825252890587
epoch£º182	 i:3 	 global-step:3643	 l-p:0.057205989956855774
epoch£º182	 i:4 	 global-step:3644	 l-p:0.057225797325372696
epoch£º182	 i:5 	 global-step:3645	 l-p:0.05746522918343544
epoch£º182	 i:6 	 global-step:3646	 l-p:0.05719335377216339
epoch£º182	 i:7 	 global-step:3647	 l-p:0.05717252567410469
epoch£º182	 i:8 	 global-step:3648	 l-p:0.05720461532473564
epoch£º182	 i:9 	 global-step:3649	 l-p:0.05737541988492012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.8657, 24.7447, 25.0873],
        [21.8657, 23.3911, 23.0710],
        [21.8657, 22.0071, 21.8909],
        [21.8657, 27.3148, 29.9928]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.05719833821058273 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05719833821058273 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8343], device='cuda:0')), ('power', tensor([-6.3980], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.05719833821058273
epoch£º183	 i:1 	 global-step:3661	 l-p:0.05717405304312706
epoch£º183	 i:2 	 global-step:3662	 l-p:0.057156771421432495
epoch£º183	 i:3 	 global-step:3663	 l-p:0.057180993258953094
epoch£º183	 i:4 	 global-step:3664	 l-p:0.05729948729276657
epoch£º183	 i:5 	 global-step:3665	 l-p:0.05720946192741394
epoch£º183	 i:6 	 global-step:3666	 l-p:0.057194869965314865
epoch£º183	 i:7 	 global-step:3667	 l-p:0.05716755613684654
epoch£º183	 i:8 	 global-step:3668	 l-p:0.05737452208995819
epoch£º183	 i:9 	 global-step:3669	 l-p:0.057392578572034836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.9652, 22.9118, 22.5238],
        [21.9652, 27.9250, 31.1575],
        [21.9652, 22.4757, 22.1692],
        [21.9652, 21.9651, 21.9652]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.05732866749167442 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05732866749167442 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7720], device='cuda:0')), ('power', tensor([-6.1730], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.05732866749167442
epoch£º184	 i:1 	 global-step:3681	 l-p:0.05725051835179329
epoch£º184	 i:2 	 global-step:3682	 l-p:0.057314012199640274
epoch£º184	 i:3 	 global-step:3683	 l-p:0.05715830624103546
epoch£º184	 i:4 	 global-step:3684	 l-p:0.05712485313415527
epoch£º184	 i:5 	 global-step:3685	 l-p:0.057267267256975174
epoch£º184	 i:6 	 global-step:3686	 l-p:0.05715436860918999
epoch£º184	 i:7 	 global-step:3687	 l-p:0.057175569236278534
epoch£º184	 i:8 	 global-step:3688	 l-p:0.05713266879320145
epoch£º184	 i:9 	 global-step:3689	 l-p:0.05709732696413994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.0659, 23.5002, 23.1526],
        [22.0659, 22.0659, 22.0659],
        [22.0659, 22.3316, 22.1358],
        [22.0659, 27.8005, 30.7612]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.057237375527620316 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057237375527620316 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7836], device='cuda:0')), ('power', tensor([-6.0772], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.057237375527620316
epoch£º185	 i:1 	 global-step:3701	 l-p:0.05712588131427765
epoch£º185	 i:2 	 global-step:3702	 l-p:0.05727623775601387
epoch£º185	 i:3 	 global-step:3703	 l-p:0.05712983384728432
epoch£º185	 i:4 	 global-step:3704	 l-p:0.057104986160993576
epoch£º185	 i:5 	 global-step:3705	 l-p:0.057244930416345596
epoch£º185	 i:6 	 global-step:3706	 l-p:0.05712607130408287
epoch£º185	 i:7 	 global-step:3707	 l-p:0.05708516761660576
epoch£º185	 i:8 	 global-step:3708	 l-p:0.0572451688349247
epoch£º185	 i:9 	 global-step:3709	 l-p:0.05707993358373642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.1665, 22.1665, 22.1665],
        [22.1665, 25.5199, 26.1821],
        [22.1665, 25.2118, 25.6458],
        [22.1665, 23.5035, 23.1351]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.0571039617061615 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0571039617061615 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8351], device='cuda:0')), ('power', tensor([-6.0423], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.0571039617061615
epoch£º186	 i:1 	 global-step:3721	 l-p:0.05719916149973869
epoch£º186	 i:2 	 global-step:3722	 l-p:0.05707894265651703
epoch£º186	 i:3 	 global-step:3723	 l-p:0.05706162378191948
epoch£º186	 i:4 	 global-step:3724	 l-p:0.05719764530658722
epoch£º186	 i:5 	 global-step:3725	 l-p:0.05726829543709755
epoch£º186	 i:6 	 global-step:3726	 l-p:0.05702173709869385
epoch£º186	 i:7 	 global-step:3727	 l-p:0.057238634675741196
epoch£º186	 i:8 	 global-step:3728	 l-p:0.05710151419043541
epoch£º186	 i:9 	 global-step:3729	 l-p:0.057039763778448105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.2674, 22.2687, 22.2674],
        [22.2674, 24.1645, 23.9482],
        [22.2674, 25.2031, 25.5529],
        [22.2674, 22.2675, 22.2674]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.0571766123175621 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0571766123175621 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8172], device='cuda:0')), ('power', tensor([-5.8699], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.0571766123175621
epoch£º187	 i:1 	 global-step:3741	 l-p:0.057070858776569366
epoch£º187	 i:2 	 global-step:3742	 l-p:0.057098422199487686
epoch£º187	 i:3 	 global-step:3743	 l-p:0.05709352344274521
epoch£º187	 i:4 	 global-step:3744	 l-p:0.05702032893896103
epoch£º187	 i:5 	 global-step:3745	 l-p:0.05701939016580582
epoch£º187	 i:6 	 global-step:3746	 l-p:0.05723316967487335
epoch£º187	 i:7 	 global-step:3747	 l-p:0.05701945722103119
epoch£º187	 i:8 	 global-step:3748	 l-p:0.05704895406961441
epoch£º187	 i:9 	 global-step:3749	 l-p:0.05718836560845375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.3687, 28.4458, 31.7430],
        [22.3687, 23.4191, 23.0218],
        [22.3687, 22.3690, 22.3687],
        [22.3687, 28.3741, 31.5895]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.05700461566448212 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05700461566448212 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8816], device='cuda:0')), ('power', tensor([-5.9222], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.05700461566448212
epoch£º188	 i:1 	 global-step:3761	 l-p:0.05701681971549988
epoch£º188	 i:2 	 global-step:3762	 l-p:0.056986331939697266
epoch£º188	 i:3 	 global-step:3763	 l-p:0.05703224614262581
epoch£º188	 i:4 	 global-step:3764	 l-p:0.05713099241256714
epoch£º188	 i:5 	 global-step:3765	 l-p:0.05706406384706497
epoch£º188	 i:6 	 global-step:3766	 l-p:0.05699072778224945
epoch£º188	 i:7 	 global-step:3767	 l-p:0.057025764137506485
epoch£º188	 i:8 	 global-step:3768	 l-p:0.057116586714982986
epoch£º188	 i:9 	 global-step:3769	 l-p:0.057260144501924515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.4701, 29.7660, 34.5135],
        [22.4701, 29.8571, 34.7200],
        [22.4701, 22.4704, 22.4701],
        [22.4701, 22.4701, 22.4701]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.05707036331295967 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05707036331295967 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8243], device='cuda:0')), ('power', tensor([-5.7562], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.05707036331295967
epoch£º189	 i:1 	 global-step:3781	 l-p:0.057063989341259
epoch£º189	 i:2 	 global-step:3782	 l-p:0.05699407309293747
epoch£º189	 i:3 	 global-step:3783	 l-p:0.057154349982738495
epoch£º189	 i:4 	 global-step:3784	 l-p:0.05693730339407921
epoch£º189	 i:5 	 global-step:3785	 l-p:0.057070549577474594
epoch£º189	 i:6 	 global-step:3786	 l-p:0.05697024241089821
epoch£º189	 i:7 	 global-step:3787	 l-p:0.05710013583302498
epoch£º189	 i:8 	 global-step:3788	 l-p:0.05697599798440933
epoch£º189	 i:9 	 global-step:3789	 l-p:0.056955497711896896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.5726, 22.5727, 22.5726],
        [22.5726, 22.5727, 22.5726],
        [22.5726, 22.5727, 22.5726],
        [22.5726, 22.8449, 22.6443]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.056977465748786926 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056977465748786926 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8681], device='cuda:0')), ('power', tensor([-5.6888], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.056977465748786926
epoch£º190	 i:1 	 global-step:3801	 l-p:0.05697760358452797
epoch£º190	 i:2 	 global-step:3802	 l-p:0.05706673488020897
epoch£º190	 i:3 	 global-step:3803	 l-p:0.056935299187898636
epoch£º190	 i:4 	 global-step:3804	 l-p:0.05693921074271202
epoch£º190	 i:5 	 global-step:3805	 l-p:0.05725741386413574
epoch£º190	 i:6 	 global-step:3806	 l-p:0.05694151669740677
epoch£º190	 i:7 	 global-step:3807	 l-p:0.056950125843286514
epoch£º190	 i:8 	 global-step:3808	 l-p:0.056978270411491394
epoch£º190	 i:9 	 global-step:3809	 l-p:0.056931667029857635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.6752, 29.8022, 34.2959],
        [22.6752, 23.4564, 23.0753],
        [22.6752, 22.6764, 22.6752],
        [22.6752, 22.6752, 22.6752]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.05707121267914772 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05707121267914772 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7660], device='cuda:0')), ('power', tensor([-5.3984], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.05707121267914772
epoch£º191	 i:1 	 global-step:3821	 l-p:0.05693823844194412
epoch£º191	 i:2 	 global-step:3822	 l-p:0.05700870230793953
epoch£º191	 i:3 	 global-step:3823	 l-p:0.0569925531744957
epoch£º191	 i:4 	 global-step:3824	 l-p:0.05690732225775719
epoch£º191	 i:5 	 global-step:3825	 l-p:0.056950051337480545
epoch£º191	 i:6 	 global-step:3826	 l-p:0.05690933018922806
epoch£º191	 i:7 	 global-step:3827	 l-p:0.056913089007139206
epoch£º191	 i:8 	 global-step:3828	 l-p:0.05689231678843498
epoch£º191	 i:9 	 global-step:3829	 l-p:0.057038143277168274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.7782, 23.1075, 22.8753],
        [22.7782, 22.9953, 22.8274],
        [22.7782, 29.4490, 33.3671],
        [22.7782, 23.2171, 22.9336]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.056907691061496735 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056907691061496735 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8697], device='cuda:0')), ('power', tensor([-5.4494], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.056907691061496735
epoch£º192	 i:1 	 global-step:3841	 l-p:0.05691276490688324
epoch£º192	 i:2 	 global-step:3842	 l-p:0.05694330483675003
epoch£º192	 i:3 	 global-step:3843	 l-p:0.05684564635157585
epoch£º192	 i:4 	 global-step:3844	 l-p:0.05708704888820648
epoch£º192	 i:5 	 global-step:3845	 l-p:0.05705396085977554
epoch£º192	 i:6 	 global-step:3846	 l-p:0.05697401985526085
epoch£º192	 i:7 	 global-step:3847	 l-p:0.056844595819711685
epoch£º192	 i:8 	 global-step:3848	 l-p:0.056863002479076385
epoch£º192	 i:9 	 global-step:3849	 l-p:0.05685669928789139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.8816, 29.1976, 32.6805],
        [22.8816, 23.1874, 22.9674],
        [22.8816, 22.9306, 22.8861],
        [22.8816, 22.8847, 22.8816]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.05684724822640419 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05684724822640419 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8994], device='cuda:0')), ('power', tensor([-5.4063], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.05684724822640419
epoch£º193	 i:1 	 global-step:3861	 l-p:0.05680902302265167
epoch£º193	 i:2 	 global-step:3862	 l-p:0.05690480023622513
epoch£º193	 i:3 	 global-step:3863	 l-p:0.05705244094133377
epoch£º193	 i:4 	 global-step:3864	 l-p:0.056857816874980927
epoch£º193	 i:5 	 global-step:3865	 l-p:0.056830935180187225
epoch£º193	 i:6 	 global-step:3866	 l-p:0.05695074796676636
epoch£º193	 i:7 	 global-step:3867	 l-p:0.05687297508120537
epoch£º193	 i:8 	 global-step:3868	 l-p:0.05702288821339607
epoch£º193	 i:9 	 global-step:3869	 l-p:0.056808777153491974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.9852, 22.9852, 22.9851],
        [22.9852, 27.7441, 29.5503],
        [22.9852, 25.1819, 25.0580],
        [22.9852, 23.5417, 23.2130]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.056789930909872055 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056789930909872055 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9411], device='cuda:0')), ('power', tensor([-5.3294], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.056789930909872055
epoch£º194	 i:1 	 global-step:3881	 l-p:0.056906603276729584
epoch£º194	 i:2 	 global-step:3882	 l-p:0.056803684681653976
epoch£º194	 i:3 	 global-step:3883	 l-p:0.05690068006515503
epoch£º194	 i:4 	 global-step:3884	 l-p:0.05677484720945358
epoch£º194	 i:5 	 global-step:3885	 l-p:0.05684250593185425
epoch£º194	 i:6 	 global-step:3886	 l-p:0.05689910799264908
epoch£º194	 i:7 	 global-step:3887	 l-p:0.056970443576574326
epoch£º194	 i:8 	 global-step:3888	 l-p:0.05690929666161537
epoch£º194	 i:9 	 global-step:3889	 l-p:0.056832119822502136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.0892, 24.0826, 23.6735],
        [23.0892, 27.5584, 29.0764],
        [23.0892, 23.1482, 23.0951],
        [23.0892, 26.9005, 27.8400]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.05680059269070625 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05680059269070625 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9008], device='cuda:0')), ('power', tensor([-5.1764], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.05680059269070625
epoch£º195	 i:1 	 global-step:3901	 l-p:0.05694226920604706
epoch£º195	 i:2 	 global-step:3902	 l-p:0.0568019263446331
epoch£º195	 i:3 	 global-step:3903	 l-p:0.056835200637578964
epoch£º195	 i:4 	 global-step:3904	 l-p:0.05686522275209427
epoch£º195	 i:5 	 global-step:3905	 l-p:0.056749846786260605
epoch£º195	 i:6 	 global-step:3906	 l-p:0.05673658102750778
epoch£º195	 i:7 	 global-step:3907	 l-p:0.05681336298584938
epoch£º195	 i:8 	 global-step:3908	 l-p:0.05672641471028328
epoch£º195	 i:9 	 global-step:3909	 l-p:0.057031597942113876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.1937, 23.1938, 23.1937],
        [23.1937, 23.3380, 23.2188],
        [23.1937, 23.2001, 23.1939],
        [23.1937, 23.3036, 23.2098]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.056825969368219376 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056825969368219376 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8349], device='cuda:0')), ('power', tensor([-4.9764], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.056825969368219376
epoch£º196	 i:1 	 global-step:3921	 l-p:0.05677925795316696
epoch£º196	 i:2 	 global-step:3922	 l-p:0.0567624568939209
epoch£º196	 i:3 	 global-step:3923	 l-p:0.056848518550395966
epoch£º196	 i:4 	 global-step:3924	 l-p:0.05678156390786171
epoch£º196	 i:5 	 global-step:3925	 l-p:0.05670984089374542
epoch£º196	 i:6 	 global-step:3926	 l-p:0.05688522383570671
epoch£º196	 i:7 	 global-step:3927	 l-p:0.056779298931360245
epoch£º196	 i:8 	 global-step:3928	 l-p:0.05673106014728546
epoch£º196	 i:9 	 global-step:3929	 l-p:0.05687510222196579
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.2987, 23.3101, 23.2991],
        [23.2987, 27.3729, 28.5128],
        [23.2987, 23.2986, 23.2987],
        [23.2987, 23.6068, 23.3845]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.05686860904097557 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05686860904097557 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8394], device='cuda:0')), ('power', tensor([-4.8370], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.05686860904097557
epoch£º197	 i:1 	 global-step:3941	 l-p:0.05671760067343712
epoch£º197	 i:2 	 global-step:3942	 l-p:0.05671693757176399
epoch£º197	 i:3 	 global-step:3943	 l-p:0.05682586878538132
epoch£º197	 i:4 	 global-step:3944	 l-p:0.05674585700035095
epoch£º197	 i:5 	 global-step:3945	 l-p:0.056671228259801865
epoch£º197	 i:6 	 global-step:3946	 l-p:0.05671820044517517
epoch£º197	 i:7 	 global-step:3947	 l-p:0.05676625296473503
epoch£º197	 i:8 	 global-step:3948	 l-p:0.05688123404979706
epoch£º197	 i:9 	 global-step:3949	 l-p:0.056743696331977844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.4040, 23.8282, 23.5483],
        [23.4040, 23.4072, 23.4041],
        [23.4040, 23.4040, 23.4040],
        [23.4040, 23.4313, 23.4058]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.05669206380844116 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05669206380844116 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9341], device='cuda:0')), ('power', tensor([-4.9029], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.05669206380844116
epoch£º198	 i:1 	 global-step:3961	 l-p:0.056758660823106766
epoch£º198	 i:2 	 global-step:3962	 l-p:0.05679367482662201
epoch£º198	 i:3 	 global-step:3963	 l-p:0.05667991191148758
epoch£º198	 i:4 	 global-step:3964	 l-p:0.05669014900922775
epoch£º198	 i:5 	 global-step:3965	 l-p:0.05667824670672417
epoch£º198	 i:6 	 global-step:3966	 l-p:0.056712787598371506
epoch£º198	 i:7 	 global-step:3967	 l-p:0.05671496316790581
epoch£º198	 i:8 	 global-step:3968	 l-p:0.05670750513672829
epoch£º198	 i:9 	 global-step:3969	 l-p:0.056905850768089294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.5096, 23.5370, 23.5114],
        [23.5096, 23.7622, 23.5713],
        [23.5096, 23.5098, 23.5096],
        [23.5096, 25.1214, 24.7668]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.056825947016477585 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056825947016477585 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8772], device='cuda:0')), ('power', tensor([-4.6438], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.056825947016477585
epoch£º199	 i:1 	 global-step:3981	 l-p:0.05666792765259743
epoch£º199	 i:2 	 global-step:3982	 l-p:0.05671290308237076
epoch£º199	 i:3 	 global-step:3983	 l-p:0.056618403643369675
epoch£º199	 i:4 	 global-step:3984	 l-p:0.05663871020078659
epoch£º199	 i:5 	 global-step:3985	 l-p:0.05664161592721939
epoch£º199	 i:6 	 global-step:3986	 l-p:0.0566592775285244
epoch£º199	 i:7 	 global-step:3987	 l-p:0.05674845725297928
epoch£º199	 i:8 	 global-step:3988	 l-p:0.056756552308797836
epoch£º199	 i:9 	 global-step:3989	 l-p:0.05674539506435394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.6159, 23.6159, 23.6159],
        [23.6159, 23.6306, 23.6165],
        [23.6159, 23.9016, 23.6910],
        [23.6159, 27.1589, 27.8343]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.05662547051906586 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05662547051906586 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9488], device='cuda:0')), ('power', tensor([-4.6648], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.05662547051906586
epoch£º200	 i:1 	 global-step:4001	 l-p:0.05660001188516617
epoch£º200	 i:2 	 global-step:4002	 l-p:0.05671121925115585
epoch£º200	 i:3 	 global-step:4003	 l-p:0.05664642155170441
epoch£º200	 i:4 	 global-step:4004	 l-p:0.056616976857185364
epoch£º200	 i:5 	 global-step:4005	 l-p:0.05662335455417633
epoch£º200	 i:6 	 global-step:4006	 l-p:0.05668440833687782
epoch£º200	 i:7 	 global-step:4007	 l-p:0.05663522705435753
epoch£º200	 i:8 	 global-step:4008	 l-p:0.056835174560546875
epoch£º200	 i:9 	 global-step:4009	 l-p:0.05680482089519501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.6584, 23.6620, 23.6584],
        [23.6584, 28.3376, 29.9828],
        [23.6584, 23.6586, 23.6584],
        [23.6584, 25.0925, 24.6978]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.05680733546614647 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05680733546614647 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8511], device='cuda:0')), ('power', tensor([-4.3948], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.05680733546614647
epoch£º201	 i:1 	 global-step:4021	 l-p:0.05677258223295212
epoch£º201	 i:2 	 global-step:4022	 l-p:0.05660555884242058
epoch£º201	 i:3 	 global-step:4023	 l-p:0.056620460003614426
epoch£º201	 i:4 	 global-step:4024	 l-p:0.05660242587327957
epoch£º201	 i:5 	 global-step:4025	 l-p:0.056597430258989334
epoch£º201	 i:6 	 global-step:4026	 l-p:0.05663444474339485
epoch£º201	 i:7 	 global-step:4027	 l-p:0.05675971508026123
epoch£º201	 i:8 	 global-step:4028	 l-p:0.05665317550301552
epoch£º201	 i:9 	 global-step:4029	 l-p:0.05660391226410866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.7013, 23.7103, 23.7016],
        [23.7013, 23.7049, 23.7014],
        [23.7013, 25.0661, 24.6603],
        [23.7013, 24.0108, 23.7868]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.056645870208740234 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056645870208740234 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9161], device='cuda:0')), ('power', tensor([-4.5791], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.056645870208740234
epoch£º202	 i:1 	 global-step:4041	 l-p:0.05677469074726105
epoch£º202	 i:2 	 global-step:4042	 l-p:0.05665731430053711
epoch£º202	 i:3 	 global-step:4043	 l-p:0.05671972781419754
epoch£º202	 i:4 	 global-step:4044	 l-p:0.05673393979668617
epoch£º202	 i:5 	 global-step:4045	 l-p:0.056595999747514725
epoch£º202	 i:6 	 global-step:4046	 l-p:0.05658449977636337
epoch£º202	 i:7 	 global-step:4047	 l-p:0.05657050013542175
epoch£º202	 i:8 	 global-step:4048	 l-p:0.0566568449139595
epoch£º202	 i:9 	 global-step:4049	 l-p:0.05659054219722748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.7443, 29.0016, 31.2007],
        [23.7443, 25.2017, 24.8083],
        [23.7443, 24.7740, 24.3522],
        [23.7443, 23.7533, 23.7446]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.05676634982228279 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05676634982228279 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8822], device='cuda:0')), ('power', tensor([-4.4279], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.05676634982228279
epoch£º203	 i:1 	 global-step:4061	 l-p:0.05670858174562454
epoch£º203	 i:2 	 global-step:4062	 l-p:0.05657863989472389
epoch£º203	 i:3 	 global-step:4063	 l-p:0.05658800154924393
epoch£º203	 i:4 	 global-step:4064	 l-p:0.05656162276864052
epoch£º203	 i:5 	 global-step:4065	 l-p:0.056673597544431686
epoch£º203	 i:6 	 global-step:4066	 l-p:0.05671459063887596
epoch£º203	 i:7 	 global-step:4067	 l-p:0.05661250650882721
epoch£º203	 i:8 	 global-step:4068	 l-p:0.056584279984235764
epoch£º203	 i:9 	 global-step:4069	 l-p:0.05661449208855629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.7875, 28.7602, 30.6705],
        [23.7875, 23.7875, 23.7875],
        [23.7875, 28.6934, 30.5388],
        [23.7875, 25.4203, 25.0615]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.05658416450023651 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05658416450023651 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9362], device='cuda:0')), ('power', tensor([-4.4447], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.05658416450023651
epoch£º204	 i:1 	 global-step:4081	 l-p:0.056642040610313416
epoch£º204	 i:2 	 global-step:4082	 l-p:0.05663662776350975
epoch£º204	 i:3 	 global-step:4083	 l-p:0.05655645206570625
epoch£º204	 i:4 	 global-step:4084	 l-p:0.05658356472849846
epoch£º204	 i:5 	 global-step:4085	 l-p:0.05660029128193855
epoch£º204	 i:6 	 global-step:4086	 l-p:0.056745510548353195
epoch£º204	 i:7 	 global-step:4087	 l-p:0.05656838417053223
epoch£º204	 i:8 	 global-step:4088	 l-p:0.056684330105781555
epoch£º204	 i:9 	 global-step:4089	 l-p:0.0566738098859787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.8307, 25.0034, 24.5793],
        [23.8307, 23.8307, 23.8307],
        [23.8307, 23.8376, 23.8309],
        [23.8307, 23.8307, 23.8307]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.05663631483912468 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05663631483912468 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8893], device='cuda:0')), ('power', tensor([-4.3965], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.05663631483912468
epoch£º205	 i:1 	 global-step:4101	 l-p:0.0565590038895607
epoch£º205	 i:2 	 global-step:4102	 l-p:0.05678349360823631
epoch£º205	 i:3 	 global-step:4103	 l-p:0.05655553564429283
epoch£º205	 i:4 	 global-step:4104	 l-p:0.056544579565525055
epoch£º205	 i:5 	 global-step:4105	 l-p:0.056564316153526306
epoch£º205	 i:6 	 global-step:4106	 l-p:0.056567877531051636
epoch£º205	 i:7 	 global-step:4107	 l-p:0.05655207484960556
epoch£º205	 i:8 	 global-step:4108	 l-p:0.056805551052093506
epoch£º205	 i:9 	 global-step:4109	 l-p:0.05657956376671791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.8743, 24.6343, 24.2438],
        [23.8743, 31.5808, 36.5536],
        [23.8743, 29.1621, 31.3741],
        [23.8743, 23.8745, 23.8743]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.05656629428267479 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05656629428267479 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9237], device='cuda:0')), ('power', tensor([-4.3272], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.05656629428267479
epoch£º206	 i:1 	 global-step:4121	 l-p:0.0565933920443058
epoch£º206	 i:2 	 global-step:4122	 l-p:0.05660422146320343
epoch£º206	 i:3 	 global-step:4123	 l-p:0.05662824586033821
epoch£º206	 i:4 	 global-step:4124	 l-p:0.056675318628549576
epoch£º206	 i:5 	 global-step:4125	 l-p:0.05655709281563759
epoch£º206	 i:6 	 global-step:4126	 l-p:0.05651460960507393
epoch£º206	 i:7 	 global-step:4127	 l-p:0.05666271969676018
epoch£º206	 i:8 	 global-step:4128	 l-p:0.05666794255375862
epoch£º206	 i:9 	 global-step:4129	 l-p:0.056551069021224976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.9181, 24.4265, 24.1093],
        [23.9181, 23.9181, 23.9181],
        [23.9181, 27.2211, 27.6935],
        [23.9181, 26.2154, 26.0890]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.05661045387387276 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05661045387387276 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9331], device='cuda:0')), ('power', tensor([-4.3413], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.05661045387387276
epoch£º207	 i:1 	 global-step:4141	 l-p:0.05653494596481323
epoch£º207	 i:2 	 global-step:4142	 l-p:0.05679793283343315
epoch£º207	 i:3 	 global-step:4143	 l-p:0.05653223395347595
epoch£º207	 i:4 	 global-step:4144	 l-p:0.05653887242078781
epoch£º207	 i:5 	 global-step:4145	 l-p:0.056579455733299255
epoch£º207	 i:6 	 global-step:4146	 l-p:0.05655747279524803
epoch£º207	 i:7 	 global-step:4147	 l-p:0.05657833069562912
epoch£º207	 i:8 	 global-step:4148	 l-p:0.056533947587013245
epoch£º207	 i:9 	 global-step:4149	 l-p:0.05662986636161804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.9622, 23.9672, 23.9623],
        [23.9622, 28.9328, 30.8184],
        [23.9622, 25.9329, 25.6694],
        [23.9622, 23.9622, 23.9622]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.056506626307964325 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056506626307964325 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9927], device='cuda:0')), ('power', tensor([-4.3917], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.056506626307964325
epoch£º208	 i:1 	 global-step:4161	 l-p:0.05669645220041275
epoch£º208	 i:2 	 global-step:4162	 l-p:0.056508053094148636
epoch£º208	 i:3 	 global-step:4163	 l-p:0.0565861277282238
epoch£º208	 i:4 	 global-step:4164	 l-p:0.05665784701704979
epoch£º208	 i:5 	 global-step:4165	 l-p:0.056552961468696594
epoch£º208	 i:6 	 global-step:4166	 l-p:0.056546229869127274
epoch£º208	 i:7 	 global-step:4167	 l-p:0.05655796825885773
epoch£º208	 i:8 	 global-step:4168	 l-p:0.056514643132686615
epoch£º208	 i:9 	 global-step:4169	 l-p:0.05663871765136719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[24.0064, 27.9021, 28.8191],
        [24.0064, 31.9768, 37.2564],
        [24.0064, 25.0424, 24.6160],
        [24.0064, 27.1713, 27.5407]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.056527070701122284 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056527070701122284 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9367], device='cuda:0')), ('power', tensor([-4.1543], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:0.056527070701122284
epoch£º209	 i:1 	 global-step:4181	 l-p:0.05659788101911545
epoch£º209	 i:2 	 global-step:4182	 l-p:0.056534163653850555
epoch£º209	 i:3 	 global-step:4183	 l-p:0.056548576802015305
epoch£º209	 i:4 	 global-step:4184	 l-p:0.05648306757211685
epoch£º209	 i:5 	 global-step:4185	 l-p:0.056546010076999664
epoch£º209	 i:6 	 global-step:4186	 l-p:0.0565926618874073
epoch£º209	 i:7 	 global-step:4187	 l-p:0.056589916348457336
epoch£º209	 i:8 	 global-step:4188	 l-p:0.056690242141485214
epoch£º209	 i:9 	 global-step:4189	 l-p:0.05652816593647003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.0508, 24.0508, 24.0508],
        [24.0508, 24.0508, 24.0508],
        [24.0508, 26.2034, 26.0064],
        [24.0508, 24.0508, 24.0508]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.05649242177605629 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05649242177605629 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9759], device='cuda:0')), ('power', tensor([-4.2399], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.05649242177605629
epoch£º210	 i:1 	 global-step:4201	 l-p:0.05657656863331795
epoch£º210	 i:2 	 global-step:4202	 l-p:0.05666889622807503
epoch£º210	 i:3 	 global-step:4203	 l-p:0.0566890612244606
epoch£º210	 i:4 	 global-step:4204	 l-p:0.05651237815618515
epoch£º210	 i:5 	 global-step:4205	 l-p:0.05647338926792145
epoch£º210	 i:6 	 global-step:4206	 l-p:0.056501712650060654
epoch£º210	 i:7 	 global-step:4207	 l-p:0.056503619998693466
epoch£º210	 i:8 	 global-step:4208	 l-p:0.056609801948070526
epoch£º210	 i:9 	 global-step:4209	 l-p:0.056482408195734024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.0955, 28.2793, 29.4282],
        [24.0955, 24.0962, 24.0955],
        [24.0955, 26.0781, 25.8130],
        [24.0955, 24.0958, 24.0955]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.05659136548638344 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05659136548638344 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9342], device='cuda:0')), ('power', tensor([-4.1212], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.05659136548638344
epoch£º211	 i:1 	 global-step:4221	 l-p:0.05649396777153015
epoch£º211	 i:2 	 global-step:4222	 l-p:0.05655856058001518
epoch£º211	 i:3 	 global-step:4223	 l-p:0.05664391815662384
epoch£º211	 i:4 	 global-step:4224	 l-p:0.056498393416404724
epoch£º211	 i:5 	 global-step:4225	 l-p:0.056568536907434464
epoch£º211	 i:6 	 global-step:4226	 l-p:0.05649518966674805
epoch£º211	 i:7 	 global-step:4227	 l-p:0.056499775499105453
epoch£º211	 i:8 	 global-step:4228	 l-p:0.05655607953667641
epoch£º211	 i:9 	 global-step:4229	 l-p:0.05647622048854828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.1405, 28.9606, 30.6792],
        [24.1405, 24.1496, 24.1408],
        [24.1405, 24.1405, 24.1405],
        [24.1405, 28.7385, 30.2503]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.05644797533750534 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05644797533750534 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0149], device='cuda:0')), ('power', tensor([-4.2096], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.05644797533750534
epoch£º212	 i:1 	 global-step:4241	 l-p:0.056489504873752594
epoch£º212	 i:2 	 global-step:4242	 l-p:0.05661603808403015
epoch£º212	 i:3 	 global-step:4243	 l-p:0.056476812809705734
epoch£º212	 i:4 	 global-step:4244	 l-p:0.05651712417602539
epoch£º212	 i:5 	 global-step:4245	 l-p:0.056515973061323166
epoch£º212	 i:6 	 global-step:4246	 l-p:0.05655546113848686
epoch£º212	 i:7 	 global-step:4247	 l-p:0.05660209432244301
epoch£º212	 i:8 	 global-step:4248	 l-p:0.05657677352428436
epoch£º212	 i:9 	 global-step:4249	 l-p:0.05645575374364853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.1855, 24.2282, 24.1890],
        [24.1855, 24.1855, 24.1855],
        [24.1855, 26.5103, 26.3825],
        [24.1855, 24.4786, 24.2627]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.05654267221689224 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05654267221689224 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8930], device='cuda:0')), ('power', tensor([-3.9818], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.05654267221689224
epoch£º213	 i:1 	 global-step:4261	 l-p:0.05673963576555252
epoch£º213	 i:2 	 global-step:4262	 l-p:0.05648604407906532
epoch£º213	 i:3 	 global-step:4263	 l-p:0.05652278661727905
epoch£º213	 i:4 	 global-step:4264	 l-p:0.05645297095179558
epoch£º213	 i:5 	 global-step:4265	 l-p:0.056469496339559555
epoch£º213	 i:6 	 global-step:4266	 l-p:0.05646263435482979
epoch£º213	 i:7 	 global-step:4267	 l-p:0.05646872520446777
epoch£º213	 i:8 	 global-step:4268	 l-p:0.05644336715340614
epoch£º213	 i:9 	 global-step:4269	 l-p:0.05653738975524902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.2309, 24.2404, 24.2313],
        [24.2309, 24.9322, 24.5517],
        [24.2309, 26.4749, 26.3082],
        [24.2309, 24.2309, 24.2309]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.05644924193620682 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05644924193620682 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9833], device='cuda:0')), ('power', tensor([-4.0685], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.05644924193620682
epoch£º214	 i:1 	 global-step:4281	 l-p:0.05645696073770523
epoch£º214	 i:2 	 global-step:4282	 l-p:0.05651890113949776
epoch£º214	 i:3 	 global-step:4283	 l-p:0.05647611990571022
epoch£º214	 i:4 	 global-step:4284	 l-p:0.05658069625496864
epoch£º214	 i:5 	 global-step:4285	 l-p:0.05659102648496628
epoch£º214	 i:6 	 global-step:4286	 l-p:0.056482914835214615
epoch£º214	 i:7 	 global-step:4287	 l-p:0.05655921250581741
epoch£º214	 i:8 	 global-step:4288	 l-p:0.05642743036150932
epoch£º214	 i:9 	 global-step:4289	 l-p:0.05645435303449631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.2764, 26.5249, 26.3579],
        [24.2764, 24.2924, 24.2772],
        [24.2764, 24.6292, 24.3805],
        [24.2764, 24.2776, 24.2764]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.056435976177453995 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056435976177453995 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9738], device='cuda:0')), ('power', tensor([-3.9987], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.056435976177453995
epoch£º215	 i:1 	 global-step:4301	 l-p:0.05644964054226875
epoch£º215	 i:2 	 global-step:4302	 l-p:0.05649574100971222
epoch£º215	 i:3 	 global-step:4303	 l-p:0.0564834289252758
epoch£º215	 i:4 	 global-step:4304	 l-p:0.05650392919778824
epoch£º215	 i:5 	 global-step:4305	 l-p:0.05654534325003624
epoch£º215	 i:6 	 global-step:4306	 l-p:0.05651956424117088
epoch£º215	 i:7 	 global-step:4307	 l-p:0.05644214525818825
epoch£º215	 i:8 	 global-step:4308	 l-p:0.05642945319414139
epoch£º215	 i:9 	 global-step:4309	 l-p:0.05656305328011513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.3221, 24.3583, 24.3248],
        [24.3221, 28.1629, 29.0049],
        [24.3221, 24.7425, 24.4606],
        [24.3221, 24.3221, 24.3221]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.056578636169433594 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056578636169433594 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9287], device='cuda:0')), ('power', tensor([-3.8717], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.056578636169433594
epoch£º216	 i:1 	 global-step:4321	 l-p:0.05651896446943283
epoch£º216	 i:2 	 global-step:4322	 l-p:0.05652691796422005
epoch£º216	 i:3 	 global-step:4323	 l-p:0.05640139430761337
epoch£º216	 i:4 	 global-step:4324	 l-p:0.05656595528125763
epoch£º216	 i:5 	 global-step:4325	 l-p:0.05640881508588791
epoch£º216	 i:6 	 global-step:4326	 l-p:0.05641506612300873
epoch£º216	 i:7 	 global-step:4327	 l-p:0.05643758550286293
epoch£º216	 i:8 	 global-step:4328	 l-p:0.056423332542181015
epoch£º216	 i:9 	 global-step:4329	 l-p:0.05646347999572754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.3683, 24.6641, 24.4461],
        [24.3683, 29.0744, 30.6586],
        [24.3683, 26.9640, 26.9616],
        [24.3683, 24.3684, 24.3683]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.05640947073698044 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05640947073698044 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0018], device='cuda:0')), ('power', tensor([-3.9414], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.05640947073698044
epoch£º217	 i:1 	 global-step:4341	 l-p:0.05652545019984245
epoch£º217	 i:2 	 global-step:4342	 l-p:0.056503694504499435
epoch£º217	 i:3 	 global-step:4343	 l-p:0.05655841529369354
epoch£º217	 i:4 	 global-step:4344	 l-p:0.05641859397292137
epoch£º217	 i:5 	 global-step:4345	 l-p:0.056545939296483994
epoch£º217	 i:6 	 global-step:4346	 l-p:0.05642000213265419
epoch£º217	 i:7 	 global-step:4347	 l-p:0.05639506131410599
epoch£º217	 i:8 	 global-step:4348	 l-p:0.056440334767103195
epoch£º217	 i:9 	 global-step:4349	 l-p:0.05639413744211197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.4145, 24.5301, 24.4314],
        [24.4145, 24.4145, 24.4145],
        [24.4145, 24.4197, 24.4146],
        [24.4145, 24.4145, 24.4145]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.056394219398498535 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056394219398498535 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9986], device='cuda:0')), ('power', tensor([-3.9156], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.056394219398498535
epoch£º218	 i:1 	 global-step:4361	 l-p:0.056400563567876816
epoch£º218	 i:2 	 global-step:4362	 l-p:0.056468404829502106
epoch£º218	 i:3 	 global-step:4363	 l-p:0.05637458339333534
epoch£º218	 i:4 	 global-step:4364	 l-p:0.05649758130311966
epoch£º218	 i:5 	 global-step:4365	 l-p:0.056578900665044785
epoch£º218	 i:6 	 global-step:4366	 l-p:0.05638295039534569
epoch£º218	 i:7 	 global-step:4367	 l-p:0.056597333401441574
epoch£º218	 i:8 	 global-step:4368	 l-p:0.05641714483499527
epoch£º218	 i:9 	 global-step:4369	 l-p:0.05637022852897644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.4608, 24.6317, 24.4927],
        [24.4608, 24.7840, 24.5506],
        [24.4608, 26.7366, 26.5724],
        [24.4608, 32.9727, 38.8533]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.05655846372246742 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05655846372246742 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9122], device='cuda:0')), ('power', tensor([-3.7027], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.05655846372246742
epoch£º219	 i:1 	 global-step:4381	 l-p:0.05642036348581314
epoch£º219	 i:2 	 global-step:4382	 l-p:0.05641641840338707
epoch£º219	 i:3 	 global-step:4383	 l-p:0.056443966925144196
epoch£º219	 i:4 	 global-step:4384	 l-p:0.05645998939871788
epoch£º219	 i:5 	 global-step:4385	 l-p:0.0563942976295948
epoch£º219	 i:6 	 global-step:4386	 l-p:0.056397661566734314
epoch£º219	 i:7 	 global-step:4387	 l-p:0.05641449987888336
epoch£º219	 i:8 	 global-step:4388	 l-p:0.056484684348106384
epoch£º219	 i:9 	 global-step:4389	 l-p:0.05636300519108772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.5076, 24.5211, 24.5081],
        [24.5076, 25.6900, 25.2524],
        [24.5076, 24.5076, 24.5076],
        [24.5076, 24.8640, 24.6128]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.056390658020973206 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056390658020973206 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9809], device='cuda:0')), ('power', tensor([-3.7856], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.056390658020973206
epoch£º220	 i:1 	 global-step:4401	 l-p:0.05642171576619148
epoch£º220	 i:2 	 global-step:4402	 l-p:0.056588802486658096
epoch£º220	 i:3 	 global-step:4403	 l-p:0.0563691221177578
epoch£º220	 i:4 	 global-step:4404	 l-p:0.05637085437774658
epoch£º220	 i:5 	 global-step:4405	 l-p:0.05644628405570984
epoch£º220	 i:6 	 global-step:4406	 l-p:0.056369662284851074
epoch£º220	 i:7 	 global-step:4407	 l-p:0.05636562779545784
epoch£º220	 i:8 	 global-step:4408	 l-p:0.05649124085903168
epoch£º220	 i:9 	 global-step:4409	 l-p:0.0564100556075573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.5544, 29.4623, 31.2126],
        [24.5544, 28.0982, 28.6909],
        [24.5544, 26.3958, 26.0657],
        [24.5544, 24.5631, 24.5547]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.05639017000794411 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05639017000794411 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9758], device='cuda:0')), ('power', tensor([-3.6523], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.05639017000794411
epoch£º221	 i:1 	 global-step:4421	 l-p:0.05650585889816284
epoch£º221	 i:2 	 global-step:4422	 l-p:0.05640910938382149
epoch£º221	 i:3 	 global-step:4423	 l-p:0.056384649127721786
epoch£º221	 i:4 	 global-step:4424	 l-p:0.05636417120695114
epoch£º221	 i:5 	 global-step:4425	 l-p:0.05634114146232605
epoch£º221	 i:6 	 global-step:4426	 l-p:0.05656927078962326
epoch£º221	 i:7 	 global-step:4427	 l-p:0.0563780851662159
epoch£º221	 i:8 	 global-step:4428	 l-p:0.05640450492501259
epoch£º221	 i:9 	 global-step:4429	 l-p:0.05634796991944313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[24.6016, 33.2458, 39.2684],
        [24.6016, 30.0778, 32.3799],
        [24.6016, 32.9647, 38.6203],
        [24.6016, 29.7642, 31.7537]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.05641402304172516 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05641402304172516 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9072], device='cuda:0')), ('power', tensor([-3.5521], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.05641402304172516
epoch£º222	 i:1 	 global-step:4441	 l-p:0.05635017156600952
epoch£º222	 i:2 	 global-step:4442	 l-p:0.05634525045752525
epoch£º222	 i:3 	 global-step:4443	 l-p:0.056346237659454346
epoch£º222	 i:4 	 global-step:4444	 l-p:0.0564710795879364
epoch£º222	 i:5 	 global-step:4445	 l-p:0.056379832327365875
epoch£º222	 i:6 	 global-step:4446	 l-p:0.056371431797742844
epoch£º222	 i:7 	 global-step:4447	 l-p:0.05638980120420456
epoch£º222	 i:8 	 global-step:4448	 l-p:0.05633742734789848
epoch£º222	 i:9 	 global-step:4449	 l-p:0.056560028344392776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.6488, 24.6708, 24.6500],
        [24.6488, 26.5758, 26.2688],
        [24.6488, 27.4717, 27.5787],
        [24.6488, 25.0141, 24.7579]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.05632179230451584 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05632179230451584 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0173], device='cuda:0')), ('power', tensor([-3.6199], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.05632179230451584
epoch£º223	 i:1 	 global-step:4461	 l-p:0.05635717883706093
epoch£º223	 i:2 	 global-step:4462	 l-p:0.056358058005571365
epoch£º223	 i:3 	 global-step:4463	 l-p:0.05636412650346756
epoch£º223	 i:4 	 global-step:4464	 l-p:0.05647052824497223
epoch£º223	 i:5 	 global-step:4465	 l-p:0.05633360892534256
epoch£º223	 i:6 	 global-step:4466	 l-p:0.05656471475958824
epoch£º223	 i:7 	 global-step:4467	 l-p:0.05642642825841904
epoch£º223	 i:8 	 global-step:4468	 l-p:0.05629907548427582
epoch£º223	 i:9 	 global-step:4469	 l-p:0.0563407726585865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.6964, 24.6966, 24.6964],
        [24.6964, 24.6965, 24.6964],
        [24.6964, 25.4329, 25.0395],
        [24.6964, 24.7388, 24.6998]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.05632857605814934 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05632857605814934 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0071], device='cuda:0')), ('power', tensor([-3.6793], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.05632857605814934
epoch£º224	 i:1 	 global-step:4481	 l-p:0.05633201450109482
epoch£º224	 i:2 	 global-step:4482	 l-p:0.05632077157497406
epoch£º224	 i:3 	 global-step:4483	 l-p:0.056414928287267685
epoch£º224	 i:4 	 global-step:4484	 l-p:0.0563371405005455
epoch£º224	 i:5 	 global-step:4485	 l-p:0.05633334442973137
epoch£º224	 i:6 	 global-step:4486	 l-p:0.05646441876888275
epoch£º224	 i:7 	 global-step:4487	 l-p:0.05648668110370636
epoch£º224	 i:8 	 global-step:4488	 l-p:0.05637787654995918
epoch£º224	 i:9 	 global-step:4489	 l-p:0.05631093680858612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.7442, 24.7442, 24.7442],
        [24.7442, 24.7442, 24.7442],
        [24.7442, 24.7443, 24.7442],
        [24.7442, 26.8087, 26.5453]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.05638398602604866 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05638398602604866 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9689], device='cuda:0')), ('power', tensor([-3.5099], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.05638398602604866
epoch£º225	 i:1 	 global-step:4501	 l-p:0.05645935982465744
epoch£º225	 i:2 	 global-step:4502	 l-p:0.05632932856678963
epoch£º225	 i:3 	 global-step:4503	 l-p:0.05631742253899574
epoch£º225	 i:4 	 global-step:4504	 l-p:0.0562976598739624
epoch£º225	 i:5 	 global-step:4505	 l-p:0.05630999431014061
epoch£º225	 i:6 	 global-step:4506	 l-p:0.05632034316658974
epoch£º225	 i:7 	 global-step:4507	 l-p:0.056439608335494995
epoch£º225	 i:8 	 global-step:4508	 l-p:0.056358106434345245
epoch£º225	 i:9 	 global-step:4509	 l-p:0.05636147782206535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.7923, 27.2113, 27.0954],
        [24.7923, 30.2958, 32.5991],
        [24.7923, 26.1092, 25.6719],
        [24.7923, 24.7975, 24.7924]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.05629371479153633 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05629371479153633 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0167], device='cuda:0')), ('power', tensor([-3.4892], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.05629371479153633
epoch£º226	 i:1 	 global-step:4521	 l-p:0.05628131702542305
epoch£º226	 i:2 	 global-step:4522	 l-p:0.05645003542304039
epoch£º226	 i:3 	 global-step:4523	 l-p:0.056361258029937744
epoch£º226	 i:4 	 global-step:4524	 l-p:0.05630109831690788
epoch£º226	 i:5 	 global-step:4525	 l-p:0.056317806243896484
epoch£º226	 i:6 	 global-step:4526	 l-p:0.05643468722701073
epoch£º226	 i:7 	 global-step:4527	 l-p:0.0564090795814991
epoch£º226	 i:8 	 global-step:4528	 l-p:0.056294456124305725
epoch£º226	 i:9 	 global-step:4529	 l-p:0.05630408972501755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.8406, 26.7849, 26.4757],
        [24.8406, 24.9107, 24.8481],
        [24.8406, 25.4681, 25.1036],
        [24.8406, 24.8487, 24.8408]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.056333988904953 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056333988904953 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9919], device='cuda:0')), ('power', tensor([-3.3840], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.056333988904953
epoch£º227	 i:1 	 global-step:4541	 l-p:0.056270893663167953
epoch£º227	 i:2 	 global-step:4542	 l-p:0.05632359907031059
epoch£º227	 i:3 	 global-step:4543	 l-p:0.056366339325904846
epoch£º227	 i:4 	 global-step:4544	 l-p:0.05631355941295624
epoch£º227	 i:5 	 global-step:4545	 l-p:0.05628795549273491
epoch£º227	 i:6 	 global-step:4546	 l-p:0.05647013708949089
epoch£º227	 i:7 	 global-step:4547	 l-p:0.05629764497280121
epoch£º227	 i:8 	 global-step:4548	 l-p:0.056393105536699295
epoch£º227	 i:9 	 global-step:4549	 l-p:0.056260526180267334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.8890, 24.8895, 24.8890],
        [24.8890, 24.8907, 24.8891],
        [24.8890, 24.9046, 24.8897],
        [24.8890, 25.6581, 25.2553]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.056271228939294815 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056271228939294815 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0119], device='cuda:0')), ('power', tensor([-3.3809], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.056271228939294815
epoch£º228	 i:1 	 global-step:4561	 l-p:0.05631019175052643
epoch£º228	 i:2 	 global-step:4562	 l-p:0.056378044188022614
epoch£º228	 i:3 	 global-step:4563	 l-p:0.05628635361790657
epoch£º228	 i:4 	 global-step:4564	 l-p:0.05639319121837616
epoch£º228	 i:5 	 global-step:4565	 l-p:0.05637729540467262
epoch£º228	 i:6 	 global-step:4566	 l-p:0.05623789131641388
epoch£º228	 i:7 	 global-step:4567	 l-p:0.05625314638018608
epoch£º228	 i:8 	 global-step:4568	 l-p:0.05635390803217888
epoch£º228	 i:9 	 global-step:4569	 l-p:0.05632668361067772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.9378, 24.9525, 24.9384],
        [24.9378, 30.6880, 33.2235],
        [24.9378, 25.3920, 25.0924],
        [24.9378, 25.2694, 25.0302]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.05643623694777489 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05643623694777489 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9117], device='cuda:0')), ('power', tensor([-3.1900], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.05643623694777489
epoch£º229	 i:1 	 global-step:4581	 l-p:0.05630931630730629
epoch£º229	 i:2 	 global-step:4582	 l-p:0.0564325787127018
epoch£º229	 i:3 	 global-step:4583	 l-p:0.056352682411670685
epoch£º229	 i:4 	 global-step:4584	 l-p:0.05628121271729469
epoch£º229	 i:5 	 global-step:4585	 l-p:0.056236572563648224
epoch£º229	 i:6 	 global-step:4586	 l-p:0.05625591054558754
epoch£º229	 i:7 	 global-step:4587	 l-p:0.05625014007091522
epoch£º229	 i:8 	 global-step:4588	 l-p:0.05625225976109505
epoch£º229	 i:9 	 global-step:4589	 l-p:0.05625149607658386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[24.9869, 25.5767, 25.2236],
        [24.9869, 33.3675, 38.9618],
        [24.9869, 25.7328, 25.3344],
        [24.9869, 27.7956, 27.8724]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.056380920112133026 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056380920112133026 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9355], device='cuda:0')), ('power', tensor([-3.1838], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.056380920112133026
epoch£º230	 i:1 	 global-step:4601	 l-p:0.05626294016838074
epoch£º230	 i:2 	 global-step:4602	 l-p:0.056245822459459305
epoch£º230	 i:3 	 global-step:4603	 l-p:0.05629991739988327
epoch£º230	 i:4 	 global-step:4604	 l-p:0.05634140968322754
epoch£º230	 i:5 	 global-step:4605	 l-p:0.05623451992869377
epoch£º230	 i:6 	 global-step:4606	 l-p:0.05626729875802994
epoch£º230	 i:7 	 global-step:4607	 l-p:0.056227754801511765
epoch£º230	 i:8 	 global-step:4608	 l-p:0.0562792532145977
epoch£º230	 i:9 	 global-step:4609	 l-p:0.056387823075056076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.0360, 25.0361, 25.0360],
        [25.0360, 25.0535, 25.0368],
        [25.0360, 33.2337, 38.5842],
        [25.0360, 26.2218, 25.7739]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.0562606044113636 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0562606044113636 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9803], device='cuda:0')), ('power', tensor([-3.1782], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.0562606044113636
epoch£º231	 i:1 	 global-step:4621	 l-p:0.05623476952314377
epoch£º231	 i:2 	 global-step:4622	 l-p:0.05649450048804283
epoch£º231	 i:3 	 global-step:4623	 l-p:0.05621980130672455
epoch£º231	 i:4 	 global-step:4624	 l-p:0.05619718134403229
epoch£º231	 i:5 	 global-step:4625	 l-p:0.05624004080891609
epoch£º231	 i:6 	 global-step:4626	 l-p:0.056317295879125595
epoch£º231	 i:7 	 global-step:4627	 l-p:0.05625195428729057
epoch£º231	 i:8 	 global-step:4628	 l-p:0.05634434521198273
epoch£º231	 i:9 	 global-step:4629	 l-p:0.05623738095164299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.0854, 25.0906, 25.0855],
        [25.0854, 25.8907, 25.4782],
        [25.0854, 29.7572, 31.2259],
        [25.0854, 25.1978, 25.1013]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.05621342360973358 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05621342360973358 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0290], device='cuda:0')), ('power', tensor([-3.1899], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.05621342360973358
epoch£º232	 i:1 	 global-step:4641	 l-p:0.05623825266957283
epoch£º232	 i:2 	 global-step:4642	 l-p:0.056217022240161896
epoch£º232	 i:3 	 global-step:4643	 l-p:0.05623546987771988
epoch£º232	 i:4 	 global-step:4644	 l-p:0.05629313364624977
epoch£º232	 i:5 	 global-step:4645	 l-p:0.05629703029990196
epoch£º232	 i:6 	 global-step:4646	 l-p:0.05633721128106117
epoch£º232	 i:7 	 global-step:4647	 l-p:0.056245096027851105
epoch£º232	 i:8 	 global-step:4648	 l-p:0.056320637464523315
epoch£º232	 i:9 	 global-step:4649	 l-p:0.05627019703388214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.1350, 26.1284, 25.6873],
        [25.1350, 25.6939, 25.3509],
        [25.1350, 28.8957, 29.5996],
        [25.1350, 32.7004, 37.2479]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.056239064782857895 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056239064782857895 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9780], device='cuda:0')), ('power', tensor([-3.0312], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.056239064782857895
epoch£º233	 i:1 	 global-step:4661	 l-p:0.056341808289289474
epoch£º233	 i:2 	 global-step:4662	 l-p:0.056277647614479065
epoch£º233	 i:3 	 global-step:4663	 l-p:0.0561993382871151
epoch£º233	 i:4 	 global-step:4664	 l-p:0.056198444217443466
epoch£º233	 i:5 	 global-step:4665	 l-p:0.05628645420074463
epoch£º233	 i:6 	 global-step:4666	 l-p:0.056253302842378616
epoch£º233	 i:7 	 global-step:4667	 l-p:0.05617596581578255
epoch£º233	 i:8 	 global-step:4668	 l-p:0.05630460008978844
epoch£º233	 i:9 	 global-step:4669	 l-p:0.056261010468006134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.1849, 25.3253, 25.2077],
        [25.1849, 25.5417, 25.2884],
        [25.1849, 25.6802, 25.3618],
        [25.1849, 25.1871, 25.1849]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.05622989684343338 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05622989684343338 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9781], device='cuda:0')), ('power', tensor([-3.0310], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.05622989684343338
epoch£º234	 i:1 	 global-step:4681	 l-p:0.056232061237096786
epoch£º234	 i:2 	 global-step:4682	 l-p:0.05629017576575279
epoch£º234	 i:3 	 global-step:4683	 l-p:0.05621460825204849
epoch£º234	 i:4 	 global-step:4684	 l-p:0.056182611733675
epoch£º234	 i:5 	 global-step:4685	 l-p:0.056295327842235565
epoch£º234	 i:6 	 global-step:4686	 l-p:0.0562320202589035
epoch£º234	 i:7 	 global-step:4687	 l-p:0.056209392845630646
epoch£º234	 i:8 	 global-step:4688	 l-p:0.056345291435718536
epoch£º234	 i:9 	 global-step:4689	 l-p:0.0561758428812027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.2351, 32.0579, 35.7151],
        [25.2351, 25.2353, 25.2351],
        [25.2351, 25.4099, 25.2674],
        [25.2351, 25.2353, 25.2351]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.056189995259046555 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056189995259046555 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0325], device='cuda:0')), ('power', tensor([-3.0813], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.056189995259046555
epoch£º235	 i:1 	 global-step:4701	 l-p:0.056191928684711456
epoch£º235	 i:2 	 global-step:4702	 l-p:0.05622703582048416
epoch£º235	 i:3 	 global-step:4703	 l-p:0.056251510977745056
epoch£º235	 i:4 	 global-step:4704	 l-p:0.05628670006990433
epoch£º235	 i:5 	 global-step:4705	 l-p:0.05618711933493614
epoch£º235	 i:6 	 global-step:4706	 l-p:0.05639619380235672
epoch£º235	 i:7 	 global-step:4707	 l-p:0.056202132254838943
epoch£º235	 i:8 	 global-step:4708	 l-p:0.05616595223546028
epoch£º235	 i:9 	 global-step:4709	 l-p:0.05617833882570267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.2855, 28.4326, 28.6937],
        [25.2855, 25.3056, 25.2865],
        [25.2855, 27.6582, 27.4952],
        [25.2855, 26.0390, 25.6361]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.05617411434650421 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05617411434650421 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0337], device='cuda:0')), ('power', tensor([-2.9744], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.05617411434650421
epoch£º236	 i:1 	 global-step:4721	 l-p:0.05615336447954178
epoch£º236	 i:2 	 global-step:4722	 l-p:0.05619262531399727
epoch£º236	 i:3 	 global-step:4723	 l-p:0.056276798248291016
epoch£º236	 i:4 	 global-step:4724	 l-p:0.0562036968767643
epoch£º236	 i:5 	 global-step:4725	 l-p:0.056331586092710495
epoch£º236	 i:6 	 global-step:4726	 l-p:0.05614635720849037
epoch£º236	 i:7 	 global-step:4727	 l-p:0.05622328072786331
epoch£º236	 i:8 	 global-step:4728	 l-p:0.056266143918037415
epoch£º236	 i:9 	 global-step:4729	 l-p:0.056178316473960876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.3360, 32.8008, 37.1892],
        [25.3360, 27.0903, 26.7088],
        [25.3360, 25.3360, 25.3360],
        [25.3360, 27.7149, 27.5520]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.056237176060676575 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056237176060676575 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9794], device='cuda:0')), ('power', tensor([-2.8454], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.056237176060676575
epoch£º237	 i:1 	 global-step:4741	 l-p:0.05617658421397209
epoch£º237	 i:2 	 global-step:4742	 l-p:0.05616038292646408
epoch£º237	 i:3 	 global-step:4743	 l-p:0.056154973804950714
epoch£º237	 i:4 	 global-step:4744	 l-p:0.056199245154857635
epoch£º237	 i:5 	 global-step:4745	 l-p:0.05614567548036575
epoch£º237	 i:6 	 global-step:4746	 l-p:0.056244052946567535
epoch£º237	 i:7 	 global-step:4747	 l-p:0.056345172226428986
epoch£º237	 i:8 	 global-step:4748	 l-p:0.05616644769906998
epoch£º237	 i:9 	 global-step:4749	 l-p:0.056186120957136154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.3869, 33.6089, 38.9169],
        [25.3869, 26.5905, 26.1359],
        [25.3869, 30.9741, 33.2799],
        [25.3869, 25.3869, 25.3869]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.05642791464924812 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05642791464924812 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.8550], device='cuda:0')), ('power', tensor([-2.6019], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.05642791464924812
epoch£º238	 i:1 	 global-step:4761	 l-p:0.056211937218904495
epoch£º238	 i:2 	 global-step:4762	 l-p:0.05612247809767723
epoch£º238	 i:3 	 global-step:4763	 l-p:0.05616934970021248
epoch£º238	 i:4 	 global-step:4764	 l-p:0.05614076927304268
epoch£º238	 i:5 	 global-step:4765	 l-p:0.056133680045604706
epoch£º238	 i:6 	 global-step:4766	 l-p:0.05615013465285301
epoch£º238	 i:7 	 global-step:4767	 l-p:0.05614762008190155
epoch£º238	 i:8 	 global-step:4768	 l-p:0.056155238300561905
epoch£º238	 i:9 	 global-step:4769	 l-p:0.05622648447751999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[25.4381, 30.9570, 33.1874],
        [25.4381, 33.0996, 37.7054],
        [25.4381, 27.1950, 26.8107],
        [25.4381, 29.8698, 31.0881]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.056139733642339706 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056139733642339706 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0215], device='cuda:0')), ('power', tensor([-2.7746], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.056139733642339706
epoch£º239	 i:1 	 global-step:4781	 l-p:0.056179821491241455
epoch£º239	 i:2 	 global-step:4782	 l-p:0.056136805564165115
epoch£º239	 i:3 	 global-step:4783	 l-p:0.05612683296203613
epoch£º239	 i:4 	 global-step:4784	 l-p:0.056300945580005646
epoch£º239	 i:5 	 global-step:4785	 l-p:0.05616309866309166
epoch£º239	 i:6 	 global-step:4786	 l-p:0.05616433918476105
epoch£º239	 i:7 	 global-step:4787	 l-p:0.05610485002398491
epoch£º239	 i:8 	 global-step:4788	 l-p:0.056200284510850906
epoch£º239	 i:9 	 global-step:4789	 l-p:0.05623767897486687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.4893, 29.9169, 31.1262],
        [25.4893, 26.5867, 26.1326],
        [25.4893, 27.0926, 26.6742],
        [25.4893, 25.5062, 25.4901]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.056115660816431046 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056115660816431046 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0593], device='cuda:0')), ('power', tensor([-2.8487], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.056115660816431046
epoch£º240	 i:1 	 global-step:4801	 l-p:0.056184519082307816
epoch£º240	 i:2 	 global-step:4802	 l-p:0.05625228211283684
epoch£º240	 i:3 	 global-step:4803	 l-p:0.05612456053495407
epoch£º240	 i:4 	 global-step:4804	 l-p:0.05615408718585968
epoch£º240	 i:5 	 global-step:4805	 l-p:0.05616011098027229
epoch£º240	 i:6 	 global-step:4806	 l-p:0.05611142888665199
epoch£º240	 i:7 	 global-step:4807	 l-p:0.05610669031739235
epoch£º240	 i:8 	 global-step:4808	 l-p:0.0562119223177433
epoch£º240	 i:9 	 global-step:4809	 l-p:0.05620269104838371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.5409, 26.8346, 26.3793],
        [25.5409, 25.5409, 25.5409],
        [25.5409, 31.2386, 33.6349],
        [25.5409, 33.0563, 37.4669]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.05611357465386391 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05611357465386391 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0382], device='cuda:0')), ('power', tensor([-2.7330], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.05611357465386391
epoch£º241	 i:1 	 global-step:4821	 l-p:0.05611688643693924
epoch£º241	 i:2 	 global-step:4822	 l-p:0.05625268816947937
epoch£º241	 i:3 	 global-step:4823	 l-p:0.05608458071947098
epoch£º241	 i:4 	 global-step:4824	 l-p:0.056100595742464066
epoch£º241	 i:5 	 global-step:4825	 l-p:0.05618714168667793
epoch£º241	 i:6 	 global-step:4826	 l-p:0.05609132722020149
epoch£º241	 i:7 	 global-step:4827	 l-p:0.056089963763952255
epoch£º241	 i:8 	 global-step:4828	 l-p:0.05627589672803879
epoch£º241	 i:9 	 global-step:4829	 l-p:0.05618045851588249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.5926, 29.4519, 30.1895],
        [25.5926, 26.1978, 25.8356],
        [25.5926, 25.5973, 25.5928],
        [25.5926, 32.5176, 36.2299]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.056109149008989334 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056109149008989334 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0307], device='cuda:0')), ('power', tensor([-2.6528], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.056109149008989334
epoch£º242	 i:1 	 global-step:4841	 l-p:0.05611368641257286
epoch£º242	 i:2 	 global-step:4842	 l-p:0.056208834052085876
epoch£º242	 i:3 	 global-step:4843	 l-p:0.05621276795864105
epoch£º242	 i:4 	 global-step:4844	 l-p:0.05609462782740593
epoch£º242	 i:5 	 global-step:4845	 l-p:0.05617628991603851
epoch£º242	 i:6 	 global-step:4846	 l-p:0.05607111379504204
epoch£º242	 i:7 	 global-step:4847	 l-p:0.05607371777296066
epoch£º242	 i:8 	 global-step:4848	 l-p:0.05620463564991951
epoch£º242	 i:9 	 global-step:4849	 l-p:0.05609777197241783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.6448, 26.4830, 26.0580],
        [25.6448, 25.6487, 25.6449],
        [25.6448, 25.6448, 25.6448],
        [25.6448, 32.1558, 35.3965]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.05612409487366676 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05612409487366676 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0370], device='cuda:0')), ('power', tensor([-2.6129], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.05612409487366676
epoch£º243	 i:1 	 global-step:4861	 l-p:0.056086327880620956
epoch£º243	 i:2 	 global-step:4862	 l-p:0.05615166574716568
epoch£º243	 i:3 	 global-step:4863	 l-p:0.056103844195604324
epoch£º243	 i:4 	 global-step:4864	 l-p:0.05609193816781044
epoch£º243	 i:5 	 global-step:4865	 l-p:0.056151412427425385
epoch£º243	 i:6 	 global-step:4866	 l-p:0.05629236251115799
epoch£º243	 i:7 	 global-step:4867	 l-p:0.056067273020744324
epoch£º243	 i:8 	 global-step:4868	 l-p:0.05606662109494209
epoch£º243	 i:9 	 global-step:4869	 l-p:0.056096043437719345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[25.6971, 26.3049, 25.9411],
        [25.6971, 29.6959, 30.5320],
        [25.6971, 26.7407, 26.2867],
        [25.6971, 31.8418, 34.6794]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.056070439517498016 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056070439517498016 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0454], device='cuda:0')), ('power', tensor([-2.5652], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.056070439517498016
epoch£º244	 i:1 	 global-step:4881	 l-p:0.05607840418815613
epoch£º244	 i:2 	 global-step:4882	 l-p:0.05604718625545502
epoch£º244	 i:3 	 global-step:4883	 l-p:0.056089334189891815
epoch£º244	 i:4 	 global-step:4884	 l-p:0.05608031526207924
epoch£º244	 i:5 	 global-step:4885	 l-p:0.05606137216091156
epoch£º244	 i:6 	 global-step:4886	 l-p:0.05622212216258049
epoch£º244	 i:7 	 global-step:4887	 l-p:0.0560687817633152
epoch£º244	 i:8 	 global-step:4888	 l-p:0.056320685893297195
epoch£º244	 i:9 	 global-step:4889	 l-p:0.05606167018413544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.7495, 25.7497, 25.7495],
        [25.7495, 28.3994, 28.3439],
        [25.7495, 29.8246, 30.7161],
        [25.7495, 25.7495, 25.7495]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.05608935281634331 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05608935281634331 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0046], device='cuda:0')), ('power', tensor([-2.4551], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.05608935281634331
epoch£º245	 i:1 	 global-step:4901	 l-p:0.05605357512831688
epoch£º245	 i:2 	 global-step:4902	 l-p:0.05606767535209656
epoch£º245	 i:3 	 global-step:4903	 l-p:0.056092508137226105
epoch£º245	 i:4 	 global-step:4904	 l-p:0.05605052039027214
epoch£º245	 i:5 	 global-step:4905	 l-p:0.05609818175435066
epoch£º245	 i:6 	 global-step:4906	 l-p:0.05606978386640549
epoch£º245	 i:7 	 global-step:4907	 l-p:0.056054119020700455
epoch£º245	 i:8 	 global-step:4908	 l-p:0.05623047053813934
epoch£º245	 i:9 	 global-step:4909	 l-p:0.05616336688399315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.8022, 31.1496, 33.1633],
        [25.8022, 25.8164, 25.8028],
        [25.8022, 27.0189, 26.5564],
        [25.8022, 25.8026, 25.8023]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.056122053414583206 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056122053414583206 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9899], device='cuda:0')), ('power', tensor([-2.3868], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.056122053414583206
epoch£º246	 i:1 	 global-step:4921	 l-p:0.05607302486896515
epoch£º246	 i:2 	 global-step:4922	 l-p:0.0560372918844223
epoch£º246	 i:3 	 global-step:4923	 l-p:0.05604018270969391
epoch£º246	 i:4 	 global-step:4924	 l-p:0.05609975755214691
epoch£º246	 i:5 	 global-step:4925	 l-p:0.05619955435395241
epoch£º246	 i:6 	 global-step:4926	 l-p:0.05606813728809357
epoch£º246	 i:7 	 global-step:4927	 l-p:0.056120507419109344
epoch£º246	 i:8 	 global-step:4928	 l-p:0.05604755878448486
epoch£º246	 i:9 	 global-step:4929	 l-p:0.05603095516562462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.8554, 25.8555, 25.8554],
        [25.8554, 26.2546, 25.9775],
        [25.8554, 27.6831, 27.3022],
        [25.8554, 25.8556, 25.8554]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.05607889965176582 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05607889965176582 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0376], device='cuda:0')), ('power', tensor([-2.4073], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.05607889965176582
epoch£º247	 i:1 	 global-step:4941	 l-p:0.056027963757514954
epoch£º247	 i:2 	 global-step:4942	 l-p:0.05602460727095604
epoch£º247	 i:3 	 global-step:4943	 l-p:0.0561656653881073
epoch£º247	 i:4 	 global-step:4944	 l-p:0.0560695081949234
epoch£º247	 i:5 	 global-step:4945	 l-p:0.05601716786623001
epoch£º247	 i:6 	 global-step:4946	 l-p:0.05616852268576622
epoch£º247	 i:7 	 global-step:4947	 l-p:0.05600681155920029
epoch£º247	 i:8 	 global-step:4948	 l-p:0.05603877827525139
epoch£º247	 i:9 	 global-step:4949	 l-p:0.05610973760485649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.9087, 25.9087, 25.9087],
        [25.9087, 25.9088, 25.9087],
        [25.9087, 26.2922, 26.0229],
        [25.9087, 25.9089, 25.9087]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.056010399013757706 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056010399013757706 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0659], device='cuda:0')), ('power', tensor([-2.4072], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.056010399013757706
epoch£º248	 i:1 	 global-step:4961	 l-p:0.05601917952299118
epoch£º248	 i:2 	 global-step:4962	 l-p:0.05599523335695267
epoch£º248	 i:3 	 global-step:4963	 l-p:0.05614030733704567
epoch£º248	 i:4 	 global-step:4964	 l-p:0.05614684149622917
epoch£º248	 i:5 	 global-step:4965	 l-p:0.05601215362548828
epoch£º248	 i:6 	 global-step:4966	 l-p:0.05599488690495491
epoch£º248	 i:7 	 global-step:4967	 l-p:0.056054387241601944
epoch£º248	 i:8 	 global-step:4968	 l-p:0.056025248020887375
epoch£º248	 i:9 	 global-step:4969	 l-p:0.05617779120802879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[25.9622, 31.0236, 32.7460],
        [25.9622, 31.4177, 33.5160],
        [25.9622, 27.6405, 27.2221],
        [25.9622, 33.6214, 38.1250]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.055993612855672836 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055993612855672836 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0812], device='cuda:0')), ('power', tensor([-2.3059], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.055993612855672836
epoch£º249	 i:1 	 global-step:4981	 l-p:0.05600859969854355
epoch£º249	 i:2 	 global-step:4982	 l-p:0.05612904578447342
epoch£º249	 i:3 	 global-step:4983	 l-p:0.05597763881087303
epoch£º249	 i:4 	 global-step:4984	 l-p:0.05603829771280289
epoch£º249	 i:5 	 global-step:4985	 l-p:0.05601751059293747
epoch£º249	 i:6 	 global-step:4986	 l-p:0.05603146553039551
epoch£º249	 i:7 	 global-step:4987	 l-p:0.056066129356622696
epoch£º249	 i:8 	 global-step:4988	 l-p:0.05620517581701279
epoch£º249	 i:9 	 global-step:4989	 l-p:0.055977948009967804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0159, 29.0296, 29.1577],
        [26.0159, 26.0159, 26.0159],
        [26.0159, 26.5837, 26.2324],
        [26.0159, 26.0182, 26.0160]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.05606699734926224 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05606699734926224 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0087], device='cuda:0')), ('power', tensor([-2.1701], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.05606699734926224
epoch£º250	 i:1 	 global-step:5001	 l-p:0.05612237751483917
epoch£º250	 i:2 	 global-step:5002	 l-p:0.05601099506020546
epoch£º250	 i:3 	 global-step:5003	 l-p:0.056050173938274384
epoch£º250	 i:4 	 global-step:5004	 l-p:0.056016020476818085
epoch£º250	 i:5 	 global-step:5005	 l-p:0.055970557034015656
epoch£º250	 i:6 	 global-step:5006	 l-p:0.05598841980099678
epoch£º250	 i:7 	 global-step:5007	 l-p:0.055963899940252304
epoch£º250	 i:8 	 global-step:5008	 l-p:0.0561518557369709
epoch£º250	 i:9 	 global-step:5009	 l-p:0.05597326159477234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0701, 28.7551, 28.6991],
        [26.0701, 26.0762, 26.0702],
        [26.0701, 27.7929, 27.3803],
        [26.0701, 26.0701, 26.0701]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.05599930137395859 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05599930137395859 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0403], device='cuda:0')), ('power', tensor([-2.1568], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.05599930137395859
epoch£º251	 i:1 	 global-step:5021	 l-p:0.055982187390327454
epoch£º251	 i:2 	 global-step:5022	 l-p:0.05607417970895767
epoch£º251	 i:3 	 global-step:5023	 l-p:0.05619172379374504
epoch£º251	 i:4 	 global-step:5024	 l-p:0.055984724313020706
epoch£º251	 i:5 	 global-step:5025	 l-p:0.056016452610492706
epoch£º251	 i:6 	 global-step:5026	 l-p:0.055979806929826736
epoch£º251	 i:7 	 global-step:5027	 l-p:0.05595369264483452
epoch£º251	 i:8 	 global-step:5028	 l-p:0.05598830431699753
epoch£º251	 i:9 	 global-step:5029	 l-p:0.056012850254774094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1244, 26.1354, 26.1248],
        [26.1244, 34.8026, 40.5336],
        [26.1244, 27.8371, 27.4206],
        [26.1244, 28.9200, 28.9186]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.05598682165145874 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05598682165145874 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0254], device='cuda:0')), ('power', tensor([-2.0727], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.05598682165145874
epoch£º252	 i:1 	 global-step:5041	 l-p:0.055950816720724106
epoch£º252	 i:2 	 global-step:5042	 l-p:0.055986013263463974
epoch£º252	 i:3 	 global-step:5043	 l-p:0.05596287548542023
epoch£º252	 i:4 	 global-step:5044	 l-p:0.05595530569553375
epoch£º252	 i:5 	 global-step:5045	 l-p:0.0561421774327755
epoch£º252	 i:6 	 global-step:5046	 l-p:0.055963676422834396
epoch£º252	 i:7 	 global-step:5047	 l-p:0.055969856679439545
epoch£º252	 i:8 	 global-step:5048	 l-p:0.0560823529958725
epoch£º252	 i:9 	 global-step:5049	 l-p:0.056051626801490784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[26.1788, 34.7707, 40.3803],
        [26.1788, 28.0291, 27.6427],
        [26.1788, 27.9017, 27.4857],
        [26.1788, 31.4235, 33.2913]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.05595019832253456 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05595019832253456 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0849], device='cuda:0')), ('power', tensor([-2.1017], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.05595019832253456
epoch£º253	 i:1 	 global-step:5061	 l-p:0.0560557059943676
epoch£º253	 i:2 	 global-step:5062	 l-p:0.056075386703014374
epoch£º253	 i:3 	 global-step:5063	 l-p:0.05600008741021156
epoch£º253	 i:4 	 global-step:5064	 l-p:0.05594082549214363
epoch£º253	 i:5 	 global-step:5065	 l-p:0.05612746253609657
epoch£º253	 i:6 	 global-step:5066	 l-p:0.05592671409249306
epoch£º253	 i:7 	 global-step:5067	 l-p:0.05595838278532028
epoch£º253	 i:8 	 global-step:5068	 l-p:0.055955324321985245
epoch£º253	 i:9 	 global-step:5069	 l-p:0.05593087524175644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.2337, 26.7512, 26.4186],
        [26.2337, 27.4458, 26.9752],
        [26.2337, 26.2339, 26.2337],
        [26.2337, 26.2338, 26.2337]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.05596121773123741 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05596121773123741 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0527], device='cuda:0')), ('power', tensor([-2.0294], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.05596121773123741
epoch£º254	 i:1 	 global-step:5081	 l-p:0.055932048708200455
epoch£º254	 i:2 	 global-step:5082	 l-p:0.05607445165514946
epoch£º254	 i:3 	 global-step:5083	 l-p:0.05598112940788269
epoch£º254	 i:4 	 global-step:5084	 l-p:0.056020963937044144
epoch£º254	 i:5 	 global-step:5085	 l-p:0.05612829700112343
epoch£º254	 i:6 	 global-step:5086	 l-p:0.05592503026127815
epoch£º254	 i:7 	 global-step:5087	 l-p:0.05592568591237068
epoch£º254	 i:8 	 global-step:5088	 l-p:0.055914536118507385
epoch£º254	 i:9 	 global-step:5089	 l-p:0.05592607334256172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[26.2888, 34.9186, 40.5533],
        [26.2888, 28.7631, 28.5942],
        [26.2888, 31.6156, 33.5478],
        [26.2888, 35.9465, 42.9253]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.0560685433447361 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0560685433447361 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9933], device='cuda:0')), ('power', tensor([-1.8544], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.0560685433447361
epoch£º255	 i:1 	 global-step:5101	 l-p:0.05594124644994736
epoch£º255	 i:2 	 global-step:5102	 l-p:0.05596166476607323
epoch£º255	 i:3 	 global-step:5103	 l-p:0.05592963099479675
epoch£º255	 i:4 	 global-step:5104	 l-p:0.05603710189461708
epoch£º255	 i:5 	 global-step:5105	 l-p:0.05590614303946495
epoch£º255	 i:6 	 global-step:5106	 l-p:0.05593584105372429
epoch£º255	 i:7 	 global-step:5107	 l-p:0.055966027081012726
epoch£º255	 i:8 	 global-step:5108	 l-p:0.056003693491220474
epoch£º255	 i:9 	 global-step:5109	 l-p:0.05590809881687164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.3441, 27.3888, 26.9251],
        [26.3441, 26.5406, 26.3821],
        [26.3441, 26.3441, 26.3441],
        [26.3441, 31.1704, 32.6343]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.05590827018022537 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05590827018022537 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0947], device='cuda:0')), ('power', tensor([-1.9573], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.05590827018022537
epoch£º256	 i:1 	 global-step:5121	 l-p:0.055955495685338974
epoch£º256	 i:2 	 global-step:5122	 l-p:0.05588342621922493
epoch£º256	 i:3 	 global-step:5123	 l-p:0.055974166840314865
epoch£º256	 i:4 	 global-step:5124	 l-p:0.05596242472529411
epoch£º256	 i:5 	 global-step:5125	 l-p:0.05602141097187996
epoch£º256	 i:6 	 global-step:5126	 l-p:0.055931705981492996
epoch£º256	 i:7 	 global-step:5127	 l-p:0.055922362953424454
epoch£º256	 i:8 	 global-step:5128	 l-p:0.05606042221188545
epoch£º256	 i:9 	 global-step:5129	 l-p:0.05590672791004181
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.3995, 26.4007, 26.3995],
        [26.3995, 26.3995, 26.3995],
        [26.3995, 29.5407, 29.7192],
        [26.3995, 26.7783, 26.5100]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.05598468333482742 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05598468333482742 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0573], device='cuda:0')), ('power', tensor([-1.8456], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.05598468333482742
epoch£º257	 i:1 	 global-step:5141	 l-p:0.05588459223508835
epoch£º257	 i:2 	 global-step:5142	 l-p:0.055971622467041016
epoch£º257	 i:3 	 global-step:5143	 l-p:0.055943168699741364
epoch£º257	 i:4 	 global-step:5144	 l-p:0.055897701531648636
epoch£º257	 i:5 	 global-step:5145	 l-p:0.056092288345098495
epoch£º257	 i:6 	 global-step:5146	 l-p:0.05590100213885307
epoch£º257	 i:7 	 global-step:5147	 l-p:0.05588263273239136
epoch£º257	 i:8 	 global-step:5148	 l-p:0.055920399725437164
epoch£º257	 i:9 	 global-step:5149	 l-p:0.05591735616326332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.4553, 26.4716, 26.4561],
        [26.4553, 27.5048, 27.0390],
        [26.4553, 26.4554, 26.4553],
        [26.4553, 31.3549, 32.8717]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.05591613054275513 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05591613054275513 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0602], device='cuda:0')), ('power', tensor([-1.7861], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.05591613054275513
epoch£º258	 i:1 	 global-step:5161	 l-p:0.05596233904361725
epoch£º258	 i:2 	 global-step:5162	 l-p:0.055914320051670074
epoch£º258	 i:3 	 global-step:5163	 l-p:0.05586464703083038
epoch£º258	 i:4 	 global-step:5164	 l-p:0.055884040892124176
epoch£º258	 i:5 	 global-step:5165	 l-p:0.055861882865428925
epoch£º258	 i:6 	 global-step:5166	 l-p:0.05604084953665733
epoch£º258	 i:7 	 global-step:5167	 l-p:0.05591021478176117
epoch£º258	 i:8 	 global-step:5168	 l-p:0.055966608226299286
epoch£º258	 i:9 	 global-step:5169	 l-p:0.05594271421432495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.5113, 31.4186, 32.9359],
        [26.5113, 26.5113, 26.5113],
        [26.5113, 26.5125, 26.5113],
        [26.5113, 28.2276, 27.7999]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.05601007863879204 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05601007863879204 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0269], device='cuda:0')), ('power', tensor([-1.6827], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.05601007863879204
epoch£º259	 i:1 	 global-step:5181	 l-p:0.0558861643075943
epoch£º259	 i:2 	 global-step:5182	 l-p:0.05586446821689606
epoch£º259	 i:3 	 global-step:5183	 l-p:0.055886391550302505
epoch£º259	 i:4 	 global-step:5184	 l-p:0.055872682482004166
epoch£º259	 i:5 	 global-step:5185	 l-p:0.055968597531318665
epoch£º259	 i:6 	 global-step:5186	 l-p:0.05593833699822426
epoch£º259	 i:7 	 global-step:5187	 l-p:0.05587390810251236
epoch£º259	 i:8 	 global-step:5188	 l-p:0.055988721549510956
epoch£º259	 i:9 	 global-step:5189	 l-p:0.05584331229329109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.5677, 32.9012, 35.8085],
        [26.5677, 30.6339, 31.4414],
        [26.5677, 26.5677, 26.5677],
        [26.5677, 28.4726, 28.0868]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.055882152169942856 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055882152169942856 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0616], device='cuda:0')), ('power', tensor([-1.6175], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.055882152169942856
epoch£º260	 i:1 	 global-step:5201	 l-p:0.05608242750167847
epoch£º260	 i:2 	 global-step:5202	 l-p:0.05584504082798958
epoch£º260	 i:3 	 global-step:5203	 l-p:0.05594787374138832
epoch£º260	 i:4 	 global-step:5204	 l-p:0.055919159203767776
epoch£º260	 i:5 	 global-step:5205	 l-p:0.05587029457092285
epoch£º260	 i:6 	 global-step:5206	 l-p:0.05582793429493904
epoch£º260	 i:7 	 global-step:5207	 l-p:0.05592924356460571
epoch£º260	 i:8 	 global-step:5208	 l-p:0.05586527660489082
epoch£º260	 i:9 	 global-step:5209	 l-p:0.05583195388317108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.6244, 26.6245, 26.6244],
        [26.6244, 31.1885, 32.3942],
        [26.6244, 31.5538, 33.0780],
        [26.6244, 27.0647, 26.7649]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.055887240916490555 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055887240916490555 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0894], device='cuda:0')), ('power', tensor([-1.6255], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.055887240916490555
epoch£º261	 i:1 	 global-step:5221	 l-p:0.05583976209163666
epoch£º261	 i:2 	 global-step:5222	 l-p:0.05592292547225952
epoch£º261	 i:3 	 global-step:5223	 l-p:0.05588908493518829
epoch£º261	 i:4 	 global-step:5224	 l-p:0.05599747598171234
epoch£º261	 i:5 	 global-step:5225	 l-p:0.05583406239748001
epoch£º261	 i:6 	 global-step:5226	 l-p:0.05583895742893219
epoch£º261	 i:7 	 global-step:5227	 l-p:0.055912084877491
epoch£º261	 i:8 	 global-step:5228	 l-p:0.055929720401763916
epoch£º261	 i:9 	 global-step:5229	 l-p:0.05581822246313095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.6811, 26.8306, 26.7053],
        [26.6811, 27.2771, 26.9114],
        [26.6811, 26.7277, 26.6849],
        [26.6811, 26.7198, 26.6839]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.055910542607307434 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055910542607307434 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0500], device='cuda:0')), ('power', tensor([-1.5303], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.055910542607307434
epoch£º262	 i:1 	 global-step:5241	 l-p:0.05583111569285393
epoch£º262	 i:2 	 global-step:5242	 l-p:0.05581114813685417
epoch£º262	 i:3 	 global-step:5243	 l-p:0.055882565677165985
epoch£º262	 i:4 	 global-step:5244	 l-p:0.0559825636446476
epoch£º262	 i:5 	 global-step:5245	 l-p:0.05591295287013054
epoch£º262	 i:6 	 global-step:5246	 l-p:0.055848944932222366
epoch£º262	 i:7 	 global-step:5247	 l-p:0.055913642048835754
epoch£º262	 i:8 	 global-step:5248	 l-p:0.05582290515303612
epoch£º262	 i:9 	 global-step:5249	 l-p:0.055821847170591354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.7382, 32.5622, 34.9214],
        [26.7382, 30.9794, 31.9081],
        [26.7382, 27.0599, 26.8221],
        [26.7382, 32.1014, 34.0120]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.05591980367898941 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05591980367898941 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0492], device='cuda:0')), ('power', tensor([-1.4749], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.05591980367898941
epoch£º263	 i:1 	 global-step:5261	 l-p:0.05583275109529495
epoch£º263	 i:2 	 global-step:5262	 l-p:0.05585603415966034
epoch£º263	 i:3 	 global-step:5263	 l-p:0.055829085409641266
epoch£º263	 i:4 	 global-step:5264	 l-p:0.055872593075037
epoch£º263	 i:5 	 global-step:5265	 l-p:0.055822305381298065
epoch£º263	 i:6 	 global-step:5266	 l-p:0.05588722229003906
epoch£º263	 i:7 	 global-step:5267	 l-p:0.055919960141181946
epoch£º263	 i:8 	 global-step:5268	 l-p:0.05580450966954231
epoch£º263	 i:9 	 global-step:5269	 l-p:0.0558624342083931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.7955, 26.8964, 26.8084],
        [26.7955, 32.7701, 35.2729],
        [26.7955, 34.7001, 39.3409],
        [26.7955, 31.4781, 32.7667]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.055810119956731796 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055810119956731796 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1022], device='cuda:0')), ('power', tensor([-1.4923], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.055810119956731796
epoch£º264	 i:1 	 global-step:5281	 l-p:0.05582021176815033
epoch£º264	 i:2 	 global-step:5282	 l-p:0.05582189932465553
epoch£º264	 i:3 	 global-step:5283	 l-p:0.05582800880074501
epoch£º264	 i:4 	 global-step:5284	 l-p:0.0558219812810421
epoch£º264	 i:5 	 global-step:5285	 l-p:0.05590987205505371
epoch£º264	 i:6 	 global-step:5286	 l-p:0.05589856952428818
epoch£º264	 i:7 	 global-step:5287	 l-p:0.05579506605863571
epoch£º264	 i:8 	 global-step:5288	 l-p:0.05587410181760788
epoch£º264	 i:9 	 global-step:5289	 l-p:0.05589521303772926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.8531, 35.3712, 40.7491],
        [26.8531, 29.7254, 29.7211],
        [26.8531, 26.8531, 26.8530],
        [26.8531, 26.8554, 26.8531]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.05584054812788963 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05584054812788963 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0455], device='cuda:0')), ('power', tensor([-1.3482], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.05584054812788963
epoch£º265	 i:1 	 global-step:5301	 l-p:0.055919867008924484
epoch£º265	 i:2 	 global-step:5302	 l-p:0.05580919235944748
epoch£º265	 i:3 	 global-step:5303	 l-p:0.05579088255763054
epoch£º265	 i:4 	 global-step:5304	 l-p:0.05583931878209114
epoch£º265	 i:5 	 global-step:5305	 l-p:0.05590997263789177
epoch£º265	 i:6 	 global-step:5306	 l-p:0.05580265447497368
epoch£º265	 i:7 	 global-step:5307	 l-p:0.055787887424230576
epoch£º265	 i:8 	 global-step:5308	 l-p:0.05578380450606346
epoch£º265	 i:9 	 global-step:5309	 l-p:0.05585992708802223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.9110, 26.9648, 26.9157],
        [26.9110, 26.9213, 26.9114],
        [26.9110, 27.1879, 26.9764],
        [26.9110, 26.9150, 26.9111]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.05579419061541557 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05579419061541557 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0936], device='cuda:0')), ('power', tensor([-1.3708], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.05579419061541557
epoch£º266	 i:1 	 global-step:5321	 l-p:0.055905330926179886
epoch£º266	 i:2 	 global-step:5322	 l-p:0.05578593909740448
epoch£º266	 i:3 	 global-step:5323	 l-p:0.055817682296037674
epoch£º266	 i:4 	 global-step:5324	 l-p:0.055815406143665314
epoch£º266	 i:5 	 global-step:5325	 l-p:0.05580172315239906
epoch£º266	 i:6 	 global-step:5326	 l-p:0.055966634303331375
epoch£º266	 i:7 	 global-step:5327	 l-p:0.05576644837856293
epoch£º266	 i:8 	 global-step:5328	 l-p:0.05578334629535675
epoch£º266	 i:9 	 global-step:5329	 l-p:0.055775728076696396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.9692, 26.9693, 26.9692],
        [26.9692, 29.0905, 28.7535],
        [26.9692, 26.9692, 26.9692],
        [26.9692, 27.1570, 27.0040]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.0558026097714901 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0558026097714901 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0651], device='cuda:0')), ('power', tensor([-1.2299], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.0558026097714901
epoch£º267	 i:1 	 global-step:5341	 l-p:0.05576420575380325
epoch£º267	 i:2 	 global-step:5342	 l-p:0.05588927119970322
epoch£º267	 i:3 	 global-step:5343	 l-p:0.05585191771388054
epoch£º267	 i:4 	 global-step:5344	 l-p:0.055854152888059616
epoch£º267	 i:5 	 global-step:5345	 l-p:0.05583678185939789
epoch£º267	 i:6 	 global-step:5346	 l-p:0.05578385666012764
epoch£º267	 i:7 	 global-step:5347	 l-p:0.05577994883060455
epoch£º267	 i:8 	 global-step:5348	 l-p:0.05577192083001137
epoch£º267	 i:9 	 global-step:5349	 l-p:0.05574631690979004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.0277, 27.0326, 27.0278],
        [27.0277, 30.6363, 31.0704],
        [27.0277, 27.2296, 27.0667],
        [27.0277, 27.0277, 27.0277]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.05577461048960686 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05577461048960686 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0880], device='cuda:0')), ('power', tensor([-1.2187], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.05577461048960686
epoch£º268	 i:1 	 global-step:5361	 l-p:0.055905018001794815
epoch£º268	 i:2 	 global-step:5362	 l-p:0.055818069726228714
epoch£º268	 i:3 	 global-step:5363	 l-p:0.05582164227962494
epoch£º268	 i:4 	 global-step:5364	 l-p:0.05580594390630722
epoch£º268	 i:5 	 global-step:5365	 l-p:0.05578240379691124
epoch£º268	 i:6 	 global-step:5366	 l-p:0.055783092975616455
epoch£º268	 i:7 	 global-step:5367	 l-p:0.055743563920259476
epoch£º268	 i:8 	 global-step:5368	 l-p:0.05577393248677254
epoch£º268	 i:9 	 global-step:5369	 l-p:0.055741194635629654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.0864, 27.0971, 27.0868],
        [27.0864, 27.1702, 27.0958],
        [27.0864, 27.7986, 27.3916],
        [27.0864, 27.5350, 27.2296]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.05574582517147064 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05574582517147064 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1099], device='cuda:0')), ('power', tensor([-1.1302], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.05574582517147064
epoch£º269	 i:1 	 global-step:5381	 l-p:0.055727098137140274
epoch£º269	 i:2 	 global-step:5382	 l-p:0.0557866171002388
epoch£º269	 i:3 	 global-step:5383	 l-p:0.055775392800569534
epoch£º269	 i:4 	 global-step:5384	 l-p:0.055754173547029495
epoch£º269	 i:5 	 global-step:5385	 l-p:0.055813100188970566
epoch£º269	 i:6 	 global-step:5386	 l-p:0.05582472309470177
epoch£º269	 i:7 	 global-step:5387	 l-p:0.055755890905857086
epoch£º269	 i:8 	 global-step:5388	 l-p:0.055846937000751495
epoch£º269	 i:9 	 global-step:5389	 l-p:0.05578770488500595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.1451, 27.2154, 27.1523],
        [27.1451, 31.3050, 32.1316],
        [27.1451, 27.1559, 27.1455],
        [27.1451, 29.6878, 29.5057]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.055728036910295486 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055728036910295486 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1298], device='cuda:0')), ('power', tensor([-1.1787], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.055728036910295486
epoch£º270	 i:1 	 global-step:5401	 l-p:0.055766571313142776
epoch£º270	 i:2 	 global-step:5402	 l-p:0.05578542500734329
epoch£º270	 i:3 	 global-step:5403	 l-p:0.05571543425321579
epoch£º270	 i:4 	 global-step:5404	 l-p:0.055715836584568024
epoch£º270	 i:5 	 global-step:5405	 l-p:0.0559675432741642
epoch£º270	 i:6 	 global-step:5406	 l-p:0.055799052119255066
epoch£º270	 i:7 	 global-step:5407	 l-p:0.055747270584106445
epoch£º270	 i:8 	 global-step:5408	 l-p:0.05572943389415741
epoch£º270	 i:9 	 global-step:5409	 l-p:0.05573186278343201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2044, 27.2044, 27.2044],
        [27.2044, 29.5320, 29.2581],
        [27.2044, 32.6584, 34.5972],
        [27.2044, 35.2580, 40.0005]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.055721577256917953 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055721577256917953 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1153], device='cuda:0')), ('power', tensor([-1.0579], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.055721577256917953
epoch£º271	 i:1 	 global-step:5421	 l-p:0.055711790919303894
epoch£º271	 i:2 	 global-step:5422	 l-p:0.05572381243109703
epoch£º271	 i:3 	 global-step:5423	 l-p:0.05591793358325958
epoch£º271	 i:4 	 global-step:5424	 l-p:0.05571524426341057
epoch£º271	 i:5 	 global-step:5425	 l-p:0.055819977074861526
epoch£º271	 i:6 	 global-step:5426	 l-p:0.055731456726789474
epoch£º271	 i:7 	 global-step:5427	 l-p:0.05577363073825836
epoch£º271	 i:8 	 global-step:5428	 l-p:0.05572887882590294
epoch£º271	 i:9 	 global-step:5429	 l-p:0.05571070685982704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2639, 27.4433, 27.2959],
        [27.2639, 33.4262, 36.0547],
        [27.2639, 27.6333, 27.3677],
        [27.2639, 27.3752, 27.2787]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.055815134197473526 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055815134197473526 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0715], device='cuda:0')), ('power', tensor([-0.9274], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.055815134197473526
epoch£º272	 i:1 	 global-step:5441	 l-p:0.05579058825969696
epoch£º272	 i:2 	 global-step:5442	 l-p:0.055726826190948486
epoch£º272	 i:3 	 global-step:5443	 l-p:0.055752791464328766
epoch£º272	 i:4 	 global-step:5444	 l-p:0.055701617151498795
epoch£º272	 i:5 	 global-step:5445	 l-p:0.055713023990392685
epoch£º272	 i:6 	 global-step:5446	 l-p:0.055723775178194046
epoch£º272	 i:7 	 global-step:5447	 l-p:0.05572841688990593
epoch£º272	 i:8 	 global-step:5448	 l-p:0.05570309981703758
epoch£º272	 i:9 	 global-step:5449	 l-p:0.05576830729842186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3236, 29.2199, 28.8058],
        [27.3236, 27.3533, 27.3254],
        [27.3236, 27.7785, 27.4693],
        [27.3236, 27.4958, 27.3536]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.05584229528903961 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05584229528903961 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0319], device='cuda:0')), ('power', tensor([-0.8029], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.05584229528903961
epoch£º273	 i:1 	 global-step:5461	 l-p:0.0557040199637413
epoch£º273	 i:2 	 global-step:5462	 l-p:0.05568957328796387
epoch£º273	 i:3 	 global-step:5463	 l-p:0.0558180995285511
epoch£º273	 i:4 	 global-step:5464	 l-p:0.05570215731859207
epoch£º273	 i:5 	 global-step:5465	 l-p:0.055723126977682114
epoch£º273	 i:6 	 global-step:5466	 l-p:0.055757589638233185
epoch£º273	 i:7 	 global-step:5467	 l-p:0.055670950561761856
epoch£º273	 i:8 	 global-step:5468	 l-p:0.05566951632499695
epoch£º273	 i:9 	 global-step:5469	 l-p:0.05571484938263893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3837, 31.1975, 31.7458],
        [27.3837, 35.4846, 40.2500],
        [27.3837, 27.4235, 27.3866],
        [27.3837, 35.4707, 40.2195]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.055675096809864044 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055675096809864044 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1438], device='cuda:0')), ('power', tensor([-0.9363], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.055675096809864044
epoch£º274	 i:1 	 global-step:5481	 l-p:0.055678024888038635
epoch£º274	 i:2 	 global-step:5482	 l-p:0.05567878484725952
epoch£º274	 i:3 	 global-step:5483	 l-p:0.055669207125902176
epoch£º274	 i:4 	 global-step:5484	 l-p:0.055846717208623886
epoch£º274	 i:5 	 global-step:5485	 l-p:0.05568598583340645
epoch£º274	 i:6 	 global-step:5486	 l-p:0.05570463091135025
epoch£º274	 i:7 	 global-step:5487	 l-p:0.05580614134669304
epoch£º274	 i:8 	 global-step:5488	 l-p:0.05570773780345917
epoch£º274	 i:9 	 global-step:5489	 l-p:0.05570794641971588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4438, 31.9337, 32.9939],
        [27.4438, 32.1567, 33.4024],
        [27.4438, 27.4438, 27.4438],
        [27.4438, 27.6169, 27.4739]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.05565716326236725 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05565716326236725 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1595], device='cuda:0')), ('power', tensor([-0.8972], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.05565716326236725
epoch£º275	 i:1 	 global-step:5501	 l-p:0.055863771587610245
epoch£º275	 i:2 	 global-step:5502	 l-p:0.055671706795692444
epoch£º275	 i:3 	 global-step:5503	 l-p:0.055735763162374496
epoch£º275	 i:4 	 global-step:5504	 l-p:0.055665768682956696
epoch£º275	 i:5 	 global-step:5505	 l-p:0.05564328655600548
epoch£º275	 i:6 	 global-step:5506	 l-p:0.05568228289484978
epoch£º275	 i:7 	 global-step:5507	 l-p:0.05573922023177147
epoch£º275	 i:8 	 global-step:5508	 l-p:0.05569634214043617
epoch£º275	 i:9 	 global-step:5509	 l-p:0.05567401647567749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5045, 35.0560, 39.1559],
        [27.5045, 27.7036, 27.5422],
        [27.5045, 27.5046, 27.5045],
        [27.5045, 27.5241, 27.5054]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.055640093982219696 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055640093982219696 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1698], device='cuda:0')), ('power', tensor([-0.8381], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.055640093982219696
epoch£º276	 i:1 	 global-step:5521	 l-p:0.055658236145973206
epoch£º276	 i:2 	 global-step:5522	 l-p:0.05591491609811783
epoch£º276	 i:3 	 global-step:5523	 l-p:0.05562901496887207
epoch£º276	 i:4 	 global-step:5524	 l-p:0.05568532645702362
epoch£º276	 i:5 	 global-step:5525	 l-p:0.055640995502471924
epoch£º276	 i:6 	 global-step:5526	 l-p:0.05575487017631531
epoch£º276	 i:7 	 global-step:5527	 l-p:0.05565880611538887
epoch£º276	 i:8 	 global-step:5528	 l-p:0.05567950755357742
epoch£º276	 i:9 	 global-step:5529	 l-p:0.05563607066869736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5653, 33.7995, 36.4590],
        [27.5653, 34.4916, 37.8821],
        [27.5653, 27.5677, 27.5654],
        [27.5653, 32.8736, 34.6325]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.05565749481320381 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05565749481320381 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1143], device='cuda:0')), ('power', tensor([-0.6465], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.05565749481320381
epoch£º277	 i:1 	 global-step:5541	 l-p:0.05564158409833908
epoch£º277	 i:2 	 global-step:5542	 l-p:0.055758483707904816
epoch£º277	 i:3 	 global-step:5543	 l-p:0.05562600865960121
epoch£º277	 i:4 	 global-step:5544	 l-p:0.05567288398742676
epoch£º277	 i:5 	 global-step:5545	 l-p:0.05566137284040451
epoch£º277	 i:6 	 global-step:5546	 l-p:0.055652957409620285
epoch£º277	 i:7 	 global-step:5547	 l-p:0.05561816692352295
epoch£º277	 i:8 	 global-step:5548	 l-p:0.05576231703162193
epoch£º277	 i:9 	 global-step:5549	 l-p:0.05571482703089714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6263, 27.7166, 27.6369],
        [27.6263, 27.9649, 27.7156],
        [27.6263, 28.6327, 28.1560],
        [27.6263, 34.2584, 37.3234]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.05573851987719536 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05573851987719536 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0953], device='cuda:0')), ('power', tensor([-0.5624], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.05573851987719536
epoch£º278	 i:1 	 global-step:5561	 l-p:0.055678289383649826
epoch£º278	 i:2 	 global-step:5562	 l-p:0.05565202236175537
epoch£º278	 i:3 	 global-step:5563	 l-p:0.05569855123758316
epoch£º278	 i:4 	 global-step:5564	 l-p:0.05570051074028015
epoch£º278	 i:5 	 global-step:5565	 l-p:0.05566637963056564
epoch£º278	 i:6 	 global-step:5566	 l-p:0.055632252246141434
epoch£º278	 i:7 	 global-step:5567	 l-p:0.05561740696430206
epoch£º278	 i:8 	 global-step:5568	 l-p:0.055635545402765274
epoch£º278	 i:9 	 global-step:5569	 l-p:0.05561579763889313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6879, 29.2460, 28.7617],
        [27.6879, 28.1089, 27.8150],
        [27.6879, 33.9512, 36.6233],
        [27.6879, 29.0649, 28.5678]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.055618949234485626 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055618949234485626 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1444], device='cuda:0')), ('power', tensor([-0.6155], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.055618949234485626
epoch£º279	 i:1 	 global-step:5581	 l-p:0.05562911927700043
epoch£º279	 i:2 	 global-step:5582	 l-p:0.05580965429544449
epoch£º279	 i:3 	 global-step:5583	 l-p:0.05567483603954315
epoch£º279	 i:4 	 global-step:5584	 l-p:0.0556194968521595
epoch£º279	 i:5 	 global-step:5585	 l-p:0.05566348135471344
epoch£º279	 i:6 	 global-step:5586	 l-p:0.055596426129341125
epoch£º279	 i:7 	 global-step:5587	 l-p:0.05570640414953232
epoch£º279	 i:8 	 global-step:5588	 l-p:0.05559971183538437
epoch£º279	 i:9 	 global-step:5589	 l-p:0.055585529655218124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7495, 27.7605, 27.7499],
        [27.7495, 28.1624, 27.8725],
        [27.7495, 28.5747, 28.1315],
        [27.7495, 29.1774, 28.6812]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.05564064532518387 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05564064532518387 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1405], device='cuda:0')), ('power', tensor([-0.5258], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.05564064532518387
epoch£º280	 i:1 	 global-step:5601	 l-p:0.05566733703017235
epoch£º280	 i:2 	 global-step:5602	 l-p:0.055612221360206604
epoch£º280	 i:3 	 global-step:5603	 l-p:0.05565794184803963
epoch£º280	 i:4 	 global-step:5604	 l-p:0.05566493794322014
epoch£º280	 i:5 	 global-step:5605	 l-p:0.05560474097728729
epoch£º280	 i:6 	 global-step:5606	 l-p:0.055588968098163605
epoch£º280	 i:7 	 global-step:5607	 l-p:0.05569769814610481
epoch£º280	 i:8 	 global-step:5608	 l-p:0.0556240975856781
epoch£º280	 i:9 	 global-step:5609	 l-p:0.05561353638768196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8114, 30.4204, 30.2339],
        [27.8114, 27.8174, 27.8115],
        [27.8114, 27.9640, 27.8358],
        [27.8114, 28.5895, 28.1579]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.05558678135275841 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05558678135275841 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1587], device='cuda:0')), ('power', tensor([-0.4754], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.05558678135275841
epoch£º281	 i:1 	 global-step:5621	 l-p:0.05557903274893761
epoch£º281	 i:2 	 global-step:5622	 l-p:0.05561970919370651
epoch£º281	 i:3 	 global-step:5623	 l-p:0.0556831918656826
epoch£º281	 i:4 	 global-step:5624	 l-p:0.055693499743938446
epoch£º281	 i:5 	 global-step:5625	 l-p:0.055623091757297516
epoch£º281	 i:6 	 global-step:5626	 l-p:0.055574677884578705
epoch£º281	 i:7 	 global-step:5627	 l-p:0.05559003725647926
epoch£º281	 i:8 	 global-step:5628	 l-p:0.05572259798645973
epoch£º281	 i:9 	 global-step:5629	 l-p:0.055568188428878784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[27.8736, 28.7027, 28.2574],
        [27.8736, 30.8574, 30.8511],
        [27.8736, 28.8332, 28.3608],
        [27.8736, 35.7610, 40.1846]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.05570255219936371 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05570255219936371 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0312], device='cuda:0')), ('power', tensor([-0.1817], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.05570255219936371
epoch£º282	 i:1 	 global-step:5641	 l-p:0.055657804012298584
epoch£º282	 i:2 	 global-step:5642	 l-p:0.055564526468515396
epoch£º282	 i:3 	 global-step:5643	 l-p:0.055647190660238266
epoch£º282	 i:4 	 global-step:5644	 l-p:0.055634524673223495
epoch£º282	 i:5 	 global-step:5645	 l-p:0.05558145046234131
epoch£º282	 i:6 	 global-step:5646	 l-p:0.055573634803295135
epoch£º282	 i:7 	 global-step:5647	 l-p:0.055582448840141296
epoch£º282	 i:8 	 global-step:5648	 l-p:0.05559166893362999
epoch£º282	 i:9 	 global-step:5649	 l-p:0.05557408556342125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9362, 32.1712, 32.9828],
        [27.9362, 35.8422, 40.2763],
        [27.9362, 32.1708, 32.9822],
        [27.9362, 27.9851, 27.9401]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.055587876588106155 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055587876588106155 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1209], device='cuda:0')), ('power', tensor([-0.2898], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.055587876588106155
epoch£º283	 i:1 	 global-step:5661	 l-p:0.0555802620947361
epoch£º283	 i:2 	 global-step:5662	 l-p:0.055562522262334824
epoch£º283	 i:3 	 global-step:5663	 l-p:0.05558467656373978
epoch£º283	 i:4 	 global-step:5664	 l-p:0.05558616667985916
epoch£º283	 i:5 	 global-step:5665	 l-p:0.055616553872823715
epoch£º283	 i:6 	 global-step:5666	 l-p:0.055726271122694016
epoch£º283	 i:7 	 global-step:5667	 l-p:0.055583756417036057
epoch£º283	 i:8 	 global-step:5668	 l-p:0.05553609877824783
epoch£º283	 i:9 	 global-step:5669	 l-p:0.05561394989490509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9988, 28.1562, 28.0243],
        [27.9988, 28.0846, 28.0084],
        [27.9988, 28.0027, 27.9989],
        [27.9988, 31.1353, 31.2050]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.0555383563041687 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0555383563041687 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1871], device='cuda:0')), ('power', tensor([-0.3095], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.0555383563041687
epoch£º284	 i:1 	 global-step:5681	 l-p:0.05558858439326286
epoch£º284	 i:2 	 global-step:5682	 l-p:0.05556638538837433
epoch£º284	 i:3 	 global-step:5683	 l-p:0.05555105581879616
epoch£º284	 i:4 	 global-step:5684	 l-p:0.055542733520269394
epoch£º284	 i:5 	 global-step:5685	 l-p:0.05563456937670708
epoch£º284	 i:6 	 global-step:5686	 l-p:0.05553710460662842
epoch£º284	 i:7 	 global-step:5687	 l-p:0.05556878447532654
epoch£º284	 i:8 	 global-step:5688	 l-p:0.05569131672382355
epoch£º284	 i:9 	 global-step:5689	 l-p:0.0556279756128788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0617, 28.0617, 28.0617],
        [28.0617, 28.1886, 28.0797],
        [28.0617, 28.9043, 28.4540],
        [28.0617, 32.1447, 32.8307]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.055541083216667175 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055541083216667175 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1653], device='cuda:0')), ('power', tensor([-0.2270], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.055541083216667175
epoch£º285	 i:1 	 global-step:5701	 l-p:0.055564478039741516
epoch£º285	 i:2 	 global-step:5702	 l-p:0.0556534081697464
epoch£º285	 i:3 	 global-step:5703	 l-p:0.05556856840848923
epoch£º285	 i:4 	 global-step:5704	 l-p:0.055517710745334625
epoch£º285	 i:5 	 global-step:5705	 l-p:0.05565386638045311
epoch£º285	 i:6 	 global-step:5706	 l-p:0.05559972673654556
epoch£º285	 i:7 	 global-step:5707	 l-p:0.055541589856147766
epoch£º285	 i:8 	 global-step:5708	 l-p:0.05555122718214989
epoch£º285	 i:9 	 global-step:5709	 l-p:0.055524569004774094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1252, 28.3557, 28.1723],
        [28.1252, 28.1255, 28.1252],
        [28.1252, 28.1252, 28.1252],
        [28.1252, 28.1538, 28.1269]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.05552290007472038 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05552290007472038 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1755], device='cuda:0')), ('power', tensor([-0.2014], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.05552290007472038
epoch£º286	 i:1 	 global-step:5721	 l-p:0.05559062957763672
epoch£º286	 i:2 	 global-step:5722	 l-p:0.055579040199518204
epoch£º286	 i:3 	 global-step:5723	 l-p:0.05552954971790314
epoch£º286	 i:4 	 global-step:5724	 l-p:0.0555436797440052
epoch£º286	 i:5 	 global-step:5725	 l-p:0.05562162399291992
epoch£º286	 i:6 	 global-step:5726	 l-p:0.05563598498702049
epoch£º286	 i:7 	 global-step:5727	 l-p:0.05552233010530472
epoch£º286	 i:8 	 global-step:5728	 l-p:0.05551229044795036
epoch£º286	 i:9 	 global-step:5729	 l-p:0.05552682653069496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1888, 31.9947, 32.4721],
        [28.1888, 32.8076, 33.8989],
        [28.1888, 29.5349, 29.0272],
        [28.1888, 28.5718, 28.2964]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.05551830679178238 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05551830679178238 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1616], device='cuda:0')), ('power', tensor([-0.0957], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.05551830679178238
epoch£º287	 i:1 	 global-step:5741	 l-p:0.05551895499229431
epoch£º287	 i:2 	 global-step:5742	 l-p:0.05550167337059975
epoch£º287	 i:3 	 global-step:5743	 l-p:0.05550879240036011
epoch£º287	 i:4 	 global-step:5744	 l-p:0.05550574138760567
epoch£º287	 i:5 	 global-step:5745	 l-p:0.05555647611618042
epoch£º287	 i:6 	 global-step:5746	 l-p:0.05563494563102722
epoch£º287	 i:7 	 global-step:5747	 l-p:0.055593863129615784
epoch£º287	 i:8 	 global-step:5748	 l-p:0.05558031052350998
epoch£º287	 i:9 	 global-step:5749	 l-p:0.05553432181477547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2525, 28.4643, 28.2935],
        [28.2525, 28.2525, 28.2525],
        [28.2525, 28.2684, 28.2532],
        [28.2525, 31.5160, 31.6423]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.0554976724088192 
model_pd.l_d.mean(): -4.017510946141556e-07 
model_pd.lagr.mean(): 0.055497270077466965 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.7410e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1748], device='cuda:0')), ('power', tensor([-0.0350], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.0554976724088192
epoch£º288	 i:1 	 global-step:5761	 l-p:0.05548124015331268
epoch£º288	 i:2 	 global-step:5762	 l-p:0.05550175532698631
epoch£º288	 i:3 	 global-step:5763	 l-p:0.05558845400810242
epoch£º288	 i:4 	 global-step:5764	 l-p:0.05552162230014801
epoch£º288	 i:5 	 global-step:5765	 l-p:0.05550095811486244
epoch£º288	 i:6 	 global-step:5766	 l-p:0.055556051433086395
epoch£º288	 i:7 	 global-step:5767	 l-p:0.05549674481153488
epoch£º288	 i:8 	 global-step:5768	 l-p:0.05550799146294594
epoch£º288	 i:9 	 global-step:5769	 l-p:0.05567125603556633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3151, 33.6250, 35.2979],
        [28.3151, 28.3307, 28.3157],
        [28.3151, 28.3262, 28.3155],
        [28.3151, 28.5206, 28.3540]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.05558352917432785 
model_pd.l_d.mean(): 3.577444886104786e-06 
model_pd.lagr.mean(): 0.055587105453014374 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.5323e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0822], device='cuda:0')), ('power', tensor([0.1225], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.05558352917432785
epoch£º289	 i:1 	 global-step:5781	 l-p:0.05550937354564667
epoch£º289	 i:2 	 global-step:5782	 l-p:0.05552910640835762
epoch£º289	 i:3 	 global-step:5783	 l-p:0.055462587624788284
epoch£º289	 i:4 	 global-step:5784	 l-p:0.05549014359712601
epoch£º289	 i:5 	 global-step:5785	 l-p:0.05548127368092537
epoch£º289	 i:6 	 global-step:5786	 l-p:0.05556473508477211
epoch£º289	 i:7 	 global-step:5787	 l-p:0.05552785098552704
epoch£º289	 i:8 	 global-step:5788	 l-p:0.05549228563904762
epoch£º289	 i:9 	 global-step:5789	 l-p:0.05555916577577591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3725, 28.3725, 28.3725],
        [28.3725, 28.4217, 28.3764],
        [28.3725, 28.3725, 28.3724],
        [28.3725, 29.6904, 29.1792]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.05547541379928589 
model_pd.l_d.mean(): 1.5090242868609494e-06 
model_pd.lagr.mean(): 0.055476922541856766 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.8187e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1900], device='cuda:0')), ('power', tensor([0.0195], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.05547541379928589
epoch£º290	 i:1 	 global-step:5801	 l-p:0.05545912683010101
epoch£º290	 i:2 	 global-step:5802	 l-p:0.05548233538866043
epoch£º290	 i:3 	 global-step:5803	 l-p:0.05550171434879303
epoch£º290	 i:4 	 global-step:5804	 l-p:0.05553895980119705
epoch£º290	 i:5 	 global-step:5805	 l-p:0.05546301603317261
epoch£º290	 i:6 	 global-step:5806	 l-p:0.055490341037511826
epoch£º290	 i:7 	 global-step:5807	 l-p:0.05549054592847824
epoch£º290	 i:8 	 global-step:5808	 l-p:0.05563816800713539
epoch£º290	 i:9 	 global-step:5809	 l-p:0.055553410202264786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4183, 30.0754, 29.5842],
        [28.4183, 32.9418, 33.9338],
        [28.4183, 28.6357, 28.4609],
        [28.4183, 28.4186, 28.4183]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.055452004075050354 
model_pd.l_d.mean(): 6.931505140528316e-06 
model_pd.lagr.mean(): 0.05545893684029579 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2177], device='cuda:0')), ('power', tensor([0.0458], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.055452004075050354
epoch£º291	 i:1 	 global-step:5821	 l-p:0.0555395781993866
epoch£º291	 i:2 	 global-step:5822	 l-p:0.05547143891453743
epoch£º291	 i:3 	 global-step:5823	 l-p:0.05551544204354286
epoch£º291	 i:4 	 global-step:5824	 l-p:0.05546186491847038
epoch£º291	 i:5 	 global-step:5825	 l-p:0.05556754395365715
epoch£º291	 i:6 	 global-step:5826	 l-p:0.05556200072169304
epoch£º291	 i:7 	 global-step:5827	 l-p:0.05550843104720116
epoch£º291	 i:8 	 global-step:5828	 l-p:0.05547483637928963
epoch£º291	 i:9 	 global-step:5829	 l-p:0.055463556200265884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4447, 28.4449, 28.4447],
        [28.4447, 28.5272, 28.4536],
        [28.4447, 28.4506, 28.4449],
        [28.4447, 28.4448, 28.4447]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.05549111217260361 
model_pd.l_d.mean(): 4.302263550926e-05 
model_pd.lagr.mean(): 0.055534135550260544 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1593], device='cuda:0')), ('power', tensor([0.1760], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.05549111217260361
epoch£º292	 i:1 	 global-step:5841	 l-p:0.05556900054216385
epoch£º292	 i:2 	 global-step:5842	 l-p:0.0554598793387413
epoch£º292	 i:3 	 global-step:5843	 l-p:0.05551609396934509
epoch£º292	 i:4 	 global-step:5844	 l-p:0.05546066164970398
epoch£º292	 i:5 	 global-step:5845	 l-p:0.05547897517681122
epoch£º292	 i:6 	 global-step:5846	 l-p:0.055451154708862305
epoch£º292	 i:7 	 global-step:5847	 l-p:0.05563459172844887
epoch£º292	 i:8 	 global-step:5848	 l-p:0.055470094084739685
epoch£º292	 i:9 	 global-step:5849	 l-p:0.055455464869737625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[28.4440, 30.4673, 30.0466],
        [28.4440, 30.4196, 29.9869],
        [28.4440, 29.2989, 28.8421],
        [28.4440, 29.1217, 28.7163]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.05545857921242714 
model_pd.l_d.mean(): 4.489993807510473e-05 
model_pd.lagr.mean(): 0.0555034801363945 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2039], device='cuda:0')), ('power', tensor([0.1302], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.05545857921242714
epoch£º293	 i:1 	 global-step:5861	 l-p:0.05555221438407898
epoch£º293	 i:2 	 global-step:5862	 l-p:0.05546269938349724
epoch£º293	 i:3 	 global-step:5863	 l-p:0.05556280538439751
epoch£º293	 i:4 	 global-step:5864	 l-p:0.0554530993103981
epoch£º293	 i:5 	 global-step:5865	 l-p:0.05547821521759033
epoch£º293	 i:6 	 global-step:5866	 l-p:0.05554498732089996
epoch£º293	 i:7 	 global-step:5867	 l-p:0.05554518103599548
epoch£º293	 i:8 	 global-step:5868	 l-p:0.05544998124241829
epoch£º293	 i:9 	 global-step:5869	 l-p:0.055508408695459366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[28.4125, 29.4494, 28.9584],
        [28.4125, 33.3011, 34.5943],
        [28.4125, 33.9875, 35.8920],
        [28.4125, 30.7638, 30.4456]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.05554524436593056 
model_pd.l_d.mean(): 7.771386299282312e-05 
model_pd.lagr.mean(): 0.05562295764684677 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1392], device='cuda:0')), ('power', tensor([0.1775], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.05554524436593056
epoch£º294	 i:1 	 global-step:5881	 l-p:0.05548040196299553
epoch£º294	 i:2 	 global-step:5882	 l-p:0.05552301183342934
epoch£º294	 i:3 	 global-step:5883	 l-p:0.05555671453475952
epoch£º294	 i:4 	 global-step:5884	 l-p:0.055505212396383286
epoch£º294	 i:5 	 global-step:5885	 l-p:0.055488113313913345
epoch£º294	 i:6 	 global-step:5886	 l-p:0.05547947809100151
epoch£º294	 i:7 	 global-step:5887	 l-p:0.05547391623258591
epoch£º294	 i:8 	 global-step:5888	 l-p:0.055504653602838516
epoch£º294	 i:9 	 global-step:5889	 l-p:0.05555111914873123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3507, 29.8712, 29.3674],
        [28.3507, 28.3540, 28.3508],
        [28.3507, 28.3735, 28.3519],
        [28.3507, 28.3509, 28.3507]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.055462565273046494 
model_pd.l_d.mean(): 1.896013054647483e-05 
model_pd.lagr.mean(): 0.055481527000665665 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2177], device='cuda:0')), ('power', tensor([0.0373], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.055462565273046494
epoch£º295	 i:1 	 global-step:5901	 l-p:0.05557768791913986
epoch£º295	 i:2 	 global-step:5902	 l-p:0.05553101375699043
epoch£º295	 i:3 	 global-step:5903	 l-p:0.055486153811216354
epoch£º295	 i:4 	 global-step:5904	 l-p:0.05551249533891678
epoch£º295	 i:5 	 global-step:5905	 l-p:0.055466167628765106
epoch£º295	 i:6 	 global-step:5906	 l-p:0.05555954948067665
epoch£º295	 i:7 	 global-step:5907	 l-p:0.05564572289586067
epoch£º295	 i:8 	 global-step:5908	 l-p:0.05550975352525711
epoch£º295	 i:9 	 global-step:5909	 l-p:0.055506329983472824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2645, 30.0563, 29.5903],
        [28.2645, 28.3596, 28.2757],
        [28.2645, 28.2756, 28.2648],
        [28.2645, 28.2645, 28.2644]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.055500879883766174 
model_pd.l_d.mean(): -2.181729541916866e-05 
model_pd.lagr.mean(): 0.05547906085848808 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1705], device='cuda:0')), ('power', tensor([-0.0402], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.055500879883766174
epoch£º296	 i:1 	 global-step:5921	 l-p:0.055509570986032486
epoch£º296	 i:2 	 global-step:5922	 l-p:0.05552032217383385
epoch£º296	 i:3 	 global-step:5923	 l-p:0.05560491979122162
epoch£º296	 i:4 	 global-step:5924	 l-p:0.0555986724793911
epoch£º296	 i:5 	 global-step:5925	 l-p:0.05555793270468712
epoch£º296	 i:6 	 global-step:5926	 l-p:0.055581234395504
epoch£º296	 i:7 	 global-step:5927	 l-p:0.05549309030175209
epoch£º296	 i:8 	 global-step:5928	 l-p:0.05556683987379074
epoch£º296	 i:9 	 global-step:5929	 l-p:0.055515483021736145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1638, 36.4928, 41.3848],
        [28.1638, 29.4997, 28.9924],
        [28.1638, 28.3727, 28.2039],
        [28.1638, 28.1748, 28.1641]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.05558214709162712 
model_pd.l_d.mean(): -1.7930904505192302e-05 
model_pd.lagr.mean(): 0.055564217269420624 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1029], device='cuda:0')), ('power', tensor([-0.0338], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.05558214709162712
epoch£º297	 i:1 	 global-step:5941	 l-p:0.05553999915719032
epoch£º297	 i:2 	 global-step:5942	 l-p:0.05559975653886795
epoch£º297	 i:3 	 global-step:5943	 l-p:0.05570589751005173
epoch£º297	 i:4 	 global-step:5944	 l-p:0.0555514320731163
epoch£º297	 i:5 	 global-step:5945	 l-p:0.05553261563181877
epoch£º297	 i:6 	 global-step:5946	 l-p:0.05552889406681061
epoch£º297	 i:7 	 global-step:5947	 l-p:0.055516429245471954
epoch£º297	 i:8 	 global-step:5948	 l-p:0.05553042143583298
epoch£º297	 i:9 	 global-step:5949	 l-p:0.055572036653757095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0614, 28.0615, 28.0614],
        [28.0614, 28.6901, 28.3044],
        [28.0614, 28.0632, 28.0614],
        [28.0614, 37.6885, 44.2198]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.05558973178267479 
model_pd.l_d.mean(): -9.680633229436353e-05 
model_pd.lagr.mean(): 0.05549292638897896 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1257], device='cuda:0')), ('power', tensor([-0.2076], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.05558973178267479
epoch£º298	 i:1 	 global-step:5961	 l-p:0.055537812411785126
epoch£º298	 i:2 	 global-step:5962	 l-p:0.05564972385764122
epoch£º298	 i:3 	 global-step:5963	 l-p:0.055615246295928955
epoch£º298	 i:4 	 global-step:5964	 l-p:0.0555802620947361
epoch£º298	 i:5 	 global-step:5965	 l-p:0.055594589561223984
epoch£º298	 i:6 	 global-step:5966	 l-p:0.05554397031664848
epoch£º298	 i:7 	 global-step:5967	 l-p:0.0555708073079586
epoch£º298	 i:8 	 global-step:5968	 l-p:0.0555591955780983
epoch£º298	 i:9 	 global-step:5969	 l-p:0.055620789527893066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9735, 28.1270, 27.9980],
        [27.9735, 28.9978, 28.5142],
        [27.9735, 29.9592, 29.5452],
        [27.9735, 31.1642, 31.2665]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.05555938184261322 
model_pd.l_d.mean(): -0.00011090002954006195 
model_pd.lagr.mean(): 0.05544847995042801 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1528], device='cuda:0')), ('power', tensor([-0.3131], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.05555938184261322
epoch£º299	 i:1 	 global-step:5981	 l-p:0.05556746944785118
epoch£º299	 i:2 	 global-step:5982	 l-p:0.05557452142238617
epoch£º299	 i:3 	 global-step:5983	 l-p:0.05565124377608299
epoch£º299	 i:4 	 global-step:5984	 l-p:0.055540360510349274
epoch£º299	 i:5 	 global-step:5985	 l-p:0.05558260902762413
epoch£º299	 i:6 	 global-step:5986	 l-p:0.05555138364434242
epoch£º299	 i:7 	 global-step:5987	 l-p:0.055581048130989075
epoch£º299	 i:8 	 global-step:5988	 l-p:0.05580592900514603
epoch£º299	 i:9 	 global-step:5989	 l-p:0.055606357753276825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9159, 36.0975, 40.8598],
        [27.9159, 28.3592, 28.0535],
        [27.9159, 27.9159, 27.9159],
        [27.9159, 28.3406, 28.0442]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.05556197836995125 
model_pd.l_d.mean(): -8.191048254957423e-05 
model_pd.lagr.mean(): 0.05548006668686867 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1703], device='cuda:0')), ('power', tensor([-0.4003], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.05556197836995125
epoch£º300	 i:1 	 global-step:6001	 l-p:0.05581820011138916
epoch£º300	 i:2 	 global-step:6002	 l-p:0.05560528486967087
epoch£º300	 i:3 	 global-step:6003	 l-p:0.05557234585285187
epoch£º300	 i:4 	 global-step:6004	 l-p:0.05555054545402527
epoch£º300	 i:5 	 global-step:6005	 l-p:0.055591318756341934
epoch£º300	 i:6 	 global-step:6006	 l-p:0.05558520182967186
epoch£º300	 i:7 	 global-step:6007	 l-p:0.05559554323554039
epoch£º300	 i:8 	 global-step:6008	 l-p:0.05566909536719322
epoch£º300	 i:9 	 global-step:6009	 l-p:0.05555231496691704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9007, 28.8613, 28.3885],
        [27.9007, 27.9007, 27.9007],
        [27.9007, 31.3945, 31.6864],
        [27.9007, 27.9007, 27.9007]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.05567222088575363 
model_pd.l_d.mean(): -9.016865078592673e-06 
model_pd.lagr.mean(): 0.055663205683231354 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.2809e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1055], device='cuda:0')), ('power', tensor([-0.2540], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.05567222088575363
epoch£º301	 i:1 	 global-step:6021	 l-p:0.05557670816779137
epoch£º301	 i:2 	 global-step:6022	 l-p:0.05566500872373581
epoch£º301	 i:3 	 global-step:6023	 l-p:0.05556797981262207
epoch£º301	 i:4 	 global-step:6024	 l-p:0.05563092976808548
epoch£º301	 i:5 	 global-step:6025	 l-p:0.0556458942592144
epoch£º301	 i:6 	 global-step:6026	 l-p:0.05556575953960419
epoch£º301	 i:7 	 global-step:6027	 l-p:0.05558900535106659
epoch£º301	 i:8 	 global-step:6028	 l-p:0.05557870864868164
epoch£º301	 i:9 	 global-step:6029	 l-p:0.05559712275862694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9315, 29.1418, 28.6414],
        [27.9315, 27.9473, 27.9322],
        [27.9315, 35.6134, 39.7889],
        [27.9315, 27.9455, 27.9321]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.055557526648044586 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055557526648044586 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1807], device='cuda:0')), ('power', tensor([-0.3836], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.055557526648044586
epoch£º302	 i:1 	 global-step:6041	 l-p:0.055561818182468414
epoch£º302	 i:2 	 global-step:6042	 l-p:0.055587079375982285
epoch£º302	 i:3 	 global-step:6043	 l-p:0.05568678304553032
epoch£º302	 i:4 	 global-step:6044	 l-p:0.055646996945142746
epoch£º302	 i:5 	 global-step:6045	 l-p:0.05556546151638031
epoch£º302	 i:6 	 global-step:6046	 l-p:0.05558605492115021
epoch£º302	 i:7 	 global-step:6047	 l-p:0.055576637387275696
epoch£º302	 i:8 	 global-step:6048	 l-p:0.055643051862716675
epoch£º302	 i:9 	 global-step:6049	 l-p:0.05558532476425171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9873, 27.9874, 27.9873],
        [27.9873, 29.0743, 28.5827],
        [27.9873, 37.3531, 43.5640],
        [27.9873, 29.8456, 29.4012]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.05565489083528519 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05565489083528519 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0992], device='cuda:0')), ('power', tensor([-0.2435], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.05565489083528519
epoch£º303	 i:1 	 global-step:6061	 l-p:0.05555722117424011
epoch£º303	 i:2 	 global-step:6062	 l-p:0.055551741272211075
epoch£º303	 i:3 	 global-step:6063	 l-p:0.055563099682331085
epoch£º303	 i:4 	 global-step:6064	 l-p:0.055549029260873795
epoch£º303	 i:5 	 global-step:6065	 l-p:0.05554766580462456
epoch£º303	 i:6 	 global-step:6066	 l-p:0.05558496341109276
epoch£º303	 i:7 	 global-step:6067	 l-p:0.05553177744150162
epoch£º303	 i:8 	 global-step:6068	 l-p:0.055663011968135834
epoch£º303	 i:9 	 global-step:6069	 l-p:0.05566653236746788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0522, 28.0521, 28.0522],
        [28.0522, 28.0522, 28.0521],
        [28.0522, 28.0673, 28.0528],
        [28.0522, 28.0573, 28.0523]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.0556558296084404 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0556558296084404 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1153], device='cuda:0')), ('power', tensor([-0.1504], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.0556558296084404
epoch£º304	 i:1 	 global-step:6081	 l-p:0.05561967194080353
epoch£º304	 i:2 	 global-step:6082	 l-p:0.05559591203927994
epoch£º304	 i:3 	 global-step:6083	 l-p:0.055566586554050446
epoch£º304	 i:4 	 global-step:6084	 l-p:0.055563438683748245
epoch£º304	 i:5 	 global-step:6085	 l-p:0.055601757019758224
epoch£º304	 i:6 	 global-step:6086	 l-p:0.05554174259305
epoch£º304	 i:7 	 global-step:6087	 l-p:0.05555272102355957
epoch£º304	 i:8 	 global-step:6088	 l-p:0.05551604554057121
epoch£º304	 i:9 	 global-step:6089	 l-p:0.055518243461847305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1207, 30.0814, 29.6560],
        [28.1207, 28.1223, 28.1207],
        [28.1207, 36.2703, 40.9569],
        [28.1207, 28.9602, 28.5101]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.05554543435573578 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554543435573578 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1342], device='cuda:0')), ('power', tensor([-0.1279], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.05554543435573578
epoch£º305	 i:1 	 global-step:6101	 l-p:0.055500030517578125
epoch£º305	 i:2 	 global-step:6102	 l-p:0.055537980049848557
epoch£º305	 i:3 	 global-step:6103	 l-p:0.055521633476018906
epoch£º305	 i:4 	 global-step:6104	 l-p:0.05554269254207611
epoch£º305	 i:5 	 global-step:6105	 l-p:0.0557146817445755
epoch£º305	 i:6 	 global-step:6106	 l-p:0.05551731958985329
epoch£º305	 i:7 	 global-step:6107	 l-p:0.05559020861983299
epoch£º305	 i:8 	 global-step:6108	 l-p:0.05558325722813606
epoch£º305	 i:9 	 global-step:6109	 l-p:0.055535152554512024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1902, 28.1902, 28.1902],
        [28.1902, 36.1718, 40.6486],
        [28.1902, 28.1902, 28.1902],
        [28.1902, 31.0118, 30.9043]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.055615559220314026 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055615559220314026 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.3903e-07], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0830], device='cuda:0')), ('power', tensor([0.0028], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.055615559220314026
epoch£º306	 i:1 	 global-step:6121	 l-p:0.055546097457408905
epoch£º306	 i:2 	 global-step:6122	 l-p:0.055522073060274124
epoch£º306	 i:3 	 global-step:6123	 l-p:0.05551047995686531
epoch£º306	 i:4 	 global-step:6124	 l-p:0.05554846674203873
epoch£º306	 i:5 	 global-step:6125	 l-p:0.05552998557686806
epoch£º306	 i:6 	 global-step:6126	 l-p:0.055503614246845245
epoch£º306	 i:7 	 global-step:6127	 l-p:0.05560998618602753
epoch£º306	 i:8 	 global-step:6128	 l-p:0.05551408603787422
epoch£º306	 i:9 	 global-step:6129	 l-p:0.05554448440670967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2606, 28.2606, 28.2605],
        [28.2606, 28.8210, 28.4609],
        [28.2606, 34.5752, 37.2195],
        [28.2606, 28.7544, 28.4234]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.055494002997875214 
model_pd.l_d.mean(): -9.265064826458058e-10 
model_pd.lagr.mean(): 0.055494002997875214 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.9540e-07], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1866], device='cuda:0')), ('power', tensor([-0.0019], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.055494002997875214
epoch£º307	 i:1 	 global-step:6141	 l-p:0.05554701387882233
epoch£º307	 i:2 	 global-step:6142	 l-p:0.05550910905003548
epoch£º307	 i:3 	 global-step:6143	 l-p:0.05560396984219551
epoch£º307	 i:4 	 global-step:6144	 l-p:0.055535200983285904
epoch£º307	 i:5 	 global-step:6145	 l-p:0.05550648272037506
epoch£º307	 i:6 	 global-step:6146	 l-p:0.055522654205560684
epoch£º307	 i:7 	 global-step:6147	 l-p:0.05560824275016785
epoch£º307	 i:8 	 global-step:6148	 l-p:0.05547850951552391
epoch£º307	 i:9 	 global-step:6149	 l-p:0.0554957315325737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3304, 28.4661, 28.3503],
        [28.3304, 28.5285, 28.3671],
        [28.3304, 29.0052, 28.6015],
        [28.3304, 31.0817, 30.9330]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.055465325713157654 
model_pd.l_d.mean(): -8.279689041046367e-07 
model_pd.lagr.mean(): 0.055464498698711395 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.2150e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2248], device='cuda:0')), ('power', tensor([-0.0347], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.055465325713157654
epoch£º308	 i:1 	 global-step:6161	 l-p:0.05558756738901138
epoch£º308	 i:2 	 global-step:6162	 l-p:0.055506352335214615
epoch£º308	 i:3 	 global-step:6163	 l-p:0.055518005043268204
epoch£º308	 i:4 	 global-step:6164	 l-p:0.05557741969823837
epoch£º308	 i:5 	 global-step:6165	 l-p:0.05554657429456711
epoch£º308	 i:6 	 global-step:6166	 l-p:0.055516283959150314
epoch£º308	 i:7 	 global-step:6167	 l-p:0.055465832352638245
epoch£º308	 i:8 	 global-step:6168	 l-p:0.05547097697854042
epoch£º308	 i:9 	 global-step:6169	 l-p:0.05550835654139519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3947, 29.7513, 29.2396],
        [28.3947, 33.0484, 34.1478],
        [28.3947, 28.3980, 28.3947],
        [28.3947, 34.3463, 36.6118]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.055575065314769745 
model_pd.l_d.mean(): 1.408160005667014e-05 
model_pd.lagr.mean(): 0.05558914691209793 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.9773e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1005], device='cuda:0')), ('power', tensor([0.1737], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.055575065314769745
epoch£º309	 i:1 	 global-step:6181	 l-p:0.055476732552051544
epoch£º309	 i:2 	 global-step:6182	 l-p:0.05546578764915466
epoch£º309	 i:3 	 global-step:6183	 l-p:0.05548980459570885
epoch£º309	 i:4 	 global-step:6184	 l-p:0.055521972477436066
epoch£º309	 i:5 	 global-step:6185	 l-p:0.055473022162914276
epoch£º309	 i:6 	 global-step:6186	 l-p:0.05562982335686684
epoch£º309	 i:7 	 global-step:6187	 l-p:0.055468376725912094
epoch£º309	 i:8 	 global-step:6188	 l-p:0.055475279688835144
epoch£º309	 i:9 	 global-step:6189	 l-p:0.055468857288360596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4432, 28.5570, 28.4582],
        [28.4432, 28.4457, 28.4433],
        [28.4432, 32.9770, 33.9749],
        [28.4432, 35.4601, 38.8120]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.055448293685913086 
model_pd.l_d.mean(): 2.5097351681324653e-05 
model_pd.lagr.mean(): 0.055473390966653824 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2102], device='cuda:0')), ('power', tensor([0.1501], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.055448293685913086
epoch£º310	 i:1 	 global-step:6201	 l-p:0.05544021353125572
epoch£º310	 i:2 	 global-step:6202	 l-p:0.05548017472028732
epoch£º310	 i:3 	 global-step:6203	 l-p:0.05546959117054939
epoch£º310	 i:4 	 global-step:6204	 l-p:0.055503811687231064
epoch£º310	 i:5 	 global-step:6205	 l-p:0.05548248067498207
epoch£º310	 i:6 	 global-step:6206	 l-p:0.05557881295681
epoch£º310	 i:7 	 global-step:6207	 l-p:0.055469855666160583
epoch£º310	 i:8 	 global-step:6208	 l-p:0.0556490421295166
epoch£º310	 i:9 	 global-step:6209	 l-p:0.055446114391088486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[28.4666, 30.5168, 30.1023],
        [28.4666, 30.4469, 30.0147],
        [28.4666, 29.7016, 29.1910],
        [28.4666, 30.9326, 30.6549]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.055566199123859406 
model_pd.l_d.mean(): 7.536966586485505e-05 
model_pd.lagr.mean(): 0.05564156919717789 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1234], device='cuda:0')), ('power', tensor([0.2769], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.055566199123859406
epoch£º311	 i:1 	 global-step:6221	 l-p:0.05547346547245979
epoch£º311	 i:2 	 global-step:6222	 l-p:0.05547706037759781
epoch£º311	 i:3 	 global-step:6223	 l-p:0.055456630885601044
epoch£º311	 i:4 	 global-step:6224	 l-p:0.055470969527959824
epoch£º311	 i:5 	 global-step:6225	 l-p:0.05555378273129463
epoch£º311	 i:6 	 global-step:6226	 l-p:0.05552823096513748
epoch£º311	 i:7 	 global-step:6227	 l-p:0.05553564801812172
epoch£º311	 i:8 	 global-step:6228	 l-p:0.05544060468673706
epoch£º311	 i:9 	 global-step:6229	 l-p:0.0554472990334034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4568, 30.5658, 30.1678],
        [28.4568, 28.5500, 28.4677],
        [28.4568, 28.4568, 28.4568],
        [28.4568, 35.2988, 38.4618]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.05544710159301758 
model_pd.l_d.mean(): 5.4483432904817164e-05 
model_pd.lagr.mean(): 0.055501583963632584 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2058], device='cuda:0')), ('power', tensor([0.1428], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.05544710159301758
epoch£º312	 i:1 	 global-step:6241	 l-p:0.055460114032030106
epoch£º312	 i:2 	 global-step:6242	 l-p:0.05550741031765938
epoch£º312	 i:3 	 global-step:6243	 l-p:0.05548292025923729
epoch£º312	 i:4 	 global-step:6244	 l-p:0.0555717758834362
epoch£º312	 i:5 	 global-step:6245	 l-p:0.05545289069414139
epoch£º312	 i:6 	 global-step:6246	 l-p:0.05546335503458977
epoch£º312	 i:7 	 global-step:6247	 l-p:0.05549414083361626
epoch£º312	 i:8 	 global-step:6248	 l-p:0.0554678812623024
epoch£º312	 i:9 	 global-step:6249	 l-p:0.05565604567527771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4103, 28.9328, 28.5883],
        [28.4103, 36.2233, 40.4664],
        [28.4103, 30.5156, 30.1183],
        [28.4103, 28.5733, 28.4370]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.055461328476667404 
model_pd.l_d.mean(): 4.5755306928185746e-05 
model_pd.lagr.mean(): 0.05550708249211311 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1919], device='cuda:0')), ('power', tensor([0.0957], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.055461328476667404
epoch£º313	 i:1 	 global-step:6261	 l-p:0.05547061935067177
epoch£º313	 i:2 	 global-step:6262	 l-p:0.055563438683748245
epoch£º313	 i:3 	 global-step:6263	 l-p:0.05550285056233406
epoch£º313	 i:4 	 global-step:6264	 l-p:0.05552605167031288
epoch£º313	 i:5 	 global-step:6265	 l-p:0.05545584857463837
epoch£º313	 i:6 	 global-step:6266	 l-p:0.05551119148731232
epoch£º313	 i:7 	 global-step:6267	 l-p:0.05549817532300949
epoch£º313	 i:8 	 global-step:6268	 l-p:0.055535491555929184
epoch£º313	 i:9 	 global-step:6269	 l-p:0.05560384690761566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3305, 28.3305, 28.3305],
        [28.3305, 28.5409, 28.3710],
        [28.3305, 29.1598, 28.7103],
        [28.3305, 29.4889, 28.9855]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.055512432008981705 
model_pd.l_d.mean(): 5.5893633543746546e-05 
model_pd.lagr.mean(): 0.05556832626461983 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1138], device='cuda:0')), ('power', tensor([0.1028], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.055512432008981705
epoch£º314	 i:1 	 global-step:6281	 l-p:0.05546605959534645
epoch£º314	 i:2 	 global-step:6282	 l-p:0.055576618760824203
epoch£º314	 i:3 	 global-step:6283	 l-p:0.05553390458226204
epoch£º314	 i:4 	 global-step:6284	 l-p:0.055488672107458115
epoch£º314	 i:5 	 global-step:6285	 l-p:0.05557980015873909
epoch£º314	 i:6 	 global-step:6286	 l-p:0.055592842400074005
epoch£º314	 i:7 	 global-step:6287	 l-p:0.05551017075777054
epoch£º314	 i:8 	 global-step:6288	 l-p:0.055563781410455704
epoch£º314	 i:9 	 global-step:6289	 l-p:0.05549226701259613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2245, 32.5054, 33.3259],
        [28.2245, 30.0913, 29.6412],
        [28.2245, 28.2245, 28.2245],
        [28.2245, 34.4478, 37.0049]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.05552544817328453 
model_pd.l_d.mean(): -2.231167491117958e-05 
model_pd.lagr.mean(): 0.05550313740968704 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1591], device='cuda:0')), ('power', tensor([-0.0396], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.05552544817328453
epoch£º315	 i:1 	 global-step:6301	 l-p:0.05566961690783501
epoch£º315	 i:2 	 global-step:6302	 l-p:0.05551106482744217
epoch£º315	 i:3 	 global-step:6303	 l-p:0.05553319677710533
epoch£º315	 i:4 	 global-step:6304	 l-p:0.05551156774163246
epoch£º315	 i:5 	 global-step:6305	 l-p:0.05550996959209442
epoch£º315	 i:6 	 global-step:6306	 l-p:0.055638208985328674
epoch£º315	 i:7 	 global-step:6307	 l-p:0.05552348867058754
epoch£º315	 i:8 	 global-step:6308	 l-p:0.05559767782688141
epoch£º315	 i:9 	 global-step:6309	 l-p:0.05552832409739494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1056, 28.2407, 28.1254],
        [28.1056, 28.1254, 28.1065],
        [28.1056, 30.4702, 30.1702],
        [28.1056, 28.6627, 28.3047]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.055509041994810104 
model_pd.l_d.mean(): -0.00013743506860919297 
model_pd.lagr.mean(): 0.05537160858511925 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2137], device='cuda:0')), ('power', tensor([-0.2611], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.055509041994810104
epoch£º316	 i:1 	 global-step:6321	 l-p:0.0555327869951725
epoch£º316	 i:2 	 global-step:6322	 l-p:0.055769871920347214
epoch£º316	 i:3 	 global-step:6323	 l-p:0.05555662140250206
epoch£º316	 i:4 	 global-step:6324	 l-p:0.05554136261343956
epoch£º316	 i:5 	 global-step:6325	 l-p:0.05553285405039787
epoch£º316	 i:6 	 global-step:6326	 l-p:0.05552869662642479
epoch£º316	 i:7 	 global-step:6327	 l-p:0.055565208196640015
epoch£º316	 i:8 	 global-step:6328	 l-p:0.0555378757417202
epoch£º316	 i:9 	 global-step:6329	 l-p:0.05571839213371277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9928, 28.6201, 28.2353],
        [27.9928, 28.0062, 27.9933],
        [27.9928, 29.3201, 28.8160],
        [27.9928, 29.1205, 28.6248]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.05556345731019974 
model_pd.l_d.mean(): -0.00011395959154469892 
model_pd.lagr.mean(): 0.05544949695467949 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1431], device='cuda:0')), ('power', tensor([-0.2644], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.05556345731019974
epoch£º317	 i:1 	 global-step:6341	 l-p:0.05557253211736679
epoch£º317	 i:2 	 global-step:6342	 l-p:0.055564939975738525
epoch£º317	 i:3 	 global-step:6343	 l-p:0.055687807500362396
epoch£º317	 i:4 	 global-step:6344	 l-p:0.05557253584265709
epoch£º317	 i:5 	 global-step:6345	 l-p:0.05556425079703331
epoch£º317	 i:6 	 global-step:6346	 l-p:0.05570215731859207
epoch£º317	 i:7 	 global-step:6347	 l-p:0.0555836521089077
epoch£º317	 i:8 	 global-step:6348	 l-p:0.055545609444379807
epoch£º317	 i:9 	 global-step:6349	 l-p:0.05565003305673599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9056, 28.5663, 28.1701],
        [27.9056, 27.9080, 27.9057],
        [27.9056, 28.0467, 27.9270],
        [27.9056, 27.9138, 27.9058]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.055588845163583755 
model_pd.l_d.mean(): -9.634522575652227e-05 
model_pd.lagr.mean(): 0.055492501705884933 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1358], device='cuda:0')), ('power', tensor([-0.3386], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.055588845163583755
epoch£º318	 i:1 	 global-step:6361	 l-p:0.055559832602739334
epoch£º318	 i:2 	 global-step:6362	 l-p:0.055610138922929764
epoch£º318	 i:3 	 global-step:6363	 l-p:0.0557730570435524
epoch£º318	 i:4 	 global-step:6364	 l-p:0.0556689091026783
epoch£º318	 i:5 	 global-step:6365	 l-p:0.05559658259153366
epoch£º318	 i:6 	 global-step:6366	 l-p:0.05556736886501312
epoch£º318	 i:7 	 global-step:6367	 l-p:0.055627718567848206
epoch£º318	 i:8 	 global-step:6368	 l-p:0.05557389184832573
epoch£º318	 i:9 	 global-step:6369	 l-p:0.055585093796253204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8618, 28.9997, 28.5051],
        [27.8618, 29.6248, 29.1655],
        [27.8618, 27.8618, 27.8618],
        [27.8618, 27.8619, 27.8618]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.05557847023010254 
model_pd.l_d.mean(): -4.151668326812796e-05 
model_pd.lagr.mean(): 0.05553695186972618 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.3665e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1596], device='cuda:0')), ('power', tensor([-0.4004], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.05557847023010254
epoch£º319	 i:1 	 global-step:6381	 l-p:0.055703070014715195
epoch£º319	 i:2 	 global-step:6382	 l-p:0.055610064417123795
epoch£º319	 i:3 	 global-step:6383	 l-p:0.05563817545771599
epoch£º319	 i:4 	 global-step:6384	 l-p:0.05556939169764519
epoch£º319	 i:5 	 global-step:6385	 l-p:0.055639103055000305
epoch£º319	 i:6 	 global-step:6386	 l-p:0.05557330325245857
epoch£º319	 i:7 	 global-step:6387	 l-p:0.055648788809776306
epoch£º319	 i:8 	 global-step:6388	 l-p:0.05566716194152832
epoch£º319	 i:9 	 global-step:6389	 l-p:0.055564507842063904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8739, 29.5849, 29.1162],
        [27.8739, 30.4791, 30.2877],
        [27.8739, 30.2625, 29.9818],
        [27.8739, 27.9298, 27.8788]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.05560942366719246 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05560942366719246 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1559], device='cuda:0')), ('power', tensor([-0.4114], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.05560942366719246
epoch£º320	 i:1 	 global-step:6401	 l-p:0.055635109543800354
epoch£º320	 i:2 	 global-step:6402	 l-p:0.05566708371043205
epoch£º320	 i:3 	 global-step:6403	 l-p:0.05560063198208809
epoch£º320	 i:4 	 global-step:6404	 l-p:0.05557651445269585
epoch£º320	 i:5 	 global-step:6405	 l-p:0.05557091906666756
epoch£º320	 i:6 	 global-step:6406	 l-p:0.05555438995361328
epoch£º320	 i:7 	 global-step:6407	 l-p:0.05554935708642006
epoch£º320	 i:8 	 global-step:6408	 l-p:0.055786583572626114
epoch£º320	 i:9 	 global-step:6409	 l-p:0.0555720217525959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9268, 28.2469, 28.0076],
        [27.9268, 33.4014, 35.2711],
        [27.9268, 28.0064, 27.9354],
        [27.9268, 27.9306, 27.9269]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.055633895099163055 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055633895099163055 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1056], device='cuda:0')), ('power', tensor([-0.3056], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.055633895099163055
epoch£º321	 i:1 	 global-step:6421	 l-p:0.055570658296346664
epoch£º321	 i:2 	 global-step:6422	 l-p:0.055564068257808685
epoch£º321	 i:3 	 global-step:6423	 l-p:0.05555597320199013
epoch£º321	 i:4 	 global-step:6424	 l-p:0.05559695139527321
epoch£º321	 i:5 	 global-step:6425	 l-p:0.055594392120838165
epoch£º321	 i:6 	 global-step:6426	 l-p:0.05556178465485573
epoch£º321	 i:7 	 global-step:6427	 l-p:0.055615320801734924
epoch£º321	 i:8 	 global-step:6428	 l-p:0.05566055327653885
epoch£º321	 i:9 	 global-step:6429	 l-p:0.055640026926994324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9951, 37.5743, 44.0582],
        [27.9951, 27.9967, 27.9951],
        [27.9951, 28.0017, 27.9953],
        [27.9951, 27.9951, 27.9951]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.05554784834384918 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554784834384918 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1744], device='cuda:0')), ('power', tensor([-0.3145], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.05554784834384918
epoch£º322	 i:1 	 global-step:6441	 l-p:0.055534012615680695
epoch£º322	 i:2 	 global-step:6442	 l-p:0.055602066218853
epoch£º322	 i:3 	 global-step:6443	 l-p:0.05554403364658356
epoch£º322	 i:4 	 global-step:6444	 l-p:0.05559727922081947
epoch£º322	 i:5 	 global-step:6445	 l-p:0.05558242276310921
epoch£º322	 i:6 	 global-step:6446	 l-p:0.055642470717430115
epoch£º322	 i:7 	 global-step:6447	 l-p:0.05562957003712654
epoch£º322	 i:8 	 global-step:6448	 l-p:0.05558561161160469
epoch£º322	 i:9 	 global-step:6449	 l-p:0.055579449981451035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0691, 28.0694, 28.0691],
        [28.0691, 28.1492, 28.0777],
        [28.0691, 31.8749, 32.3618],
        [28.0691, 28.4503, 28.1763]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.05554499477148056 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554499477148056 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1494], device='cuda:0')), ('power', tensor([-0.1935], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.05554499477148056
epoch£º323	 i:1 	 global-step:6461	 l-p:0.0556722916662693
epoch£º323	 i:2 	 global-step:6462	 l-p:0.05564015358686447
epoch£º323	 i:3 	 global-step:6463	 l-p:0.05560192093253136
epoch£º323	 i:4 	 global-step:6464	 l-p:0.055521734058856964
epoch£º323	 i:5 	 global-step:6465	 l-p:0.05553462356328964
epoch£º323	 i:6 	 global-step:6466	 l-p:0.05551711097359657
epoch£º323	 i:7 	 global-step:6467	 l-p:0.05558571219444275
epoch£º323	 i:8 	 global-step:6468	 l-p:0.05554017052054405
epoch£º323	 i:9 	 global-step:6469	 l-p:0.05553044378757477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1456, 28.1458, 28.1456],
        [28.1456, 28.1458, 28.1456],
        [28.1456, 28.3224, 28.1763],
        [28.1456, 29.3369, 28.8341]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.05556312948465347 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05556312948465347 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0899], device='cuda:0')), ('power', tensor([-0.0037], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.05556312948465347
epoch£º324	 i:1 	 global-step:6481	 l-p:0.055621445178985596
epoch£º324	 i:2 	 global-step:6482	 l-p:0.05549993738532066
epoch£º324	 i:3 	 global-step:6483	 l-p:0.05553952977061272
epoch£º324	 i:4 	 global-step:6484	 l-p:0.05552634969353676
epoch£º324	 i:5 	 global-step:6485	 l-p:0.05549977719783783
epoch£º324	 i:6 	 global-step:6486	 l-p:0.05554848536849022
epoch£º324	 i:7 	 global-step:6487	 l-p:0.05558179318904877
epoch£º324	 i:8 	 global-step:6488	 l-p:0.05559791624546051
epoch£º324	 i:9 	 global-step:6489	 l-p:0.05555170774459839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2228, 30.1828, 29.7538],
        [28.2228, 28.4011, 28.2538],
        [28.2228, 28.2253, 28.2228],
        [28.2228, 31.4203, 31.5103]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.05560021102428436 
model_pd.l_d.mean(): 1.2141458949344042e-09 
model_pd.lagr.mean(): 0.05560021102428436 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.5709e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1239], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.05560021102428436
epoch£º325	 i:1 	 global-step:6501	 l-p:0.055607881397008896
epoch£º325	 i:2 	 global-step:6502	 l-p:0.05555654317140579
epoch£º325	 i:3 	 global-step:6503	 l-p:0.055518049746751785
epoch£º325	 i:4 	 global-step:6504	 l-p:0.05553874373435974
epoch£º325	 i:5 	 global-step:6505	 l-p:0.055510014295578
epoch£º325	 i:6 	 global-step:6506	 l-p:0.05550989881157875
epoch£º325	 i:7 	 global-step:6507	 l-p:0.05548912286758423
epoch£º325	 i:8 	 global-step:6508	 l-p:0.0555475689470768
epoch£º325	 i:9 	 global-step:6509	 l-p:0.05549399182200432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2998, 31.3367, 31.3330],
        [28.2998, 28.3154, 28.3005],
        [28.2998, 37.7233, 43.9409],
        [28.2998, 31.1330, 31.0251]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.05550507828593254 
model_pd.l_d.mean(): 2.53558511076335e-07 
model_pd.lagr.mean(): 0.055505331605672836 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.0196e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1587], device='cuda:0')), ('power', tensor([0.0290], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.05550507828593254
epoch£º326	 i:1 	 global-step:6521	 l-p:0.05548429489135742
epoch£º326	 i:2 	 global-step:6522	 l-p:0.05553659424185753
epoch£º326	 i:3 	 global-step:6523	 l-p:0.05553959682583809
epoch£º326	 i:4 	 global-step:6524	 l-p:0.0555109903216362
epoch£º326	 i:5 	 global-step:6525	 l-p:0.055495552718639374
epoch£º326	 i:6 	 global-step:6526	 l-p:0.055471163243055344
epoch£º326	 i:7 	 global-step:6527	 l-p:0.05567933991551399
epoch£º326	 i:8 	 global-step:6528	 l-p:0.05552949756383896
epoch£º326	 i:9 	 global-step:6529	 l-p:0.055464114993810654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3739, 37.9514, 44.3503],
        [28.3739, 28.3763, 28.3739],
        [28.3739, 29.2189, 28.7651],
        [28.3739, 28.3908, 28.3746]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.05550134927034378 
model_pd.l_d.mean(): 5.584534392255591e-06 
model_pd.lagr.mean(): 0.055506933480501175 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.8055e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1541], device='cuda:0')), ('power', tensor([0.1058], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.05550134927034378
epoch£º327	 i:1 	 global-step:6541	 l-p:0.055512964725494385
epoch£º327	 i:2 	 global-step:6542	 l-p:0.05548034608364105
epoch£º327	 i:3 	 global-step:6543	 l-p:0.055520590394735336
epoch£º327	 i:4 	 global-step:6544	 l-p:0.0555579848587513
epoch£º327	 i:5 	 global-step:6545	 l-p:0.05545435845851898
epoch£º327	 i:6 	 global-step:6546	 l-p:0.05561494827270508
epoch£º327	 i:7 	 global-step:6547	 l-p:0.055487025529146194
epoch£º327	 i:8 	 global-step:6548	 l-p:0.055473994463682175
epoch£º327	 i:9 	 global-step:6549	 l-p:0.05547165125608444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4357, 28.5522, 28.4512],
        [28.4357, 28.4786, 28.4388],
        [28.4357, 31.3570, 31.2852],
        [28.4357, 35.5921, 39.0964]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.05548664554953575 
model_pd.l_d.mean(): 2.625151137181092e-05 
model_pd.lagr.mean(): 0.05551289767026901 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1433], device='cuda:0')), ('power', tensor([0.1998], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.05548664554953575
epoch£º328	 i:1 	 global-step:6561	 l-p:0.055457953363657
epoch£º328	 i:2 	 global-step:6562	 l-p:0.055484265089035034
epoch£º328	 i:3 	 global-step:6563	 l-p:0.05546269193291664
epoch£º328	 i:4 	 global-step:6564	 l-p:0.05544247478246689
epoch£º328	 i:5 	 global-step:6565	 l-p:0.05567548796534538
epoch£º328	 i:6 	 global-step:6566	 l-p:0.05544755235314369
epoch£º328	 i:7 	 global-step:6567	 l-p:0.055470842868089676
epoch£º328	 i:8 	 global-step:6568	 l-p:0.05544431507587433
epoch£º328	 i:9 	 global-step:6569	 l-p:0.05559828504920006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4731, 28.4732, 28.4731],
        [28.4731, 33.3728, 34.6689],
        [28.4731, 28.4873, 28.4737],
        [28.4731, 29.1482, 28.7434]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.055554378777742386 
model_pd.l_d.mean(): 7.854049181332812e-05 
model_pd.lagr.mean(): 0.05563291907310486 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0977], device='cuda:0')), ('power', tensor([0.3331], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.055554378777742386
epoch£º329	 i:1 	 global-step:6581	 l-p:0.05557563528418541
epoch£º329	 i:2 	 global-step:6582	 l-p:0.055446311831474304
epoch£º329	 i:3 	 global-step:6583	 l-p:0.055491216480731964
epoch£º329	 i:4 	 global-step:6584	 l-p:0.05548088997602463
epoch£º329	 i:5 	 global-step:6585	 l-p:0.05545704811811447
epoch£º329	 i:6 	 global-step:6586	 l-p:0.055441081523895264
epoch£º329	 i:7 	 global-step:6587	 l-p:0.05556183308362961
epoch£º329	 i:8 	 global-step:6588	 l-p:0.05543552339076996
epoch£º329	 i:9 	 global-step:6589	 l-p:0.05548161268234253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4749, 28.4869, 28.4753],
        [28.4749, 28.4929, 28.4757],
        [28.4749, 28.4851, 28.4752],
        [28.4749, 28.4749, 28.4749]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.05545807629823685 
model_pd.l_d.mean(): 6.011666846461594e-05 
model_pd.lagr.mean(): 0.05551819130778313 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1889], device='cuda:0')), ('power', tensor([0.1711], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.05545807629823685
epoch£º330	 i:1 	 global-step:6601	 l-p:0.05551864951848984
epoch£º330	 i:2 	 global-step:6602	 l-p:0.05544344335794449
epoch£º330	 i:3 	 global-step:6603	 l-p:0.05548920854926109
epoch£º330	 i:4 	 global-step:6604	 l-p:0.05544861778616905
epoch£º330	 i:5 	 global-step:6605	 l-p:0.055470213294029236
epoch£º330	 i:6 	 global-step:6606	 l-p:0.0554933063685894
epoch£º330	 i:7 	 global-step:6607	 l-p:0.05547702684998512
epoch£º330	 i:8 	 global-step:6608	 l-p:0.0554773174226284
epoch£º330	 i:9 	 global-step:6609	 l-p:0.05568403750658035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4355, 31.0964, 30.9012],
        [28.4355, 37.5984, 43.4581],
        [28.4355, 28.4904, 28.4402],
        [28.4355, 36.7769, 41.6330]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.055479105561971664 
model_pd.l_d.mean(): 7.918795017758384e-05 
model_pd.lagr.mean(): 0.05555829405784607 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1515], device='cuda:0')), ('power', tensor([0.1727], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.055479105561971664
epoch£º331	 i:1 	 global-step:6621	 l-p:0.05544740706682205
epoch£º331	 i:2 	 global-step:6622	 l-p:0.0555739663541317
epoch£º331	 i:3 	 global-step:6623	 l-p:0.055576544255018234
epoch£º331	 i:4 	 global-step:6624	 l-p:0.055540651082992554
epoch£º331	 i:5 	 global-step:6625	 l-p:0.055458974093198776
epoch£º331	 i:6 	 global-step:6626	 l-p:0.05544789135456085
epoch£º331	 i:7 	 global-step:6627	 l-p:0.05548328906297684
epoch£º331	 i:8 	 global-step:6628	 l-p:0.05549352616071701
epoch£º331	 i:9 	 global-step:6629	 l-p:0.055574458092451096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3557, 30.1238, 29.6509],
        [28.3557, 32.4278, 33.0804],
        [28.3557, 31.0192, 30.8292],
        [28.3557, 28.4026, 28.3593]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.05548892542719841 
model_pd.l_d.mean(): 6.690614827675745e-05 
model_pd.lagr.mean(): 0.055555831640958786 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1551], device='cuda:0')), ('power', tensor([0.1246], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.05548892542719841
epoch£º332	 i:1 	 global-step:6641	 l-p:0.05548940598964691
epoch£º332	 i:2 	 global-step:6642	 l-p:0.055628735572099686
epoch£º332	 i:3 	 global-step:6643	 l-p:0.05552859976887703
epoch£º332	 i:4 	 global-step:6644	 l-p:0.05547288805246353
epoch£º332	 i:5 	 global-step:6645	 l-p:0.05547890067100525
epoch£º332	 i:6 	 global-step:6646	 l-p:0.05548442155122757
epoch£º332	 i:7 	 global-step:6647	 l-p:0.05559511110186577
epoch£º332	 i:8 	 global-step:6648	 l-p:0.05559222027659416
epoch£º332	 i:9 	 global-step:6649	 l-p:0.055512234568595886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2418, 28.2418, 28.2418],
        [28.2418, 33.0603, 34.3119],
        [28.2418, 28.2418, 28.2418],
        [28.2418, 28.2420, 28.2418]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.05551168695092201 
model_pd.l_d.mean(): -1.571214852447156e-05 
model_pd.lagr.mean(): 0.0554959736764431 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1688], device='cuda:0')), ('power', tensor([-0.0277], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.05551168695092201
epoch£º333	 i:1 	 global-step:6661	 l-p:0.055626533925533295
epoch£º333	 i:2 	 global-step:6662	 l-p:0.05550754442811012
epoch£º333	 i:3 	 global-step:6663	 l-p:0.05550502985715866
epoch£º333	 i:4 	 global-step:6664	 l-p:0.05552542954683304
epoch£º333	 i:5 	 global-step:6665	 l-p:0.055516891181468964
epoch£º333	 i:6 	 global-step:6666	 l-p:0.05557329207658768
epoch£º333	 i:7 	 global-step:6667	 l-p:0.055529773235321045
epoch£º333	 i:8 	 global-step:6668	 l-p:0.055601052939891815
epoch£º333	 i:9 	 global-step:6669	 l-p:0.05562710762023926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1107, 28.1167, 28.1108],
        [28.1107, 28.1530, 28.1138],
        [28.1107, 30.3425, 29.9957],
        [28.1107, 29.4528, 28.9466]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.05551093444228172 
model_pd.l_d.mean(): -0.00012140917533542961 
model_pd.lagr.mean(): 0.05538952350616455 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2057], device='cuda:0')), ('power', tensor([-0.2262], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.05551093444228172
epoch£º334	 i:1 	 global-step:6681	 l-p:0.055599067360162735
epoch£º334	 i:2 	 global-step:6682	 l-p:0.055549506098032
epoch£º334	 i:3 	 global-step:6683	 l-p:0.055554673075675964
epoch£º334	 i:4 	 global-step:6684	 l-p:0.05554196238517761
epoch£º334	 i:5 	 global-step:6685	 l-p:0.05559461563825607
epoch£º334	 i:6 	 global-step:6686	 l-p:0.05555413290858269
epoch£º334	 i:7 	 global-step:6687	 l-p:0.05560697987675667
epoch£º334	 i:8 	 global-step:6688	 l-p:0.055710598826408386
epoch£º334	 i:9 	 global-step:6689	 l-p:0.05557266250252724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9845, 28.8029, 28.3593],
        [27.9845, 36.8798, 42.4975],
        [27.9845, 27.9845, 27.9845],
        [27.9845, 37.4510, 43.7917]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.055638331919908524 
model_pd.l_d.mean(): -9.980989852920175e-05 
model_pd.lagr.mean(): 0.055538520216941833 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0684], device='cuda:0')), ('power', tensor([-0.2263], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.055638331919908524
epoch£º335	 i:1 	 global-step:6701	 l-p:0.05556719750165939
epoch£º335	 i:2 	 global-step:6702	 l-p:0.05557502806186676
epoch£º335	 i:3 	 global-step:6703	 l-p:0.05556970462203026
epoch£º335	 i:4 	 global-step:6704	 l-p:0.05558737739920616
epoch£º335	 i:5 	 global-step:6705	 l-p:0.05562340095639229
epoch£º335	 i:6 	 global-step:6706	 l-p:0.055576909333467484
epoch£º335	 i:7 	 global-step:6707	 l-p:0.05570719763636589
epoch£º335	 i:8 	 global-step:6708	 l-p:0.055551547557115555
epoch£º335	 i:9 	 global-step:6709	 l-p:0.05563898757100105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[27.8859, 28.6576, 28.2271],
        [27.8859, 29.3069, 28.8073],
        [27.8859, 32.7056, 33.9956],
        [27.8859, 28.7878, 28.3262]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.05567540228366852 
model_pd.l_d.mean(): -0.00010713726078392938 
model_pd.lagr.mean(): 0.05556826665997505 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1243], device='cuda:0')), ('power', tensor([-0.3724], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.05567540228366852
epoch£º336	 i:1 	 global-step:6721	 l-p:0.05562257021665573
epoch£º336	 i:2 	 global-step:6722	 l-p:0.05565076321363449
epoch£º336	 i:3 	 global-step:6723	 l-p:0.05562571436166763
epoch£º336	 i:4 	 global-step:6724	 l-p:0.0556328110396862
epoch£º336	 i:5 	 global-step:6725	 l-p:0.05557797849178314
epoch£º336	 i:6 	 global-step:6726	 l-p:0.05566193163394928
epoch£º336	 i:7 	 global-step:6727	 l-p:0.055573657155036926
epoch£º336	 i:8 	 global-step:6728	 l-p:0.05557297170162201
epoch£º336	 i:9 	 global-step:6729	 l-p:0.05560595542192459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8360, 38.0904, 45.5032],
        [27.8360, 28.4286, 28.0578],
        [27.8360, 31.0485, 31.1725],
        [27.8360, 34.6954, 37.9714]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.05574801191687584 
model_pd.l_d.mean(): -2.987703737744596e-05 
model_pd.lagr.mean(): 0.055718135088682175 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.9715e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0698], device='cuda:0')), ('power', tensor([-0.3133], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.05574801191687584
epoch£º337	 i:1 	 global-step:6741	 l-p:0.05561538413167
epoch£º337	 i:2 	 global-step:6742	 l-p:0.055622462183237076
epoch£º337	 i:3 	 global-step:6743	 l-p:0.055583611130714417
epoch£º337	 i:4 	 global-step:6744	 l-p:0.05563811585307121
epoch£º337	 i:5 	 global-step:6745	 l-p:0.05559878051280975
epoch£º337	 i:6 	 global-step:6746	 l-p:0.05566977337002754
epoch£º337	 i:7 	 global-step:6747	 l-p:0.055620353668928146
epoch£º337	 i:8 	 global-step:6748	 l-p:0.055563315749168396
epoch£º337	 i:9 	 global-step:6749	 l-p:0.055585846304893494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8503, 28.8653, 28.3846],
        [27.8503, 28.2668, 27.9748],
        [27.8503, 27.9239, 27.8578],
        [27.8503, 27.8503, 27.8502]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.05559873953461647 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05559873953461647 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1286], device='cuda:0')), ('power', tensor([-0.4139], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.05559873953461647
epoch£º338	 i:1 	 global-step:6761	 l-p:0.05559264123439789
epoch£º338	 i:2 	 global-step:6762	 l-p:0.05557576194405556
epoch£º338	 i:3 	 global-step:6763	 l-p:0.0556141659617424
epoch£º338	 i:4 	 global-step:6764	 l-p:0.05560360848903656
epoch£º338	 i:5 	 global-step:6765	 l-p:0.055656637996435165
epoch£º338	 i:6 	 global-step:6766	 l-p:0.05567242577672005
epoch£º338	 i:7 	 global-step:6767	 l-p:0.05561337247490883
epoch£º338	 i:8 	 global-step:6768	 l-p:0.05556997284293175
epoch£º338	 i:9 	 global-step:6769	 l-p:0.05566932633519173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9088, 35.5853, 39.7585],
        [27.9088, 28.5946, 28.1899],
        [27.9088, 28.3421, 28.0414],
        [27.9088, 27.9088, 27.9088]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.055576153099536896 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055576153099536896 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1459], device='cuda:0')), ('power', tensor([-0.3572], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.055576153099536896
epoch£º339	 i:1 	 global-step:6781	 l-p:0.05556744337081909
epoch£º339	 i:2 	 global-step:6782	 l-p:0.055605676025152206
epoch£º339	 i:3 	 global-step:6783	 l-p:0.055652692914009094
epoch£º339	 i:4 	 global-step:6784	 l-p:0.05555024370551109
epoch£º339	 i:5 	 global-step:6785	 l-p:0.055620260536670685
epoch£º339	 i:6 	 global-step:6786	 l-p:0.05554373562335968
epoch£º339	 i:7 	 global-step:6787	 l-p:0.055689726024866104
epoch£º339	 i:8 	 global-step:6788	 l-p:0.0556352324783802
epoch£º339	 i:9 	 global-step:6789	 l-p:0.05558380112051964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9841, 34.1761, 36.7347],
        [27.9841, 28.0172, 27.9862],
        [27.9841, 27.9861, 27.9841],
        [27.9841, 28.9479, 28.4735]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.05559020861983299 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05559020861983299 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1132], device='cuda:0')), ('power', tensor([-0.2451], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.05559020861983299
epoch£º340	 i:1 	 global-step:6801	 l-p:0.055542510002851486
epoch£º340	 i:2 	 global-step:6802	 l-p:0.05564623698592186
epoch£º340	 i:3 	 global-step:6803	 l-p:0.05562884360551834
epoch£º340	 i:4 	 global-step:6804	 l-p:0.055546656250953674
epoch£º340	 i:5 	 global-step:6805	 l-p:0.055609043687582016
epoch£º340	 i:6 	 global-step:6806	 l-p:0.055557992309331894
epoch£º340	 i:7 	 global-step:6807	 l-p:0.05565529689192772
epoch£º340	 i:8 	 global-step:6808	 l-p:0.05552494898438454
epoch£º340	 i:9 	 global-step:6809	 l-p:0.05555953457951546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0655, 28.0766, 28.0659],
        [28.0655, 29.4626, 28.9584],
        [28.0655, 34.9844, 38.2892],
        [28.0655, 35.1240, 38.5799]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.05555911734700203 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555911734700203 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1285], device='cuda:0')), ('power', tensor([-0.1439], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.05555911734700203
epoch£º341	 i:1 	 global-step:6821	 l-p:0.05556920915842056
epoch£º341	 i:2 	 global-step:6822	 l-p:0.05559549853205681
epoch£º341	 i:3 	 global-step:6823	 l-p:0.05553344637155533
epoch£º341	 i:4 	 global-step:6824	 l-p:0.05553874000906944
epoch£º341	 i:5 	 global-step:6825	 l-p:0.05554196611046791
epoch£º341	 i:6 	 global-step:6826	 l-p:0.055657871067523956
epoch£º341	 i:7 	 global-step:6827	 l-p:0.05554644390940666
epoch£º341	 i:8 	 global-step:6828	 l-p:0.05551816523075104
epoch£º341	 i:9 	 global-step:6829	 l-p:0.0556289367377758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1492, 28.1492, 28.1492],
        [28.1492, 35.8868, 40.0886],
        [28.1492, 37.3906, 43.4088],
        [28.1492, 35.8939, 40.1039]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.05551189184188843 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05551189184188843 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1864], device='cuda:0')), ('power', tensor([-0.1490], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.05551189184188843
epoch£º342	 i:1 	 global-step:6841	 l-p:0.0555250383913517
epoch£º342	 i:2 	 global-step:6842	 l-p:0.055523212999105453
epoch£º342	 i:3 	 global-step:6843	 l-p:0.05560147762298584
epoch£º342	 i:4 	 global-step:6844	 l-p:0.05562176927924156
epoch£º342	 i:5 	 global-step:6845	 l-p:0.05554652959108353
epoch£º342	 i:6 	 global-step:6846	 l-p:0.05556836351752281
epoch£º342	 i:7 	 global-step:6847	 l-p:0.05550067871809006
epoch£º342	 i:8 	 global-step:6848	 l-p:0.055631738156080246
epoch£º342	 i:9 	 global-step:6849	 l-p:0.0554850772023201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2339, 36.0030, 40.2264],
        [28.2339, 28.2339, 28.2338],
        [28.2339, 37.4270, 43.3665],
        [28.2339, 28.5749, 28.3228]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.0554925873875618 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0554925873875618 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1956], device='cuda:0')), ('power', tensor([-0.0952], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.0554925873875618
epoch£º343	 i:1 	 global-step:6861	 l-p:0.05551455542445183
epoch£º343	 i:2 	 global-step:6862	 l-p:0.05549135431647301
epoch£º343	 i:3 	 global-step:6863	 l-p:0.0556250736117363
epoch£º343	 i:4 	 global-step:6864	 l-p:0.05549076944589615
epoch£º343	 i:5 	 global-step:6865	 l-p:0.055643100291490555
epoch£º343	 i:6 	 global-step:6866	 l-p:0.05551190674304962
epoch£º343	 i:7 	 global-step:6867	 l-p:0.05551168695092201
epoch£º343	 i:8 	 global-step:6868	 l-p:0.05549555644392967
epoch£º343	 i:9 	 global-step:6869	 l-p:0.055565256625413895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[28.3185, 37.5406, 43.4989],
        [28.3185, 36.2614, 40.6703],
        [28.3185, 29.6337, 29.1236],
        [28.3185, 29.5550, 29.0469]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.055475518107414246 
model_pd.l_d.mean(): -3.6098717259847035e-07 
model_pd.lagr.mean(): 0.055475156754255295 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.2023e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2002], device='cuda:0')), ('power', tensor([-0.0158], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.055475518107414246
epoch£º344	 i:1 	 global-step:6881	 l-p:0.05551038309931755
epoch£º344	 i:2 	 global-step:6882	 l-p:0.05559057742357254
epoch£º344	 i:3 	 global-step:6883	 l-p:0.05551959574222565
epoch£º344	 i:4 	 global-step:6884	 l-p:0.05559646338224411
epoch£º344	 i:5 	 global-step:6885	 l-p:0.05553891509771347
epoch£º344	 i:6 	 global-step:6886	 l-p:0.05550570413470268
epoch£º344	 i:7 	 global-step:6887	 l-p:0.055489107966423035
epoch£º344	 i:8 	 global-step:6888	 l-p:0.05545646697282791
epoch£º344	 i:9 	 global-step:6889	 l-p:0.055490970611572266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3966, 28.6215, 28.4416],
        [28.3966, 37.7319, 43.8169],
        [28.3966, 28.6899, 28.4659],
        [28.3966, 31.6150, 31.7057]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.05555976927280426 
model_pd.l_d.mean(): 1.1170002835569903e-05 
model_pd.lagr.mean(): 0.05557093769311905 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.4561e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1537], device='cuda:0')), ('power', tensor([0.1444], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.05555976927280426
epoch£º345	 i:1 	 global-step:6901	 l-p:0.05553499236702919
epoch£º345	 i:2 	 global-step:6902	 l-p:0.05548712611198425
epoch£º345	 i:3 	 global-step:6903	 l-p:0.055447787046432495
epoch£º345	 i:4 	 global-step:6904	 l-p:0.055481452494859695
epoch£º345	 i:5 	 global-step:6905	 l-p:0.05554601922631264
epoch£º345	 i:6 	 global-step:6906	 l-p:0.0555306151509285
epoch£º345	 i:7 	 global-step:6907	 l-p:0.055461835116147995
epoch£º345	 i:8 	 global-step:6908	 l-p:0.05550592392683029
epoch£º345	 i:9 	 global-step:6909	 l-p:0.055474936962127686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4560, 28.5848, 28.4742],
        [28.4560, 28.4574, 28.4560],
        [28.4560, 28.8685, 28.5767],
        [28.4560, 32.2473, 32.6935]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.05549260973930359 
model_pd.l_d.mean(): 4.15241920563858e-05 
model_pd.lagr.mean(): 0.055534135550260544 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1174], device='cuda:0')), ('power', tensor([0.2486], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.05549260973930359
epoch£º346	 i:1 	 global-step:6921	 l-p:0.05552844703197479
epoch£º346	 i:2 	 global-step:6922	 l-p:0.055449217557907104
epoch£º346	 i:3 	 global-step:6923	 l-p:0.05557022616267204
epoch£º346	 i:4 	 global-step:6924	 l-p:0.05560958757996559
epoch£º346	 i:5 	 global-step:6925	 l-p:0.055458515882492065
epoch£º346	 i:6 	 global-step:6926	 l-p:0.055455923080444336
epoch£º346	 i:7 	 global-step:6927	 l-p:0.05544494464993477
epoch£º346	 i:8 	 global-step:6928	 l-p:0.0554654486477375
epoch£º346	 i:9 	 global-step:6929	 l-p:0.05546259880065918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4832, 28.4832, 28.4832],
        [28.4832, 31.7749, 31.9024],
        [28.4832, 38.2317, 44.8274],
        [28.4832, 28.5357, 28.4875]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.055485352873802185 
model_pd.l_d.mean(): 7.23619305063039e-05 
model_pd.lagr.mean(): 0.05555771663784981 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1294], device='cuda:0')), ('power', tensor([0.2588], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.055485352873802185
epoch£º347	 i:1 	 global-step:6941	 l-p:0.05550486594438553
epoch£º347	 i:2 	 global-step:6942	 l-p:0.055471427738666534
epoch£º347	 i:3 	 global-step:6943	 l-p:0.05552534759044647
epoch£º347	 i:4 	 global-step:6944	 l-p:0.055478647351264954
epoch£º347	 i:5 	 global-step:6945	 l-p:0.0554577112197876
epoch£º347	 i:6 	 global-step:6946	 l-p:0.05549035966396332
epoch£º347	 i:7 	 global-step:6947	 l-p:0.055437736213207245
epoch£º347	 i:8 	 global-step:6948	 l-p:0.05557510256767273
epoch£º347	 i:9 	 global-step:6949	 l-p:0.055493924766778946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4673, 29.2184, 28.7892],
        [28.4673, 28.8542, 28.5760],
        [28.4673, 29.1436, 28.7384],
        [28.4673, 28.4673, 28.4673]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.05546630173921585 
model_pd.l_d.mean(): 6.301902612904087e-05 
model_pd.lagr.mean(): 0.05552932247519493 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1792], device='cuda:0')), ('power', tensor([0.1590], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.05546630173921585
epoch£º348	 i:1 	 global-step:6961	 l-p:0.0554625503718853
epoch£º348	 i:2 	 global-step:6962	 l-p:0.055449143052101135
epoch£º348	 i:3 	 global-step:6963	 l-p:0.05547372251749039
epoch£º348	 i:4 	 global-step:6964	 l-p:0.055487263947725296
epoch£º348	 i:5 	 global-step:6965	 l-p:0.05553537607192993
epoch£º348	 i:6 	 global-step:6966	 l-p:0.05547874793410301
epoch£º348	 i:7 	 global-step:6967	 l-p:0.05556938052177429
epoch£º348	 i:8 	 global-step:6968	 l-p:0.05550243332982063
epoch£º348	 i:9 	 global-step:6969	 l-p:0.05557012930512428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4055, 28.4055, 28.4055],
        [28.4055, 28.4212, 28.4062],
        [28.4055, 28.4251, 28.4065],
        [28.4055, 28.7850, 28.5110]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.05558834969997406 
model_pd.l_d.mean(): 0.00012278082431294024 
model_pd.lagr.mean(): 0.055711131542921066 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0960], device='cuda:0')), ('power', tensor([0.2481], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.05558834969997406
epoch£º349	 i:1 	 global-step:6981	 l-p:0.05549664422869682
epoch£º349	 i:2 	 global-step:6982	 l-p:0.05545838177204132
epoch£º349	 i:3 	 global-step:6983	 l-p:0.05562359839677811
epoch£º349	 i:4 	 global-step:6984	 l-p:0.05551167204976082
epoch£º349	 i:5 	 global-step:6985	 l-p:0.05547511950135231
epoch£º349	 i:6 	 global-step:6986	 l-p:0.05547422170639038
epoch£º349	 i:7 	 global-step:6987	 l-p:0.055495698004961014
epoch£º349	 i:8 	 global-step:6988	 l-p:0.05548090487718582
epoch£º349	 i:9 	 global-step:6989	 l-p:0.05555341765284538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3019, 28.3214, 28.3029],
        [28.3019, 35.7610, 39.6213],
        [28.3019, 34.5195, 37.0602],
        [28.3019, 35.8348, 39.7779]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.0554942786693573 
model_pd.l_d.mean(): 3.179922714480199e-05 
model_pd.lagr.mean(): 0.05552607774734497 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1711], device='cuda:0')), ('power', tensor([0.0575], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.0554942786693573
epoch£º350	 i:1 	 global-step:7001	 l-p:0.055496033281087875
epoch£º350	 i:2 	 global-step:7002	 l-p:0.05551808699965477
epoch£º350	 i:3 	 global-step:7003	 l-p:0.055633142590522766
epoch£º350	 i:4 	 global-step:7004	 l-p:0.05549081787467003
epoch£º350	 i:5 	 global-step:7005	 l-p:0.055510494858026505
epoch£º350	 i:6 	 global-step:7006	 l-p:0.05555072799324989
epoch£º350	 i:7 	 global-step:7007	 l-p:0.05556066706776619
epoch£º350	 i:8 	 global-step:7008	 l-p:0.05551930144429207
epoch£º350	 i:9 	 global-step:7009	 l-p:0.05562715232372284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1691, 28.8004, 28.4131],
        [28.1691, 29.0766, 28.6108],
        [28.1691, 32.4411, 33.2598],
        [28.1691, 32.6507, 33.6333]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.05553557351231575 
model_pd.l_d.mean(): -3.179333725711331e-05 
model_pd.lagr.mean(): 0.055503781884908676 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1327], device='cuda:0')), ('power', tensor([-0.0576], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.05553557351231575
epoch£º351	 i:1 	 global-step:7021	 l-p:0.055585455149412155
epoch£º351	 i:2 	 global-step:7022	 l-p:0.055526383221149445
epoch£º351	 i:3 	 global-step:7023	 l-p:0.05556036904454231
epoch£º351	 i:4 	 global-step:7024	 l-p:0.055520590394735336
epoch£º351	 i:5 	 global-step:7025	 l-p:0.05553145706653595
epoch£º351	 i:6 	 global-step:7026	 l-p:0.05554555729031563
epoch£º351	 i:7 	 global-step:7027	 l-p:0.055641669780015945
epoch£º351	 i:8 	 global-step:7028	 l-p:0.05553599074482918
epoch£º351	 i:9 	 global-step:7029	 l-p:0.055701836943626404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0296, 28.5851, 28.2282],
        [28.0296, 31.4372, 31.6655],
        [28.0296, 29.1459, 28.6507],
        [28.0296, 32.9253, 34.2650]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.05552409589290619 
model_pd.l_d.mean(): -0.00015485959011130035 
model_pd.lagr.mean(): 0.05536923557519913 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2138], device='cuda:0')), ('power', tensor([-0.3206], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.05552409589290619
epoch£º352	 i:1 	 global-step:7041	 l-p:0.05563068762421608
epoch£º352	 i:2 	 global-step:7042	 l-p:0.05558578297495842
epoch£º352	 i:3 	 global-step:7043	 l-p:0.05557975172996521
epoch£º352	 i:4 	 global-step:7044	 l-p:0.055562131106853485
epoch£º352	 i:5 	 global-step:7045	 l-p:0.05565989390015602
epoch£º352	 i:6 	 global-step:7046	 l-p:0.055586524307727814
epoch£º352	 i:7 	 global-step:7047	 l-p:0.05557095259428024
epoch£º352	 i:8 	 global-step:7048	 l-p:0.05568672716617584
epoch£º352	 i:9 	 global-step:7049	 l-p:0.05557352304458618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9101, 28.0039, 27.9212],
        [27.9101, 27.9419, 27.9121],
        [27.9101, 27.9384, 27.9117],
        [27.9101, 27.9102, 27.9101]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.05557432398200035 
model_pd.l_d.mean(): -0.00013318851415533572 
model_pd.lagr.mean(): 0.05544113367795944 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1549], device='cuda:0')), ('power', tensor([-0.3831], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.05557432398200035
epoch£º353	 i:1 	 global-step:7061	 l-p:0.05566415563225746
epoch£º353	 i:2 	 global-step:7062	 l-p:0.05566425248980522
epoch£º353	 i:3 	 global-step:7063	 l-p:0.05563969537615776
epoch£º353	 i:4 	 global-step:7064	 l-p:0.05557147040963173
epoch£º353	 i:5 	 global-step:7065	 l-p:0.05557835102081299
epoch£º353	 i:6 	 global-step:7066	 l-p:0.05567488446831703
epoch£º353	 i:7 	 global-step:7067	 l-p:0.05557461082935333
epoch£º353	 i:8 	 global-step:7068	 l-p:0.055604685097932816
epoch£º353	 i:9 	 global-step:7069	 l-p:0.05562571808695793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8356, 28.2936, 27.9810],
        [27.8356, 33.2530, 35.0805],
        [27.8356, 37.6177, 44.4019],
        [27.8356, 27.9266, 27.8462]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.05557837337255478 
model_pd.l_d.mean(): -8.275287109427154e-05 
model_pd.lagr.mean(): 0.055495619773864746 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1878], device='cuda:0')), ('power', tensor([-0.5110], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.05557837337255478
epoch£º354	 i:1 	 global-step:7081	 l-p:0.05568833276629448
epoch£º354	 i:2 	 global-step:7082	 l-p:0.05561623349785805
epoch£º354	 i:3 	 global-step:7083	 l-p:0.055584803223609924
epoch£º354	 i:4 	 global-step:7084	 l-p:0.05559727922081947
epoch£º354	 i:5 	 global-step:7085	 l-p:0.05579846352338791
epoch£º354	 i:6 	 global-step:7086	 l-p:0.05559635907411575
epoch£º354	 i:7 	 global-step:7087	 l-p:0.055615417659282684
epoch£º354	 i:8 	 global-step:7088	 l-p:0.05559391900897026
epoch£º354	 i:9 	 global-step:7089	 l-p:0.05559912696480751
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8291, 37.6683, 44.5292],
        [27.8291, 28.0985, 27.8903],
        [27.8291, 27.8773, 27.8330],
        [27.8291, 27.8455, 27.8298]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.055614862591028214 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055614862591028214 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1229], device='cuda:0')), ('power', tensor([-0.4649], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.055614862591028214
epoch£º355	 i:1 	 global-step:7101	 l-p:0.05567268654704094
epoch£º355	 i:2 	 global-step:7102	 l-p:0.055603254586458206
epoch£º355	 i:3 	 global-step:7103	 l-p:0.05566069483757019
epoch£º355	 i:4 	 global-step:7104	 l-p:0.05567910894751549
epoch£º355	 i:5 	 global-step:7105	 l-p:0.05557845160365105
epoch£º355	 i:6 	 global-step:7106	 l-p:0.05557738244533539
epoch£º355	 i:7 	 global-step:7107	 l-p:0.05563654750585556
epoch£º355	 i:8 	 global-step:7108	 l-p:0.05559540539979935
epoch£º355	 i:9 	 global-step:7109	 l-p:0.05559969320893288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8831, 28.7890, 28.3266],
        [27.8831, 34.9941, 38.5364],
        [27.8831, 27.8832, 27.8831],
        [27.8831, 28.8432, 28.3707]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.055629462003707886 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055629462003707886 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1345], device='cuda:0')), ('power', tensor([-0.3728], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.055629462003707886
epoch£º356	 i:1 	 global-step:7121	 l-p:0.05555938929319382
epoch£º356	 i:2 	 global-step:7122	 l-p:0.05560016632080078
epoch£º356	 i:3 	 global-step:7123	 l-p:0.055698052048683167
epoch£º356	 i:4 	 global-step:7124	 l-p:0.05559263005852699
epoch£º356	 i:5 	 global-step:7125	 l-p:0.05556909739971161
epoch£º356	 i:6 	 global-step:7126	 l-p:0.05557171627879143
epoch£º356	 i:7 	 global-step:7127	 l-p:0.055613067001104355
epoch£º356	 i:8 	 global-step:7128	 l-p:0.0556940883398056
epoch£º356	 i:9 	 global-step:7129	 l-p:0.05554906278848648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9617, 37.5233, 43.9917],
        [27.9617, 27.9868, 27.9630],
        [27.9617, 28.0955, 27.9813],
        [27.9617, 29.3960, 28.8954]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.05558817833662033 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05558817833662033 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0954], device='cuda:0')), ('power', tensor([-0.2500], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.05558817833662033
epoch£º357	 i:1 	 global-step:7141	 l-p:0.055544108152389526
epoch£º357	 i:2 	 global-step:7142	 l-p:0.05556655302643776
epoch£º357	 i:3 	 global-step:7143	 l-p:0.05561450868844986
epoch£º357	 i:4 	 global-step:7144	 l-p:0.055575478821992874
epoch£º357	 i:5 	 global-step:7145	 l-p:0.05560024455189705
epoch£º357	 i:6 	 global-step:7146	 l-p:0.05563855171203613
epoch£º357	 i:7 	 global-step:7147	 l-p:0.055590372532606125
epoch£º357	 i:8 	 global-step:7148	 l-p:0.05554451420903206
epoch£º357	 i:9 	 global-step:7149	 l-p:0.05563974380493164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0491, 28.0602, 28.0495],
        [28.0491, 28.0492, 28.0491],
        [28.0491, 30.2741, 29.9276],
        [28.0491, 36.3655, 41.2639]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.055539317429065704 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055539317429065704 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1698], device='cuda:0')), ('power', tensor([-0.2575], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.055539317429065704
epoch£º358	 i:1 	 global-step:7161	 l-p:0.05560583993792534
epoch£º358	 i:2 	 global-step:7162	 l-p:0.05562470853328705
epoch£º358	 i:3 	 global-step:7163	 l-p:0.055568989366292953
epoch£º358	 i:4 	 global-step:7164	 l-p:0.05553216487169266
epoch£º358	 i:5 	 global-step:7165	 l-p:0.05570370703935623
epoch£º358	 i:6 	 global-step:7166	 l-p:0.05552075430750847
epoch£º358	 i:7 	 global-step:7167	 l-p:0.055537231266498566
epoch£º358	 i:8 	 global-step:7168	 l-p:0.05555737391114235
epoch£º358	 i:9 	 global-step:7169	 l-p:0.05552702769637108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1402, 28.5178, 28.2455],
        [28.1402, 28.1401, 28.1402],
        [28.1402, 29.9690, 29.5138],
        [28.1402, 28.1405, 28.1402]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.055539797991514206 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055539797991514206 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1484], device='cuda:0')), ('power', tensor([-0.1321], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.055539797991514206
epoch£º359	 i:1 	 global-step:7181	 l-p:0.05551961436867714
epoch£º359	 i:2 	 global-step:7182	 l-p:0.05561421439051628
epoch£º359	 i:3 	 global-step:7183	 l-p:0.055601026862859726
epoch£º359	 i:4 	 global-step:7184	 l-p:0.05563207343220711
epoch£º359	 i:5 	 global-step:7185	 l-p:0.05551661550998688
epoch£º359	 i:6 	 global-step:7186	 l-p:0.05550879240036011
epoch£º359	 i:7 	 global-step:7187	 l-p:0.05552968010306358
epoch£º359	 i:8 	 global-step:7188	 l-p:0.05555285885930061
epoch£º359	 i:9 	 global-step:7189	 l-p:0.055513013154268265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2325, 29.6385, 29.1311],
        [28.2325, 29.6869, 29.1816],
        [28.2325, 29.2210, 28.7395],
        [28.2325, 28.2328, 28.2325]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.05552799627184868 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05552799627184868 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.0064e-07], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1349], device='cuda:0')), ('power', tensor([0.0140], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.05552799627184868
epoch£º360	 i:1 	 global-step:7201	 l-p:0.05551175773143768
epoch£º360	 i:2 	 global-step:7202	 l-p:0.055546607822179794
epoch£º360	 i:3 	 global-step:7203	 l-p:0.05549135431647301
epoch£º360	 i:4 	 global-step:7204	 l-p:0.05559217184782028
epoch£º360	 i:5 	 global-step:7205	 l-p:0.05551975965499878
epoch£º360	 i:6 	 global-step:7206	 l-p:0.05548634007573128
epoch£º360	 i:7 	 global-step:7207	 l-p:0.05566219985485077
epoch£º360	 i:8 	 global-step:7208	 l-p:0.055516455322504044
epoch£º360	 i:9 	 global-step:7209	 l-p:0.05548294261097908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3248, 28.3248, 28.3248],
        [28.3248, 28.3250, 28.3248],
        [28.3248, 28.3267, 28.3248],
        [28.3248, 28.3449, 28.3257]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.055481478571891785 
model_pd.l_d.mean(): 8.777487892075442e-07 
model_pd.lagr.mean(): 0.05548235774040222 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.1510e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1781], device='cuda:0')), ('power', tensor([0.0456], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.055481478571891785
epoch£º361	 i:1 	 global-step:7221	 l-p:0.055489856749773026
epoch£º361	 i:2 	 global-step:7222	 l-p:0.055523939430713654
epoch£º361	 i:3 	 global-step:7223	 l-p:0.05547501519322395
epoch£º361	 i:4 	 global-step:7224	 l-p:0.0556870736181736
epoch£º361	 i:5 	 global-step:7225	 l-p:0.055475153028964996
epoch£º361	 i:6 	 global-step:7226	 l-p:0.055513571947813034
epoch£º361	 i:7 	 global-step:7227	 l-p:0.055469855666160583
epoch£º361	 i:8 	 global-step:7228	 l-p:0.05555190518498421
epoch£º361	 i:9 	 global-step:7229	 l-p:0.05548631027340889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4099, 30.0035, 29.5050],
        [28.4099, 28.4099, 28.4098],
        [28.4099, 28.4100, 28.4099],
        [28.4099, 35.4648, 38.8630]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.055477093905210495 
model_pd.l_d.mean(): 1.2805919141101185e-05 
model_pd.lagr.mean(): 0.05548990145325661 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.6621e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1719], device='cuda:0')), ('power', tensor([0.1632], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.055477093905210495
epoch£º362	 i:1 	 global-step:7241	 l-p:0.05545883998274803
epoch£º362	 i:2 	 global-step:7242	 l-p:0.055480025708675385
epoch£º362	 i:3 	 global-step:7243	 l-p:0.055553123354911804
epoch£º362	 i:4 	 global-step:7244	 l-p:0.05550844594836235
epoch£º362	 i:5 	 global-step:7245	 l-p:0.055462416261434555
epoch£º362	 i:6 	 global-step:7246	 l-p:0.055603865534067154
epoch£º362	 i:7 	 global-step:7247	 l-p:0.055489540100097656
epoch£º362	 i:8 	 global-step:7248	 l-p:0.05546361207962036
epoch£º362	 i:9 	 global-step:7249	 l-p:0.05550144985318184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4741, 29.3223, 28.8668],
        [28.4741, 28.4821, 28.4744],
        [28.4741, 28.4937, 28.4750],
        [28.4741, 30.4554, 30.0232]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.055446870625019073 
model_pd.l_d.mean(): 2.5793509848881513e-05 
model_pd.lagr.mean(): 0.055472664535045624 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2016], device='cuda:0')), ('power', tensor([0.1466], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.055446870625019073
epoch£º363	 i:1 	 global-step:7261	 l-p:0.0555889829993248
epoch£º363	 i:2 	 global-step:7262	 l-p:0.055484771728515625
epoch£º363	 i:3 	 global-step:7263	 l-p:0.05548429489135742
epoch£º363	 i:4 	 global-step:7264	 l-p:0.0554923489689827
epoch£º363	 i:5 	 global-step:7265	 l-p:0.05547956004738808
epoch£º363	 i:6 	 global-step:7266	 l-p:0.05544246733188629
epoch£º363	 i:7 	 global-step:7267	 l-p:0.055529192090034485
epoch£º363	 i:8 	 global-step:7268	 l-p:0.05543363466858864
epoch£º363	 i:9 	 global-step:7269	 l-p:0.05551813542842865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5009, 39.0119, 46.6112],
        [28.5009, 29.3525, 28.8959],
        [28.5009, 28.5023, 28.5009],
        [28.5009, 30.7505, 30.3938]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.055543795228004456 
model_pd.l_d.mean(): 9.02839019545354e-05 
model_pd.lagr.mean(): 0.05563407763838768 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1385], device='cuda:0')), ('power', tensor([0.3032], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.055543795228004456
epoch£º364	 i:1 	 global-step:7281	 l-p:0.055515363812446594
epoch£º364	 i:2 	 global-step:7282	 l-p:0.05546552687883377
epoch£º364	 i:3 	 global-step:7283	 l-p:0.0555211566388607
epoch£º364	 i:4 	 global-step:7284	 l-p:0.05549240484833717
epoch£º364	 i:5 	 global-step:7285	 l-p:0.055439334362745285
epoch£º364	 i:6 	 global-step:7286	 l-p:0.055540792644023895
epoch£º364	 i:7 	 global-step:7287	 l-p:0.05544589087367058
epoch£º364	 i:8 	 global-step:7288	 l-p:0.055444926023483276
epoch£º364	 i:9 	 global-step:7289	 l-p:0.05548091605305672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4774, 28.4774, 28.4774],
        [28.4774, 30.4562, 30.0232],
        [28.4774, 29.9399, 29.4297],
        [28.4774, 28.5659, 28.4874]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.055446188896894455 
model_pd.l_d.mean(): 6.487526115961373e-05 
model_pd.lagr.mean(): 0.05551106482744217 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2012], device='cuda:0')), ('power', tensor([0.1537], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.055446188896894455
epoch£º365	 i:1 	 global-step:7301	 l-p:0.05546604096889496
epoch£º365	 i:2 	 global-step:7302	 l-p:0.05544495955109596
epoch£º365	 i:3 	 global-step:7303	 l-p:0.05559392645955086
epoch£º365	 i:4 	 global-step:7304	 l-p:0.05547291040420532
epoch£º365	 i:5 	 global-step:7305	 l-p:0.05545446276664734
epoch£º365	 i:6 	 global-step:7306	 l-p:0.055546991527080536
epoch£º365	 i:7 	 global-step:7307	 l-p:0.055534932762384415
epoch£º365	 i:8 	 global-step:7308	 l-p:0.05556086450815201
epoch£º365	 i:9 	 global-step:7309	 l-p:0.05546648055315018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3997, 28.4520, 28.4040],
        [28.3997, 37.7360, 43.8217],
        [28.3997, 38.3895, 45.3187],
        [28.3997, 28.4410, 28.4027]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.05558988079428673 
model_pd.l_d.mean(): 0.0001594718632986769 
model_pd.lagr.mean(): 0.055749353021383286 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0523], device='cuda:0')), ('power', tensor([0.3053], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.05558988079428673
epoch£º366	 i:1 	 global-step:7321	 l-p:0.05546903982758522
epoch£º366	 i:2 	 global-step:7322	 l-p:0.05546337366104126
epoch£º366	 i:3 	 global-step:7323	 l-p:0.05559885501861572
epoch£º366	 i:4 	 global-step:7324	 l-p:0.05549262836575508
epoch£º366	 i:5 	 global-step:7325	 l-p:0.05545806884765625
epoch£º366	 i:6 	 global-step:7326	 l-p:0.055492103099823
epoch£º366	 i:7 	 global-step:7327	 l-p:0.055544931441545486
epoch£º366	 i:8 	 global-step:7328	 l-p:0.055477116256952286
epoch£º366	 i:9 	 global-step:7329	 l-p:0.055603884160518646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[28.2743, 37.6888, 43.9005],
        [28.2743, 30.0668, 29.6006],
        [28.2743, 29.5004, 28.9935],
        [28.2743, 31.9262, 32.2930]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.05558072030544281 
model_pd.l_d.mean(): 6.390486669261009e-05 
model_pd.lagr.mean(): 0.055644623935222626 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1149], device='cuda:0')), ('power', tensor([0.1116], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.05558072030544281
epoch£º367	 i:1 	 global-step:7341	 l-p:0.05547664687037468
epoch£º367	 i:2 	 global-step:7342	 l-p:0.055517829954624176
epoch£º367	 i:3 	 global-step:7343	 l-p:0.05551575496792793
epoch£º367	 i:4 	 global-step:7344	 l-p:0.055515095591545105
epoch£º367	 i:5 	 global-step:7345	 l-p:0.0555160790681839
epoch£º367	 i:6 	 global-step:7346	 l-p:0.05564892664551735
epoch£º367	 i:7 	 global-step:7347	 l-p:0.055502548813819885
epoch£º367	 i:8 	 global-step:7348	 l-p:0.05566735938191414
epoch£º367	 i:9 	 global-step:7349	 l-p:0.05553646385669708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1205, 28.1205, 28.1205],
        [28.1205, 28.1205, 28.1205],
        [28.1205, 28.1537, 28.1226],
        [28.1205, 31.0306, 30.9712]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.05553625151515007 
model_pd.l_d.mean(): -7.833119889255613e-05 
model_pd.lagr.mean(): 0.05545791983604431 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1412], device='cuda:0')), ('power', tensor([-0.1415], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.05553625151515007
epoch£º368	 i:1 	 global-step:7361	 l-p:0.05556529760360718
epoch£º368	 i:2 	 global-step:7362	 l-p:0.055521707981824875
epoch£º368	 i:3 	 global-step:7363	 l-p:0.05554717406630516
epoch£º368	 i:4 	 global-step:7364	 l-p:0.05572015792131424
epoch£º368	 i:5 	 global-step:7365	 l-p:0.05555807054042816
epoch£º368	 i:6 	 global-step:7366	 l-p:0.05556104704737663
epoch£º368	 i:7 	 global-step:7367	 l-p:0.05553954839706421
epoch£º368	 i:8 	 global-step:7368	 l-p:0.055592987686395645
epoch£º368	 i:9 	 global-step:7369	 l-p:0.05565832182765007
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9663, 32.0346, 32.7180],
        [27.9663, 27.9895, 27.9675],
        [27.9663, 27.9663, 27.9663],
        [27.9663, 37.3247, 43.5307]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.05560283362865448 
model_pd.l_d.mean(): -9.587783279130235e-05 
model_pd.lagr.mean(): 0.055506955832242966 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0771], device='cuda:0')), ('power', tensor([-0.2101], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.05560283362865448
epoch£º369	 i:1 	 global-step:7381	 l-p:0.055659741163253784
epoch£º369	 i:2 	 global-step:7382	 l-p:0.05555910989642143
epoch£º369	 i:3 	 global-step:7383	 l-p:0.05557917430996895
epoch£º369	 i:4 	 global-step:7384	 l-p:0.055582787841558456
epoch£º369	 i:5 	 global-step:7385	 l-p:0.05556255206465721
epoch£º369	 i:6 	 global-step:7386	 l-p:0.055667389184236526
epoch£º369	 i:7 	 global-step:7387	 l-p:0.055647190660238266
epoch£º369	 i:8 	 global-step:7388	 l-p:0.055616721510887146
epoch£º369	 i:9 	 global-step:7389	 l-p:0.05562077462673187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8435, 30.9614, 31.0306],
        [27.8435, 27.8752, 27.8454],
        [27.8435, 35.6127, 39.9043],
        [27.8435, 28.7398, 28.2797]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.05578850582242012 
model_pd.l_d.mean(): -7.296600961126387e-05 
model_pd.lagr.mean(): 0.05571553856134415 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0474], device='cuda:0')), ('power', tensor([-0.2533], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.05578850582242012
epoch£º370	 i:1 	 global-step:7401	 l-p:0.055594876408576965
epoch£º370	 i:2 	 global-step:7402	 l-p:0.055597934871912
epoch£º370	 i:3 	 global-step:7403	 l-p:0.05569295585155487
epoch£º370	 i:4 	 global-step:7404	 l-p:0.05556763708591461
epoch£º370	 i:5 	 global-step:7405	 l-p:0.05558353289961815
epoch£º370	 i:6 	 global-step:7406	 l-p:0.05558830127120018
epoch£º370	 i:7 	 global-step:7407	 l-p:0.055648431181907654
epoch£º370	 i:8 	 global-step:7408	 l-p:0.055618830025196075
epoch£º370	 i:9 	 global-step:7409	 l-p:0.05562085658311844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7837, 27.7839, 27.7837],
        [27.7837, 27.7992, 27.7843],
        [27.7837, 37.0214, 43.1122],
        [27.7837, 27.9580, 27.8139]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.05561155080795288 
model_pd.l_d.mean(): -3.495166311040521e-05 
model_pd.lagr.mean(): 0.05557660013437271 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.7286e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1212], device='cuda:0')), ('power', tensor([-0.4877], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.05561155080795288
epoch£º371	 i:1 	 global-step:7421	 l-p:0.05562669783830643
epoch£º371	 i:2 	 global-step:7422	 l-p:0.055608734488487244
epoch£º371	 i:3 	 global-step:7423	 l-p:0.055628884583711624
epoch£º371	 i:4 	 global-step:7424	 l-p:0.05570603162050247
epoch£º371	 i:5 	 global-step:7425	 l-p:0.05559591203927994
epoch£º371	 i:6 	 global-step:7426	 l-p:0.05561744421720505
epoch£º371	 i:7 	 global-step:7427	 l-p:0.055698197335004807
epoch£º371	 i:8 	 global-step:7428	 l-p:0.055643901228904724
epoch£º371	 i:9 	 global-step:7429	 l-p:0.055612754076719284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8067, 38.0162, 45.3754],
        [27.8067, 27.8069, 27.8067],
        [27.8067, 27.8230, 27.8074],
        [27.8067, 30.6823, 30.6235]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.0556778647005558 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0556778647005558 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0950], device='cuda:0')), ('power', tensor([-0.3946], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.0556778647005558
epoch£º372	 i:1 	 global-step:7441	 l-p:0.055599845945835114
epoch£º372	 i:2 	 global-step:7442	 l-p:0.0556139275431633
epoch£º372	 i:3 	 global-step:7443	 l-p:0.05560043081641197
epoch£º372	 i:4 	 global-step:7444	 l-p:0.05558450520038605
epoch£º372	 i:5 	 global-step:7445	 l-p:0.055628761649131775
epoch£º372	 i:6 	 global-step:7446	 l-p:0.055601589381694794
epoch£º372	 i:7 	 global-step:7447	 l-p:0.05576116591691971
epoch£º372	 i:8 	 global-step:7448	 l-p:0.055554457008838654
epoch£º372	 i:9 	 global-step:7449	 l-p:0.055622994899749756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8794, 37.1562, 43.2764],
        [27.8794, 28.1670, 27.9474],
        [27.8794, 27.8794, 27.8794],
        [27.8794, 31.3704, 31.6620]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.05568951368331909 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05568951368331909 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0740], device='cuda:0')), ('power', tensor([-0.3086], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.05568951368331909
epoch£º373	 i:1 	 global-step:7461	 l-p:0.055601004511117935
epoch£º373	 i:2 	 global-step:7462	 l-p:0.05563434585928917
epoch£º373	 i:3 	 global-step:7463	 l-p:0.05557479336857796
epoch£º373	 i:4 	 global-step:7464	 l-p:0.05559660866856575
epoch£º373	 i:5 	 global-step:7465	 l-p:0.055565305054187775
epoch£º373	 i:6 	 global-step:7466	 l-p:0.055652301758527756
epoch£º373	 i:7 	 global-step:7467	 l-p:0.05563196539878845
epoch£º373	 i:8 	 global-step:7468	 l-p:0.05554188787937164
epoch£º373	 i:9 	 global-step:7469	 l-p:0.055584926158189774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9701, 30.4985, 30.2694],
        [27.9701, 28.4305, 28.1163],
        [27.9701, 28.2908, 28.0511],
        [27.9701, 37.7693, 44.5454]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.05555642023682594 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555642023682594 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1757], device='cuda:0')), ('power', tensor([-0.3107], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.05555642023682594
epoch£º374	 i:1 	 global-step:7481	 l-p:0.05569351091980934
epoch£º374	 i:2 	 global-step:7482	 l-p:0.05558619275689125
epoch£º374	 i:3 	 global-step:7483	 l-p:0.05556687340140343
epoch£º374	 i:4 	 global-step:7484	 l-p:0.055546343326568604
epoch£º374	 i:5 	 global-step:7485	 l-p:0.055533651262521744
epoch£º374	 i:6 	 global-step:7486	 l-p:0.055566079914569855
epoch£º374	 i:7 	 global-step:7487	 l-p:0.055628929287195206
epoch£º374	 i:8 	 global-step:7488	 l-p:0.0555533766746521
epoch£º374	 i:9 	 global-step:7489	 l-p:0.05564407259225845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0672, 28.1159, 28.0711],
        [28.0672, 28.0672, 28.0672],
        [28.0672, 28.1252, 28.0723],
        [28.0672, 28.0684, 28.0672]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.055550139397382736 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055550139397382736 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1698], device='cuda:0')), ('power', tensor([-0.2537], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.055550139397382736
epoch£º375	 i:1 	 global-step:7501	 l-p:0.05556926503777504
epoch£º375	 i:2 	 global-step:7502	 l-p:0.05552222579717636
epoch£º375	 i:3 	 global-step:7503	 l-p:0.05559787154197693
epoch£º375	 i:4 	 global-step:7504	 l-p:0.05552295967936516
epoch£º375	 i:5 	 global-step:7505	 l-p:0.055645912885665894
epoch£º375	 i:6 	 global-step:7506	 l-p:0.055522236973047256
epoch£º375	 i:7 	 global-step:7507	 l-p:0.05554137006402016
epoch£º375	 i:8 	 global-step:7508	 l-p:0.05550374463200569
epoch£º375	 i:9 	 global-step:7509	 l-p:0.05569516494870186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1668, 28.8341, 28.4340],
        [28.1668, 31.4198, 31.5456],
        [28.1668, 28.1682, 28.1668],
        [28.1668, 30.0225, 29.5720]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.05557306110858917 
model_pd.l_d.mean(): -3.730915807409474e-07 
model_pd.lagr.mean(): 0.055572688579559326 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.1991e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1038], device='cuda:0')), ('power', tensor([-0.0752], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.05557306110858917
epoch£º376	 i:1 	 global-step:7521	 l-p:0.0555858351290226
epoch£º376	 i:2 	 global-step:7522	 l-p:0.05556555464863777
epoch£º376	 i:3 	 global-step:7523	 l-p:0.055521268397569656
epoch£º376	 i:4 	 global-step:7524	 l-p:0.055548522621393204
epoch£º376	 i:5 	 global-step:7525	 l-p:0.05557073652744293
epoch£º376	 i:6 	 global-step:7526	 l-p:0.05551094934344292
epoch£º376	 i:7 	 global-step:7527	 l-p:0.05561406910419464
epoch£º376	 i:8 	 global-step:7528	 l-p:0.055490151047706604
epoch£º376	 i:9 	 global-step:7529	 l-p:0.05548499524593353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2676, 30.3266, 29.9215],
        [28.2676, 28.9161, 28.5220],
        [28.2676, 28.7873, 28.4446],
        [28.2676, 37.4723, 43.4193]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.055515874177217484 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055515874177217484 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1632], device='cuda:0')), ('power', tensor([-0.0131], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.055515874177217484
epoch£º377	 i:1 	 global-step:7541	 l-p:0.055495623499155045
epoch£º377	 i:2 	 global-step:7542	 l-p:0.05563664436340332
epoch£º377	 i:3 	 global-step:7543	 l-p:0.0555022656917572
epoch£º377	 i:4 	 global-step:7544	 l-p:0.055520884692668915
epoch£º377	 i:5 	 global-step:7545	 l-p:0.05564635246992111
epoch£º377	 i:6 	 global-step:7546	 l-p:0.05548352748155594
epoch£º377	 i:7 	 global-step:7547	 l-p:0.05547254905104637
epoch£º377	 i:8 	 global-step:7548	 l-p:0.05547395721077919
epoch£º377	 i:9 	 global-step:7549	 l-p:0.055511098355054855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3675, 31.2079, 31.0998],
        [28.3675, 28.3676, 28.3675],
        [28.3675, 29.4073, 28.9164],
        [28.3675, 28.8634, 28.5311]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.055509939789772034 
model_pd.l_d.mean(): 5.438578682515072e-06 
model_pd.lagr.mean(): 0.05551537871360779 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.2306e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1206], device='cuda:0')), ('power', tensor([0.1581], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.055509939789772034
epoch£º378	 i:1 	 global-step:7561	 l-p:0.05550803616642952
epoch£º378	 i:2 	 global-step:7562	 l-p:0.055605195462703705
epoch£º378	 i:3 	 global-step:7563	 l-p:0.05550133436918259
epoch£º378	 i:4 	 global-step:7564	 l-p:0.0554816909134388
epoch£º378	 i:5 	 global-step:7565	 l-p:0.05548233911395073
epoch£º378	 i:6 	 global-step:7566	 l-p:0.055490121245384216
epoch£º378	 i:7 	 global-step:7567	 l-p:0.05545045807957649
epoch£º378	 i:8 	 global-step:7568	 l-p:0.05551706999540329
epoch£º378	 i:9 	 global-step:7569	 l-p:0.055518824607133865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4539, 30.1132, 29.6213],
        [28.4539, 28.4539, 28.4539],
        [28.4539, 28.5826, 28.4721],
        [28.4539, 30.5272, 30.1194]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.055566299706697464 
model_pd.l_d.mean(): 3.61374877684284e-05 
model_pd.lagr.mean(): 0.05560243874788284 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1158], device='cuda:0')), ('power', tensor([0.3126], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.055566299706697464
epoch£º379	 i:1 	 global-step:7581	 l-p:0.05548182502388954
epoch£º379	 i:2 	 global-step:7582	 l-p:0.05559904873371124
epoch£º379	 i:3 	 global-step:7583	 l-p:0.05545618385076523
epoch£º379	 i:4 	 global-step:7584	 l-p:0.05546407401561737
epoch£º379	 i:5 	 global-step:7585	 l-p:0.05548922345042229
epoch£º379	 i:6 	 global-step:7586	 l-p:0.055469147861003876
epoch£º379	 i:7 	 global-step:7587	 l-p:0.05545256659388542
epoch£º379	 i:8 	 global-step:7588	 l-p:0.055444393306970596
epoch£º379	 i:9 	 global-step:7589	 l-p:0.055494535714387894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5075, 33.8057, 35.4458],
        [28.5075, 28.6877, 28.5388],
        [28.5075, 28.5106, 28.5075],
        [28.5075, 28.5400, 28.5095]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.05546333268284798 
model_pd.l_d.mean(): 5.084898293716833e-05 
model_pd.lagr.mean(): 0.05551418289542198 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1781], device='cuda:0')), ('power', tensor([0.2180], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.05546333268284798
epoch£º380	 i:1 	 global-step:7601	 l-p:0.05544265732169151
epoch£º380	 i:2 	 global-step:7602	 l-p:0.05550520122051239
epoch£º380	 i:3 	 global-step:7603	 l-p:0.055436696857213974
epoch£º380	 i:4 	 global-step:7604	 l-p:0.05555851384997368
epoch£º380	 i:5 	 global-step:7605	 l-p:0.05545835942029953
epoch£º380	 i:6 	 global-step:7606	 l-p:0.05545579642057419
epoch£º380	 i:7 	 global-step:7607	 l-p:0.05551834776997566
epoch£º380	 i:8 	 global-step:7608	 l-p:0.05555250868201256
epoch£º380	 i:9 	 global-step:7609	 l-p:0.05546083301305771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5113, 28.5121, 28.5113],
        [28.5113, 36.3534, 40.6125],
        [28.5113, 29.1194, 28.7389],
        [28.5113, 29.9756, 29.4648]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.05547310784459114 
model_pd.l_d.mean(): 0.00011910846660612151 
model_pd.lagr.mean(): 0.055592216551303864 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1327], device='cuda:0')), ('power', tensor([0.3246], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.05547310784459114
epoch£º381	 i:1 	 global-step:7621	 l-p:0.05547752231359482
epoch£º381	 i:2 	 global-step:7622	 l-p:0.05546938627958298
epoch£º381	 i:3 	 global-step:7623	 l-p:0.05549336224794388
epoch£º381	 i:4 	 global-step:7624	 l-p:0.05551018938422203
epoch£º381	 i:5 	 global-step:7625	 l-p:0.05555262416601181
epoch£º381	 i:6 	 global-step:7626	 l-p:0.055439405143260956
epoch£º381	 i:7 	 global-step:7627	 l-p:0.055502261966466904
epoch£º381	 i:8 	 global-step:7628	 l-p:0.05551902577280998
epoch£º381	 i:9 	 global-step:7629	 l-p:0.05546122044324875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4547, 29.4514, 28.9660],
        [28.4547, 28.4571, 28.4547],
        [28.4547, 28.4626, 28.4549],
        [28.4547, 29.8693, 29.3576]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.055448830127716064 
model_pd.l_d.mean(): 3.9987255149753764e-05 
model_pd.lagr.mean(): 0.055488817393779755 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2135], device='cuda:0')), ('power', tensor([0.0817], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.055448830127716064
epoch£º382	 i:1 	 global-step:7641	 l-p:0.05548655241727829
epoch£º382	 i:2 	 global-step:7642	 l-p:0.05549811199307442
epoch£º382	 i:3 	 global-step:7643	 l-p:0.055453862994909286
epoch£º382	 i:4 	 global-step:7644	 l-p:0.05551174655556679
epoch£º382	 i:5 	 global-step:7645	 l-p:0.05550185963511467
epoch£º382	 i:6 	 global-step:7646	 l-p:0.05545686557888985
epoch£º382	 i:7 	 global-step:7647	 l-p:0.055496785789728165
epoch£º382	 i:8 	 global-step:7648	 l-p:0.05562259629368782
epoch£º382	 i:9 	 global-step:7649	 l-p:0.055591411888599396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3393, 28.3728, 28.3414],
        [28.3393, 30.9032, 30.6710],
        [28.3393, 28.3393, 28.3393],
        [28.3393, 28.9435, 28.5654]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.05550391972064972 
model_pd.l_d.mean(): 3.8512916944455355e-05 
model_pd.lagr.mean(): 0.05554243177175522 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1500], device='cuda:0')), ('power', tensor([0.0676], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.05550391972064972
epoch£º383	 i:1 	 global-step:7661	 l-p:0.055532559752464294
epoch£º383	 i:2 	 global-step:7662	 l-p:0.055474042892456055
epoch£º383	 i:3 	 global-step:7663	 l-p:0.055489681661129
epoch£º383	 i:4 	 global-step:7664	 l-p:0.05551021546125412
epoch£º383	 i:5 	 global-step:7665	 l-p:0.055580250918865204
epoch£º383	 i:6 	 global-step:7666	 l-p:0.05552391707897186
epoch£º383	 i:7 	 global-step:7667	 l-p:0.055483799427747726
epoch£º383	 i:8 	 global-step:7668	 l-p:0.05555434897542
epoch£º383	 i:9 	 global-step:7669	 l-p:0.05569353327155113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1805, 36.3964, 41.1505],
        [28.1805, 29.9666, 29.5021],
        [28.1805, 34.0807, 36.3238],
        [28.1805, 28.5587, 28.2860]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.05550229921936989 
model_pd.l_d.mean(): -5.926322774030268e-05 
model_pd.lagr.mean(): 0.055443037301301956 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1937], device='cuda:0')), ('power', tensor([-0.1018], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.05550229921936989
epoch£º384	 i:1 	 global-step:7681	 l-p:0.05555430427193642
epoch£º384	 i:2 	 global-step:7682	 l-p:0.055525172501802444
epoch£º384	 i:3 	 global-step:7683	 l-p:0.05559110641479492
epoch£º384	 i:4 	 global-step:7684	 l-p:0.05563312768936157
epoch£º384	 i:5 	 global-step:7685	 l-p:0.055544402450323105
epoch£º384	 i:6 	 global-step:7686	 l-p:0.055558979511260986
epoch£º384	 i:7 	 global-step:7687	 l-p:0.055585797876119614
epoch£º384	 i:8 	 global-step:7688	 l-p:0.055594004690647125
epoch£º384	 i:9 	 global-step:7689	 l-p:0.05560367926955223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0060, 30.6514, 30.4711],
        [28.0060, 28.0279, 28.0071],
        [28.0060, 28.0063, 28.0060],
        [28.0060, 35.8744, 40.2522]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.05556933209300041 
model_pd.l_d.mean(): -0.00012105666974093765 
model_pd.lagr.mean(): 0.055448275059461594 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1349], device='cuda:0')), ('power', tensor([-0.2369], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.05556933209300041
epoch£º385	 i:1 	 global-step:7701	 l-p:0.05557975172996521
epoch£º385	 i:2 	 global-step:7702	 l-p:0.055638812482357025
epoch£º385	 i:3 	 global-step:7703	 l-p:0.055606283247470856
epoch£º385	 i:4 	 global-step:7704	 l-p:0.05554303899407387
epoch£º385	 i:5 	 global-step:7705	 l-p:0.055619630962610245
epoch£º385	 i:6 	 global-step:7706	 l-p:0.055589523166418076
epoch£º385	 i:7 	 global-step:7707	 l-p:0.05566956475377083
epoch£º385	 i:8 	 global-step:7708	 l-p:0.05557098239660263
epoch£º385	 i:9 	 global-step:7709	 l-p:0.05565619841217995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8525, 28.5118, 28.1165],
        [27.8525, 33.4448, 35.4341],
        [27.8525, 27.8525, 27.8525],
        [27.8525, 28.3147, 28.0001]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.05560421943664551 
model_pd.l_d.mean(): -0.00012921681627631187 
model_pd.lagr.mean(): 0.05547500401735306 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1081], device='cuda:0')), ('power', tensor([-0.3630], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.05560421943664551
epoch£º386	 i:1 	 global-step:7721	 l-p:0.055581338703632355
epoch£º386	 i:2 	 global-step:7722	 l-p:0.05557378754019737
epoch£º386	 i:3 	 global-step:7723	 l-p:0.05575789511203766
epoch£º386	 i:4 	 global-step:7724	 l-p:0.05558238923549652
epoch£º386	 i:5 	 global-step:7725	 l-p:0.05563436448574066
epoch£º386	 i:6 	 global-step:7726	 l-p:0.055739179253578186
epoch£º386	 i:7 	 global-step:7727	 l-p:0.055593252182006836
epoch£º386	 i:8 	 global-step:7728	 l-p:0.05560493469238281
epoch£º386	 i:9 	 global-step:7729	 l-p:0.055644452571868896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7591, 28.0995, 27.8488],
        [27.7591, 35.1167, 38.9532],
        [27.7591, 27.7643, 27.7592],
        [27.7591, 28.4410, 28.0386]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.05559438467025757 
model_pd.l_d.mean(): -7.193217606982216e-05 
model_pd.lagr.mean(): 0.05552245303988457 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1702], device='cuda:0')), ('power', tensor([-0.5270], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.05559438467025757
epoch£º387	 i:1 	 global-step:7741	 l-p:0.055738773196935654
epoch£º387	 i:2 	 global-step:7742	 l-p:0.05562041327357292
epoch£º387	 i:3 	 global-step:7743	 l-p:0.0555972084403038
epoch£º387	 i:4 	 global-step:7744	 l-p:0.055603548884391785
epoch£º387	 i:5 	 global-step:7745	 l-p:0.055612727999687195
epoch£º387	 i:6 	 global-step:7746	 l-p:0.055733874440193176
epoch£º387	 i:7 	 global-step:7747	 l-p:0.055624235421419144
epoch£º387	 i:8 	 global-step:7748	 l-p:0.05560256540775299
epoch£º387	 i:9 	 global-step:7749	 l-p:0.05570182204246521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7575, 27.7904, 27.7596],
        [27.7575, 28.1077, 27.8515],
        [27.7575, 27.7979, 27.7604],
        [27.7575, 30.1354, 29.8559]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.055664148181676865 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055664148181676865 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0402], device='cuda:0')), ('power', tensor([-0.3766], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.055664148181676865
epoch£º388	 i:1 	 global-step:7761	 l-p:0.05566643178462982
epoch£º388	 i:2 	 global-step:7762	 l-p:0.05574670806527138
epoch£º388	 i:3 	 global-step:7763	 l-p:0.05560159683227539
epoch£º388	 i:4 	 global-step:7764	 l-p:0.0556049719452858
epoch£º388	 i:5 	 global-step:7765	 l-p:0.055582769215106964
epoch£º388	 i:6 	 global-step:7766	 l-p:0.05561477690935135
epoch£º388	 i:7 	 global-step:7767	 l-p:0.05559869483113289
epoch£º388	 i:8 	 global-step:7768	 l-p:0.05562027171254158
epoch£º388	 i:9 	 global-step:7769	 l-p:0.05565584450960159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8259, 31.7474, 32.3358],
        [27.8259, 30.0203, 29.6728],
        [27.8259, 27.8279, 27.8260],
        [27.8259, 28.4652, 28.0770]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.05567360669374466 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05567360669374466 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0890], device='cuda:0')), ('power', tensor([-0.3483], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.05567360669374466
epoch£º389	 i:1 	 global-step:7781	 l-p:0.05572548136115074
epoch£º389	 i:2 	 global-step:7782	 l-p:0.055578604340553284
epoch£º389	 i:3 	 global-step:7783	 l-p:0.05570385232567787
epoch£º389	 i:4 	 global-step:7784	 l-p:0.05557422339916229
epoch£º389	 i:5 	 global-step:7785	 l-p:0.0555921271443367
epoch£º389	 i:6 	 global-step:7786	 l-p:0.055578768253326416
epoch£º389	 i:7 	 global-step:7787	 l-p:0.055610086768865585
epoch£º389	 i:8 	 global-step:7788	 l-p:0.05558701232075691
epoch£º389	 i:9 	 global-step:7789	 l-p:0.055558670312166214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9207, 27.9232, 27.9207],
        [27.9207, 29.3101, 28.8086],
        [27.9207, 36.1885, 41.0528],
        [27.9207, 27.9231, 27.9208]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.05556173995137215 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05556173995137215 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1676], device='cuda:0')), ('power', tensor([-0.3831], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.05556173995137215
epoch£º390	 i:1 	 global-step:7801	 l-p:0.05558909475803375
epoch£º390	 i:2 	 global-step:7802	 l-p:0.055581409484148026
epoch£º390	 i:3 	 global-step:7803	 l-p:0.05564342439174652
epoch£º390	 i:4 	 global-step:7804	 l-p:0.05557513236999512
epoch£º390	 i:5 	 global-step:7805	 l-p:0.055595312267541885
epoch£º390	 i:6 	 global-step:7806	 l-p:0.055622100830078125
epoch£º390	 i:7 	 global-step:7807	 l-p:0.05563681572675705
epoch£º390	 i:8 	 global-step:7808	 l-p:0.05553340166807175
epoch£º390	 i:9 	 global-step:7809	 l-p:0.05563400313258171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0246, 28.1188, 28.0358],
        [28.0246, 30.6718, 30.4914],
        [28.0246, 30.2343, 29.8837],
        [28.0246, 28.0653, 28.0275]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.05553887411952019 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05553887411952019 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1786], device='cuda:0')), ('power', tensor([-0.2928], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.05553887411952019
epoch£º391	 i:1 	 global-step:7821	 l-p:0.05557246133685112
epoch£º391	 i:2 	 global-step:7822	 l-p:0.05558061972260475
epoch£º391	 i:3 	 global-step:7823	 l-p:0.05556660145521164
epoch£º391	 i:4 	 global-step:7824	 l-p:0.05553366616368294
epoch£º391	 i:5 	 global-step:7825	 l-p:0.05560116097331047
epoch£º391	 i:6 	 global-step:7826	 l-p:0.05553080514073372
epoch£º391	 i:7 	 global-step:7827	 l-p:0.05565594509243965
epoch£º391	 i:8 	 global-step:7828	 l-p:0.055536672472953796
epoch£º391	 i:9 	 global-step:7829	 l-p:0.0556349940598011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1322, 28.7084, 28.3424],
        [28.1322, 28.1346, 28.1322],
        [28.1322, 38.4217, 45.8109],
        [28.1322, 28.1322, 28.1322]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.055577944964170456 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055577944964170456 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1437], device='cuda:0')), ('power', tensor([-0.1489], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.055577944964170456
epoch£º392	 i:1 	 global-step:7841	 l-p:0.05560896918177605
epoch£º392	 i:2 	 global-step:7842	 l-p:0.05551062151789665
epoch£º392	 i:3 	 global-step:7843	 l-p:0.055645331740379333
epoch£º392	 i:4 	 global-step:7844	 l-p:0.05552089959383011
epoch£º392	 i:5 	 global-step:7845	 l-p:0.05551068112254143
epoch£º392	 i:6 	 global-step:7846	 l-p:0.055513277649879456
epoch£º392	 i:7 	 global-step:7847	 l-p:0.055549077689647675
epoch£º392	 i:8 	 global-step:7848	 l-p:0.05556219443678856
epoch£º392	 i:9 	 global-step:7849	 l-p:0.055529311299324036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2416, 37.3390, 43.1565],
        [28.2416, 28.2416, 28.2416],
        [28.2416, 33.9316, 35.9651],
        [28.2416, 28.2417, 28.2416]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.055513571947813034 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055513571947813034 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.6441e-07], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1467], device='cuda:0')), ('power', tensor([0.0093], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.055513571947813034
epoch£º393	 i:1 	 global-step:7861	 l-p:0.05557619035243988
epoch£º393	 i:2 	 global-step:7862	 l-p:0.05548350140452385
epoch£º393	 i:3 	 global-step:7863	 l-p:0.05567228049039841
epoch£º393	 i:4 	 global-step:7864	 l-p:0.05551914870738983
epoch£º393	 i:5 	 global-step:7865	 l-p:0.05550326406955719
epoch£º393	 i:6 	 global-step:7866	 l-p:0.055511172860860825
epoch£º393	 i:7 	 global-step:7867	 l-p:0.05546823889017105
epoch£º393	 i:8 	 global-step:7868	 l-p:0.05557771399617195
epoch£º393	 i:9 	 global-step:7869	 l-p:0.05547879636287689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3503, 28.9862, 28.5961],
        [28.3503, 28.3543, 28.3504],
        [28.3503, 30.4509, 30.0544],
        [28.3503, 38.0568, 44.6274]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.05548075586557388 
model_pd.l_d.mean(): 1.1589925179578131e-06 
model_pd.lagr.mean(): 0.055481914430856705 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.5286e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1838], device='cuda:0')), ('power', tensor([0.0510], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.05548075586557388
epoch£º394	 i:1 	 global-step:7881	 l-p:0.055500999093055725
epoch£º394	 i:2 	 global-step:7882	 l-p:0.055474814027547836
epoch£º394	 i:3 	 global-step:7883	 l-p:0.05553055182099342
epoch£º394	 i:4 	 global-step:7884	 l-p:0.05548153817653656
epoch£º394	 i:5 	 global-step:7885	 l-p:0.055519260466098785
epoch£º394	 i:6 	 global-step:7886	 l-p:0.05545889586210251
epoch£º394	 i:7 	 global-step:7887	 l-p:0.055542442947626114
epoch£º394	 i:8 	 global-step:7888	 l-p:0.055537622421979904
epoch£º394	 i:9 	 global-step:7889	 l-p:0.055561818182468414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4493, 28.4650, 28.4500],
        [28.4493, 29.3749, 28.9025],
        [28.4493, 28.4493, 28.4493],
        [28.4493, 29.0742, 28.6877]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.05548598989844322 
model_pd.l_d.mean(): 2.0134297301410697e-05 
model_pd.lagr.mean(): 0.05550612509250641 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1368], device='cuda:0')), ('power', tensor([0.2056], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.05548598989844322
epoch£º395	 i:1 	 global-step:7901	 l-p:0.055492885410785675
epoch£º395	 i:2 	 global-step:7902	 l-p:0.055436134338378906
epoch£º395	 i:3 	 global-step:7903	 l-p:0.05543883144855499
epoch£º395	 i:4 	 global-step:7904	 l-p:0.05558916926383972
epoch£º395	 i:5 	 global-step:7905	 l-p:0.05551403388381004
epoch£º395	 i:6 	 global-step:7906	 l-p:0.05544266104698181
epoch£º395	 i:7 	 global-step:7907	 l-p:0.055469054728746414
epoch£º395	 i:8 	 global-step:7908	 l-p:0.05556880310177803
epoch£º395	 i:9 	 global-step:7909	 l-p:0.055474571883678436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5186, 34.2508, 36.2898],
        [28.5186, 34.8352, 37.4458],
        [28.5186, 28.7445, 28.5638],
        [28.5186, 28.6477, 28.5369]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.05542441084980965 
model_pd.l_d.mean(): 3.4012140531558543e-05 
model_pd.lagr.mean(): 0.055458422750234604 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2412], device='cuda:0')), ('power', tensor([0.1569], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.05542441084980965
epoch£º396	 i:1 	 global-step:7921	 l-p:0.05548296496272087
epoch£º396	 i:2 	 global-step:7922	 l-p:0.05555272102355957
epoch£º396	 i:3 	 global-step:7923	 l-p:0.055448055267333984
epoch£º396	 i:4 	 global-step:7924	 l-p:0.05546967312693596
epoch£º396	 i:5 	 global-step:7925	 l-p:0.05545467883348465
epoch£º396	 i:6 	 global-step:7926	 l-p:0.05551563575863838
epoch£º396	 i:7 	 global-step:7927	 l-p:0.05544580891728401
epoch£º396	 i:8 	 global-step:7928	 l-p:0.05544044077396393
epoch£º396	 i:9 	 global-step:7929	 l-p:0.0555836483836174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5340, 38.3064, 44.9219],
        [28.5340, 28.5913, 28.5390],
        [28.5340, 29.3866, 28.9295],
        [28.5340, 29.2870, 28.8568]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.05551614612340927 
model_pd.l_d.mean(): 0.00013900133490096778 
model_pd.lagr.mean(): 0.05565514788031578 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0729], device='cuda:0')), ('power', tensor([0.3874], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.05551614612340927
epoch£º397	 i:1 	 global-step:7941	 l-p:0.055602386593818665
epoch£º397	 i:2 	 global-step:7942	 l-p:0.055445823818445206
epoch£º397	 i:3 	 global-step:7943	 l-p:0.0554409883916378
epoch£º397	 i:4 	 global-step:7944	 l-p:0.05545928701758385
epoch£º397	 i:5 	 global-step:7945	 l-p:0.05545062571763992
epoch£º397	 i:6 	 global-step:7946	 l-p:0.05556972324848175
epoch£º397	 i:7 	 global-step:7947	 l-p:0.05544436722993851
epoch£º397	 i:8 	 global-step:7948	 l-p:0.05543852597475052
epoch£º397	 i:9 	 global-step:7949	 l-p:0.05548129975795746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4780, 34.4927, 36.8088],
        [28.4780, 28.4781, 28.4780],
        [28.4780, 28.4937, 28.4786],
        [28.4780, 33.8201, 35.5032]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.055537089705467224 
model_pd.l_d.mean(): 0.00015593820717185736 
model_pd.lagr.mean(): 0.05569302663207054 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1139], device='cuda:0')), ('power', tensor([0.3162], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.055537089705467224
epoch£º398	 i:1 	 global-step:7961	 l-p:0.05559912696480751
epoch£º398	 i:2 	 global-step:7962	 l-p:0.05544319003820419
epoch£º398	 i:3 	 global-step:7963	 l-p:0.05545356497168541
epoch£º398	 i:4 	 global-step:7964	 l-p:0.055448196828365326
epoch£º398	 i:5 	 global-step:7965	 l-p:0.0555076003074646
epoch£º398	 i:6 	 global-step:7966	 l-p:0.055470362305641174
epoch£º398	 i:7 	 global-step:7967	 l-p:0.05549078807234764
epoch£º398	 i:8 	 global-step:7968	 l-p:0.055509526282548904
epoch£º398	 i:9 	 global-step:7969	 l-p:0.05557041987776756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3495, 28.3538, 28.3496],
        [28.3495, 29.1227, 28.6880],
        [28.3495, 32.7585, 33.6667],
        [28.3495, 30.8047, 30.5282]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.05551247298717499 
model_pd.l_d.mean(): 5.970701022306457e-05 
model_pd.lagr.mean(): 0.05557217821478844 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1413], device='cuda:0')), ('power', tensor([0.1024], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.05551247298717499
epoch£º399	 i:1 	 global-step:7981	 l-p:0.05558915063738823
epoch£º399	 i:2 	 global-step:7982	 l-p:0.05552782863378525
epoch£º399	 i:3 	 global-step:7983	 l-p:0.05548298731446266
epoch£º399	 i:4 	 global-step:7984	 l-p:0.05563077703118324
epoch£º399	 i:5 	 global-step:7985	 l-p:0.05548584461212158
epoch£º399	 i:6 	 global-step:7986	 l-p:0.055518388748168945
epoch£º399	 i:7 	 global-step:7987	 l-p:0.0555599182844162
epoch£º399	 i:8 	 global-step:7988	 l-p:0.05553177744150162
epoch£º399	 i:9 	 global-step:7989	 l-p:0.05550356209278107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1693, 31.3259, 31.3962],
        [28.1693, 33.0907, 34.4375],
        [28.1693, 28.3551, 28.2025],
        [28.1693, 31.1329, 31.0983]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.055589575320482254 
model_pd.l_d.mean(): -3.209511123714037e-05 
model_pd.lagr.mean(): 0.055557481944561005 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1379], device='cuda:0')), ('power', tensor([-0.0538], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.055589575320482254
epoch£º400	 i:1 	 global-step:8001	 l-p:0.055507052689790726
epoch£º400	 i:2 	 global-step:8002	 l-p:0.05557506904006004
epoch£º400	 i:3 	 global-step:8003	 l-p:0.055597152560949326
epoch£º400	 i:4 	 global-step:8004	 l-p:0.055645618587732315
epoch£º400	 i:5 	 global-step:8005	 l-p:0.055527929216623306
epoch£º400	 i:6 	 global-step:8006	 l-p:0.055541057139635086
epoch£º400	 i:7 	 global-step:8007	 l-p:0.055582206696271896
epoch£º400	 i:8 	 global-step:8008	 l-p:0.05552221089601517
epoch£º400	 i:9 	 global-step:8009	 l-p:0.0555386058986187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0896, 33.4872, 35.2661],
        [28.0896, 28.0916, 28.0896],
        [28.0896, 32.9196, 34.1969],
        [28.0896, 34.8058, 37.8905]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.05559347942471504 
model_pd.l_d.mean(): -1.3752463928540237e-05 
model_pd.lagr.mean(): 0.05557972565293312 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0676], device='cuda:0')), ('power', tensor([-0.0254], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.05559347942471504
epoch£º401	 i:1 	 global-step:8021	 l-p:0.055526964366436005
epoch£º401	 i:2 	 global-step:8022	 l-p:0.055543992668390274
epoch£º401	 i:3 	 global-step:8023	 l-p:0.055641382932662964
epoch£º401	 i:4 	 global-step:8024	 l-p:0.05553744360804558
epoch£º401	 i:5 	 global-step:8025	 l-p:0.055602338165044785
epoch£º401	 i:6 	 global-step:8026	 l-p:0.05552343279123306
epoch£º401	 i:7 	 global-step:8027	 l-p:0.05573466047644615
epoch£º401	 i:8 	 global-step:8028	 l-p:0.05553964152932167
epoch£º401	 i:9 	 global-step:8029	 l-p:0.0555456206202507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0151, 28.0151, 28.0151],
        [28.0151, 28.0195, 28.0152],
        [28.0151, 28.7906, 28.3580],
        [28.0151, 28.0151, 28.0151]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.05554179474711418 
model_pd.l_d.mean(): -0.00015155297296587378 
model_pd.lagr.mean(): 0.055390242487192154 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1829], device='cuda:0')), ('power', tensor([-0.3398], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.05554179474711418
epoch£º402	 i:1 	 global-step:8041	 l-p:0.05566345900297165
epoch£º402	 i:2 	 global-step:8042	 l-p:0.05569160357117653
epoch£º402	 i:3 	 global-step:8043	 l-p:0.055600427091121674
epoch£º402	 i:4 	 global-step:8044	 l-p:0.055546537041664124
epoch£º402	 i:5 	 global-step:8045	 l-p:0.05555776506662369
epoch£º402	 i:6 	 global-step:8046	 l-p:0.05556156113743782
epoch£º402	 i:7 	 global-step:8047	 l-p:0.055565036833286285
epoch£º402	 i:8 	 global-step:8048	 l-p:0.05562533810734749
epoch£º402	 i:9 	 global-step:8049	 l-p:0.05557669699192047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9569, 28.5701, 28.1907],
        [27.9569, 27.9572, 27.9569],
        [27.9569, 28.5041, 28.1510],
        [27.9569, 28.1334, 27.9876]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.05556650832295418 
model_pd.l_d.mean(): -0.00011725857621058822 
model_pd.lagr.mean(): 0.05544925108551979 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1607], device='cuda:0')), ('power', tensor([-0.3692], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.05556650832295418
epoch£º403	 i:1 	 global-step:8061	 l-p:0.05556662380695343
epoch£º403	 i:2 	 global-step:8062	 l-p:0.055642880499362946
epoch£º403	 i:3 	 global-step:8063	 l-p:0.055738724768161774
epoch£º403	 i:4 	 global-step:8064	 l-p:0.055560700595378876
epoch£º403	 i:5 	 global-step:8065	 l-p:0.05557085946202278
epoch£º403	 i:6 	 global-step:8066	 l-p:0.05559869483113289
epoch£º403	 i:7 	 global-step:8067	 l-p:0.05556950345635414
epoch£º403	 i:8 	 global-step:8068	 l-p:0.055655643343925476
epoch£º403	 i:9 	 global-step:8069	 l-p:0.055560026317834854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9233, 32.7231, 33.9923],
        [27.9233, 35.7924, 40.1857],
        [27.9233, 28.2978, 28.0278],
        [27.9233, 27.9718, 27.9272]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.05566510930657387 
model_pd.l_d.mean(): -3.4615197364473715e-05 
model_pd.lagr.mean(): 0.05563049390912056 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0740], device='cuda:0')), ('power', tensor([-0.2091], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.05566510930657387
epoch£º404	 i:1 	 global-step:8081	 l-p:0.05562688410282135
epoch£º404	 i:2 	 global-step:8082	 l-p:0.05562165006995201
epoch£º404	 i:3 	 global-step:8083	 l-p:0.055580250918865204
epoch£º404	 i:4 	 global-step:8084	 l-p:0.05567898601293564
epoch£º404	 i:5 	 global-step:8085	 l-p:0.0555616170167923
epoch£º404	 i:6 	 global-step:8086	 l-p:0.055585429072380066
epoch£º404	 i:7 	 global-step:8087	 l-p:0.05563269183039665
epoch£º404	 i:8 	 global-step:8088	 l-p:0.055548399686813354
epoch£º404	 i:9 	 global-step:8089	 l-p:0.05557192116975784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9198, 30.7967, 30.7319],
        [27.9198, 27.9197, 27.9197],
        [27.9198, 27.9197, 27.9198],
        [27.9198, 29.6527, 29.1864]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.055577315390110016 
model_pd.l_d.mean(): -1.125265953305643e-06 
model_pd.lagr.mean(): 0.05557619035243988 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1378], device='cuda:0')), ('power', tensor([-0.3413], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.055577315390110016
epoch£º405	 i:1 	 global-step:8101	 l-p:0.05567600950598717
epoch£º405	 i:2 	 global-step:8102	 l-p:0.05558865889906883
epoch£º405	 i:3 	 global-step:8103	 l-p:0.05555286630988121
epoch£º405	 i:4 	 global-step:8104	 l-p:0.055703360587358475
epoch£º405	 i:5 	 global-step:8105	 l-p:0.05559825897216797
epoch£º405	 i:6 	 global-step:8106	 l-p:0.05557030439376831
epoch£º405	 i:7 	 global-step:8107	 l-p:0.05555117875337601
epoch£º405	 i:8 	 global-step:8108	 l-p:0.05555910989642143
epoch£º405	 i:9 	 global-step:8109	 l-p:0.05567396432161331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9456, 28.7277, 28.2939],
        [27.9456, 28.1652, 27.9893],
        [27.9456, 29.6592, 29.1890],
        [27.9456, 33.1824, 34.8319]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.05562197417020798 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05562197417020798 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1048], device='cuda:0')), ('power', tensor([-0.2644], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.05562197417020798
epoch£º406	 i:1 	 global-step:8121	 l-p:0.05557706207036972
epoch£º406	 i:2 	 global-step:8122	 l-p:0.05562949180603027
epoch£º406	 i:3 	 global-step:8123	 l-p:0.055568303912878036
epoch£º406	 i:4 	 global-step:8124	 l-p:0.05557200312614441
epoch£º406	 i:5 	 global-step:8125	 l-p:0.05556066706776619
epoch£º406	 i:6 	 global-step:8126	 l-p:0.05558476224541664
epoch£º406	 i:7 	 global-step:8127	 l-p:0.055564310401678085
epoch£º406	 i:8 	 global-step:8128	 l-p:0.055616192519664764
epoch£º406	 i:9 	 global-step:8129	 l-p:0.05568654462695122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9853, 28.0463, 27.9909],
        [27.9853, 28.0393, 27.9899],
        [27.9853, 32.4419, 33.4224],
        [27.9853, 30.9928, 30.9925]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.05555061250925064 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555061250925064 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1722], device='cuda:0')), ('power', tensor([-0.3281], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.05555061250925064
epoch£º407	 i:1 	 global-step:8141	 l-p:0.05563799664378166
epoch£º407	 i:2 	 global-step:8142	 l-p:0.05567651614546776
epoch£º407	 i:3 	 global-step:8143	 l-p:0.0555884912610054
epoch£º407	 i:4 	 global-step:8144	 l-p:0.05554280802607536
epoch£º407	 i:5 	 global-step:8145	 l-p:0.05555376037955284
epoch£º407	 i:6 	 global-step:8146	 l-p:0.055614173412323
epoch£º407	 i:7 	 global-step:8147	 l-p:0.05563492700457573
epoch£º407	 i:8 	 global-step:8148	 l-p:0.05555552989244461
epoch£º407	 i:9 	 global-step:8149	 l-p:0.055538177490234375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0301, 28.4360, 28.1489],
        [28.0301, 28.3737, 28.1206],
        [28.0301, 29.7703, 29.3021],
        [28.0301, 32.2515, 33.0442]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.055540140718221664 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055540140718221664 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1710], device='cuda:0')), ('power', tensor([-0.2267], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.055540140718221664
epoch£º408	 i:1 	 global-step:8161	 l-p:0.05560843273997307
epoch£º408	 i:2 	 global-step:8162	 l-p:0.055592168122529984
epoch£º408	 i:3 	 global-step:8163	 l-p:0.05560184270143509
epoch£º408	 i:4 	 global-step:8164	 l-p:0.05555431544780731
epoch£º408	 i:5 	 global-step:8165	 l-p:0.05553438514471054
epoch£º408	 i:6 	 global-step:8166	 l-p:0.05556284636259079
epoch£º408	 i:7 	 global-step:8167	 l-p:0.0555521659553051
epoch£º408	 i:8 	 global-step:8168	 l-p:0.055614594370126724
epoch£º408	 i:9 	 global-step:8169	 l-p:0.05563650652766228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0767, 28.3181, 28.1276],
        [28.0767, 28.3488, 28.1385],
        [28.0767, 28.0926, 28.0774],
        [28.0767, 28.0878, 28.0771]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.05553504824638367 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05553504824638367 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1851], device='cuda:0')), ('power', tensor([-0.2507], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.05553504824638367
epoch£º409	 i:1 	 global-step:8181	 l-p:0.05551648512482643
epoch£º409	 i:2 	 global-step:8182	 l-p:0.05553016439080238
epoch£º409	 i:3 	 global-step:8183	 l-p:0.05555965006351471
epoch£º409	 i:4 	 global-step:8184	 l-p:0.05552690476179123
epoch£º409	 i:5 	 global-step:8185	 l-p:0.055665310472249985
epoch£º409	 i:6 	 global-step:8186	 l-p:0.05553700402379036
epoch£º409	 i:7 	 global-step:8187	 l-p:0.05569331347942352
epoch£º409	 i:8 	 global-step:8188	 l-p:0.05558399856090546
epoch£º409	 i:9 	 global-step:8189	 l-p:0.05555185303092003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1242, 28.5374, 28.2463],
        [28.1242, 29.0495, 28.5806],
        [28.1242, 28.1823, 28.1293],
        [28.1242, 28.1242, 28.1242]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.055529527366161346 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055529527366161346 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1586], device='cuda:0')), ('power', tensor([-0.1480], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.055529527366161346
epoch£º410	 i:1 	 global-step:8201	 l-p:0.0555448904633522
epoch£º410	 i:2 	 global-step:8202	 l-p:0.055719442665576935
epoch£º410	 i:3 	 global-step:8203	 l-p:0.0555291548371315
epoch£º410	 i:4 	 global-step:8204	 l-p:0.055613331496715546
epoch£º410	 i:5 	 global-step:8205	 l-p:0.05551350489258766
epoch£º410	 i:6 	 global-step:8206	 l-p:0.055616822093725204
epoch£º410	 i:7 	 global-step:8207	 l-p:0.05551300570368767
epoch£º410	 i:8 	 global-step:8208	 l-p:0.05549772083759308
epoch£º410	 i:9 	 global-step:8209	 l-p:0.055524129420518875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1722, 28.1722, 28.1722],
        [28.1722, 28.1781, 28.1724],
        [28.1722, 28.1736, 28.1722],
        [28.1722, 28.1830, 28.1726]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.05552743002772331 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05552743002772331 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1427], device='cuda:0')), ('power', tensor([-0.1022], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.05552743002772331
epoch£º411	 i:1 	 global-step:8221	 l-p:0.05557145178318024
epoch£º411	 i:2 	 global-step:8222	 l-p:0.05550282448530197
epoch£º411	 i:3 	 global-step:8223	 l-p:0.05552910640835762
epoch£º411	 i:4 	 global-step:8224	 l-p:0.055669061839580536
epoch£º411	 i:5 	 global-step:8225	 l-p:0.05555685982108116
epoch£º411	 i:6 	 global-step:8226	 l-p:0.0555589459836483
epoch£º411	 i:7 	 global-step:8227	 l-p:0.05550265312194824
epoch£º411	 i:8 	 global-step:8228	 l-p:0.05559835210442543
epoch£º411	 i:9 	 global-step:8229	 l-p:0.05548563972115517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2203, 28.2770, 28.2253],
        [28.2203, 38.6231, 46.1436],
        [28.2203, 28.2203, 28.2203],
        [28.2203, 29.1296, 28.6629]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.0556136891245842 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0556136891245842 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.5522e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1056], device='cuda:0')), ('power', tensor([0.0510], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.0556136891245842
epoch£º412	 i:1 	 global-step:8241	 l-p:0.05550381541252136
epoch£º412	 i:2 	 global-step:8242	 l-p:0.05552215874195099
epoch£º412	 i:3 	 global-step:8243	 l-p:0.05551576614379883
epoch£º412	 i:4 	 global-step:8244	 l-p:0.05552436783909798
epoch£º412	 i:5 	 global-step:8245	 l-p:0.055515725165605545
epoch£º412	 i:6 	 global-step:8246	 l-p:0.05549009144306183
epoch£º412	 i:7 	 global-step:8247	 l-p:0.05557660013437271
epoch£º412	 i:8 	 global-step:8248	 l-p:0.055620621889829636
epoch£º412	 i:9 	 global-step:8249	 l-p:0.05552057921886444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2685, 28.9170, 28.5228],
        [28.2685, 28.2685, 28.2685],
        [28.2685, 38.2705, 45.2455],
        [28.2685, 28.2685, 28.2685]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.05553815886378288 
model_pd.l_d.mean(): 2.931181199983257e-07 
model_pd.lagr.mean(): 0.05553845316171646 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.0692e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1291], device='cuda:0')), ('power', tensor([0.0323], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.05553815886378288
epoch£º413	 i:1 	 global-step:8261	 l-p:0.05563291534781456
epoch£º413	 i:2 	 global-step:8262	 l-p:0.055491216480731964
epoch£º413	 i:3 	 global-step:8263	 l-p:0.055527668446302414
epoch£º413	 i:4 	 global-step:8264	 l-p:0.055525556206703186
epoch£º413	 i:5 	 global-step:8265	 l-p:0.05564994737505913
epoch£º413	 i:6 	 global-step:8266	 l-p:0.05547422170639038
epoch£º413	 i:7 	 global-step:8267	 l-p:0.05548470467329025
epoch£º413	 i:8 	 global-step:8268	 l-p:0.05549392104148865
epoch£º413	 i:9 	 global-step:8269	 l-p:0.05548761412501335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3152, 28.3487, 28.3174],
        [28.3152, 33.5238, 35.1056],
        [28.3152, 28.3152, 28.3152],
        [28.3152, 28.5884, 28.3771]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.055558718740940094 
model_pd.l_d.mean(): 3.3906051157828188e-06 
model_pd.lagr.mean(): 0.055562108755111694 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.6724e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1197], device='cuda:0')), ('power', tensor([0.1083], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.055558718740940094
epoch£º414	 i:1 	 global-step:8281	 l-p:0.055473294109106064
epoch£º414	 i:2 	 global-step:8282	 l-p:0.05550582706928253
epoch£º414	 i:3 	 global-step:8283	 l-p:0.055514268577098846
epoch£º414	 i:4 	 global-step:8284	 l-p:0.055611513555049896
epoch£º414	 i:5 	 global-step:8285	 l-p:0.055509962141513824
epoch£º414	 i:6 	 global-step:8286	 l-p:0.05547322705388069
epoch£º414	 i:7 	 global-step:8287	 l-p:0.05549353361129761
epoch£º414	 i:8 	 global-step:8288	 l-p:0.05557481572031975
epoch£º414	 i:9 	 global-step:8289	 l-p:0.055499132722616196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3573, 33.1965, 34.4537],
        [28.3573, 36.6746, 41.5166],
        [28.3573, 32.5399, 33.2741],
        [28.3573, 28.5008, 28.3791]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.05551379173994064 
model_pd.l_d.mean(): 1.404733757226495e-05 
model_pd.lagr.mean(): 0.05552783980965614 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.5105e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1018], device='cuda:0')), ('power', tensor([0.1852], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.05551379173994064
epoch£º415	 i:1 	 global-step:8301	 l-p:0.05546599254012108
epoch£º415	 i:2 	 global-step:8302	 l-p:0.05556317791342735
epoch£º415	 i:3 	 global-step:8303	 l-p:0.05546044185757637
epoch£º415	 i:4 	 global-step:8304	 l-p:0.0554763600230217
epoch£º415	 i:5 	 global-step:8305	 l-p:0.055566269904375076
epoch£º415	 i:6 	 global-step:8306	 l-p:0.05550439655780792
epoch£º415	 i:7 	 global-step:8307	 l-p:0.05550750717520714
epoch£º415	 i:8 	 global-step:8308	 l-p:0.05561167746782303
epoch£º415	 i:9 	 global-step:8309	 l-p:0.05546599254012108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3911, 28.7703, 28.4965],
        [28.3911, 28.3913, 28.3911],
        [28.3911, 33.0441, 34.1433],
        [28.3911, 28.3911, 28.3911]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.055466942489147186 
model_pd.l_d.mean(): 1.7486845536041074e-05 
model_pd.lagr.mean(): 0.055484429001808167 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1903], device='cuda:0')), ('power', tensor([0.1252], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.055466942489147186
epoch£º416	 i:1 	 global-step:8321	 l-p:0.0554867647588253
epoch£º416	 i:2 	 global-step:8322	 l-p:0.05547506362199783
epoch£º416	 i:3 	 global-step:8323	 l-p:0.05545883998274803
epoch£º416	 i:4 	 global-step:8324	 l-p:0.055628206580877304
epoch£º416	 i:5 	 global-step:8325	 l-p:0.05559409409761429
epoch£º416	 i:6 	 global-step:8326	 l-p:0.055491622537374496
epoch£º416	 i:7 	 global-step:8327	 l-p:0.05551362782716751
epoch£º416	 i:8 	 global-step:8328	 l-p:0.05547897145152092
epoch£º416	 i:9 	 global-step:8329	 l-p:0.055484045296907425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4119, 28.4943, 28.4208],
        [28.4119, 28.5995, 28.4455],
        [28.4119, 29.0656, 28.6687],
        [28.4119, 28.4119, 28.4119]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.055476583540439606 
model_pd.l_d.mean(): 2.6378580514574423e-05 
model_pd.lagr.mean(): 0.055502962321043015 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1646], device='cuda:0')), ('power', tensor([0.1213], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.055476583540439606
epoch£º417	 i:1 	 global-step:8341	 l-p:0.05552675947546959
epoch£º417	 i:2 	 global-step:8342	 l-p:0.05546851083636284
epoch£º417	 i:3 	 global-step:8343	 l-p:0.0554729625582695
epoch£º417	 i:4 	 global-step:8344	 l-p:0.055468738079071045
epoch£º417	 i:5 	 global-step:8345	 l-p:0.05546238645911217
epoch£º417	 i:6 	 global-step:8346	 l-p:0.05564761906862259
epoch£º417	 i:7 	 global-step:8347	 l-p:0.05547357350587845
epoch£º417	 i:8 	 global-step:8348	 l-p:0.05553995817899704
epoch£º417	 i:9 	 global-step:8349	 l-p:0.055513896048069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4160, 28.4174, 28.4160],
        [28.4160, 28.4168, 28.4160],
        [28.4160, 34.3723, 36.6395],
        [28.4160, 29.6570, 29.1470]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.05553774535655975 
model_pd.l_d.mean(): 6.589015538338572e-05 
model_pd.lagr.mean(): 0.05560363456606865 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1432], device='cuda:0')), ('power', tensor([0.2181], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:0.05553774535655975
epoch£º418	 i:1 	 global-step:8361	 l-p:0.05544881522655487
epoch£º418	 i:2 	 global-step:8362	 l-p:0.055573564022779465
epoch£º418	 i:3 	 global-step:8363	 l-p:0.0555589534342289
epoch£º418	 i:4 	 global-step:8364	 l-p:0.0554669164121151
epoch£º418	 i:5 	 global-step:8365	 l-p:0.05551883950829506
epoch£º418	 i:6 	 global-step:8366	 l-p:0.05549195036292076
epoch£º418	 i:7 	 global-step:8367	 l-p:0.05549148842692375
epoch£º418	 i:8 	 global-step:8368	 l-p:0.055503275245428085
epoch£º418	 i:9 	 global-step:8369	 l-p:0.05546804144978523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4008, 29.6328, 29.1235],
        [28.4008, 32.5359, 33.2310],
        [28.4008, 31.4558, 31.4557],
        [28.4008, 28.4891, 28.4108]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.05548401176929474 
model_pd.l_d.mean(): 6.996998854447156e-05 
model_pd.lagr.mean(): 0.05555398017168045 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1585], device='cuda:0')), ('power', tensor([0.1819], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.05548401176929474
epoch£º419	 i:1 	 global-step:8381	 l-p:0.05559001863002777
epoch£º419	 i:2 	 global-step:8382	 l-p:0.055454038083553314
epoch£º419	 i:3 	 global-step:8383	 l-p:0.055544160306453705
epoch£º419	 i:4 	 global-step:8384	 l-p:0.05547317862510681
epoch£º419	 i:5 	 global-step:8385	 l-p:0.05553198605775833
epoch£º419	 i:6 	 global-step:8386	 l-p:0.055503588169813156
epoch£º419	 i:7 	 global-step:8387	 l-p:0.05548880994319916
epoch£º419	 i:8 	 global-step:8388	 l-p:0.05552707612514496
epoch£º419	 i:9 	 global-step:8389	 l-p:0.055512331426143646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3651, 33.8185, 35.6161],
        [28.3651, 29.4047, 28.9139],
        [28.3651, 30.1250, 29.6505],
        [28.3651, 28.3691, 28.3652]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.055545464158058167 
model_pd.l_d.mean(): 5.563360537053086e-05 
model_pd.lagr.mean(): 0.0556010976433754 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1327], device='cuda:0')), ('power', tensor([0.1223], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.055545464158058167
epoch£º420	 i:1 	 global-step:8401	 l-p:0.05547378584742546
epoch£º420	 i:2 	 global-step:8402	 l-p:0.05558079481124878
epoch£º420	 i:3 	 global-step:8403	 l-p:0.055535148829221725
epoch£º420	 i:4 	 global-step:8404	 l-p:0.05545540153980255
epoch£º420	 i:5 	 global-step:8405	 l-p:0.05553358793258667
epoch£º420	 i:6 	 global-step:8406	 l-p:0.0554945282638073
epoch£º420	 i:7 	 global-step:8407	 l-p:0.05559859052300453
epoch£º420	 i:8 	 global-step:8408	 l-p:0.05548109859228134
epoch£º420	 i:9 	 global-step:8409	 l-p:0.05550001561641693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3119, 30.3232, 29.9040],
        [28.3119, 37.5316, 43.4885],
        [28.3119, 28.7619, 28.4516],
        [28.3119, 28.3704, 28.3171]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.0555032342672348 
model_pd.l_d.mean(): 9.698815119918436e-06 
model_pd.lagr.mean(): 0.055512934923172 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1474], device='cuda:0')), ('power', tensor([0.0193], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.0555032342672348
epoch£º421	 i:1 	 global-step:8421	 l-p:0.0555557906627655
epoch£º421	 i:2 	 global-step:8422	 l-p:0.05551490560173988
epoch£º421	 i:3 	 global-step:8423	 l-p:0.05559087544679642
epoch£º421	 i:4 	 global-step:8424	 l-p:0.055519018322229385
epoch£º421	 i:5 	 global-step:8425	 l-p:0.05556454136967659
epoch£º421	 i:6 	 global-step:8426	 l-p:0.05550593137741089
epoch£º421	 i:7 	 global-step:8427	 l-p:0.05548796430230141
epoch£º421	 i:8 	 global-step:8428	 l-p:0.05558057501912117
epoch£º421	 i:9 	 global-step:8429	 l-p:0.055497169494628906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2451, 28.4002, 28.2699],
        [28.2451, 28.2451, 28.2451],
        [28.2451, 28.2512, 28.2453],
        [28.2451, 32.0261, 32.4819]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.05551714822649956 
model_pd.l_d.mean(): -1.0427487723063678e-05 
model_pd.lagr.mean(): 0.05550672113895416 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1405], device='cuda:0')), ('power', tensor([-0.0200], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.05551714822649956
epoch£º422	 i:1 	 global-step:8441	 l-p:0.055488958954811096
epoch£º422	 i:2 	 global-step:8442	 l-p:0.05555969104170799
epoch£º422	 i:3 	 global-step:8443	 l-p:0.05553026497364044
epoch£º422	 i:4 	 global-step:8444	 l-p:0.055636364966630936
epoch£º422	 i:5 	 global-step:8445	 l-p:0.05551418662071228
epoch£º422	 i:6 	 global-step:8446	 l-p:0.05557627975940704
epoch£º422	 i:7 	 global-step:8447	 l-p:0.05551362782716751
epoch£º422	 i:8 	 global-step:8448	 l-p:0.055566225200891495
epoch£º422	 i:9 	 global-step:8449	 l-p:0.05556216090917587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1714, 28.1717, 28.1713],
        [28.1714, 32.5789, 33.5027],
        [28.1714, 28.3581, 28.2049],
        [28.1714, 31.0869, 31.0273]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.05554123595356941 
model_pd.l_d.mean(): -4.83851772514754e-06 
model_pd.lagr.mean(): 0.05553639680147171 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1453], device='cuda:0')), ('power', tensor([-0.0096], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.05554123595356941
epoch£º423	 i:1 	 global-step:8461	 l-p:0.05550921708345413
epoch£º423	 i:2 	 global-step:8462	 l-p:0.05553698539733887
epoch£º423	 i:3 	 global-step:8463	 l-p:0.05551784485578537
epoch£º423	 i:4 	 global-step:8464	 l-p:0.05563811957836151
epoch£º423	 i:5 	 global-step:8465	 l-p:0.05566947162151337
epoch£º423	 i:6 	 global-step:8466	 l-p:0.055530671030282974
epoch£º423	 i:7 	 global-step:8467	 l-p:0.05553361773490906
epoch£º423	 i:8 	 global-step:8468	 l-p:0.05557144060730934
epoch£º423	 i:9 	 global-step:8469	 l-p:0.05556853115558624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0987, 29.2469, 28.7478],
        [28.0987, 28.1188, 28.0996],
        [28.0987, 34.3999, 37.0530],
        [28.0987, 28.0987, 28.0987]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.05555001646280289 
model_pd.l_d.mean(): -4.282276495359838e-05 
model_pd.lagr.mean(): 0.05550719425082207 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1307], device='cuda:0')), ('power', tensor([-0.0947], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.05555001646280289
epoch£º424	 i:1 	 global-step:8481	 l-p:0.05565166845917702
epoch£º424	 i:2 	 global-step:8482	 l-p:0.05552830919623375
epoch£º424	 i:3 	 global-step:8483	 l-p:0.05559086799621582
epoch£º424	 i:4 	 global-step:8484	 l-p:0.055545881390571594
epoch£º424	 i:5 	 global-step:8485	 l-p:0.055548571050167084
epoch£º424	 i:6 	 global-step:8486	 l-p:0.05552723631262779
epoch£º424	 i:7 	 global-step:8487	 l-p:0.05555250868201256
epoch£º424	 i:8 	 global-step:8488	 l-p:0.05562088266015053
epoch£º424	 i:9 	 global-step:8489	 l-p:0.05564449727535248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0351, 29.0007, 28.5255],
        [28.0351, 30.1135, 29.7224],
        [28.0351, 33.4216, 35.1968],
        [28.0351, 28.0350, 28.0351]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.05563443899154663 
model_pd.l_d.mean(): -5.9671630879165605e-05 
model_pd.lagr.mean(): 0.05557476729154587 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1348], device='cuda:0')), ('power', tensor([-0.1636], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.05563443899154663
epoch£º425	 i:1 	 global-step:8501	 l-p:0.05555938184261322
epoch£º425	 i:2 	 global-step:8502	 l-p:0.055548932403326035
epoch£º425	 i:3 	 global-step:8503	 l-p:0.055555541068315506
epoch£º425	 i:4 	 global-step:8504	 l-p:0.05558309331536293
epoch£º425	 i:5 	 global-step:8505	 l-p:0.05555873364210129
epoch£º425	 i:6 	 global-step:8506	 l-p:0.05561705306172371
epoch£º425	 i:7 	 global-step:8507	 l-p:0.05559353157877922
epoch£º425	 i:8 	 global-step:8508	 l-p:0.05560433119535446
epoch£º425	 i:9 	 global-step:8509	 l-p:0.055622100830078125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9896, 28.0694, 27.9982],
        [27.9896, 27.9954, 27.9897],
        [27.9896, 28.0121, 27.9907],
        [27.9896, 35.0644, 38.5501]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.05566757172346115 
model_pd.l_d.mean(): -3.071852188440971e-05 
model_pd.lagr.mean(): 0.055636852979660034 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0773], device='cuda:0')), ('power', tensor([-0.1234], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.05566757172346115
epoch£º426	 i:1 	 global-step:8521	 l-p:0.05554816126823425
epoch£º426	 i:2 	 global-step:8522	 l-p:0.05556466802954674
epoch£º426	 i:3 	 global-step:8523	 l-p:0.05554576590657234
epoch£º426	 i:4 	 global-step:8524	 l-p:0.055671606212854385
epoch£º426	 i:5 	 global-step:8525	 l-p:0.055600062012672424
epoch£º426	 i:6 	 global-step:8526	 l-p:0.055571991950273514
epoch£º426	 i:7 	 global-step:8527	 l-p:0.055637434124946594
epoch£º426	 i:8 	 global-step:8528	 l-p:0.0555599108338356
epoch£º426	 i:9 	 global-step:8529	 l-p:0.0555822029709816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9692, 28.0232, 27.9738],
        [27.9692, 27.9924, 27.9704],
        [27.9692, 28.6120, 28.2218],
        [27.9692, 31.4721, 31.7648]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.0556243397295475 
model_pd.l_d.mean(): -3.616573667386547e-05 
model_pd.lagr.mean(): 0.05558817461133003 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1432], device='cuda:0')), ('power', tensor([-0.3114], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.0556243397295475
epoch£º427	 i:1 	 global-step:8541	 l-p:0.055571600794792175
epoch£º427	 i:2 	 global-step:8542	 l-p:0.05557674169540405
epoch£º427	 i:3 	 global-step:8543	 l-p:0.055684030055999756
epoch£º427	 i:4 	 global-step:8544	 l-p:0.055653855204582214
epoch£º427	 i:5 	 global-step:8545	 l-p:0.055578526109457016
epoch£º427	 i:6 	 global-step:8546	 l-p:0.05554939806461334
epoch£º427	 i:7 	 global-step:8547	 l-p:0.05554943531751633
epoch£º427	 i:8 	 global-step:8548	 l-p:0.05559933930635452
epoch£º427	 i:9 	 global-step:8549	 l-p:0.055576715618371964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[27.9793, 36.0858, 40.7475],
        [27.9793, 30.0507, 29.6596],
        [27.9793, 35.0150, 38.4596],
        [27.9793, 30.4005, 30.1276]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.05555450916290283 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555450916290283 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1638], device='cuda:0')), ('power', tensor([-0.3135], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.05555450916290283
epoch£º428	 i:1 	 global-step:8561	 l-p:0.05559197813272476
epoch£º428	 i:2 	 global-step:8562	 l-p:0.055542901158332825
epoch£º428	 i:3 	 global-step:8563	 l-p:0.055641479790210724
epoch£º428	 i:4 	 global-step:8564	 l-p:0.055590834468603134
epoch£º428	 i:5 	 global-step:8565	 l-p:0.05557817220687866
epoch£º428	 i:6 	 global-step:8566	 l-p:0.055611178278923035
epoch£º428	 i:7 	 global-step:8567	 l-p:0.05554389953613281
epoch£º428	 i:8 	 global-step:8568	 l-p:0.055598385632038116
epoch£º428	 i:9 	 global-step:8569	 l-p:0.055662598460912704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0153, 30.8908, 30.8198],
        [28.0153, 28.0160, 28.0153],
        [28.0153, 33.8479, 36.0468],
        [28.0153, 37.8636, 44.6939]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.05561808496713638 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05561808496713638 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0583], device='cuda:0')), ('power', tensor([-0.1288], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.05561808496713638
epoch£º429	 i:1 	 global-step:8581	 l-p:0.05570807307958603
epoch£º429	 i:2 	 global-step:8582	 l-p:0.05559613183140755
epoch£º429	 i:3 	 global-step:8583	 l-p:0.05552314221858978
epoch£º429	 i:4 	 global-step:8584	 l-p:0.05556628108024597
epoch£º429	 i:5 	 global-step:8585	 l-p:0.05552804470062256
epoch£º429	 i:6 	 global-step:8586	 l-p:0.0555538535118103
epoch£º429	 i:7 	 global-step:8587	 l-p:0.05564132705330849
epoch£º429	 i:8 	 global-step:8588	 l-p:0.055543024092912674
epoch£º429	 i:9 	 global-step:8589	 l-p:0.05555090308189392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0625, 28.4068, 28.1532],
        [28.0625, 37.8955, 44.6952],
        [28.0625, 28.9746, 28.5090],
        [28.0625, 28.0735, 28.0629]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.0556095652282238 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0556095652282238 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0500], device='cuda:0')), ('power', tensor([-0.0688], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.0556095652282238
epoch£º430	 i:1 	 global-step:8601	 l-p:0.055577415972948074
epoch£º430	 i:2 	 global-step:8602	 l-p:0.055523283779621124
epoch£º430	 i:3 	 global-step:8603	 l-p:0.05553680285811424
epoch£º430	 i:4 	 global-step:8604	 l-p:0.05553528666496277
epoch£º430	 i:5 	 global-step:8605	 l-p:0.0555584579706192
epoch£º430	 i:6 	 global-step:8606	 l-p:0.05562705546617508
epoch£º430	 i:7 	 global-step:8607	 l-p:0.055629290640354156
epoch£º430	 i:8 	 global-step:8608	 l-p:0.05553748831152916
epoch£º430	 i:9 	 global-step:8609	 l-p:0.055591728538274765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1135, 33.3345, 34.9504],
        [28.1135, 38.0518, 44.9783],
        [28.1135, 28.2259, 28.1283],
        [28.1135, 30.7708, 30.5904]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.05555637553334236 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555637553334236 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1217], device='cuda:0')), ('power', tensor([-0.1277], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.05555637553334236
epoch£º431	 i:1 	 global-step:8621	 l-p:0.05561346933245659
epoch£º431	 i:2 	 global-step:8622	 l-p:0.05557132512331009
epoch£º431	 i:3 	 global-step:8623	 l-p:0.055535368621349335
epoch£º431	 i:4 	 global-step:8624	 l-p:0.055518101900815964
epoch£º431	 i:5 	 global-step:8625	 l-p:0.05554010346531868
epoch£º431	 i:6 	 global-step:8626	 l-p:0.05552562698721886
epoch£º431	 i:7 	 global-step:8627	 l-p:0.05569874495267868
epoch£º431	 i:8 	 global-step:8628	 l-p:0.055532872676849365
epoch£º431	 i:9 	 global-step:8629	 l-p:0.0555272102355957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1662, 37.5946, 43.8473],
        [28.1662, 28.1667, 28.1661],
        [28.1662, 28.3441, 28.1971],
        [28.1662, 28.5309, 28.2656]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.05554123595356941 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554123595356941 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1332], device='cuda:0')), ('power', tensor([-0.0661], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.05554123595356941
epoch£º432	 i:1 	 global-step:8641	 l-p:0.05549533665180206
epoch£º432	 i:2 	 global-step:8642	 l-p:0.055544547736644745
epoch£º432	 i:3 	 global-step:8643	 l-p:0.055576447397470474
epoch£º432	 i:4 	 global-step:8644	 l-p:0.055583953857421875
epoch£º432	 i:5 	 global-step:8645	 l-p:0.055553242564201355
epoch£º432	 i:6 	 global-step:8646	 l-p:0.055510587990283966
epoch£º432	 i:7 	 global-step:8647	 l-p:0.05555953085422516
epoch£º432	 i:8 	 global-step:8648	 l-p:0.055509548634290695
epoch£º432	 i:9 	 global-step:8649	 l-p:0.05563545972108841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2194, 30.2257, 29.8084],
        [28.2194, 34.1282, 36.3747],
        [28.2194, 28.2194, 28.2194],
        [28.2194, 28.4915, 28.2810]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.05553411319851875 
model_pd.l_d.mean(): -3.3231675189426824e-08 
model_pd.lagr.mean(): 0.05553407967090607 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.8643e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1307], device='cuda:0')), ('power', tensor([-0.0078], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.05553411319851875
epoch£º433	 i:1 	 global-step:8661	 l-p:0.05550861358642578
epoch£º433	 i:2 	 global-step:8662	 l-p:0.05551803484559059
epoch£º433	 i:3 	 global-step:8663	 l-p:0.05550212785601616
epoch£º433	 i:4 	 global-step:8664	 l-p:0.05552724748849869
epoch£º433	 i:5 	 global-step:8665	 l-p:0.05558756738901138
epoch£º433	 i:6 	 global-step:8666	 l-p:0.05565350502729416
epoch£º433	 i:7 	 global-step:8667	 l-p:0.05558227375149727
epoch£º433	 i:8 	 global-step:8668	 l-p:0.0555105097591877
epoch£º433	 i:9 	 global-step:8669	 l-p:0.05547639727592468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[28.2728, 31.3985, 31.4449],
        [28.2728, 34.6755, 37.4077],
        [28.2728, 29.6142, 29.1049],
        [28.2728, 30.2357, 29.8057]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.055477626621723175 
model_pd.l_d.mean(): -2.5756148147593194e-07 
model_pd.lagr.mean(): 0.05547736957669258 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.3626e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2168], device='cuda:0')), ('power', tensor([-0.0323], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.055477626621723175
epoch£º434	 i:1 	 global-step:8681	 l-p:0.05548657849431038
epoch£º434	 i:2 	 global-step:8682	 l-p:0.05560366436839104
epoch£º434	 i:3 	 global-step:8683	 l-p:0.05552779138088226
epoch£º434	 i:4 	 global-step:8684	 l-p:0.05549081042408943
epoch£º434	 i:5 	 global-step:8685	 l-p:0.05555591359734535
epoch£º434	 i:6 	 global-step:8686	 l-p:0.05558552220463753
epoch£º434	 i:7 	 global-step:8687	 l-p:0.055522620677948
epoch£º434	 i:8 	 global-step:8688	 l-p:0.05555471405386925
epoch£º434	 i:9 	 global-step:8689	 l-p:0.05548686906695366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3249, 28.3261, 28.3249],
        [28.3249, 28.3249, 28.3249],
        [28.3249, 28.3253, 28.3249],
        [28.3249, 37.6354, 43.7042]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.055473897606134415 
model_pd.l_d.mean(): -6.86079545175744e-07 
model_pd.lagr.mean(): 0.0554732121527195 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.2459e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2024], device='cuda:0')), ('power', tensor([-0.0205], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.055473897606134415
epoch£º435	 i:1 	 global-step:8701	 l-p:0.055484965443611145
epoch£º435	 i:2 	 global-step:8702	 l-p:0.05551845580339432
epoch£º435	 i:3 	 global-step:8703	 l-p:0.05549892410635948
epoch£º435	 i:4 	 global-step:8704	 l-p:0.05552011355757713
epoch£º435	 i:5 	 global-step:8705	 l-p:0.055533573031425476
epoch£º435	 i:6 	 global-step:8706	 l-p:0.055556658655405045
epoch£º435	 i:7 	 global-step:8707	 l-p:0.0554877333343029
epoch£º435	 i:8 	 global-step:8708	 l-p:0.055486612021923065
epoch£º435	 i:9 	 global-step:8709	 l-p:0.05562853813171387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3722, 37.5399, 43.4188],
        [28.3722, 35.5487, 39.0850],
        [28.3722, 28.4432, 28.3793],
        [28.3722, 33.3311, 34.6883]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.05547862872481346 
model_pd.l_d.mean(): 1.2648973097384442e-05 
model_pd.lagr.mean(): 0.05549127608537674 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.1545e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1616], device='cuda:0')), ('power', tensor([0.1506], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.05547862872481346
epoch£º436	 i:1 	 global-step:8721	 l-p:0.05546703189611435
epoch£º436	 i:2 	 global-step:8722	 l-p:0.05552877485752106
epoch£º436	 i:3 	 global-step:8723	 l-p:0.055573802441358566
epoch£º436	 i:4 	 global-step:8724	 l-p:0.05551518499851227
epoch£º436	 i:5 	 global-step:8725	 l-p:0.05548590049147606
epoch£º436	 i:6 	 global-step:8726	 l-p:0.055500857532024384
epoch£º436	 i:7 	 global-step:8727	 l-p:0.05546504259109497
epoch£º436	 i:8 	 global-step:8728	 l-p:0.05556187406182289
epoch£º436	 i:9 	 global-step:8729	 l-p:0.05552501603960991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4088, 28.4426, 28.4110],
        [28.4088, 38.8843, 46.4577],
        [28.4088, 34.4016, 36.7053],
        [28.4088, 34.6998, 37.2997]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.055560577660799026 
model_pd.l_d.mean(): 2.776443761831615e-05 
model_pd.lagr.mean(): 0.05558834224939346 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1445], device='cuda:0')), ('power', tensor([0.1779], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.055560577660799026
epoch£º437	 i:1 	 global-step:8741	 l-p:0.05550037696957588
epoch£º437	 i:2 	 global-step:8742	 l-p:0.05549554526805878
epoch£º437	 i:3 	 global-step:8743	 l-p:0.055489446967840195
epoch£º437	 i:4 	 global-step:8744	 l-p:0.05546781048178673
epoch£º437	 i:5 	 global-step:8745	 l-p:0.055600106716156006
epoch£º437	 i:6 	 global-step:8746	 l-p:0.055448178201913834
epoch£º437	 i:7 	 global-step:8747	 l-p:0.05545505881309509
epoch£º437	 i:8 	 global-step:8748	 l-p:0.05551038682460785
epoch£º437	 i:9 	 global-step:8749	 l-p:0.05551522597670555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4283, 30.4035, 29.9713],
        [28.4283, 32.7131, 33.5180],
        [28.4283, 28.7026, 28.4904],
        [28.4283, 28.4283, 28.4283]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.055562399327754974 
model_pd.l_d.mean(): 4.4177482777740806e-05 
model_pd.lagr.mean(): 0.055606577545404434 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1468], device='cuda:0')), ('power', tensor([0.1820], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.055562399327754974
epoch£º438	 i:1 	 global-step:8761	 l-p:0.05547359213232994
epoch£º438	 i:2 	 global-step:8762	 l-p:0.05555332079529762
epoch£º438	 i:3 	 global-step:8763	 l-p:0.05549662560224533
epoch£º438	 i:4 	 global-step:8764	 l-p:0.05548705905675888
epoch£º438	 i:5 	 global-step:8765	 l-p:0.05545674264431
epoch£º438	 i:6 	 global-step:8766	 l-p:0.05548499524593353
epoch£º438	 i:7 	 global-step:8767	 l-p:0.05546733736991882
epoch£º438	 i:8 	 global-step:8768	 l-p:0.05556788668036461
epoch£º438	 i:9 	 global-step:8769	 l-p:0.0554724782705307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4263, 31.2729, 31.1646],
        [28.4263, 31.6718, 31.7762],
        [28.4263, 31.7371, 31.8795],
        [28.4263, 28.4282, 28.4263]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.0554526224732399 
model_pd.l_d.mean(): 2.7647245588013902e-05 
model_pd.lagr.mean(): 0.05548027157783508 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2091], device='cuda:0')), ('power', tensor([0.0827], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.0554526224732399
epoch£º439	 i:1 	 global-step:8781	 l-p:0.055603932589292526
epoch£º439	 i:2 	 global-step:8782	 l-p:0.05545119568705559
epoch£º439	 i:3 	 global-step:8783	 l-p:0.055491168051958084
epoch£º439	 i:4 	 global-step:8784	 l-p:0.05549227446317673
epoch£º439	 i:5 	 global-step:8785	 l-p:0.05548550561070442
epoch£º439	 i:6 	 global-step:8786	 l-p:0.055480487644672394
epoch£º439	 i:7 	 global-step:8787	 l-p:0.055521052330732346
epoch£º439	 i:8 	 global-step:8788	 l-p:0.05560534819960594
epoch£º439	 i:9 	 global-step:8789	 l-p:0.05546383559703827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4013, 28.6753, 28.4633],
        [28.4013, 28.6122, 28.4418],
        [28.4013, 29.8131, 29.3024],
        [28.4013, 34.1169, 36.1549]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.05556609854102135 
model_pd.l_d.mean(): 7.731251389486715e-05 
model_pd.lagr.mean(): 0.05564340949058533 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1315], device='cuda:0')), ('power', tensor([0.1842], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.05556609854102135
epoch£º440	 i:1 	 global-step:8801	 l-p:0.055565595626831055
epoch£º440	 i:2 	 global-step:8802	 l-p:0.0555240660905838
epoch£º440	 i:3 	 global-step:8803	 l-p:0.05552537366747856
epoch£º440	 i:4 	 global-step:8804	 l-p:0.05545505881309509
epoch£º440	 i:5 	 global-step:8805	 l-p:0.055481165647506714
epoch£º440	 i:6 	 global-step:8806	 l-p:0.05551684647798538
epoch£º440	 i:7 	 global-step:8807	 l-p:0.055517539381980896
epoch£º440	 i:8 	 global-step:8808	 l-p:0.05545901507139206
epoch£º440	 i:9 	 global-step:8809	 l-p:0.055508021265268326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3533, 28.4242, 28.3603],
        [28.3533, 34.0516, 36.0791],
        [28.3533, 28.3600, 28.3535],
        [28.3533, 30.8687, 30.6159]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.05557481199502945 
model_pd.l_d.mean(): 8.997543045552447e-05 
model_pd.lagr.mean(): 0.0556647889316082 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0902], device='cuda:0')), ('power', tensor([0.1845], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.05557481199502945
epoch£º441	 i:1 	 global-step:8821	 l-p:0.05556820333003998
epoch£º441	 i:2 	 global-step:8822	 l-p:0.05550207942724228
epoch£º441	 i:3 	 global-step:8823	 l-p:0.055523138493299484
epoch£º441	 i:4 	 global-step:8824	 l-p:0.05549503117799759
epoch£º441	 i:5 	 global-step:8825	 l-p:0.055472809821367264
epoch£º441	 i:6 	 global-step:8826	 l-p:0.05547714978456497
epoch£º441	 i:7 	 global-step:8827	 l-p:0.0555245466530323
epoch£º441	 i:8 	 global-step:8828	 l-p:0.05547883361577988
epoch£º441	 i:9 	 global-step:8829	 l-p:0.055619239807128906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2850, 28.2979, 28.2855],
        [28.2850, 35.9786, 40.1067],
        [28.2850, 32.4564, 33.1885],
        [28.2850, 28.4848, 28.3223]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.05551338940858841 
model_pd.l_d.mean(): -2.750273552010185e-06 
model_pd.lagr.mean(): 0.055510640144348145 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1699], device='cuda:0')), ('power', tensor([-0.0052], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.05551338940858841
epoch£º442	 i:1 	 global-step:8841	 l-p:0.05547264590859413
epoch£º442	 i:2 	 global-step:8842	 l-p:0.05550157278776169
epoch£º442	 i:3 	 global-step:8843	 l-p:0.05557771399617195
epoch£º442	 i:4 	 global-step:8844	 l-p:0.05550352483987808
epoch£º442	 i:5 	 global-step:8845	 l-p:0.05552373826503754
epoch£º442	 i:6 	 global-step:8846	 l-p:0.05557708442211151
epoch£º442	 i:7 	 global-step:8847	 l-p:0.05566846579313278
epoch£º442	 i:8 	 global-step:8848	 l-p:0.05551566183567047
epoch£º442	 i:9 	 global-step:8849	 l-p:0.05553489923477173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2046, 28.6313, 28.3330],
        [28.2046, 28.2105, 28.2048],
        [28.2046, 28.2129, 28.2049],
        [28.2046, 28.8238, 28.4408]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.055549222975969315 
model_pd.l_d.mean(): -4.0850289224181324e-05 
model_pd.lagr.mean(): 0.05550837144255638 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1572], device='cuda:0')), ('power', tensor([-0.0772], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.055549222975969315
epoch£º443	 i:1 	 global-step:8861	 l-p:0.05550961196422577
epoch£º443	 i:2 	 global-step:8862	 l-p:0.05557449162006378
epoch£º443	 i:3 	 global-step:8863	 l-p:0.05561709776520729
epoch£º443	 i:4 	 global-step:8864	 l-p:0.05552123859524727
epoch£º443	 i:5 	 global-step:8865	 l-p:0.055519189685583115
epoch£º443	 i:6 	 global-step:8866	 l-p:0.05552523583173752
epoch£º443	 i:7 	 global-step:8867	 l-p:0.0555417537689209
epoch£º443	 i:8 	 global-step:8868	 l-p:0.05561203882098198
epoch£º443	 i:9 	 global-step:8869	 l-p:0.05558842048048973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1205, 33.6352, 35.5187],
        [28.1205, 28.7197, 28.3448],
        [28.1205, 29.9480, 29.4931],
        [28.1205, 33.0329, 34.3773]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.05564841255545616 
model_pd.l_d.mean(): -1.0328846656193491e-05 
model_pd.lagr.mean(): 0.05563808232545853 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0854], device='cuda:0')), ('power', tensor([-0.0211], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:0.05564841255545616
epoch£º444	 i:1 	 global-step:8881	 l-p:0.055518053472042084
epoch£º444	 i:2 	 global-step:8882	 l-p:0.05551890283823013
epoch£º444	 i:3 	 global-step:8883	 l-p:0.055531710386276245
epoch£º444	 i:4 	 global-step:8884	 l-p:0.05560125783085823
epoch£º444	 i:5 	 global-step:8885	 l-p:0.0555400550365448
epoch£º444	 i:6 	 global-step:8886	 l-p:0.05556284636259079
epoch£º444	 i:7 	 global-step:8887	 l-p:0.055608801543712616
epoch£º444	 i:8 	 global-step:8888	 l-p:0.055624961853027344
epoch£º444	 i:9 	 global-step:8889	 l-p:0.05557301267981529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0428, 28.0461, 28.0429],
        [28.0428, 30.3735, 30.0638],
        [28.0428, 28.1346, 28.0535],
        [28.0428, 29.1597, 28.6643]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.05553940683603287 
model_pd.l_d.mean(): -9.727871656650677e-05 
model_pd.lagr.mean(): 0.05544212833046913 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1694], device='cuda:0')), ('power', tensor([-0.2372], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.05553940683603287
epoch£º445	 i:1 	 global-step:8901	 l-p:0.055572252720594406
epoch£º445	 i:2 	 global-step:8902	 l-p:0.05563637614250183
epoch£º445	 i:3 	 global-step:8903	 l-p:0.05564962700009346
epoch£º445	 i:4 	 global-step:8904	 l-p:0.055542148649692535
epoch£º445	 i:5 	 global-step:8905	 l-p:0.0555541105568409
epoch£º445	 i:6 	 global-step:8906	 l-p:0.05565379559993744
epoch£º445	 i:7 	 global-step:8907	 l-p:0.05554606392979622
epoch£º445	 i:8 	 global-step:8908	 l-p:0.05556914210319519
epoch£º445	 i:9 	 global-step:8909	 l-p:0.05561220645904541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[27.9822, 30.2957, 29.9824],
        [27.9822, 36.2548, 41.1134],
        [27.9822, 31.1739, 31.2763],
        [27.9822, 36.4514, 41.5464]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.05557744950056076 
model_pd.l_d.mean(): -7.455512240994722e-05 
model_pd.lagr.mean(): 0.05550289526581764 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1272], device='cuda:0')), ('power', tensor([-0.2528], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.05557744950056076
epoch£º446	 i:1 	 global-step:8921	 l-p:0.055612750351428986
epoch£º446	 i:2 	 global-step:8922	 l-p:0.05557873472571373
epoch£º446	 i:3 	 global-step:8923	 l-p:0.05555913969874382
epoch£º446	 i:4 	 global-step:8924	 l-p:0.055659759789705276
epoch£º446	 i:5 	 global-step:8925	 l-p:0.055590204894542694
epoch£º446	 i:6 	 global-step:8926	 l-p:0.05553735792636871
epoch£º446	 i:7 	 global-step:8927	 l-p:0.055738113820552826
epoch£º446	 i:8 	 global-step:8928	 l-p:0.055566947907209396
epoch£º446	 i:9 	 global-step:8929	 l-p:0.05555788055062294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9482, 27.9888, 27.9511],
        [27.9482, 28.3664, 28.0732],
        [27.9482, 30.5889, 30.4096],
        [27.9482, 30.6601, 30.5133]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.05557356774806976 
model_pd.l_d.mean(): -4.7259167331503704e-05 
model_pd.lagr.mean(): 0.055526308715343475 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1419], device='cuda:0')), ('power', tensor([-0.3046], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.05557356774806976
epoch£º447	 i:1 	 global-step:8941	 l-p:0.05561070144176483
epoch£º447	 i:2 	 global-step:8942	 l-p:0.05564114451408386
epoch£º447	 i:3 	 global-step:8943	 l-p:0.05555621162056923
epoch£º447	 i:4 	 global-step:8944	 l-p:0.055569838732481
epoch£º447	 i:5 	 global-step:8945	 l-p:0.05569487437605858
epoch£º447	 i:6 	 global-step:8946	 l-p:0.055567000061273575
epoch£º447	 i:7 	 global-step:8947	 l-p:0.05561528727412224
epoch£º447	 i:8 	 global-step:8948	 l-p:0.055631015449762344
epoch£º447	 i:9 	 global-step:8949	 l-p:0.0555582232773304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9488, 37.1298, 43.1135],
        [27.9488, 35.7489, 40.0576],
        [27.9488, 28.2371, 28.0169],
        [27.9488, 27.9494, 27.9488]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.055610187351703644 
model_pd.l_d.mean(): -1.5270774156306288e-06 
model_pd.lagr.mean(): 0.055608659982681274 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1012], device='cuda:0')), ('power', tensor([-0.2535], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.055610187351703644
epoch£º448	 i:1 	 global-step:8961	 l-p:0.055607087910175323
epoch£º448	 i:2 	 global-step:8962	 l-p:0.05565604940056801
epoch£º448	 i:3 	 global-step:8963	 l-p:0.0556115098297596
epoch£º448	 i:4 	 global-step:8964	 l-p:0.05562900751829147
epoch£º448	 i:5 	 global-step:8965	 l-p:0.0555935800075531
epoch£º448	 i:6 	 global-step:8966	 l-p:0.05554189532995224
epoch£º448	 i:7 	 global-step:8967	 l-p:0.055622927844524384
epoch£º448	 i:8 	 global-step:8968	 l-p:0.055570438504219055
epoch£º448	 i:9 	 global-step:8969	 l-p:0.05554024502635002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9831, 27.9998, 27.9838],
        [27.9831, 29.4238, 28.9231],
        [27.9831, 28.4089, 28.1117],
        [27.9831, 28.5376, 28.1813]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.05554446205496788 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554446205496788 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1776], device='cuda:0')), ('power', tensor([-0.3044], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.05554446205496788
epoch£º449	 i:1 	 global-step:8981	 l-p:0.05554533749818802
epoch£º449	 i:2 	 global-step:8982	 l-p:0.055539220571517944
epoch£º449	 i:3 	 global-step:8983	 l-p:0.05558546259999275
epoch£º449	 i:4 	 global-step:8984	 l-p:0.0556589774787426
epoch£º449	 i:5 	 global-step:8985	 l-p:0.055570654571056366
epoch£º449	 i:6 	 global-step:8986	 l-p:0.055642079561948776
epoch£º449	 i:7 	 global-step:8987	 l-p:0.05565979704260826
epoch£º449	 i:8 	 global-step:8988	 l-p:0.05559684336185455
epoch£º449	 i:9 	 global-step:8989	 l-p:0.05555078759789467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0330, 28.0362, 28.0330],
        [28.0330, 28.0342, 28.0330],
        [28.0330, 28.0582, 28.0343],
        [28.0330, 34.7349, 37.8130]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.055538471788167953 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055538471788167953 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1752], device='cuda:0')), ('power', tensor([-0.2319], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.055538471788167953
epoch£º450	 i:1 	 global-step:9001	 l-p:0.05553257837891579
epoch£º450	 i:2 	 global-step:9002	 l-p:0.05561425909399986
epoch£º450	 i:3 	 global-step:9003	 l-p:0.0556258000433445
epoch£º450	 i:4 	 global-step:9004	 l-p:0.055546052753925323
epoch£º450	 i:5 	 global-step:9005	 l-p:0.05559424310922623
epoch£º450	 i:6 	 global-step:9006	 l-p:0.0556686595082283
epoch£º450	 i:7 	 global-step:9007	 l-p:0.05551554635167122
epoch£º450	 i:8 	 global-step:9008	 l-p:0.05555827543139458
epoch£º450	 i:9 	 global-step:9009	 l-p:0.05558953061699867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0887, 28.3059, 28.1316],
        [28.0887, 28.7345, 28.3424],
        [28.0887, 28.0893, 28.0887],
        [28.0887, 28.0906, 28.0887]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.055658970028162 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055658970028162 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0521], device='cuda:0')), ('power', tensor([-0.0223], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.055658970028162
epoch£º451	 i:1 	 global-step:9021	 l-p:0.05554446578025818
epoch£º451	 i:2 	 global-step:9022	 l-p:0.055647630244493484
epoch£º451	 i:3 	 global-step:9023	 l-p:0.055519621819257736
epoch£º451	 i:4 	 global-step:9024	 l-p:0.055525120347738266
epoch£º451	 i:5 	 global-step:9025	 l-p:0.05554334074258804
epoch£º451	 i:6 	 global-step:9026	 l-p:0.05563122779130936
epoch£º451	 i:7 	 global-step:9027	 l-p:0.05552738904953003
epoch£º451	 i:8 	 global-step:9028	 l-p:0.0555100254714489
epoch£º451	 i:9 	 global-step:9029	 l-p:0.055557917803525925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1468, 29.0090, 28.5533],
        [28.1468, 28.1575, 28.1471],
        [28.1468, 28.1688, 28.1479],
        [28.1468, 28.1468, 28.1467]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.05554978549480438 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554978549480438 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1249], device='cuda:0')), ('power', tensor([-0.0826], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.05554978549480438
epoch£º452	 i:1 	 global-step:9041	 l-p:0.055510394275188446
epoch£º452	 i:2 	 global-step:9042	 l-p:0.05563010275363922
epoch£º452	 i:3 	 global-step:9043	 l-p:0.055500831454992294
epoch£º452	 i:4 	 global-step:9044	 l-p:0.05558400601148605
epoch£º452	 i:5 	 global-step:9045	 l-p:0.05553840473294258
epoch£º452	 i:6 	 global-step:9046	 l-p:0.05569513142108917
epoch£º452	 i:7 	 global-step:9047	 l-p:0.055509209632873535
epoch£º452	 i:8 	 global-step:9048	 l-p:0.05550392344594002
epoch£º452	 i:9 	 global-step:9049	 l-p:0.05552314594388008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2056, 28.2167, 28.2060],
        [28.2056, 28.2056, 28.2056],
        [28.2056, 28.2224, 28.2063],
        [28.2056, 28.6122, 28.3243]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.05550164729356766 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05550164729356766 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1858], device='cuda:0')), ('power', tensor([-0.0814], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.05550164729356766
epoch£º453	 i:1 	 global-step:9061	 l-p:0.05561145767569542
epoch£º453	 i:2 	 global-step:9062	 l-p:0.05558964982628822
epoch£º453	 i:3 	 global-step:9063	 l-p:0.05563626065850258
epoch£º453	 i:4 	 global-step:9064	 l-p:0.055501919239759445
epoch£º453	 i:5 	 global-step:9065	 l-p:0.055515632033348083
epoch£º453	 i:6 	 global-step:9066	 l-p:0.055547427386045456
epoch£º453	 i:7 	 global-step:9067	 l-p:0.0555591806769371
epoch£º453	 i:8 	 global-step:9068	 l-p:0.055479150265455246
epoch£º453	 i:9 	 global-step:9069	 l-p:0.05548151955008507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2648, 29.4042, 28.9034],
        [28.2648, 33.6979, 35.4887],
        [28.2648, 28.2875, 28.2659],
        [28.2648, 28.3716, 28.2784]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.055511441081762314 
model_pd.l_d.mean(): -3.828205308309407e-08 
model_pd.lagr.mean(): 0.05551140382885933 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1709], device='cuda:0')), ('power', tensor([-0.0662], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.055511441081762314
epoch£º454	 i:1 	 global-step:9081	 l-p:0.055562179535627365
epoch£º454	 i:2 	 global-step:9082	 l-p:0.05555368959903717
epoch£º454	 i:3 	 global-step:9083	 l-p:0.05548711493611336
epoch£º454	 i:4 	 global-step:9084	 l-p:0.05558403208851814
epoch£º454	 i:5 	 global-step:9085	 l-p:0.05551382899284363
epoch£º454	 i:6 	 global-step:9086	 l-p:0.05550520867109299
epoch£º454	 i:7 	 global-step:9087	 l-p:0.05556975677609444
epoch£º454	 i:8 	 global-step:9088	 l-p:0.05550770461559296
epoch£º454	 i:9 	 global-step:9089	 l-p:0.05550777167081833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3235, 30.7076, 30.4052],
        [28.3235, 28.3274, 28.3236],
        [28.3235, 28.3248, 28.3235],
        [28.3235, 28.3235, 28.3235]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.05550716444849968 
model_pd.l_d.mean(): 2.1404050585260848e-06 
model_pd.lagr.mean(): 0.055509306490421295 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.0341e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1451], device='cuda:0')), ('power', tensor([0.0815], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.05550716444849968
epoch£º455	 i:1 	 global-step:9101	 l-p:0.05565943568944931
epoch£º455	 i:2 	 global-step:9102	 l-p:0.055516213178634644
epoch£º455	 i:3 	 global-step:9103	 l-p:0.05547443404793739
epoch£º455	 i:4 	 global-step:9104	 l-p:0.05548785999417305
epoch£º455	 i:5 	 global-step:9105	 l-p:0.055489759892225266
epoch£º455	 i:6 	 global-step:9106	 l-p:0.055461786687374115
epoch£º455	 i:7 	 global-step:9107	 l-p:0.05549795180559158
epoch£º455	 i:8 	 global-step:9108	 l-p:0.05559572950005531
epoch£º455	 i:9 	 global-step:9109	 l-p:0.05549618974328041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3772, 28.3772, 28.3772],
        [28.3772, 29.2320, 28.7758],
        [28.3772, 28.3812, 28.3773],
        [28.3772, 28.3995, 28.3784]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.05548775568604469 
model_pd.l_d.mean(): 6.149020009615924e-06 
model_pd.lagr.mean(): 0.055493906140327454 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.1593e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1663], device='cuda:0')), ('power', tensor([0.0792], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.05548775568604469
epoch£º456	 i:1 	 global-step:9121	 l-p:0.055562399327754974
epoch£º456	 i:2 	 global-step:9122	 l-p:0.055555954575538635
epoch£º456	 i:3 	 global-step:9123	 l-p:0.055562976747751236
epoch£º456	 i:4 	 global-step:9124	 l-p:0.055515091866254807
epoch£º456	 i:5 	 global-step:9125	 l-p:0.05546257272362709
epoch£º456	 i:6 	 global-step:9126	 l-p:0.055527858436107635
epoch£º456	 i:7 	 global-step:9127	 l-p:0.055461395531892776
epoch£º456	 i:8 	 global-step:9128	 l-p:0.05548842251300812
epoch£º456	 i:9 	 global-step:9129	 l-p:0.05546281859278679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4189, 34.6635, 37.2153],
        [28.4189, 29.3987, 28.9166],
        [28.4189, 32.7313, 33.5581],
        [28.4189, 28.4189, 28.4189]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.05545149743556976 
model_pd.l_d.mean(): 1.859704752860125e-05 
model_pd.lagr.mean(): 0.055470094084739685 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2138], device='cuda:0')), ('power', tensor([0.1213], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.05545149743556976
epoch£º457	 i:1 	 global-step:9141	 l-p:0.05553855374455452
epoch£º457	 i:2 	 global-step:9142	 l-p:0.05548122152686119
epoch£º457	 i:3 	 global-step:9143	 l-p:0.05547220632433891
epoch£º457	 i:4 	 global-step:9144	 l-p:0.05547601729631424
epoch£º457	 i:5 	 global-step:9145	 l-p:0.05552683770656586
epoch£º457	 i:6 	 global-step:9146	 l-p:0.05554984137415886
epoch£º457	 i:7 	 global-step:9147	 l-p:0.055479325354099274
epoch£º457	 i:8 	 global-step:9148	 l-p:0.055444248020648956
epoch£º457	 i:9 	 global-step:9149	 l-p:0.055599793791770935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4411, 28.4416, 28.4410],
        [28.4411, 28.9097, 28.5899],
        [28.4411, 28.4410, 28.4411],
        [28.4411, 28.4418, 28.4411]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.055452678352594376 
model_pd.l_d.mean(): 3.485315028228797e-05 
model_pd.lagr.mean(): 0.055487532168626785 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1969], device='cuda:0')), ('power', tensor([0.1419], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.055452678352594376
epoch£º458	 i:1 	 global-step:9161	 l-p:0.055513039231300354
epoch£º458	 i:2 	 global-step:9162	 l-p:0.05550016835331917
epoch£º458	 i:3 	 global-step:9163	 l-p:0.05544578284025192
epoch£º458	 i:4 	 global-step:9164	 l-p:0.05550222098827362
epoch£º458	 i:5 	 global-step:9165	 l-p:0.05549423024058342
epoch£º458	 i:6 	 global-step:9166	 l-p:0.05546444654464722
epoch£º458	 i:7 	 global-step:9167	 l-p:0.055573124438524246
epoch£º458	 i:8 	 global-step:9168	 l-p:0.055478036403656006
epoch£º458	 i:9 	 global-step:9169	 l-p:0.05557241290807724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4392, 28.4392, 28.4392],
        [28.4392, 33.9251, 35.7437],
        [28.4392, 38.8058, 46.2257],
        [28.4392, 28.4394, 28.4392]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.05549237132072449 
model_pd.l_d.mean(): 4.893550431006588e-05 
model_pd.lagr.mean(): 0.05554130673408508 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1838], device='cuda:0')), ('power', tensor([0.1423], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.05549237132072449
epoch£º459	 i:1 	 global-step:9181	 l-p:0.055456552654504776
epoch£º459	 i:2 	 global-step:9182	 l-p:0.055462948977947235
epoch£º459	 i:3 	 global-step:9183	 l-p:0.055512093007564545
epoch£º459	 i:4 	 global-step:9184	 l-p:0.055564746260643005
epoch£º459	 i:5 	 global-step:9185	 l-p:0.05557295307517052
epoch£º459	 i:6 	 global-step:9186	 l-p:0.05545412749052048
epoch£º459	 i:7 	 global-step:9187	 l-p:0.05552690848708153
epoch£º459	 i:8 	 global-step:9188	 l-p:0.05547989904880524
epoch£º459	 i:9 	 global-step:9189	 l-p:0.055501263588666916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4103, 28.4914, 28.4190],
        [28.4103, 30.7734, 30.4596],
        [28.4103, 38.3710, 45.2595],
        [28.4103, 32.8574, 33.7896]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.05559808015823364 
model_pd.l_d.mean(): 0.0001239114790223539 
model_pd.lagr.mean(): 0.05572199076414108 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0749], device='cuda:0')), ('power', tensor([0.2848], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.05559808015823364
epoch£º460	 i:1 	 global-step:9201	 l-p:0.055464617908000946
epoch£º460	 i:2 	 global-step:9202	 l-p:0.05546291545033455
epoch£º460	 i:3 	 global-step:9203	 l-p:0.055485401302576065
epoch£º460	 i:4 	 global-step:9204	 l-p:0.0555538609623909
epoch£º460	 i:5 	 global-step:9205	 l-p:0.055454254150390625
epoch£º460	 i:6 	 global-step:9206	 l-p:0.0555587001144886
epoch£º460	 i:7 	 global-step:9207	 l-p:0.055511217564344406
epoch£º460	 i:8 	 global-step:9208	 l-p:0.055530499666929245
epoch£º460	 i:9 	 global-step:9209	 l-p:0.05548854544758797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3537, 28.3694, 28.3544],
        [28.3537, 30.5924, 30.2380],
        [28.3537, 28.8494, 28.5172],
        [28.3537, 33.6218, 35.2525]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.055630557239055634 
model_pd.l_d.mean(): 0.00010838738671736792 
model_pd.lagr.mean(): 0.05573894456028938 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0777], device='cuda:0')), ('power', tensor([0.2144], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.055630557239055634
epoch£º461	 i:1 	 global-step:9221	 l-p:0.05549836531281471
epoch£º461	 i:2 	 global-step:9222	 l-p:0.055468786507844925
epoch£º461	 i:3 	 global-step:9223	 l-p:0.0555465891957283
epoch£º461	 i:4 	 global-step:9224	 l-p:0.05555400624871254
epoch£º461	 i:5 	 global-step:9225	 l-p:0.05551321431994438
epoch£º461	 i:6 	 global-step:9226	 l-p:0.05548395216464996
epoch£º461	 i:7 	 global-step:9227	 l-p:0.055482205003499985
epoch£º461	 i:8 	 global-step:9228	 l-p:0.05552433803677559
epoch£º461	 i:9 	 global-step:9229	 l-p:0.05554240942001343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2745, 31.6656, 31.8667],
        [28.2745, 31.0566, 30.9255],
        [28.2745, 28.7051, 28.4046],
        [28.2745, 28.6979, 28.4011]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.0555020272731781 
model_pd.l_d.mean(): 7.825508873793297e-06 
model_pd.lagr.mean(): 0.05550985410809517 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1660], device='cuda:0')), ('power', tensor([0.0144], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.0555020272731781
epoch£º462	 i:1 	 global-step:9241	 l-p:0.055563148111104965
epoch£º462	 i:2 	 global-step:9242	 l-p:0.055512648075819016
epoch£º462	 i:3 	 global-step:9243	 l-p:0.05551140755414963
epoch£º462	 i:4 	 global-step:9244	 l-p:0.055519912391901016
epoch£º462	 i:5 	 global-step:9245	 l-p:0.05556584149599075
epoch£º462	 i:6 	 global-step:9246	 l-p:0.05561945214867592
epoch£º462	 i:7 	 global-step:9247	 l-p:0.0555582270026207
epoch£º462	 i:8 	 global-step:9248	 l-p:0.055529702454805374
epoch£º462	 i:9 	 global-step:9249	 l-p:0.055539339780807495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1811, 28.1930, 28.1816],
        [28.1811, 28.2734, 28.1919],
        [28.1811, 28.2628, 28.1900],
        [28.1811, 28.1822, 28.1812]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.055681418627500534 
model_pd.l_d.mean(): 5.2863866585539654e-05 
model_pd.lagr.mean(): 0.05573428422212601 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0277], device='cuda:0')), ('power', tensor([0.0985], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.055681418627500534
epoch£º463	 i:1 	 global-step:9261	 l-p:0.05557013303041458
epoch£º463	 i:2 	 global-step:9262	 l-p:0.05556466802954674
epoch£º463	 i:3 	 global-step:9263	 l-p:0.05550599843263626
epoch£º463	 i:4 	 global-step:9264	 l-p:0.05552014335989952
epoch£º463	 i:5 	 global-step:9265	 l-p:0.05560744181275368
epoch£º463	 i:6 	 global-step:9266	 l-p:0.0555613711476326
epoch£º463	 i:7 	 global-step:9267	 l-p:0.055526990443468094
epoch£º463	 i:8 	 global-step:9268	 l-p:0.05554119125008583
epoch£º463	 i:9 	 global-step:9269	 l-p:0.05553864315152168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0843, 28.1128, 28.0859],
        [28.0843, 33.0507, 34.4454],
        [28.0843, 34.1914, 36.6512],
        [28.0843, 28.0843, 28.0843]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.055532533675432205 
model_pd.l_d.mean(): -0.00010227460006717592 
model_pd.lagr.mean(): 0.05543025955557823 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1676], device='cuda:0')), ('power', tensor([-0.2117], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.055532533675432205
epoch£º464	 i:1 	 global-step:9281	 l-p:0.055544834583997726
epoch£º464	 i:2 	 global-step:9282	 l-p:0.05556011572480202
epoch£º464	 i:3 	 global-step:9283	 l-p:0.055554989725351334
epoch£º464	 i:4 	 global-step:9284	 l-p:0.05553055554628372
epoch£º464	 i:5 	 global-step:9285	 l-p:0.055654920637607574
epoch£º464	 i:6 	 global-step:9286	 l-p:0.055598944425582886
epoch£º464	 i:7 	 global-step:9287	 l-p:0.05556100606918335
epoch£º464	 i:8 	 global-step:9288	 l-p:0.05554884299635887
epoch£º464	 i:9 	 global-step:9289	 l-p:0.05572608858346939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9983, 27.9983, 27.9983],
        [27.9983, 28.0093, 27.9987],
        [27.9983, 28.1829, 28.0313],
        [27.9983, 29.8491, 29.4028]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.055575497448444366 
model_pd.l_d.mean(): -8.35176688269712e-05 
model_pd.lagr.mean(): 0.05549198016524315 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1164], device='cuda:0')), ('power', tensor([-0.2180], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.055575497448444366
epoch£º465	 i:1 	 global-step:9301	 l-p:0.05557677149772644
epoch£º465	 i:2 	 global-step:9302	 l-p:0.055538009852170944
epoch£º465	 i:3 	 global-step:9303	 l-p:0.05558406561613083
epoch£º465	 i:4 	 global-step:9304	 l-p:0.05553662031888962
epoch£º465	 i:5 	 global-step:9305	 l-p:0.055584218353033066
epoch£º465	 i:6 	 global-step:9306	 l-p:0.05556386336684227
epoch£º465	 i:7 	 global-step:9307	 l-p:0.0556512288749218
epoch£º465	 i:8 	 global-step:9308	 l-p:0.05559379607439041
epoch£º465	 i:9 	 global-step:9309	 l-p:0.055766429752111435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9375, 29.2977, 28.7951],
        [27.9375, 29.7840, 29.3387],
        [27.9375, 28.0711, 27.9571],
        [27.9375, 31.9460, 32.5880]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.055691011250019073 
model_pd.l_d.mean(): -4.976452692062594e-05 
model_pd.lagr.mean(): 0.05564124509692192 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0856], device='cuda:0')), ('power', tensor([-0.2029], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.055691011250019073
epoch£º466	 i:1 	 global-step:9321	 l-p:0.05561160296201706
epoch£º466	 i:2 	 global-step:9322	 l-p:0.05568518489599228
epoch£º466	 i:3 	 global-step:9323	 l-p:0.05554942786693573
epoch£º466	 i:4 	 global-step:9324	 l-p:0.0555671826004982
epoch£º466	 i:5 	 global-step:9325	 l-p:0.05559317767620087
epoch£º466	 i:6 	 global-step:9326	 l-p:0.05555193871259689
epoch£º466	 i:7 	 global-step:9327	 l-p:0.055548250675201416
epoch£º466	 i:8 	 global-step:9328	 l-p:0.05565449595451355
epoch£º466	 i:9 	 global-step:9329	 l-p:0.05561224743723869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9125, 27.9130, 27.9126],
        [27.9125, 33.4528, 35.3856],
        [27.9125, 28.1258, 27.9543],
        [27.9125, 29.5383, 29.0562]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.05558513477444649 
model_pd.l_d.mean(): -2.9541057301685214e-05 
model_pd.lagr.mean(): 0.055555593222379684 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.7634e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1400], device='cuda:0')), ('power', tensor([-0.3475], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.05558513477444649
epoch£º467	 i:1 	 global-step:9341	 l-p:0.05562597140669823
epoch£º467	 i:2 	 global-step:9342	 l-p:0.05563254654407501
epoch£º467	 i:3 	 global-step:9343	 l-p:0.05564560741186142
epoch£º467	 i:4 	 global-step:9344	 l-p:0.0556667223572731
epoch£º467	 i:5 	 global-step:9345	 l-p:0.05567946657538414
epoch£º467	 i:6 	 global-step:9346	 l-p:0.05554806441068649
epoch£º467	 i:7 	 global-step:9347	 l-p:0.05555635318160057
epoch£º467	 i:8 	 global-step:9348	 l-p:0.055574286729097366
epoch£º467	 i:9 	 global-step:9349	 l-p:0.05556342378258705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9300, 28.2723, 28.0202],
        [27.9300, 33.5376, 35.5317],
        [27.9300, 28.5702, 28.1811],
        [27.9300, 37.5099, 44.0089]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.05556538328528404 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05556538328528404 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1613], device='cuda:0')), ('power', tensor([-0.3279], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.05556538328528404
epoch£º468	 i:1 	 global-step:9361	 l-p:0.05565236136317253
epoch£º468	 i:2 	 global-step:9362	 l-p:0.055645931512117386
epoch£º468	 i:3 	 global-step:9363	 l-p:0.055662307888269424
epoch£º468	 i:4 	 global-step:9364	 l-p:0.0555947870016098
epoch£º468	 i:5 	 global-step:9365	 l-p:0.055582188069820404
epoch£º468	 i:6 	 global-step:9366	 l-p:0.055539678782224655
epoch£º468	 i:7 	 global-step:9367	 l-p:0.05562455952167511
epoch£º468	 i:8 	 global-step:9368	 l-p:0.05556096136569977
epoch£º468	 i:9 	 global-step:9369	 l-p:0.05558058246970177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9773, 28.4377, 28.1235],
        [27.9773, 27.9773, 27.9773],
        [27.9773, 27.9997, 27.9784],
        [27.9773, 27.9773, 27.9773]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.05556391179561615 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05556391179561615 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1524], device='cuda:0')), ('power', tensor([-0.3662], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.05556391179561615
epoch£º469	 i:1 	 global-step:9381	 l-p:0.055549852550029755
epoch£º469	 i:2 	 global-step:9382	 l-p:0.05563218146562576
epoch£º469	 i:3 	 global-step:9383	 l-p:0.05557374656200409
epoch£º469	 i:4 	 global-step:9384	 l-p:0.0555577352643013
epoch£º469	 i:5 	 global-step:9385	 l-p:0.05561317875981331
epoch£º469	 i:6 	 global-step:9386	 l-p:0.05562090128660202
epoch£º469	 i:7 	 global-step:9387	 l-p:0.055557746440172195
epoch£º469	 i:8 	 global-step:9388	 l-p:0.05565149337053299
epoch£º469	 i:9 	 global-step:9389	 l-p:0.05557651072740555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0357, 28.0376, 28.0357],
        [28.0357, 29.4648, 28.9625],
        [28.0357, 28.6508, 28.2703],
        [28.0357, 28.0369, 28.0357]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.05554515868425369 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554515868425369 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1641], device='cuda:0')), ('power', tensor([-0.2862], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.05554515868425369
epoch£º470	 i:1 	 global-step:9401	 l-p:0.055526189506053925
epoch£º470	 i:2 	 global-step:9402	 l-p:0.055588964372873306
epoch£º470	 i:3 	 global-step:9403	 l-p:0.055594053119421005
epoch£º470	 i:4 	 global-step:9404	 l-p:0.05556754395365715
epoch£º470	 i:5 	 global-step:9405	 l-p:0.055675208568573
epoch£º470	 i:6 	 global-step:9406	 l-p:0.05552714318037033
epoch£º470	 i:7 	 global-step:9407	 l-p:0.05563652887940407
epoch£º470	 i:8 	 global-step:9408	 l-p:0.055538251996040344
epoch£º470	 i:9 	 global-step:9409	 l-p:0.055571988224983215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0984, 28.0984, 28.0984],
        [28.0984, 28.0984, 28.0984],
        [28.0984, 28.0984, 28.0984],
        [28.0984, 28.0984, 28.0984]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.05556182563304901 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05556182563304901 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1157], device='cuda:0')), ('power', tensor([-0.1322], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.05556182563304901
epoch£º471	 i:1 	 global-step:9421	 l-p:0.05552827566862106
epoch£º471	 i:2 	 global-step:9422	 l-p:0.055559318512678146
epoch£º471	 i:3 	 global-step:9423	 l-p:0.05550478771328926
epoch£º471	 i:4 	 global-step:9424	 l-p:0.055646076798439026
epoch£º471	 i:5 	 global-step:9425	 l-p:0.055553290992975235
epoch£º471	 i:6 	 global-step:9426	 l-p:0.055639851838350296
epoch£º471	 i:7 	 global-step:9427	 l-p:0.05551810935139656
epoch£º471	 i:8 	 global-step:9428	 l-p:0.05555705353617668
epoch£º471	 i:9 	 global-step:9429	 l-p:0.055570777505636215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1628, 30.9814, 30.8740],
        [28.1628, 28.3487, 28.1960],
        [28.1628, 35.6567, 39.5792],
        [28.1628, 28.1628, 28.1628]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.05551593005657196 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05551593005657196 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1785], device='cuda:0')), ('power', tensor([-0.1322], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.05551593005657196
epoch£º472	 i:1 	 global-step:9441	 l-p:0.05559089034795761
epoch£º472	 i:2 	 global-step:9442	 l-p:0.05553240701556206
epoch£º472	 i:3 	 global-step:9443	 l-p:0.05554264038801193
epoch£º472	 i:4 	 global-step:9444	 l-p:0.05555017665028572
epoch£º472	 i:5 	 global-step:9445	 l-p:0.05554017424583435
epoch£º472	 i:6 	 global-step:9446	 l-p:0.05561276897788048
epoch£º472	 i:7 	 global-step:9447	 l-p:0.05550061911344528
epoch£º472	 i:8 	 global-step:9448	 l-p:0.05560145899653435
epoch£º472	 i:9 	 global-step:9449	 l-p:0.05551893636584282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2279, 30.2348, 29.8174],
        [28.2279, 28.2281, 28.2279],
        [28.2279, 28.2282, 28.2279],
        [28.2279, 28.4251, 28.2644]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.0555555522441864 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0555555522441864 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.4963e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0868], device='cuda:0')), ('power', tensor([0.1099], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.0555555522441864
epoch£º473	 i:1 	 global-step:9461	 l-p:0.05561858043074608
epoch£º473	 i:2 	 global-step:9462	 l-p:0.0554887093603611
epoch£º473	 i:3 	 global-step:9463	 l-p:0.05550691485404968
epoch£º473	 i:4 	 global-step:9464	 l-p:0.05555524304509163
epoch£º473	 i:5 	 global-step:9465	 l-p:0.05562416464090347
epoch£º473	 i:6 	 global-step:9466	 l-p:0.05553925782442093
epoch£º473	 i:7 	 global-step:9467	 l-p:0.0555071085691452
epoch£º473	 i:8 	 global-step:9468	 l-p:0.055481988936662674
epoch£º473	 i:9 	 global-step:9469	 l-p:0.05549504980444908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2929, 28.3082, 28.2936],
        [28.2929, 28.2930, 28.2929],
        [28.2929, 28.2929, 28.2929],
        [28.2929, 29.0026, 28.5876]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.055511415004730225 
model_pd.l_d.mean(): 4.117259209124313e-07 
model_pd.lagr.mean(): 0.055511828511953354 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.8398e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1460], device='cuda:0')), ('power', tensor([0.0603], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.055511415004730225
epoch£º474	 i:1 	 global-step:9481	 l-p:0.05557485297322273
epoch£º474	 i:2 	 global-step:9482	 l-p:0.0555146299302578
epoch£º474	 i:3 	 global-step:9483	 l-p:0.05551181361079216
epoch£º474	 i:4 	 global-step:9484	 l-p:0.05558358505368233
epoch£º474	 i:5 	 global-step:9485	 l-p:0.055537834763526917
epoch£º474	 i:6 	 global-step:9486	 l-p:0.05554252862930298
epoch£º474	 i:7 	 global-step:9487	 l-p:0.055481795221567154
epoch£º474	 i:8 	 global-step:9488	 l-p:0.055469632148742676
epoch£º474	 i:9 	 global-step:9489	 l-p:0.05551312491297722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3556, 29.3330, 28.8520],
        [28.3556, 29.8167, 29.3091],
        [28.3556, 28.3556, 28.3556],
        [28.3556, 28.3556, 28.3556]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.055509909987449646 
model_pd.l_d.mean(): 6.962199222471099e-06 
model_pd.lagr.mean(): 0.05551687255501747 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.2592e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1308], device='cuda:0')), ('power', tensor([0.1553], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.055509909987449646
epoch£º475	 i:1 	 global-step:9501	 l-p:0.05554160848259926
epoch£º475	 i:2 	 global-step:9502	 l-p:0.05551324412226677
epoch£º475	 i:3 	 global-step:9503	 l-p:0.05558456853032112
epoch£º475	 i:4 	 global-step:9504	 l-p:0.055562179535627365
epoch£º475	 i:5 	 global-step:9505	 l-p:0.05546453967690468
epoch£º475	 i:6 	 global-step:9506	 l-p:0.05545368045568466
epoch£º475	 i:7 	 global-step:9507	 l-p:0.055490702390670776
epoch£º475	 i:8 	 global-step:9508	 l-p:0.05548880621790886
epoch£º475	 i:9 	 global-step:9509	 l-p:0.05551161617040634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4090, 36.1380, 40.2852],
        [28.4090, 28.4091, 28.4090],
        [28.4090, 35.5952, 39.1363],
        [28.4090, 28.9726, 28.6105]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.055474456399679184 
model_pd.l_d.mean(): 1.2847212929045781e-05 
model_pd.lagr.mean(): 0.05548730492591858 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1721], device='cuda:0')), ('power', tensor([0.1144], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.055474456399679184
epoch£º476	 i:1 	 global-step:9521	 l-p:0.055570680648088455
epoch£º476	 i:2 	 global-step:9522	 l-p:0.055472515523433685
epoch£º476	 i:3 	 global-step:9523	 l-p:0.05544579401612282
epoch£º476	 i:4 	 global-step:9524	 l-p:0.055538348853588104
epoch£º476	 i:5 	 global-step:9525	 l-p:0.05546542629599571
epoch£º476	 i:6 	 global-step:9526	 l-p:0.0555650070309639
epoch£º476	 i:7 	 global-step:9527	 l-p:0.055460475385189056
epoch£º476	 i:8 	 global-step:9528	 l-p:0.05544999614357948
epoch£º476	 i:9 	 global-step:9529	 l-p:0.05558451637625694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4448, 28.4516, 28.4450],
        [28.4448, 33.7804, 35.4614],
        [28.4448, 28.9100, 28.5919],
        [28.4448, 30.4236, 29.9917]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.05555067956447601 
model_pd.l_d.mean(): 5.027473525842652e-05 
model_pd.lagr.mean(): 0.055600956082344055 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1115], device='cuda:0')), ('power', tensor([0.2480], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.05555067956447601
epoch£º477	 i:1 	 global-step:9541	 l-p:0.055452365428209305
epoch£º477	 i:2 	 global-step:9542	 l-p:0.055524520576000214
epoch£º477	 i:3 	 global-step:9543	 l-p:0.05543777346611023
epoch£º477	 i:4 	 global-step:9544	 l-p:0.05553960055112839
epoch£º477	 i:5 	 global-step:9545	 l-p:0.0554526261985302
epoch£º477	 i:6 	 global-step:9546	 l-p:0.055609166622161865
epoch£º477	 i:7 	 global-step:9547	 l-p:0.05545532703399658
epoch£º477	 i:8 	 global-step:9548	 l-p:0.05550147965550423
epoch£º477	 i:9 	 global-step:9549	 l-p:0.05545306205749512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4556, 28.4556, 28.4556],
        [28.4556, 36.8751, 41.8206],
        [28.4556, 28.4556, 28.4556],
        [28.4556, 31.5168, 31.5167]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.05559276044368744 
model_pd.l_d.mean(): 0.00010464360821060836 
model_pd.lagr.mean(): 0.055697403848171234 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0839], device='cuda:0')), ('power', tensor([0.3424], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.05559276044368744
epoch£º478	 i:1 	 global-step:9561	 l-p:0.05549734830856323
epoch£º478	 i:2 	 global-step:9562	 l-p:0.05547128990292549
epoch£º478	 i:3 	 global-step:9563	 l-p:0.05552671477198601
epoch£º478	 i:4 	 global-step:9564	 l-p:0.055461496114730835
epoch£º478	 i:5 	 global-step:9565	 l-p:0.055538151413202286
epoch£º478	 i:6 	 global-step:9566	 l-p:0.05548013001680374
epoch£º478	 i:7 	 global-step:9567	 l-p:0.05546724051237106
epoch£º478	 i:8 	 global-step:9568	 l-p:0.055473364889621735
epoch£º478	 i:9 	 global-step:9569	 l-p:0.055473893880844116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4349, 36.7761, 41.6321],
        [28.4349, 28.5711, 28.4549],
        [28.4349, 31.4825, 31.4764],
        [28.4349, 29.2674, 28.8162]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.055507224053144455 
model_pd.l_d.mean(): 8.51161967148073e-05 
model_pd.lagr.mean(): 0.05559233948588371 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1198], device='cuda:0')), ('power', tensor([0.2091], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.055507224053144455
epoch£º479	 i:1 	 global-step:9581	 l-p:0.05547555908560753
epoch£º479	 i:2 	 global-step:9582	 l-p:0.055474285036325455
epoch£º479	 i:3 	 global-step:9583	 l-p:0.055470578372478485
epoch£º479	 i:4 	 global-step:9584	 l-p:0.05545364320278168
epoch£º479	 i:5 	 global-step:9585	 l-p:0.05559301748871803
epoch£º479	 i:6 	 global-step:9586	 l-p:0.0555272176861763
epoch£º479	 i:7 	 global-step:9587	 l-p:0.05557584762573242
epoch£º479	 i:8 	 global-step:9588	 l-p:0.05548680201172829
epoch£º479	 i:9 	 global-step:9589	 l-p:0.05549020692706108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3819, 28.3823, 28.3819],
        [28.3819, 29.7820, 29.2713],
        [28.3819, 28.3823, 28.3819],
        [28.3819, 29.0547, 28.6513]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.055576398968696594 
model_pd.l_d.mean(): 8.840762893669307e-05 
model_pd.lagr.mean(): 0.05566480755805969 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1378], device='cuda:0')), ('power', tensor([0.1801], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.055576398968696594
epoch£º480	 i:1 	 global-step:9601	 l-p:0.0554923415184021
epoch£º480	 i:2 	 global-step:9602	 l-p:0.055526476353406906
epoch£º480	 i:3 	 global-step:9603	 l-p:0.055500879883766174
epoch£º480	 i:4 	 global-step:9604	 l-p:0.0555509552359581
epoch£º480	 i:5 	 global-step:9605	 l-p:0.055486924946308136
epoch£º480	 i:6 	 global-step:9606	 l-p:0.055572059005498886
epoch£º480	 i:7 	 global-step:9607	 l-p:0.055493857711553574
epoch£º480	 i:8 	 global-step:9608	 l-p:0.05549272522330284
epoch£º480	 i:9 	 global-step:9609	 l-p:0.055495165288448334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3009, 28.6256, 28.3829],
        [28.3009, 28.3010, 28.3009],
        [28.3009, 28.3328, 28.3029],
        [28.3009, 28.3011, 28.3009]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.05547612905502319 
model_pd.l_d.mean(): 5.61011665922706e-06 
model_pd.lagr.mean(): 0.05548173934221268 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2059], device='cuda:0')), ('power', tensor([0.0104], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.05547612905502319
epoch£º481	 i:1 	 global-step:9621	 l-p:0.05549682676792145
epoch£º481	 i:2 	 global-step:9622	 l-p:0.055495698004961014
epoch£º481	 i:3 	 global-step:9623	 l-p:0.05554788559675217
epoch£º481	 i:4 	 global-step:9624	 l-p:0.055545270442962646
epoch£º481	 i:5 	 global-step:9625	 l-p:0.05550972372293472
epoch£º481	 i:6 	 global-step:9626	 l-p:0.05553150177001953
epoch£º481	 i:7 	 global-step:9627	 l-p:0.05555236339569092
epoch£º481	 i:8 	 global-step:9628	 l-p:0.05552229657769203
epoch£º481	 i:9 	 global-step:9629	 l-p:0.05569661036133766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2006, 28.3923, 28.2355],
        [28.2006, 28.6933, 28.3631],
        [28.2006, 30.8486, 30.6596],
        [28.2006, 33.4386, 35.0598]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.055494364351034164 
model_pd.l_d.mean(): -7.201277912827209e-05 
model_pd.lagr.mean(): 0.0554223507642746 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2051], device='cuda:0')), ('power', tensor([-0.1315], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.055494364351034164
epoch£º482	 i:1 	 global-step:9641	 l-p:0.05551761016249657
epoch£º482	 i:2 	 global-step:9642	 l-p:0.05551322177052498
epoch£º482	 i:3 	 global-step:9643	 l-p:0.05559295043349266
epoch£º482	 i:4 	 global-step:9644	 l-p:0.055640220642089844
epoch£º482	 i:5 	 global-step:9645	 l-p:0.05555064231157303
epoch£º482	 i:6 	 global-step:9646	 l-p:0.05560753121972084
epoch£º482	 i:7 	 global-step:9647	 l-p:0.0555279403924942
epoch£º482	 i:8 	 global-step:9648	 l-p:0.05552031844854355
epoch£º482	 i:9 	 global-step:9649	 l-p:0.05562252923846245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0944, 28.0944, 28.0944],
        [28.0944, 33.3151, 34.9329],
        [28.0944, 28.0974, 28.0944],
        [28.0944, 28.0944, 28.0943]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.05566537752747536 
model_pd.l_d.mean(): -2.761701944109518e-05 
model_pd.lagr.mean(): 0.05563776195049286 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0520], device='cuda:0')), ('power', tensor([-0.0551], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.05566537752747536
epoch£º483	 i:1 	 global-step:9661	 l-p:0.055717211216688156
epoch£º483	 i:2 	 global-step:9662	 l-p:0.05552007257938385
epoch£º483	 i:3 	 global-step:9663	 l-p:0.055559203028678894
epoch£º483	 i:4 	 global-step:9664	 l-p:0.055532436817884445
epoch£º483	 i:5 	 global-step:9665	 l-p:0.05556637793779373
epoch£º483	 i:6 	 global-step:9666	 l-p:0.055556219071149826
epoch£º483	 i:7 	 global-step:9667	 l-p:0.055586013942956924
epoch£º483	 i:8 	 global-step:9668	 l-p:0.055551812052726746
epoch£º483	 i:9 	 global-step:9669	 l-p:0.05554656684398651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[27.9958, 34.5027, 37.3818],
        [27.9958, 30.5267, 30.2973],
        [27.9958, 31.5022, 31.7952],
        [27.9958, 34.7212, 37.8298]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.05560620501637459 
model_pd.l_d.mean(): -6.938282604096457e-05 
model_pd.lagr.mean(): 0.055536821484565735 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0669], device='cuda:0')), ('power', tensor([-0.1718], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.05560620501637459
epoch£º484	 i:1 	 global-step:9681	 l-p:0.055635590106248856
epoch£º484	 i:2 	 global-step:9682	 l-p:0.05564916506409645
epoch£º484	 i:3 	 global-step:9683	 l-p:0.05564231798052788
epoch£º484	 i:4 	 global-step:9684	 l-p:0.055564288049936295
epoch£º484	 i:5 	 global-step:9685	 l-p:0.055620912462472916
epoch£º484	 i:6 	 global-step:9686	 l-p:0.05556662380695343
epoch£º484	 i:7 	 global-step:9687	 l-p:0.05558303743600845
epoch£º484	 i:8 	 global-step:9688	 l-p:0.055552929639816284
epoch£º484	 i:9 	 global-step:9689	 l-p:0.05556615814566612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9216, 28.2096, 27.9896],
        [27.9216, 28.9394, 28.4574],
        [27.9216, 27.9216, 27.9216],
        [27.9216, 27.9216, 27.9216]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.05554080754518509 
model_pd.l_d.mean(): -0.00012331320613157004 
model_pd.lagr.mean(): 0.0554174929857254 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2235], device='cuda:0')), ('power', tensor([-0.4708], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.05554080754518509
epoch£º485	 i:1 	 global-step:9701	 l-p:0.05574335902929306
epoch£º485	 i:2 	 global-step:9702	 l-p:0.05558425188064575
epoch£º485	 i:3 	 global-step:9703	 l-p:0.05561814084649086
epoch£º485	 i:4 	 global-step:9704	 l-p:0.05562501773238182
epoch£º485	 i:5 	 global-step:9705	 l-p:0.0555768720805645
epoch£º485	 i:6 	 global-step:9706	 l-p:0.05555180832743645
epoch£º485	 i:7 	 global-step:9707	 l-p:0.0556747242808342
epoch£º485	 i:8 	 global-step:9708	 l-p:0.05559734255075455
epoch£º485	 i:9 	 global-step:9709	 l-p:0.05559580773115158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8876, 31.5988, 32.0351],
        [27.8876, 29.6160, 29.1498],
        [27.8876, 34.1985, 36.8911],
        [27.8876, 28.4872, 28.1134]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.05560853332281113 
model_pd.l_d.mean(): -2.850516466423869e-05 
model_pd.lagr.mean(): 0.055580027401447296 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.5752e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1008], device='cuda:0')), ('power', tensor([-0.3120], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.05560853332281113
epoch£º486	 i:1 	 global-step:9721	 l-p:0.05555996298789978
epoch£º486	 i:2 	 global-step:9722	 l-p:0.055664148181676865
epoch£º486	 i:3 	 global-step:9723	 l-p:0.0555574893951416
epoch£º486	 i:4 	 global-step:9724	 l-p:0.05560716614127159
epoch£º486	 i:5 	 global-step:9725	 l-p:0.05559695512056351
epoch£º486	 i:6 	 global-step:9726	 l-p:0.05555332079529762
epoch£º486	 i:7 	 global-step:9727	 l-p:0.05571427196264267
epoch£º486	 i:8 	 global-step:9728	 l-p:0.05559547245502472
epoch£º486	 i:9 	 global-step:9729	 l-p:0.05567475035786629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9042, 29.6423, 29.1773],
        [27.9042, 27.9053, 27.9042],
        [27.9042, 27.9042, 27.9042],
        [27.9042, 32.6618, 33.8974]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.05560099333524704 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05560099333524704 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1246], device='cuda:0')), ('power', tensor([-0.3408], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.05560099333524704
epoch£º487	 i:1 	 global-step:9741	 l-p:0.055566925555467606
epoch£º487	 i:2 	 global-step:9742	 l-p:0.055599287152290344
epoch£º487	 i:3 	 global-step:9743	 l-p:0.05563092976808548
epoch£º487	 i:4 	 global-step:9744	 l-p:0.05566127598285675
epoch£º487	 i:5 	 global-step:9745	 l-p:0.055585190653800964
epoch£º487	 i:6 	 global-step:9746	 l-p:0.05555474013090134
epoch£º487	 i:7 	 global-step:9747	 l-p:0.055623870342969894
epoch£º487	 i:8 	 global-step:9748	 l-p:0.05557914450764656
epoch£º487	 i:9 	 global-step:9749	 l-p:0.05565676465630531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9555, 27.9637, 27.9558],
        [27.9555, 31.2082, 31.3479],
        [27.9555, 28.1399, 27.9885],
        [27.9555, 35.6443, 39.8235]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.05557039752602577 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05557039752602577 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1409], device='cuda:0')), ('power', tensor([-0.2954], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.05557039752602577
epoch£º488	 i:1 	 global-step:9761	 l-p:0.05557392165064812
epoch£º488	 i:2 	 global-step:9762	 l-p:0.05555301904678345
epoch£º488	 i:3 	 global-step:9763	 l-p:0.05570323020219803
epoch£º488	 i:4 	 global-step:9764	 l-p:0.0555914007127285
epoch£º488	 i:5 	 global-step:9765	 l-p:0.05561596155166626
epoch£º488	 i:6 	 global-step:9766	 l-p:0.055639684200286865
epoch£º488	 i:7 	 global-step:9767	 l-p:0.05556148290634155
epoch£º488	 i:8 	 global-step:9768	 l-p:0.05557669699192047
epoch£º488	 i:9 	 global-step:9769	 l-p:0.055551789700984955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0197, 28.3088, 28.0880],
        [28.0197, 28.0222, 28.0198],
        [28.0197, 28.3410, 28.1009],
        [28.0197, 28.0197, 28.0197]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.05554145574569702 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554145574569702 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1790], device='cuda:0')), ('power', tensor([-0.3143], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.05554145574569702
epoch£º489	 i:1 	 global-step:9781	 l-p:0.05555758252739906
epoch£º489	 i:2 	 global-step:9782	 l-p:0.05557166039943695
epoch£º489	 i:3 	 global-step:9783	 l-p:0.05555548518896103
epoch£º489	 i:4 	 global-step:9784	 l-p:0.055621929466724396
epoch£º489	 i:5 	 global-step:9785	 l-p:0.05562042072415352
epoch£º489	 i:6 	 global-step:9786	 l-p:0.055690206587314606
epoch£º489	 i:7 	 global-step:9787	 l-p:0.05555405095219612
epoch£º489	 i:8 	 global-step:9788	 l-p:0.05554576218128204
epoch£º489	 i:9 	 global-step:9789	 l-p:0.05554010346531868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0885, 28.0885, 28.0885],
        [28.0885, 28.0918, 28.0886],
        [28.0885, 28.0937, 28.0886],
        [28.0885, 28.0885, 28.0885]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.055520132184028625 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055520132184028625 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1933], device='cuda:0')), ('power', tensor([-0.2395], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.055520132184028625
epoch£º490	 i:1 	 global-step:9801	 l-p:0.055538758635520935
epoch£º490	 i:2 	 global-step:9802	 l-p:0.05556787922978401
epoch£º490	 i:3 	 global-step:9803	 l-p:0.055556315928697586
epoch£º490	 i:4 	 global-step:9804	 l-p:0.05561460554599762
epoch£º490	 i:5 	 global-step:9805	 l-p:0.05551571398973465
epoch£º490	 i:6 	 global-step:9806	 l-p:0.05556735023856163
epoch£º490	 i:7 	 global-step:9807	 l-p:0.055569130927324295
epoch£º490	 i:8 	 global-step:9808	 l-p:0.05551651865243912
epoch£º490	 i:9 	 global-step:9809	 l-p:0.05568743497133255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1591, 36.3684, 41.1187],
        [28.1591, 29.7454, 29.2524],
        [28.1591, 28.5118, 28.2532],
        [28.1591, 34.3918, 36.9674]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.05561508238315582 
model_pd.l_d.mean(): -1.6428963078851666e-07 
model_pd.lagr.mean(): 0.05561491847038269 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.9062e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1245], device='cuda:0')), ('power', tensor([-0.0413], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.05561508238315582
epoch£º491	 i:1 	 global-step:9821	 l-p:0.055502526462078094
epoch£º491	 i:2 	 global-step:9822	 l-p:0.055521104484796524
epoch£º491	 i:3 	 global-step:9823	 l-p:0.05556553229689598
epoch£º491	 i:4 	 global-step:9824	 l-p:0.05551113188266754
epoch£º491	 i:5 	 global-step:9825	 l-p:0.05553095042705536
epoch£º491	 i:6 	 global-step:9826	 l-p:0.055583205074071884
epoch£º491	 i:7 	 global-step:9827	 l-p:0.05554797127842903
epoch£º491	 i:8 	 global-step:9828	 l-p:0.05560443922877312
epoch£º491	 i:9 	 global-step:9829	 l-p:0.05552588775753975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2305, 28.2306, 28.2304],
        [28.2305, 31.3512, 31.3974],
        [28.2305, 28.2871, 28.2354],
        [28.2305, 28.2715, 28.2334]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.05549529939889908 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05549529939889908 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1866], device='cuda:0')), ('power', tensor([-0.0276], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.05549529939889908
epoch£º492	 i:1 	 global-step:9841	 l-p:0.055528268218040466
epoch£º492	 i:2 	 global-step:9842	 l-p:0.055596884340047836
epoch£º492	 i:3 	 global-step:9843	 l-p:0.055514153093099594
epoch£º492	 i:4 	 global-step:9844	 l-p:0.05551005154848099
epoch£º492	 i:5 	 global-step:9845	 l-p:0.05555054917931557
epoch£º492	 i:6 	 global-step:9846	 l-p:0.05557845160365105
epoch£º492	 i:7 	 global-step:9847	 l-p:0.055477604269981384
epoch£º492	 i:8 	 global-step:9848	 l-p:0.05558530613780022
epoch£º492	 i:9 	 global-step:9849	 l-p:0.05552457273006439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3018, 38.7016, 46.1988],
        [28.3018, 28.3036, 28.3018],
        [28.3018, 29.7114, 29.2027],
        [28.3018, 37.5181, 43.4726]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.05570875108242035 
model_pd.l_d.mean(): 2.646110260684509e-06 
model_pd.lagr.mean(): 0.05571139603853226 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.3210e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9900], device='cuda:0')), ('power', tensor([0.2629], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.05570875108242035
epoch£º493	 i:1 	 global-step:9861	 l-p:0.05549939349293709
epoch£º493	 i:2 	 global-step:9862	 l-p:0.05546621233224869
epoch£º493	 i:3 	 global-step:9863	 l-p:0.05552556738257408
epoch£º493	 i:4 	 global-step:9864	 l-p:0.05550231784582138
epoch£º493	 i:5 	 global-step:9865	 l-p:0.055491745471954346
epoch£º493	 i:6 	 global-step:9866	 l-p:0.055554408580064774
epoch£º493	 i:7 	 global-step:9867	 l-p:0.055509284138679504
epoch£º493	 i:8 	 global-step:9868	 l-p:0.05547123774886131
epoch£º493	 i:9 	 global-step:9869	 l-p:0.05548932030797005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3696, 31.5066, 31.5532],
        [28.3696, 38.3153, 45.1934],
        [28.3696, 29.4049, 28.9146],
        [28.3696, 28.5109, 28.3908]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.05564184859395027 
model_pd.l_d.mean(): 1.7171802028315142e-05 
model_pd.lagr.mean(): 0.05565902218222618 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.9734e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0406], device='cuda:0')), ('power', tensor([0.3194], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.05564184859395027
epoch£º494	 i:1 	 global-step:9881	 l-p:0.055514369159936905
epoch£º494	 i:2 	 global-step:9882	 l-p:0.05546778440475464
epoch£º494	 i:3 	 global-step:9883	 l-p:0.055525634437799454
epoch£º494	 i:4 	 global-step:9884	 l-p:0.05545015260577202
epoch£º494	 i:5 	 global-step:9885	 l-p:0.05548424646258354
epoch£º494	 i:6 	 global-step:9886	 l-p:0.05549312010407448
epoch£º494	 i:7 	 global-step:9887	 l-p:0.05547739937901497
epoch£º494	 i:8 	 global-step:9888	 l-p:0.05557578057050705
epoch£º494	 i:9 	 global-step:9889	 l-p:0.05545968934893608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4251, 29.7455, 29.2334],
        [28.4251, 28.4290, 28.4251],
        [28.4251, 35.9327, 39.8270],
        [28.4251, 28.5122, 28.4348]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.05547424778342247 
model_pd.l_d.mean(): 1.8156815713155083e-05 
model_pd.lagr.mean(): 0.05549240484833717 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1753], device='cuda:0')), ('power', tensor([0.1409], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.05547424778342247
epoch£º495	 i:1 	 global-step:9901	 l-p:0.05551169440150261
epoch£º495	 i:2 	 global-step:9902	 l-p:0.055501751601696014
epoch£º495	 i:3 	 global-step:9903	 l-p:0.05555785074830055
epoch£º495	 i:4 	 global-step:9904	 l-p:0.05548414587974548
epoch£º495	 i:5 	 global-step:9905	 l-p:0.05556100606918335
epoch£º495	 i:6 	 global-step:9906	 l-p:0.05545121058821678
epoch£º495	 i:7 	 global-step:9907	 l-p:0.055517103523015976
epoch£º495	 i:8 	 global-step:9908	 l-p:0.05545269697904587
epoch£º495	 i:9 	 global-step:9909	 l-p:0.05548392981290817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4592, 28.4592, 28.4591],
        [28.4592, 29.0977, 28.7060],
        [28.4592, 28.4593, 28.4592],
        [28.4592, 35.9762, 39.8755]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.05545872077345848 
model_pd.l_d.mean(): 3.602527795010246e-05 
model_pd.lagr.mean(): 0.05549474433064461 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1843], device='cuda:0')), ('power', tensor([0.1587], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.05545872077345848
epoch£º496	 i:1 	 global-step:9921	 l-p:0.05561194196343422
epoch£º496	 i:2 	 global-step:9922	 l-p:0.05546003580093384
epoch£º496	 i:3 	 global-step:9923	 l-p:0.055578574538230896
epoch£º496	 i:4 	 global-step:9924	 l-p:0.05545172467827797
epoch£º496	 i:5 	 global-step:9925	 l-p:0.05544041469693184
epoch£º496	 i:6 	 global-step:9926	 l-p:0.05551101639866829
epoch£º496	 i:7 	 global-step:9927	 l-p:0.055493082851171494
epoch£º496	 i:8 	 global-step:9928	 l-p:0.055494923144578934
epoch£º496	 i:9 	 global-step:9929	 l-p:0.05545279383659363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4627, 31.3132, 31.2047],
        [28.4627, 28.4627, 28.4627],
        [28.4627, 28.4629, 28.4627],
        [28.4627, 28.4739, 28.4631]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.055472467094659805 
model_pd.l_d.mean(): 7.923434895928949e-05 
model_pd.lagr.mean(): 0.05555170029401779 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1580], device='cuda:0')), ('power', tensor([0.2360], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.055472467094659805
epoch£º497	 i:1 	 global-step:9941	 l-p:0.05546750873327255
epoch£º497	 i:2 	 global-step:9942	 l-p:0.05544726178050041
epoch£º497	 i:3 	 global-step:9943	 l-p:0.05547938123345375
epoch£º497	 i:4 	 global-step:9944	 l-p:0.05545565113425255
epoch£º497	 i:5 	 global-step:9945	 l-p:0.05549328774213791
epoch£º497	 i:6 	 global-step:9946	 l-p:0.055461280047893524
epoch£º497	 i:7 	 global-step:9947	 l-p:0.05550137534737587
epoch£º497	 i:8 	 global-step:9948	 l-p:0.05553341656923294
epoch£º497	 i:9 	 global-step:9949	 l-p:0.05566701665520668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4306, 28.8514, 28.5555],
        [28.4306, 36.7230, 41.5218],
        [28.4306, 34.6171, 37.1092],
        [28.4306, 28.4850, 28.4352]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.0554770901799202 
model_pd.l_d.mean(): 7.60181646910496e-05 
model_pd.lagr.mean(): 0.05555310845375061 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1622], device='cuda:0')), ('power', tensor([0.1735], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.0554770901799202
epoch£º498	 i:1 	 global-step:9961	 l-p:0.055475395172834396
epoch£º498	 i:2 	 global-step:9962	 l-p:0.05548780411481857
epoch£º498	 i:3 	 global-step:9963	 l-p:0.05546864494681358
epoch£º498	 i:4 	 global-step:9964	 l-p:0.05546482279896736
epoch£º498	 i:5 	 global-step:9965	 l-p:0.05554826557636261
epoch£º498	 i:6 	 global-step:9966	 l-p:0.055483944714069366
epoch£º498	 i:7 	 global-step:9967	 l-p:0.0555056594312191
epoch£º498	 i:8 	 global-step:9968	 l-p:0.055660102516412735
epoch£º498	 i:9 	 global-step:9969	 l-p:0.05550362542271614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3638, 28.3691, 28.3639],
        [28.3638, 36.3969, 40.9028],
        [28.3638, 28.3638, 28.3638],
        [28.3638, 28.3638, 28.3638]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.05545753240585327 
model_pd.l_d.mean(): -3.629944330896251e-06 
model_pd.lagr.mean(): 0.05545390397310257 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2254], device='cuda:0')), ('power', tensor([-0.0070], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.05545753240585327
epoch£º499	 i:1 	 global-step:9981	 l-p:0.055620450526475906
epoch£º499	 i:2 	 global-step:9982	 l-p:0.05547378584742546
epoch£º499	 i:3 	 global-step:9983	 l-p:0.055531661957502365
epoch£º499	 i:4 	 global-step:9984	 l-p:0.05547510087490082
epoch£º499	 i:5 	 global-step:9985	 l-p:0.0555548258125782
epoch£º499	 i:6 	 global-step:9986	 l-p:0.055502407252788544
epoch£º499	 i:7 	 global-step:9987	 l-p:0.055492013692855835
epoch£º499	 i:8 	 global-step:9988	 l-p:0.055539071559906006
epoch£º499	 i:9 	 global-step:9989	 l-p:0.055592864751815796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2668, 29.1777, 28.7102],
        [28.2668, 31.0481, 30.9170],
        [28.2668, 28.6850, 28.3909],
        [28.2668, 33.8117, 35.7057]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.0554826445877552 
model_pd.l_d.mean(): -1.00126953839208e-05 
model_pd.lagr.mean(): 0.05547263100743294 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2054], device='cuda:0')), ('power', tensor([-0.0180], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.0554826445877552
epoch£º500	 i:1 	 global-step:10001	 l-p:0.05549195036292076
epoch£º500	 i:2 	 global-step:10002	 l-p:0.055502403527498245
epoch£º500	 i:3 	 global-step:10003	 l-p:0.05563085526227951
epoch£º500	 i:4 	 global-step:10004	 l-p:0.055509090423583984
epoch£º500	 i:5 	 global-step:10005	 l-p:0.055571578443050385
epoch£º500	 i:6 	 global-step:10006	 l-p:0.055675480514764786
epoch£º500	 i:7 	 global-step:10007	 l-p:0.055535122752189636
epoch£º500	 i:8 	 global-step:10008	 l-p:0.05553021281957626
epoch£º500	 i:9 	 global-step:10009	 l-p:0.055528078228235245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1518, 29.3722, 28.8677],
        [28.1518, 31.0421, 30.9709],
        [28.1518, 31.4287, 31.5695],
        [28.1518, 28.3695, 28.1948]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.05555865541100502 
model_pd.l_d.mean(): -3.9825114072300494e-05 
model_pd.lagr.mean(): 0.05551883205771446 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1228], device='cuda:0')), ('power', tensor([-0.0736], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:0.05555865541100502
epoch£º501	 i:1 	 global-step:10021	 l-p:0.05553627386689186
epoch£º501	 i:2 	 global-step:10022	 l-p:0.0555139034986496
epoch£º501	 i:3 	 global-step:10023	 l-p:0.055650267750024796
epoch£º501	 i:4 	 global-step:10024	 l-p:0.05550893768668175
epoch£º501	 i:5 	 global-step:10025	 l-p:0.0556517019867897
epoch£º501	 i:6 	 global-step:10026	 l-p:0.0555449053645134
epoch£º501	 i:7 	 global-step:10027	 l-p:0.05556860938668251
epoch£º501	 i:8 	 global-step:10028	 l-p:0.05554667487740517
epoch£º501	 i:9 	 global-step:10029	 l-p:0.05561884865164757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0355, 30.2475, 29.8972],
        [28.0355, 28.0355, 28.0355],
        [28.0355, 35.8950, 40.2572],
        [28.0355, 28.0371, 28.0355]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.055613286793231964 
model_pd.l_d.mean(): -9.171276906272396e-05 
model_pd.lagr.mean(): 0.05552157387137413 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0945], device='cuda:0')), ('power', tensor([-0.1960], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.055613286793231964
epoch£º502	 i:1 	 global-step:10041	 l-p:0.055560026317834854
epoch£º502	 i:2 	 global-step:10042	 l-p:0.05553590506315231
epoch£º502	 i:3 	 global-step:10043	 l-p:0.0556194968521595
epoch£º502	 i:4 	 global-step:10044	 l-p:0.05558620020747185
epoch£º502	 i:5 	 global-step:10045	 l-p:0.05558255687355995
epoch£º502	 i:6 	 global-step:10046	 l-p:0.055726636201143265
epoch£º502	 i:7 	 global-step:10047	 l-p:0.05555235594511032
epoch£º502	 i:8 	 global-step:10048	 l-p:0.05558208376169205
epoch£º502	 i:9 	 global-step:10049	 l-p:0.055568140000104904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9372, 27.9377, 27.9372],
        [27.9372, 35.6219, 39.7996],
        [27.9372, 27.9372, 27.9372],
        [27.9372, 28.1216, 27.9702]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.05562273785471916 
model_pd.l_d.mean(): -0.00011218392319278792 
model_pd.lagr.mean(): 0.05551055446267128 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1106], device='cuda:0')), ('power', tensor([-0.3295], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.05562273785471916
epoch£º503	 i:1 	 global-step:10061	 l-p:0.05557623878121376
epoch£º503	 i:2 	 global-step:10062	 l-p:0.05555886775255203
epoch£º503	 i:3 	 global-step:10063	 l-p:0.05558246001601219
epoch£º503	 i:4 	 global-step:10064	 l-p:0.05558446794748306
epoch£º503	 i:5 	 global-step:10065	 l-p:0.0556485652923584
epoch£º503	 i:6 	 global-step:10066	 l-p:0.055680517107248306
epoch£º503	 i:7 	 global-step:10067	 l-p:0.055598706007003784
epoch£º503	 i:8 	 global-step:10068	 l-p:0.05556744337081909
epoch£º503	 i:9 	 global-step:10069	 l-p:0.05568086355924606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8762, 27.9557, 27.8848],
        [27.8762, 30.7376, 30.6675],
        [27.8762, 28.1157, 27.9267],
        [27.8762, 29.0554, 28.5576]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.055585477501153946 
model_pd.l_d.mean(): -6.414038216462359e-05 
model_pd.lagr.mean(): 0.05552133545279503 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1313], device='cuda:0')), ('power', tensor([-0.3735], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:0.055585477501153946
epoch£º504	 i:1 	 global-step:10081	 l-p:0.055677201598882675
epoch£º504	 i:2 	 global-step:10082	 l-p:0.05558609962463379
epoch£º504	 i:3 	 global-step:10083	 l-p:0.05556620657444
epoch£º504	 i:4 	 global-step:10084	 l-p:0.055572789162397385
epoch£º504	 i:5 	 global-step:10085	 l-p:0.055648546665906906
epoch£º504	 i:6 	 global-step:10086	 l-p:0.0555623359978199
epoch£º504	 i:7 	 global-step:10087	 l-p:0.055593203753232956
epoch£º504	 i:8 	 global-step:10088	 l-p:0.0556735023856163
epoch£º504	 i:9 	 global-step:10089	 l-p:0.05571539327502251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8685, 29.3614, 28.8667],
        [27.8685, 28.3110, 28.0059],
        [27.8685, 27.8988, 27.8704],
        [27.8685, 28.5316, 28.1348]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.05557670444250107 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05557670444250107 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1586], device='cuda:0')), ('power', tensor([-0.4170], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.05557670444250107
epoch£º505	 i:1 	 global-step:10101	 l-p:0.05560788884758949
epoch£º505	 i:2 	 global-step:10102	 l-p:0.05556970089673996
epoch£º505	 i:3 	 global-step:10103	 l-p:0.05555829033255577
epoch£º505	 i:4 	 global-step:10104	 l-p:0.05560046434402466
epoch£º505	 i:5 	 global-step:10105	 l-p:0.055598314851522446
epoch£º505	 i:6 	 global-step:10106	 l-p:0.05563169717788696
epoch£º505	 i:7 	 global-step:10107	 l-p:0.05572787672281265
epoch£º505	 i:8 	 global-step:10108	 l-p:0.055594462901353836
epoch£º505	 i:9 	 global-step:10109	 l-p:0.05567832663655281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9121, 27.9917, 27.9207],
        [27.9121, 37.9378, 45.0264],
        [27.9121, 36.9240, 42.7022],
        [27.9121, 31.0954, 31.1974]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.05562388896942139 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05562388896942139 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0818], device='cuda:0')), ('power', tensor([-0.2983], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.05562388896942139
epoch£º506	 i:1 	 global-step:10121	 l-p:0.055550120770931244
epoch£º506	 i:2 	 global-step:10122	 l-p:0.055634625256061554
epoch£º506	 i:3 	 global-step:10123	 l-p:0.05554887279868126
epoch£º506	 i:4 	 global-step:10124	 l-p:0.055572815239429474
epoch£º506	 i:5 	 global-step:10125	 l-p:0.05564489588141441
epoch£º506	 i:6 	 global-step:10126	 l-p:0.05562260374426842
epoch£º506	 i:7 	 global-step:10127	 l-p:0.05562466010451317
epoch£º506	 i:8 	 global-step:10128	 l-p:0.05564858019351959
epoch£º506	 i:9 	 global-step:10129	 l-p:0.0555565170943737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9779, 28.3774, 28.0938],
        [27.9779, 29.8355, 29.3912],
        [27.9779, 30.3759, 30.0941],
        [27.9779, 37.5749, 44.0857]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.055564820766448975 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055564820766448975 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1439], device='cuda:0')), ('power', tensor([-0.2621], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.055564820766448975
epoch£º507	 i:1 	 global-step:10141	 l-p:0.0556214302778244
epoch£º507	 i:2 	 global-step:10142	 l-p:0.055644094944000244
epoch£º507	 i:3 	 global-step:10143	 l-p:0.05553970858454704
epoch£º507	 i:4 	 global-step:10144	 l-p:0.05566171184182167
epoch£º507	 i:5 	 global-step:10145	 l-p:0.0555376335978508
epoch£º507	 i:6 	 global-step:10146	 l-p:0.055555444210767746
epoch£º507	 i:7 	 global-step:10147	 l-p:0.05553782358765602
epoch£º507	 i:8 	 global-step:10148	 l-p:0.05567425116896629
epoch£º507	 i:9 	 global-step:10149	 l-p:0.05554454028606415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0516, 28.2376, 28.0850],
        [28.0516, 28.0667, 28.0523],
        [28.0516, 28.0555, 28.0517],
        [28.0516, 32.5195, 33.5024]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.055618103593587875 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055618103593587875 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0954], device='cuda:0')), ('power', tensor([-0.1325], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.055618103593587875
epoch£º508	 i:1 	 global-step:10161	 l-p:0.05555122718214989
epoch£º508	 i:2 	 global-step:10162	 l-p:0.055624283850193024
epoch£º508	 i:3 	 global-step:10163	 l-p:0.055542606860399246
epoch£º508	 i:4 	 global-step:10164	 l-p:0.0555139034986496
epoch£º508	 i:5 	 global-step:10165	 l-p:0.05552608519792557
epoch£º508	 i:6 	 global-step:10166	 l-p:0.055542439222335815
epoch£º508	 i:7 	 global-step:10167	 l-p:0.05561099946498871
epoch£º508	 i:8 	 global-step:10168	 l-p:0.05559265986084938
epoch£º508	 i:9 	 global-step:10169	 l-p:0.05560257285833359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1283, 34.8541, 37.9434],
        [28.1283, 28.1313, 28.1283],
        [28.1283, 38.3762, 45.7107],
        [28.1283, 28.4734, 28.2192]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.05557238310575485 
model_pd.l_d.mean(): -2.1105869052462367e-07 
model_pd.lagr.mean(): 0.05557217076420784 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1304], device='cuda:0')), ('power', tensor([-0.0965], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.05557238310575485
epoch£º509	 i:1 	 global-step:10181	 l-p:0.05569253861904144
epoch£º509	 i:2 	 global-step:10182	 l-p:0.055528391152620316
epoch£º509	 i:3 	 global-step:10183	 l-p:0.05554351583123207
epoch£º509	 i:4 	 global-step:10184	 l-p:0.055526331067085266
epoch£º509	 i:5 	 global-step:10185	 l-p:0.05553346499800682
epoch£º509	 i:6 	 global-step:10186	 l-p:0.055586546659469604
epoch£º509	 i:7 	 global-step:10187	 l-p:0.05550450086593628
epoch£º509	 i:8 	 global-step:10188	 l-p:0.05554300174117088
epoch£º509	 i:9 	 global-step:10189	 l-p:0.055534910410642624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2062, 28.2155, 28.2065],
        [28.2062, 34.0807, 36.2956],
        [28.2062, 34.4253, 36.9805],
        [28.2062, 32.7964, 33.8628]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.05567186325788498 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05567186325788498 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1133], device='cuda:0')), ('power', tensor([-0.0116], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.05567186325788498
epoch£º510	 i:1 	 global-step:10201	 l-p:0.05550403147935867
epoch£º510	 i:2 	 global-step:10202	 l-p:0.05551666021347046
epoch£º510	 i:3 	 global-step:10203	 l-p:0.05553130805492401
epoch£º510	 i:4 	 global-step:10204	 l-p:0.055549196898937225
epoch£º510	 i:5 	 global-step:10205	 l-p:0.05555090680718422
epoch£º510	 i:6 	 global-step:10206	 l-p:0.055505502969026566
epoch£º510	 i:7 	 global-step:10207	 l-p:0.05552462488412857
epoch£º510	 i:8 	 global-step:10208	 l-p:0.05554596707224846
epoch£º510	 i:9 	 global-step:10209	 l-p:0.05550479143857956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2846, 29.0059, 28.5872],
        [28.2846, 33.7390, 35.5471],
        [28.2846, 28.5574, 28.3464],
        [28.2846, 28.2847, 28.2846]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.055522408336400986 
model_pd.l_d.mean(): 5.104724891680235e-07 
model_pd.lagr.mean(): 0.055522918701171875 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.0106e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1307], device='cuda:0')), ('power', tensor([0.1032], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.055522408336400986
epoch£º511	 i:1 	 global-step:10221	 l-p:0.055507559329271317
epoch£º511	 i:2 	 global-step:10222	 l-p:0.05554788559675217
epoch£º511	 i:3 	 global-step:10223	 l-p:0.05555233731865883
epoch£º511	 i:4 	 global-step:10224	 l-p:0.05548202246427536
epoch£º511	 i:5 	 global-step:10225	 l-p:0.05548704043030739
epoch£º511	 i:6 	 global-step:10226	 l-p:0.055476702749729156
epoch£º511	 i:7 	 global-step:10227	 l-p:0.05570152774453163
epoch£º511	 i:8 	 global-step:10228	 l-p:0.05549945309758186
epoch£º511	 i:9 	 global-step:10229	 l-p:0.05546846613287926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3611, 28.4433, 28.3700],
        [28.3611, 28.7093, 28.4528],
        [28.3611, 33.6341, 35.2683],
        [28.3611, 29.5809, 29.0730]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.055478837341070175 
model_pd.l_d.mean(): 2.4405251224379754e-06 
model_pd.lagr.mean(): 0.05548127740621567 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.4778e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1942], device='cuda:0')), ('power', tensor([0.0583], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.055478837341070175
epoch£º512	 i:1 	 global-step:10241	 l-p:0.055541813373565674
epoch£º512	 i:2 	 global-step:10242	 l-p:0.05549187585711479
epoch£º512	 i:3 	 global-step:10243	 l-p:0.055581144988536835
epoch£º512	 i:4 	 global-step:10244	 l-p:0.0555809922516346
epoch£º512	 i:5 	 global-step:10245	 l-p:0.05548016354441643
epoch£º512	 i:6 	 global-step:10246	 l-p:0.05549361929297447
epoch£º512	 i:7 	 global-step:10247	 l-p:0.05548607558012009
epoch£º512	 i:8 	 global-step:10248	 l-p:0.055453233420848846
epoch£º512	 i:9 	 global-step:10249	 l-p:0.055509764701128006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4272, 30.2324, 29.7641],
        [28.4272, 36.4199, 40.8674],
        [28.4272, 29.9520, 29.4469],
        [28.4272, 28.4414, 28.4278]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.05547204241156578 
model_pd.l_d.mean(): 1.8919898138847202e-05 
model_pd.lagr.mean(): 0.05549096316099167 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1647], device='cuda:0')), ('power', tensor([0.1644], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.05547204241156578
epoch£º513	 i:1 	 global-step:10261	 l-p:0.05550655350089073
epoch£º513	 i:2 	 global-step:10262	 l-p:0.0554581880569458
epoch£º513	 i:3 	 global-step:10263	 l-p:0.05545102804899216
epoch£º513	 i:4 	 global-step:10264	 l-p:0.05558083578944206
epoch£º513	 i:5 	 global-step:10265	 l-p:0.055480461567640305
epoch£º513	 i:6 	 global-step:10266	 l-p:0.05546930059790611
epoch£º513	 i:7 	 global-step:10267	 l-p:0.055509209632873535
epoch£º513	 i:8 	 global-step:10268	 l-p:0.055509913712739944
epoch£º513	 i:9 	 global-step:10269	 l-p:0.05554509907960892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4703, 37.9679, 44.2438],
        [28.4703, 36.3078, 40.5686],
        [28.4703, 29.7931, 29.2800],
        [28.4703, 28.4716, 28.4703]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.055465735495090485 
model_pd.l_d.mean(): 3.557728632586077e-05 
model_pd.lagr.mean(): 0.055501312017440796 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1678], device='cuda:0')), ('power', tensor([0.1644], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.055465735495090485
epoch£º514	 i:1 	 global-step:10281	 l-p:0.05544798821210861
epoch£º514	 i:2 	 global-step:10282	 l-p:0.05544072017073631
epoch£º514	 i:3 	 global-step:10283	 l-p:0.05546092987060547
epoch£º514	 i:4 	 global-step:10284	 l-p:0.05562806874513626
epoch£º514	 i:5 	 global-step:10285	 l-p:0.05555322393774986
epoch£º514	 i:6 	 global-step:10286	 l-p:0.05550847947597504
epoch£º514	 i:7 	 global-step:10287	 l-p:0.055492155253887177
epoch£º514	 i:8 	 global-step:10288	 l-p:0.05547729507088661
epoch£º514	 i:9 	 global-step:10289	 l-p:0.055449943989515305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4799, 36.0624, 40.0317],
        [28.4799, 35.6479, 39.1579],
        [28.4799, 28.9456, 28.6271],
        [28.4799, 32.8020, 33.6308]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.05556749179959297 
model_pd.l_d.mean(): 0.00011847143468912691 
model_pd.lagr.mean(): 0.05568596348166466 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0976], device='cuda:0')), ('power', tensor([0.3566], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.05556749179959297
epoch£º515	 i:1 	 global-step:10301	 l-p:0.055527981370687485
epoch£º515	 i:2 	 global-step:10302	 l-p:0.055526215583086014
epoch£º515	 i:3 	 global-step:10303	 l-p:0.055450599640607834
epoch£º515	 i:4 	 global-step:10304	 l-p:0.055440112948417664
epoch£º515	 i:5 	 global-step:10305	 l-p:0.05553222447633743
epoch£º515	 i:6 	 global-step:10306	 l-p:0.0555247962474823
epoch£º515	 i:7 	 global-step:10307	 l-p:0.05545393377542496
epoch£º515	 i:8 	 global-step:10308	 l-p:0.05546678602695465
epoch£º515	 i:9 	 global-step:10309	 l-p:0.05545271188020706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4465, 28.5111, 28.4526],
        [28.4465, 29.9127, 29.4033],
        [28.4465, 35.9956, 39.9329],
        [28.4465, 28.5716, 28.4639]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.05549618601799011 
model_pd.l_d.mean(): 0.00011823394743259996 
model_pd.lagr.mean(): 0.0556144192814827 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1297], device='cuda:0')), ('power', tensor([0.2666], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.05549618601799011
epoch£º516	 i:1 	 global-step:10321	 l-p:0.05547420307993889
epoch£º516	 i:2 	 global-step:10322	 l-p:0.05547270551323891
epoch£º516	 i:3 	 global-step:10323	 l-p:0.05550408363342285
epoch£º516	 i:4 	 global-step:10324	 l-p:0.0554862879216671
epoch£º516	 i:5 	 global-step:10325	 l-p:0.05554203316569328
epoch£º516	 i:6 	 global-step:10326	 l-p:0.05550655350089073
epoch£º516	 i:7 	 global-step:10327	 l-p:0.055475275963544846
epoch£º516	 i:8 	 global-step:10328	 l-p:0.05547190085053444
epoch£º516	 i:9 	 global-step:10329	 l-p:0.055621437728405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3706, 28.4204, 28.3746],
        [28.3706, 31.3567, 31.3219],
        [28.3706, 31.0537, 30.8716],
        [28.3706, 28.3708, 28.3706]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.055517230182886124 
model_pd.l_d.mean(): 6.071108873584308e-05 
model_pd.lagr.mean(): 0.05557794123888016 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1521], device='cuda:0')), ('power', tensor([0.1149], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.055517230182886124
epoch£º517	 i:1 	 global-step:10341	 l-p:0.055492743849754333
epoch£º517	 i:2 	 global-step:10342	 l-p:0.055499810725450516
epoch£º517	 i:3 	 global-step:10343	 l-p:0.05547685548663139
epoch£º517	 i:4 	 global-step:10344	 l-p:0.05547872930765152
epoch£º517	 i:5 	 global-step:10345	 l-p:0.055505990982055664
epoch£º517	 i:6 	 global-step:10346	 l-p:0.05555710196495056
epoch£º517	 i:7 	 global-step:10347	 l-p:0.05550770461559296
epoch£º517	 i:8 	 global-step:10348	 l-p:0.05561394616961479
epoch£º517	 i:9 	 global-step:10349	 l-p:0.0555875301361084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[28.2610, 34.4928, 37.0533],
        [28.2610, 32.0442, 32.5003],
        [28.2610, 29.0948, 28.6448],
        [28.2610, 30.1303, 29.6797]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.05551150068640709 
model_pd.l_d.mean(): 1.4790224668104202e-05 
model_pd.lagr.mean(): 0.05552629008889198 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1451], device='cuda:0')), ('power', tensor([0.0261], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.05551150068640709
epoch£º518	 i:1 	 global-step:10361	 l-p:0.05556539446115494
epoch£º518	 i:2 	 global-step:10362	 l-p:0.05554753169417381
epoch£º518	 i:3 	 global-step:10363	 l-p:0.0556047223508358
epoch£º518	 i:4 	 global-step:10364	 l-p:0.055515553802251816
epoch£º518	 i:5 	 global-step:10365	 l-p:0.05551743507385254
epoch£º518	 i:6 	 global-step:10366	 l-p:0.05554027855396271
epoch£º518	 i:7 	 global-step:10367	 l-p:0.05556590482592583
epoch£º518	 i:8 	 global-step:10368	 l-p:0.055525969713926315
epoch£º518	 i:9 	 global-step:10369	 l-p:0.05558782443404198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1315, 28.1499, 28.1323],
        [28.1315, 38.0225, 44.8826],
        [28.1315, 28.1516, 28.1324],
        [28.1315, 28.7766, 28.3845]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.055532798171043396 
model_pd.l_d.mean(): -8.362542575923726e-05 
model_pd.lagr.mean(): 0.05544917285442352 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1623], device='cuda:0')), ('power', tensor([-0.1530], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.055532798171043396
epoch£º519	 i:1 	 global-step:10381	 l-p:0.055514056235551834
epoch£º519	 i:2 	 global-step:10382	 l-p:0.055558886379003525
epoch£º519	 i:3 	 global-step:10383	 l-p:0.055525150150060654
epoch£º519	 i:4 	 global-step:10384	 l-p:0.055545624345541
epoch£º519	 i:5 	 global-step:10385	 l-p:0.05579841881990433
epoch£º519	 i:6 	 global-step:10386	 l-p:0.05553784221410751
epoch£º519	 i:7 	 global-step:10387	 l-p:0.05558926612138748
epoch£º519	 i:8 	 global-step:10388	 l-p:0.05558835715055466
epoch£º519	 i:9 	 global-step:10389	 l-p:0.055563583970069885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0022, 28.0021, 28.0022],
        [28.0022, 37.0186, 42.7841],
        [28.0022, 28.0022, 28.0021],
        [28.0022, 28.0513, 28.0061]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.05564159154891968 
model_pd.l_d.mean(): -0.00010709952766774222 
model_pd.lagr.mean(): 0.055534493178129196 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1356], device='cuda:0')), ('power', tensor([-0.2326], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.05564159154891968
epoch£º520	 i:1 	 global-step:10401	 l-p:0.05565883591771126
epoch£º520	 i:2 	 global-step:10402	 l-p:0.05556078255176544
epoch£º520	 i:3 	 global-step:10403	 l-p:0.05559856817126274
epoch£º520	 i:4 	 global-step:10404	 l-p:0.05565401539206505
epoch£º520	 i:5 	 global-step:10405	 l-p:0.05555911734700203
epoch£º520	 i:6 	 global-step:10406	 l-p:0.05561831220984459
epoch£º520	 i:7 	 global-step:10407	 l-p:0.05556587502360344
epoch£º520	 i:8 	 global-step:10408	 l-p:0.055578697472810745
epoch£º520	 i:9 	 global-step:10409	 l-p:0.05556824430823326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8963, 30.1084, 29.7638],
        [27.8963, 28.1032, 27.9361],
        [27.8963, 37.4640, 43.9548],
        [27.8963, 29.6067, 29.1374]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.05556539446115494 
model_pd.l_d.mean(): -0.00013223804126027972 
model_pd.lagr.mean(): 0.055433157831430435 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1773], device='cuda:0')), ('power', tensor([-0.4205], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.05556539446115494
epoch£º521	 i:1 	 global-step:10421	 l-p:0.05562139302492142
epoch£º521	 i:2 	 global-step:10422	 l-p:0.055576346814632416
epoch£º521	 i:3 	 global-step:10423	 l-p:0.05560779571533203
epoch£º521	 i:4 	 global-step:10424	 l-p:0.055610571056604385
epoch£º521	 i:5 	 global-step:10425	 l-p:0.05571358650922775
epoch£º521	 i:6 	 global-step:10426	 l-p:0.05564011633396149
epoch£º521	 i:7 	 global-step:10427	 l-p:0.0556766651570797
epoch£º521	 i:8 	 global-step:10428	 l-p:0.055576130747795105
epoch£º521	 i:9 	 global-step:10429	 l-p:0.05559904873371124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8374, 27.8604, 27.8386],
        [27.8374, 27.8375, 27.8374],
        [27.8374, 27.8932, 27.8422],
        [27.8374, 27.9005, 27.8433]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.055577199906110764 
model_pd.l_d.mean(): -5.549719571718015e-05 
model_pd.lagr.mean(): 0.055521704256534576 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1682], device='cuda:0')), ('power', tensor([-0.4429], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.055577199906110764
epoch£º522	 i:1 	 global-step:10441	 l-p:0.055655162781476974
epoch£º522	 i:2 	 global-step:10442	 l-p:0.05559006705880165
epoch£º522	 i:3 	 global-step:10443	 l-p:0.05562521889805794
epoch£º522	 i:4 	 global-step:10444	 l-p:0.055609796196222305
epoch£º522	 i:5 	 global-step:10445	 l-p:0.05567184463143349
epoch£º522	 i:6 	 global-step:10446	 l-p:0.055605895817279816
epoch£º522	 i:7 	 global-step:10447	 l-p:0.05565372854471207
epoch£º522	 i:8 	 global-step:10448	 l-p:0.055666483938694
epoch£º522	 i:9 	 global-step:10449	 l-p:0.05559617280960083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8430, 27.9688, 27.8608],
        [27.8430, 27.8430, 27.8430],
        [27.8430, 37.3915, 43.8692],
        [27.8430, 27.8441, 27.8430]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.05575329810380936 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05575329810380936 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0123], device='cuda:0')), ('power', tensor([-0.2257], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.05575329810380936
epoch£º523	 i:1 	 global-step:10461	 l-p:0.05558276176452637
epoch£º523	 i:2 	 global-step:10462	 l-p:0.05569136142730713
epoch£º523	 i:3 	 global-step:10463	 l-p:0.055582210421562195
epoch£º523	 i:4 	 global-step:10464	 l-p:0.05557459592819214
epoch£º523	 i:5 	 global-step:10465	 l-p:0.05559053272008896
epoch£º523	 i:6 	 global-step:10466	 l-p:0.05558650940656662
epoch£º523	 i:7 	 global-step:10467	 l-p:0.05558770149946213
epoch£º523	 i:8 	 global-step:10468	 l-p:0.05557442456483841
epoch£º523	 i:9 	 global-step:10469	 l-p:0.055662479251623154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8984, 27.9722, 27.9060],
        [27.8984, 35.2948, 39.1517],
        [27.8984, 27.9002, 27.8985],
        [27.8984, 27.9682, 27.9053]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.05559144914150238 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05559144914150238 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1414], device='cuda:0')), ('power', tensor([-0.3508], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.05559144914150238
epoch£º524	 i:1 	 global-step:10481	 l-p:0.05556720122694969
epoch£º524	 i:2 	 global-step:10482	 l-p:0.055563896894454956
epoch£º524	 i:3 	 global-step:10483	 l-p:0.05557437613606453
epoch£º524	 i:4 	 global-step:10484	 l-p:0.05561608821153641
epoch£º524	 i:5 	 global-step:10485	 l-p:0.05569076910614967
epoch£º524	 i:6 	 global-step:10486	 l-p:0.05571720004081726
epoch£º524	 i:7 	 global-step:10487	 l-p:0.05558186024427414
epoch£º524	 i:8 	 global-step:10488	 l-p:0.05558580160140991
epoch£º524	 i:9 	 global-step:10489	 l-p:0.05555940419435501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9727, 27.9871, 27.9733],
        [27.9727, 27.9729, 27.9727],
        [27.9727, 27.9925, 27.9736],
        [27.9727, 29.9169, 29.4926]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.0556228905916214 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0556228905916214 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1208], device='cuda:0')), ('power', tensor([-0.2305], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.0556228905916214
epoch£º525	 i:1 	 global-step:10501	 l-p:0.055542439222335815
epoch£º525	 i:2 	 global-step:10502	 l-p:0.05557893589138985
epoch£º525	 i:3 	 global-step:10503	 l-p:0.055586185306310654
epoch£º525	 i:4 	 global-step:10504	 l-p:0.05562547966837883
epoch£º525	 i:5 	 global-step:10505	 l-p:0.05558347702026367
epoch£º525	 i:6 	 global-step:10506	 l-p:0.055540360510349274
epoch£º525	 i:7 	 global-step:10507	 l-p:0.055666547268629074
epoch£º525	 i:8 	 global-step:10508	 l-p:0.055582914501428604
epoch£º525	 i:9 	 global-step:10509	 l-p:0.055555857717990875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0540, 28.1268, 28.0614],
        [28.0540, 29.4841, 28.9814],
        [28.0540, 33.6959, 35.7073],
        [28.0540, 34.4045, 37.1142]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.05553547292947769 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05553547292947769 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1696], device='cuda:0')), ('power', tensor([-0.2033], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.05553547292947769
epoch£º526	 i:1 	 global-step:10521	 l-p:0.055546920746564865
epoch£º526	 i:2 	 global-step:10522	 l-p:0.05562792718410492
epoch£º526	 i:3 	 global-step:10523	 l-p:0.055700913071632385
epoch£º526	 i:4 	 global-step:10524	 l-p:0.055530138313770294
epoch£º526	 i:5 	 global-step:10525	 l-p:0.055615052580833435
epoch£º526	 i:6 	 global-step:10526	 l-p:0.055528149008750916
epoch£º526	 i:7 	 global-step:10527	 l-p:0.05551700294017792
epoch£º526	 i:8 	 global-step:10528	 l-p:0.055579546838998795
epoch£º526	 i:9 	 global-step:10529	 l-p:0.05553235858678818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1380, 28.1380, 28.1380],
        [28.1380, 30.9540, 30.8466],
        [28.1380, 28.1380, 28.1380],
        [28.1380, 28.1418, 28.1380]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.05552805960178375 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05552805960178375 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1690], device='cuda:0')), ('power', tensor([-0.1608], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.05552805960178375
epoch£º527	 i:1 	 global-step:10541	 l-p:0.055629391223192215
epoch£º527	 i:2 	 global-step:10542	 l-p:0.05550548434257507
epoch£º527	 i:3 	 global-step:10543	 l-p:0.05551948398351669
epoch£º527	 i:4 	 global-step:10544	 l-p:0.05562080442905426
epoch£º527	 i:5 	 global-step:10545	 l-p:0.055575743317604065
epoch£º527	 i:6 	 global-step:10546	 l-p:0.05551067739725113
epoch£º527	 i:7 	 global-step:10547	 l-p:0.055525779724121094
epoch£º527	 i:8 	 global-step:10548	 l-p:0.055619802325963974
epoch£º527	 i:9 	 global-step:10549	 l-p:0.05550355464220047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2229, 28.5794, 28.3186],
        [28.2229, 28.2654, 28.2260],
        [28.2229, 35.9889, 40.2104],
        [28.2229, 35.9901, 40.2130]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): 0.05555929243564606 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555929243564606 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.6526e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1122], device='cuda:0')), ('power', tensor([0.0331], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:0.05555929243564606
epoch£º528	 i:1 	 global-step:10561	 l-p:0.055530890822410583
epoch£º528	 i:2 	 global-step:10562	 l-p:0.05550948157906532
epoch£º528	 i:3 	 global-step:10563	 l-p:0.0555536113679409
epoch£º528	 i:4 	 global-step:10564	 l-p:0.055604238063097
epoch£º528	 i:5 	 global-step:10565	 l-p:0.05554661527276039
epoch£º528	 i:6 	 global-step:10566	 l-p:0.055525634437799454
epoch£º528	 i:7 	 global-step:10567	 l-p:0.05550488457083702
epoch£º528	 i:8 	 global-step:10568	 l-p:0.055552899837493896
epoch£º528	 i:9 	 global-step:10569	 l-p:0.055476754903793335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[28.3078, 31.3522, 31.3520],
        [28.3078, 32.7379, 33.6666],
        [28.3078, 29.0179, 28.6027],
        [28.3078, 35.4672, 38.9949]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.055483561009168625 
model_pd.l_d.mean(): 4.025489488412859e-07 
model_pd.lagr.mean(): 0.05548396334052086 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.2515e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1821], device='cuda:0')), ('power', tensor([0.0379], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.055483561009168625
epoch£º529	 i:1 	 global-step:10581	 l-p:0.055625952780246735
epoch£º529	 i:2 	 global-step:10582	 l-p:0.05552750825881958
epoch£º529	 i:3 	 global-step:10583	 l-p:0.055501632392406464
epoch£º529	 i:4 	 global-step:10584	 l-p:0.05552859976887703
epoch£º529	 i:5 	 global-step:10585	 l-p:0.05549140274524689
epoch£º529	 i:6 	 global-step:10586	 l-p:0.055504102259874344
epoch£º529	 i:7 	 global-step:10587	 l-p:0.055476244539022446
epoch£º529	 i:8 	 global-step:10588	 l-p:0.055563896894454956
epoch£º529	 i:9 	 global-step:10589	 l-p:0.05549097806215286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3882, 28.3882, 28.3882],
        [28.3882, 30.6418, 30.2910],
        [28.3882, 28.3882, 28.3882],
        [28.3882, 34.5845, 37.0920]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.055488500744104385 
model_pd.l_d.mean(): 8.599428838351741e-06 
model_pd.lagr.mean(): 0.055497098714113235 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.7335e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1575], device='cuda:0')), ('power', tensor([0.1429], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.055488500744104385
epoch£º530	 i:1 	 global-step:10601	 l-p:0.055487409234046936
epoch£º530	 i:2 	 global-step:10602	 l-p:0.05552951619029045
epoch£º530	 i:3 	 global-step:10603	 l-p:0.05549108237028122
epoch£º530	 i:4 	 global-step:10604	 l-p:0.05545487254858017
epoch£º530	 i:5 	 global-step:10605	 l-p:0.05550508201122284
epoch£º530	 i:6 	 global-step:10606	 l-p:0.055516455322504044
epoch£º530	 i:7 	 global-step:10607	 l-p:0.05550730600953102
epoch£º530	 i:8 	 global-step:10608	 l-p:0.05558881163597107
epoch£º530	 i:9 	 global-step:10609	 l-p:0.05547337234020233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4535, 28.4860, 28.4555],
        [28.4535, 28.4535, 28.4535],
        [28.4535, 28.4535, 28.4535],
        [28.4535, 28.4535, 28.4535]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.05555756017565727 
model_pd.l_d.mean(): 3.3864092984003946e-05 
model_pd.lagr.mean(): 0.05559142306447029 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1452], device='cuda:0')), ('power', tensor([0.2306], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.05555756017565727
epoch£º531	 i:1 	 global-step:10621	 l-p:0.055470988154411316
epoch£º531	 i:2 	 global-step:10622	 l-p:0.055535655468702316
epoch£º531	 i:3 	 global-step:10623	 l-p:0.055475395172834396
epoch£º531	 i:4 	 global-step:10624	 l-p:0.05556167662143707
epoch£º531	 i:5 	 global-step:10625	 l-p:0.055458515882492065
epoch£º531	 i:6 	 global-step:10626	 l-p:0.05545632168650627
epoch£º531	 i:7 	 global-step:10627	 l-p:0.05544606223702431
epoch£º531	 i:8 	 global-step:10628	 l-p:0.055480439215898514
epoch£º531	 i:9 	 global-step:10629	 l-p:0.055492501705884933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4889, 33.3913, 34.6882],
        [28.4889, 28.8661, 28.5932],
        [28.4889, 34.1497, 36.1252],
        [28.4889, 28.5065, 28.4896]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.0555463582277298 
model_pd.l_d.mean(): 7.992469181772321e-05 
model_pd.lagr.mean(): 0.0556262843310833 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1408], device='cuda:0')), ('power', tensor([0.3073], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:0.0555463582277298
epoch£º532	 i:1 	 global-step:10641	 l-p:0.05559882894158363
epoch£º532	 i:2 	 global-step:10642	 l-p:0.05544429272413254
epoch£º532	 i:3 	 global-step:10643	 l-p:0.05544684827327728
epoch£º532	 i:4 	 global-step:10644	 l-p:0.05544836074113846
epoch£º532	 i:5 	 global-step:10645	 l-p:0.05549049377441406
epoch£º532	 i:6 	 global-step:10646	 l-p:0.05553651601076126
epoch£º532	 i:7 	 global-step:10647	 l-p:0.05546381697058678
epoch£º532	 i:8 	 global-step:10648	 l-p:0.05547064542770386
epoch£º532	 i:9 	 global-step:10649	 l-p:0.055455513298511505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4809, 36.0395, 39.9818],
        [28.4809, 28.5013, 28.4818],
        [28.4809, 28.6599, 28.5119],
        [28.4809, 30.3586, 29.9028]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.05552158132195473 
model_pd.l_d.mean(): 9.915939153870568e-05 
model_pd.lagr.mean(): 0.055620741099119186 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1614], device='cuda:0')), ('power', tensor([0.2599], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.05552158132195473
epoch£º533	 i:1 	 global-step:10661	 l-p:0.05546243116259575
epoch£º533	 i:2 	 global-step:10662	 l-p:0.055436257272958755
epoch£º533	 i:3 	 global-step:10663	 l-p:0.055460505187511444
epoch£º533	 i:4 	 global-step:10664	 l-p:0.055554721504449844
epoch£º533	 i:5 	 global-step:10665	 l-p:0.05548867583274841
epoch£º533	 i:6 	 global-step:10666	 l-p:0.055592384189367294
epoch£º533	 i:7 	 global-step:10667	 l-p:0.055507857352495193
epoch£º533	 i:8 	 global-step:10668	 l-p:0.05547439306974411
epoch£º533	 i:9 	 global-step:10669	 l-p:0.05546233430504799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4251, 30.1890, 29.7134],
        [28.4251, 32.0976, 32.4666],
        [28.4251, 28.4775, 28.4294],
        [28.4251, 30.7775, 30.4592]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.05550003424286842 
model_pd.l_d.mean(): 0.00011455571075202897 
model_pd.lagr.mean(): 0.055614590644836426 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1254], device='cuda:0')), ('power', tensor([0.2346], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.05550003424286842
epoch£º534	 i:1 	 global-step:10681	 l-p:0.055466458201408386
epoch£º534	 i:2 	 global-step:10682	 l-p:0.05553349480032921
epoch£º534	 i:3 	 global-step:10683	 l-p:0.05545826628804207
epoch£º534	 i:4 	 global-step:10684	 l-p:0.05555107817053795
epoch£º534	 i:5 	 global-step:10685	 l-p:0.05548384040594101
epoch£º534	 i:6 	 global-step:10686	 l-p:0.05551256984472275
epoch£º534	 i:7 	 global-step:10687	 l-p:0.055586542934179306
epoch£º534	 i:8 	 global-step:10688	 l-p:0.05546881631016731
epoch£º534	 i:9 	 global-step:10689	 l-p:0.05555526912212372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3242, 28.3255, 28.3242],
        [28.3242, 28.4675, 28.3459],
        [28.3242, 28.3244, 28.3242],
        [28.3242, 28.6716, 28.4157]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.05550450086593628 
model_pd.l_d.mean(): 9.759367458173074e-06 
model_pd.lagr.mean(): 0.05551426112651825 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1555], device='cuda:0')), ('power', tensor([0.0175], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:0.05550450086593628
epoch£º535	 i:1 	 global-step:10701	 l-p:0.05549849569797516
epoch£º535	 i:2 	 global-step:10702	 l-p:0.05561712011694908
epoch£º535	 i:3 	 global-step:10703	 l-p:0.055482491850852966
epoch£º535	 i:4 	 global-step:10704	 l-p:0.05547608435153961
epoch£º535	 i:5 	 global-step:10705	 l-p:0.05557073652744293
epoch£º535	 i:6 	 global-step:10706	 l-p:0.0555262565612793
epoch£º535	 i:7 	 global-step:10707	 l-p:0.05557670071721077
epoch£º535	 i:8 	 global-step:10708	 l-p:0.05557284131646156
epoch£º535	 i:9 	 global-step:10709	 l-p:0.05552906543016434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1912, 28.2617, 28.1981],
        [28.1912, 28.2245, 28.1933],
        [28.1912, 28.2551, 28.1972],
        [28.1912, 34.3228, 36.7925]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.055591318756341934 
model_pd.l_d.mean(): 9.810082701733336e-06 
model_pd.lagr.mean(): 0.055601127445697784 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1236], device='cuda:0')), ('power', tensor([0.0173], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.055591318756341934
epoch£º536	 i:1 	 global-step:10721	 l-p:0.055530745536088943
epoch£º536	 i:2 	 global-step:10722	 l-p:0.055517978966236115
epoch£º536	 i:3 	 global-step:10723	 l-p:0.0555109828710556
epoch£º536	 i:4 	 global-step:10724	 l-p:0.055557798594236374
epoch£º536	 i:5 	 global-step:10725	 l-p:0.05559903010725975
epoch£º536	 i:6 	 global-step:10726	 l-p:0.05554606392979622
epoch£º536	 i:7 	 global-step:10727	 l-p:0.055678997188806534
epoch£º536	 i:8 	 global-step:10728	 l-p:0.05553186312317848
epoch£º536	 i:9 	 global-step:10729	 l-p:0.055577658116817474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0467, 28.0470, 28.0467],
        [28.0467, 30.0400, 29.6253],
        [28.0467, 29.9091, 29.4637],
        [28.0467, 28.0467, 28.0467]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.055529139935970306 
model_pd.l_d.mean(): -0.00016571629384998232 
model_pd.lagr.mean(): 0.05536342412233353 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1890], device='cuda:0')), ('power', tensor([-0.3262], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.055529139935970306
epoch£º537	 i:1 	 global-step:10741	 l-p:0.055608805269002914
epoch£º537	 i:2 	 global-step:10742	 l-p:0.055601052939891815
epoch£º537	 i:3 	 global-step:10743	 l-p:0.05557052418589592
epoch£º537	 i:4 	 global-step:10744	 l-p:0.055584151297807693
epoch£º537	 i:5 	 global-step:10745	 l-p:0.055640190839767456
epoch£º537	 i:6 	 global-step:10746	 l-p:0.05558745190501213
epoch£º537	 i:7 	 global-step:10747	 l-p:0.055574554949998856
epoch£º537	 i:8 	 global-step:10748	 l-p:0.05566020682454109
epoch£º537	 i:9 	 global-step:10749	 l-p:0.05557671934366226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9176, 28.1574, 27.9681],
        [27.9176, 27.9206, 27.9176],
        [27.9176, 27.9178, 27.9176],
        [27.9176, 28.3396, 28.0445]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.055564235895872116 
model_pd.l_d.mean(): -0.00014648001524619758 
model_pd.lagr.mean(): 0.055417757481336594 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1675], device='cuda:0')), ('power', tensor([-0.3863], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.055564235895872116
epoch£º538	 i:1 	 global-step:10761	 l-p:0.0555805005133152
epoch£º538	 i:2 	 global-step:10762	 l-p:0.055565398186445236
epoch£º538	 i:3 	 global-step:10763	 l-p:0.05559000372886658
epoch£º538	 i:4 	 global-step:10764	 l-p:0.05565357208251953
epoch£º538	 i:5 	 global-step:10765	 l-p:0.055601466447114944
epoch£º538	 i:6 	 global-step:10766	 l-p:0.055592168122529984
epoch£º538	 i:7 	 global-step:10767	 l-p:0.05557212233543396
epoch£º538	 i:8 	 global-step:10768	 l-p:0.05577320605516434
epoch£º538	 i:9 	 global-step:10769	 l-p:0.055675607174634933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8319, 28.6455, 28.2044],
        [27.8319, 27.8320, 27.8319],
        [27.8319, 27.9171, 27.8414],
        [27.8319, 27.8323, 27.8319]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.055588074028491974 
model_pd.l_d.mean(): -8.608099597040564e-05 
model_pd.lagr.mean(): 0.055501993745565414 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1430], device='cuda:0')), ('power', tensor([-0.4424], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.055588074028491974
epoch£º539	 i:1 	 global-step:10781	 l-p:0.0555848591029644
epoch£º539	 i:2 	 global-step:10782	 l-p:0.05561047047376633
epoch£º539	 i:3 	 global-step:10783	 l-p:0.05558445677161217
epoch£º539	 i:4 	 global-step:10784	 l-p:0.05569763109087944
epoch£º539	 i:5 	 global-step:10785	 l-p:0.055631160736083984
epoch£º539	 i:6 	 global-step:10786	 l-p:0.05561019852757454
epoch£º539	 i:7 	 global-step:10787	 l-p:0.055709484964609146
epoch£º539	 i:8 	 global-step:10788	 l-p:0.05563981458544731
epoch£º539	 i:9 	 global-step:10789	 l-p:0.055630967020988464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8140, 31.0238, 31.1477],
        [27.8140, 27.8140, 27.8140],
        [27.8140, 29.0188, 28.5207],
        [27.8140, 33.8783, 36.3319]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.055673856288194656 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055673856288194656 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0485], device='cuda:0')), ('power', tensor([-0.3157], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.055673856288194656
epoch£º540	 i:1 	 global-step:10801	 l-p:0.055607233196496964
epoch£º540	 i:2 	 global-step:10802	 l-p:0.0556822195649147
epoch£º540	 i:3 	 global-step:10803	 l-p:0.05558372288942337
epoch£º540	 i:4 	 global-step:10804	 l-p:0.055596303194761276
epoch£º540	 i:5 	 global-step:10805	 l-p:0.05557190626859665
epoch£º540	 i:6 	 global-step:10806	 l-p:0.05566493049263954
epoch£º540	 i:7 	 global-step:10807	 l-p:0.0556534081697464
epoch£º540	 i:8 	 global-step:10808	 l-p:0.055644311010837555
epoch£º540	 i:9 	 global-step:10809	 l-p:0.05557866394519806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8621, 28.1305, 27.9228],
        [27.8621, 28.0686, 27.9018],
        [27.8621, 36.8570, 42.6242],
        [27.8621, 27.8620, 27.8620]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.05569598078727722 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05569598078727722 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0936], device='cuda:0')), ('power', tensor([-0.3672], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.05569598078727722
epoch£º541	 i:1 	 global-step:10821	 l-p:0.0555957555770874
epoch£º541	 i:2 	 global-step:10822	 l-p:0.05556569620966911
epoch£º541	 i:3 	 global-step:10823	 l-p:0.05573597177863121
epoch£º541	 i:4 	 global-step:10824	 l-p:0.05557595193386078
epoch£º541	 i:5 	 global-step:10825	 l-p:0.055598895996809006
epoch£º541	 i:6 	 global-step:10826	 l-p:0.05560071021318436
epoch£º541	 i:7 	 global-step:10827	 l-p:0.055621612817049026
epoch£º541	 i:8 	 global-step:10828	 l-p:0.05556409806013107
epoch£º541	 i:9 	 global-step:10829	 l-p:0.0555688850581646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9386, 27.9387, 27.9386],
        [27.9386, 30.8068, 30.7365],
        [27.9386, 38.2326, 45.6741],
        [27.9386, 31.5446, 31.9066]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.05572417005896568 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05572417005896568 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0531], device='cuda:0')), ('power', tensor([-0.2105], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.05572417005896568
epoch£º542	 i:1 	 global-step:10841	 l-p:0.05556148290634155
epoch£º542	 i:2 	 global-step:10842	 l-p:0.05560721829533577
epoch£º542	 i:3 	 global-step:10843	 l-p:0.05557436868548393
epoch£º542	 i:4 	 global-step:10844	 l-p:0.0556616485118866
epoch£º542	 i:5 	 global-step:10845	 l-p:0.05555734410881996
epoch£º542	 i:6 	 global-step:10846	 l-p:0.05553446710109711
epoch£º542	 i:7 	 global-step:10847	 l-p:0.05559207871556282
epoch£º542	 i:8 	 global-step:10848	 l-p:0.055573537945747375
epoch£º542	 i:9 	 global-step:10849	 l-p:0.05556545779109001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0254, 31.5357, 31.8290],
        [28.0254, 37.9310, 44.8346],
        [28.0254, 28.6925, 28.2933],
        [28.0254, 28.0333, 28.0257]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.0555826835334301 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0555826835334301 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1315], device='cuda:0')), ('power', tensor([-0.2177], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.0555826835334301
epoch£º543	 i:1 	 global-step:10861	 l-p:0.055540695786476135
epoch£º543	 i:2 	 global-step:10862	 l-p:0.05562952905893326
epoch£º543	 i:3 	 global-step:10863	 l-p:0.055586669594049454
epoch£º543	 i:4 	 global-step:10864	 l-p:0.05563837289810181
epoch£º543	 i:5 	 global-step:10865	 l-p:0.055537156760692596
epoch£º543	 i:6 	 global-step:10866	 l-p:0.05553647503256798
epoch£º543	 i:7 	 global-step:10867	 l-p:0.055533263832330704
epoch£º543	 i:8 	 global-step:10868	 l-p:0.05566532909870148
epoch£º543	 i:9 	 global-step:10869	 l-p:0.055516500025987625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1159, 28.1225, 28.1160],
        [28.1159, 32.0805, 32.6757],
        [28.1159, 28.1159, 28.1159],
        [28.1159, 36.4442, 41.3444]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.05553232133388519 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05553232133388519 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1747], device='cuda:0')), ('power', tensor([-0.1919], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.05553232133388519
epoch£º544	 i:1 	 global-step:10881	 l-p:0.05560429394245148
epoch£º544	 i:2 	 global-step:10882	 l-p:0.05561579763889313
epoch£º544	 i:3 	 global-step:10883	 l-p:0.05556432157754898
epoch£º544	 i:4 	 global-step:10884	 l-p:0.05551735311746597
epoch£º544	 i:5 	 global-step:10885	 l-p:0.05550244823098183
epoch£º544	 i:6 	 global-step:10886	 l-p:0.0555507093667984
epoch£º544	 i:7 	 global-step:10887	 l-p:0.05561394616961479
epoch£º544	 i:8 	 global-step:10888	 l-p:0.05555953457951546
epoch£º544	 i:9 	 global-step:10889	 l-p:0.05551714450120926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2079, 28.2830, 28.2156],
        [28.2079, 28.5541, 28.2992],
        [28.2079, 29.6610, 29.1561],
        [28.2079, 29.5988, 29.0915]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.055499136447906494 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055499136447906494 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1916], device='cuda:0')), ('power', tensor([-0.0789], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.055499136447906494
epoch£º545	 i:1 	 global-step:10901	 l-p:0.05556868389248848
epoch£º545	 i:2 	 global-step:10902	 l-p:0.055747032165527344
epoch£º545	 i:3 	 global-step:10903	 l-p:0.055527158081531525
epoch£º545	 i:4 	 global-step:10904	 l-p:0.055485013872385025
epoch£º545	 i:5 	 global-step:10905	 l-p:0.05552050471305847
epoch£º545	 i:6 	 global-step:10906	 l-p:0.055510733276605606
epoch£º545	 i:7 	 global-step:10907	 l-p:0.055500276386737823
epoch£º545	 i:8 	 global-step:10908	 l-p:0.05552114546298981
epoch£º545	 i:9 	 global-step:10909	 l-p:0.05550884082913399
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3003, 28.4872, 28.3337],
        [28.3003, 28.4781, 28.3311],
        [28.3003, 28.3011, 28.3002],
        [28.3003, 31.0850, 30.9537]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.05556783452630043 
model_pd.l_d.mean(): 4.718563317851476e-08 
model_pd.lagr.mean(): 0.05556788295507431 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.5418e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1794], device='cuda:0')), ('power', tensor([0.0077], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.05556783452630043
epoch£º546	 i:1 	 global-step:10921	 l-p:0.05548595264554024
epoch£º546	 i:2 	 global-step:10922	 l-p:0.05557392165064812
epoch£º546	 i:3 	 global-step:10923	 l-p:0.055527105927467346
epoch£º546	 i:4 	 global-step:10924	 l-p:0.055545028299093246
epoch£º546	 i:5 	 global-step:10925	 l-p:0.05549311637878418
epoch£º546	 i:6 	 global-step:10926	 l-p:0.05548182874917984
epoch£º546	 i:7 	 global-step:10927	 l-p:0.05546904355287552
epoch£º546	 i:8 	 global-step:10928	 l-p:0.05557055398821831
epoch£º546	 i:9 	 global-step:10929	 l-p:0.05548685044050217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3900, 29.3687, 28.8871],
        [28.3900, 37.7142, 43.7866],
        [28.3900, 28.3959, 28.3901],
        [28.3900, 28.3900, 28.3900]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.05547646805644035 
model_pd.l_d.mean(): 5.686366876034299e-06 
model_pd.lagr.mean(): 0.055482152849435806 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.9226e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1739], device='cuda:0')), ('power', tensor([0.1054], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.05547646805644035
epoch£º547	 i:1 	 global-step:10941	 l-p:0.05555276572704315
epoch£º547	 i:2 	 global-step:10942	 l-p:0.055552709847688675
epoch£º547	 i:3 	 global-step:10943	 l-p:0.05558134615421295
epoch£º547	 i:4 	 global-step:10944	 l-p:0.055484965443611145
epoch£º547	 i:5 	 global-step:10945	 l-p:0.05548645928502083
epoch£º547	 i:6 	 global-step:10946	 l-p:0.05550902336835861
epoch£º547	 i:7 	 global-step:10947	 l-p:0.055456165224313736
epoch£º547	 i:8 	 global-step:10948	 l-p:0.05549042671918869
epoch£º547	 i:9 	 global-step:10949	 l-p:0.05544097349047661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4631, 28.4645, 28.4631],
        [28.4631, 29.3032, 28.8498],
        [28.4631, 28.6871, 28.5078],
        [28.4631, 28.8124, 28.5551]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.05554267764091492 
model_pd.l_d.mean(): 3.8566005969187245e-05 
model_pd.lagr.mean(): 0.055581241846084595 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1144], device='cuda:0')), ('power', tensor([0.2688], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.05554267764091492
epoch£º548	 i:1 	 global-step:10961	 l-p:0.05544402822852135
epoch£º548	 i:2 	 global-step:10962	 l-p:0.055476102977991104
epoch£º548	 i:3 	 global-step:10963	 l-p:0.05547598749399185
epoch£º548	 i:4 	 global-step:10964	 l-p:0.055484361946582794
epoch£º548	 i:5 	 global-step:10965	 l-p:0.05550720915198326
epoch£º548	 i:6 	 global-step:10966	 l-p:0.055609967559576035
epoch£º548	 i:7 	 global-step:10967	 l-p:0.05544916167855263
epoch£º548	 i:8 	 global-step:10968	 l-p:0.0554625503718853
epoch£º548	 i:9 	 global-step:10969	 l-p:0.055459871888160706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5023, 36.8167, 41.6282],
        [28.5023, 28.5333, 28.5042],
        [28.5023, 28.5518, 28.5063],
        [28.5023, 30.3562, 29.8949]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.055454377084970474 
model_pd.l_d.mean(): 4.997302676201798e-05 
model_pd.lagr.mean(): 0.05550435185432434 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1887], device='cuda:0')), ('power', tensor([0.1904], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.055454377084970474
epoch£º549	 i:1 	 global-step:10981	 l-p:0.05552104860544205
epoch£º549	 i:2 	 global-step:10982	 l-p:0.05546130985021591
epoch£º549	 i:3 	 global-step:10983	 l-p:0.055443983525037766
epoch£º549	 i:4 	 global-step:10984	 l-p:0.05548432096838951
epoch£º549	 i:5 	 global-step:10985	 l-p:0.055543720722198486
epoch£º549	 i:6 	 global-step:10986	 l-p:0.055457066744565964
epoch£º549	 i:7 	 global-step:10987	 l-p:0.055427756160497665
epoch£º549	 i:8 	 global-step:10988	 l-p:0.0554511584341526
epoch£º549	 i:9 	 global-step:10989	 l-p:0.05562928691506386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4941, 28.7312, 28.5430],
        [28.4941, 31.4451, 31.3851],
        [28.4941, 30.7445, 30.3883],
        [28.4941, 28.9188, 28.6206]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.05560735985636711 
model_pd.l_d.mean(): 0.000148709790664725 
model_pd.lagr.mean(): 0.05575606971979141 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0826], device='cuda:0')), ('power', tensor([0.3806], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.05560735985636711
epoch£º550	 i:1 	 global-step:11001	 l-p:0.05548154562711716
epoch£º550	 i:2 	 global-step:11002	 l-p:0.055571045726537704
epoch£º550	 i:3 	 global-step:11003	 l-p:0.055454619228839874
epoch£º550	 i:4 	 global-step:11004	 l-p:0.055487409234046936
epoch£º550	 i:5 	 global-step:11005	 l-p:0.05546888709068298
epoch£º550	 i:6 	 global-step:11006	 l-p:0.055479586124420166
epoch£º550	 i:7 	 global-step:11007	 l-p:0.05544577166438103
epoch£º550	 i:8 	 global-step:11008	 l-p:0.055445391684770584
epoch£º550	 i:9 	 global-step:11009	 l-p:0.05549795553088188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4301, 29.5766, 29.0727],
        [28.4301, 34.6359, 37.1474],
        [28.4301, 38.4310, 45.3679],
        [28.4301, 28.4301, 28.4301]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.05564601719379425 
model_pd.l_d.mean(): 0.00015668068954255432 
model_pd.lagr.mean(): 0.05580269917845726 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0559], device='cuda:0')), ('power', tensor([0.3117], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.05564601719379425
epoch£º551	 i:1 	 global-step:11021	 l-p:0.055534422397613525
epoch£º551	 i:2 	 global-step:11022	 l-p:0.0554591566324234
epoch£º551	 i:3 	 global-step:11023	 l-p:0.055486030876636505
epoch£º551	 i:4 	 global-step:11024	 l-p:0.0555279403924942
epoch£º551	 i:5 	 global-step:11025	 l-p:0.05545881763100624
epoch£º551	 i:6 	 global-step:11026	 l-p:0.05549567937850952
epoch£º551	 i:7 	 global-step:11027	 l-p:0.05548306182026863
epoch£º551	 i:8 	 global-step:11028	 l-p:0.05555122345685959
epoch£º551	 i:9 	 global-step:11029	 l-p:0.05547596514225006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3138, 31.2936, 31.2589],
        [28.3138, 28.3140, 28.3138],
        [28.3138, 28.3139, 28.3138],
        [28.3138, 28.3138, 28.3138]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.055530790239572525 
model_pd.l_d.mean(): 3.5910106817027554e-05 
model_pd.lagr.mean(): 0.0555667020380497 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1212], device='cuda:0')), ('power', tensor([0.0629], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.055530790239572525
epoch£º552	 i:1 	 global-step:11041	 l-p:0.055569179356098175
epoch£º552	 i:2 	 global-step:11042	 l-p:0.05550996959209442
epoch£º552	 i:3 	 global-step:11043	 l-p:0.05548742040991783
epoch£º552	 i:4 	 global-step:11044	 l-p:0.05548593029379845
epoch£º552	 i:5 	 global-step:11045	 l-p:0.05553237348794937
epoch£º552	 i:6 	 global-step:11046	 l-p:0.05556843802332878
epoch£º552	 i:7 	 global-step:11047	 l-p:0.05556130409240723
epoch£º552	 i:8 	 global-step:11048	 l-p:0.05562330782413483
epoch£º552	 i:9 	 global-step:11049	 l-p:0.05552385374903679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1623, 28.1623, 28.1623],
        [28.1623, 31.1789, 31.1727],
        [28.1623, 37.8013, 44.3260],
        [28.1623, 28.4853, 28.2438]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.05553698539733887 
model_pd.l_d.mean(): -2.11085189221194e-05 
model_pd.lagr.mean(): 0.05551587790250778 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1328], device='cuda:0')), ('power', tensor([-0.0369], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.05553698539733887
epoch£º553	 i:1 	 global-step:11061	 l-p:0.05554166063666344
epoch£º553	 i:2 	 global-step:11062	 l-p:0.055715806782245636
epoch£º553	 i:3 	 global-step:11063	 l-p:0.05553259700536728
epoch£º553	 i:4 	 global-step:11064	 l-p:0.055544156581163406
epoch£º553	 i:5 	 global-step:11065	 l-p:0.05555714666843414
epoch£º553	 i:6 	 global-step:11066	 l-p:0.05551416426897049
epoch£º553	 i:7 	 global-step:11067	 l-p:0.055635493248701096
epoch£º553	 i:8 	 global-step:11068	 l-p:0.055607929825782776
epoch£º553	 i:9 	 global-step:11069	 l-p:0.05553174391388893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0011, 28.1404, 28.0220],
        [28.0011, 28.0062, 28.0012],
        [28.0011, 28.1779, 28.0318],
        [28.0011, 32.3804, 33.2982]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.05560071021318436 
model_pd.l_d.mean(): -0.00014396585174836218 
model_pd.lagr.mean(): 0.05545674264431 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1436], device='cuda:0')), ('power', tensor([-0.2913], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.05560071021318436
epoch£º554	 i:1 	 global-step:11081	 l-p:0.055579375475645065
epoch£º554	 i:2 	 global-step:11082	 l-p:0.055559415370225906
epoch£º554	 i:3 	 global-step:11083	 l-p:0.055598556995391846
epoch£º554	 i:4 	 global-step:11084	 l-p:0.055568840354681015
epoch£º554	 i:5 	 global-step:11085	 l-p:0.05557584762573242
epoch£º554	 i:6 	 global-step:11086	 l-p:0.05560322478413582
epoch£º554	 i:7 	 global-step:11087	 l-p:0.05558757111430168
epoch£º554	 i:8 	 global-step:11088	 l-p:0.05563900247216225
epoch£º554	 i:9 	 global-step:11089	 l-p:0.05572643131017685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8636, 27.8638, 27.8636],
        [27.8636, 28.5038, 28.1151],
        [27.8636, 27.9442, 27.8723],
        [27.8636, 28.6924, 28.2473]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.05559080094099045 
model_pd.l_d.mean(): -0.00014533198555000126 
model_pd.lagr.mean(): 0.05544546991586685 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1468], device='cuda:0')), ('power', tensor([-0.4272], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.05559080094099045
epoch£º555	 i:1 	 global-step:11101	 l-p:0.05562705546617508
epoch£º555	 i:2 	 global-step:11102	 l-p:0.05580431967973709
epoch£º555	 i:3 	 global-step:11103	 l-p:0.05561519041657448
epoch£º555	 i:4 	 global-step:11104	 l-p:0.05560009926557541
epoch£º555	 i:5 	 global-step:11105	 l-p:0.055633943527936935
epoch£º555	 i:6 	 global-step:11106	 l-p:0.055576175451278687
epoch£º555	 i:7 	 global-step:11107	 l-p:0.055628154426813126
epoch£º555	 i:8 	 global-step:11108	 l-p:0.05560776963829994
epoch£º555	 i:9 	 global-step:11109	 l-p:0.05559408664703369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7834, 30.2451, 29.9974],
        [27.7834, 27.7833, 27.7833],
        [27.7834, 33.2283, 35.0876],
        [27.7834, 27.7940, 27.7837]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.05559227243065834 
model_pd.l_d.mean(): -6.323503475869074e-05 
model_pd.lagr.mean(): 0.05552903562784195 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1527], device='cuda:0')), ('power', tensor([-0.4881], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.05559227243065834
epoch£º556	 i:1 	 global-step:11121	 l-p:0.055590566247701645
epoch£º556	 i:2 	 global-step:11122	 l-p:0.05561681464314461
epoch£º556	 i:3 	 global-step:11123	 l-p:0.055712807923555374
epoch£º556	 i:4 	 global-step:11124	 l-p:0.05558048188686371
epoch£º556	 i:5 	 global-step:11125	 l-p:0.05557791143655777
epoch£º556	 i:6 	 global-step:11126	 l-p:0.055771779268980026
epoch£º556	 i:7 	 global-step:11127	 l-p:0.055602315813302994
epoch£º556	 i:8 	 global-step:11128	 l-p:0.0557066909968853
epoch£º556	 i:9 	 global-step:11129	 l-p:0.05562172830104828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7850, 27.8480, 27.7909],
        [27.7850, 31.3700, 31.7297],
        [27.7850, 28.2421, 27.9302],
        [27.7850, 33.3323, 35.2874]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.055632781237363815 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055632781237363815 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1011], device='cuda:0')), ('power', tensor([-0.3879], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.055632781237363815
epoch£º557	 i:1 	 global-step:11141	 l-p:0.05573992058634758
epoch£º557	 i:2 	 global-step:11142	 l-p:0.055641692131757736
epoch£º557	 i:3 	 global-step:11143	 l-p:0.055615510791540146
epoch£º557	 i:4 	 global-step:11144	 l-p:0.05559578910470009
epoch£º557	 i:5 	 global-step:11145	 l-p:0.055657509714365005
epoch£º557	 i:6 	 global-step:11146	 l-p:0.055577438324689865
epoch£º557	 i:7 	 global-step:11147	 l-p:0.05564732849597931
epoch£º557	 i:8 	 global-step:11148	 l-p:0.055586278438568115
epoch£º557	 i:9 	 global-step:11149	 l-p:0.05560711771249771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8491, 30.1626, 29.8551],
        [27.8491, 33.0408, 34.6607],
        [27.8491, 28.0435, 27.8851],
        [27.8491, 27.8896, 27.8520]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.05563710257411003 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05563710257411003 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1257], device='cuda:0')), ('power', tensor([-0.3905], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.05563710257411003
epoch£º558	 i:1 	 global-step:11161	 l-p:0.055592820048332214
epoch£º558	 i:2 	 global-step:11162	 l-p:0.055728815495967865
epoch£º558	 i:3 	 global-step:11163	 l-p:0.05565366521477699
epoch£º558	 i:4 	 global-step:11164	 l-p:0.05559549108147621
epoch£º558	 i:5 	 global-step:11165	 l-p:0.05558105185627937
epoch£º558	 i:6 	 global-step:11166	 l-p:0.05556827783584595
epoch£º558	 i:7 	 global-step:11167	 l-p:0.05562323331832886
epoch£º558	 i:8 	 global-step:11168	 l-p:0.05558520182967186
epoch£º558	 i:9 	 global-step:11169	 l-p:0.05557476729154587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9362, 32.2773, 33.1712],
        [27.9362, 27.9909, 27.9409],
        [27.9362, 27.9364, 27.9362],
        [27.9362, 27.9400, 27.9363]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.05560597777366638 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05560597777366638 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0894], device='cuda:0')), ('power', tensor([-0.2154], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.05560597777366638
epoch£º559	 i:1 	 global-step:11181	 l-p:0.05569995939731598
epoch£º559	 i:2 	 global-step:11182	 l-p:0.055620696395635605
epoch£º559	 i:3 	 global-step:11183	 l-p:0.055551331490278244
epoch£º559	 i:4 	 global-step:11184	 l-p:0.0555567741394043
epoch£º559	 i:5 	 global-step:11185	 l-p:0.055595774203538895
epoch£º559	 i:6 	 global-step:11186	 l-p:0.05553458258509636
epoch£º559	 i:7 	 global-step:11187	 l-p:0.055656492710113525
epoch£º559	 i:8 	 global-step:11188	 l-p:0.055579688400030136
epoch£º559	 i:9 	 global-step:11189	 l-p:0.05554714426398277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[28.0317, 38.2826, 45.6440],
        [28.0317, 31.1291, 31.1749],
        [28.0317, 38.3276, 45.7494],
        [28.0317, 33.5201, 35.3899]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.055691976100206375 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055691976100206375 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0657], device='cuda:0')), ('power', tensor([-0.1272], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.055691976100206375
epoch£º560	 i:1 	 global-step:11201	 l-p:0.05556299164891243
epoch£º560	 i:2 	 global-step:11202	 l-p:0.055556461215019226
epoch£º560	 i:3 	 global-step:11203	 l-p:0.05556737631559372
epoch£º560	 i:4 	 global-step:11204	 l-p:0.0555783212184906
epoch£º560	 i:5 	 global-step:11205	 l-p:0.05553828179836273
epoch£º560	 i:6 	 global-step:11206	 l-p:0.055534400045871735
epoch£º560	 i:7 	 global-step:11207	 l-p:0.05551048740744591
epoch£º560	 i:8 	 global-step:11208	 l-p:0.05551648139953613
epoch£º560	 i:9 	 global-step:11209	 l-p:0.05568927526473999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1302, 28.1302, 28.1302],
        [28.1302, 30.9970, 30.9152],
        [28.1302, 28.1866, 28.1351],
        [28.1302, 33.7812, 35.7917]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.055596984922885895 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055596984922885895 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1365], device='cuda:0')), ('power', tensor([-0.1139], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.055596984922885895
epoch£º561	 i:1 	 global-step:11221	 l-p:0.05569544807076454
epoch£º561	 i:2 	 global-step:11222	 l-p:0.055518824607133865
epoch£º561	 i:3 	 global-step:11223	 l-p:0.05554119870066643
epoch£º561	 i:4 	 global-step:11224	 l-p:0.055602602660655975
epoch£º561	 i:5 	 global-step:11225	 l-p:0.055513910949230194
epoch£º561	 i:6 	 global-step:11226	 l-p:0.055512044578790665
epoch£º561	 i:7 	 global-step:11227	 l-p:0.05550212785601616
epoch£º561	 i:8 	 global-step:11228	 l-p:0.05552711337804794
epoch£º561	 i:9 	 global-step:11229	 l-p:0.05553115904331207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2303, 28.5025, 28.2919],
        [28.2303, 28.2333, 28.2303],
        [28.2303, 28.2306, 28.2302],
        [28.2303, 32.3391, 33.0295]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.055556099861860275 
model_pd.l_d.mean(): -3.4811421301128576e-08 
model_pd.lagr.mean(): 0.05555606633424759 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1476], device='cuda:0')), ('power', tensor([-0.0524], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.055556099861860275
epoch£º562	 i:1 	 global-step:11241	 l-p:0.05548985302448273
epoch£º562	 i:2 	 global-step:11242	 l-p:0.055518414825201035
epoch£º562	 i:3 	 global-step:11243	 l-p:0.055564019829034805
epoch£º562	 i:4 	 global-step:11244	 l-p:0.05560549721121788
epoch£º562	 i:5 	 global-step:11245	 l-p:0.05548951029777527
epoch£º562	 i:6 	 global-step:11246	 l-p:0.05553876608610153
epoch£º562	 i:7 	 global-step:11247	 l-p:0.05554597079753876
epoch£º562	 i:8 	 global-step:11248	 l-p:0.05551692470908165
epoch£º562	 i:9 	 global-step:11249	 l-p:0.055510297417640686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3301, 28.3728, 28.3332],
        [28.3301, 29.2515, 28.7812],
        [28.3301, 28.8954, 28.5330],
        [28.3301, 28.5544, 28.3750]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.05548468604683876 
model_pd.l_d.mean(): 1.5686671304138144e-06 
model_pd.lagr.mean(): 0.05548625439405441 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.8034e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1769], device='cuda:0')), ('power', tensor([0.0630], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.05548468604683876
epoch£º563	 i:1 	 global-step:11261	 l-p:0.055467795580625534
epoch£º563	 i:2 	 global-step:11262	 l-p:0.05549130216240883
epoch£º563	 i:3 	 global-step:11263	 l-p:0.05546713247895241
epoch£º563	 i:4 	 global-step:11264	 l-p:0.05566488206386566
epoch£º563	 i:5 	 global-step:11265	 l-p:0.055619239807128906
epoch£º563	 i:6 	 global-step:11266	 l-p:0.05547063797712326
epoch£º563	 i:7 	 global-step:11267	 l-p:0.05548352748155594
epoch£º563	 i:8 	 global-step:11268	 l-p:0.05546336993575096
epoch£º563	 i:9 	 global-step:11269	 l-p:0.05552462860941887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[28.4216, 32.2080, 32.6536],
        [28.4216, 33.9905, 35.8882],
        [28.4216, 30.7857, 30.4717],
        [28.4216, 29.5266, 29.0269]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.055540990084409714 
model_pd.l_d.mean(): 1.546193743706681e-05 
model_pd.lagr.mean(): 0.05555645376443863 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.7018e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1530], device='cuda:0')), ('power', tensor([0.1752], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.055540990084409714
epoch£º564	 i:1 	 global-step:11281	 l-p:0.05553547665476799
epoch£º564	 i:2 	 global-step:11282	 l-p:0.055460162460803986
epoch£º564	 i:3 	 global-step:11283	 l-p:0.055512718856334686
epoch£º564	 i:4 	 global-step:11284	 l-p:0.055492281913757324
epoch£º564	 i:5 	 global-step:11285	 l-p:0.05547570064663887
epoch£º564	 i:6 	 global-step:11286	 l-p:0.05547701567411423
epoch£º564	 i:7 	 global-step:11287	 l-p:0.05545180290937424
epoch£º564	 i:8 	 global-step:11288	 l-p:0.055461786687374115
epoch£º564	 i:9 	 global-step:11289	 l-p:0.055564288049936295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4876, 28.5165, 28.4893],
        [28.4876, 29.4103, 28.9381],
        [28.4876, 38.4389, 45.2977],
        [28.4876, 38.4766, 45.3848]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.05548101291060448 
model_pd.l_d.mean(): 4.798916052095592e-05 
model_pd.lagr.mean(): 0.05552900210022926 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1399], device='cuda:0')), ('power', tensor([0.2496], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.05548101291060448
epoch£º565	 i:1 	 global-step:11301	 l-p:0.055437564849853516
epoch£º565	 i:2 	 global-step:11302	 l-p:0.055513083934783936
epoch£º565	 i:3 	 global-step:11303	 l-p:0.05545602738857269
epoch£º565	 i:4 	 global-step:11304	 l-p:0.05544726178050041
epoch£º565	 i:5 	 global-step:11305	 l-p:0.055490508675575256
epoch£º565	 i:6 	 global-step:11306	 l-p:0.05550755560398102
epoch£º565	 i:7 	 global-step:11307	 l-p:0.0555863231420517
epoch£º565	 i:8 	 global-step:11308	 l-p:0.05549732223153114
epoch£º565	 i:9 	 global-step:11309	 l-p:0.05545893684029579
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5109, 28.5923, 28.5196],
        [28.5109, 28.5203, 28.5112],
        [28.5109, 29.3850, 28.9231],
        [28.5109, 28.8684, 28.6064]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.055463917553424835 
model_pd.l_d.mean(): 7.669338083360344e-05 
model_pd.lagr.mean(): 0.05554061010479927 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1599], device='cuda:0')), ('power', tensor([0.2396], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.055463917553424835
epoch£º566	 i:1 	 global-step:11321	 l-p:0.055455531924963
epoch£º566	 i:2 	 global-step:11322	 l-p:0.055438559502363205
epoch£º566	 i:3 	 global-step:11323	 l-p:0.05551769956946373
epoch£º566	 i:4 	 global-step:11324	 l-p:0.055508799850940704
epoch£º566	 i:5 	 global-step:11325	 l-p:0.055462274700403214
epoch£º566	 i:6 	 global-step:11326	 l-p:0.05546056479215622
epoch£º566	 i:7 	 global-step:11327	 l-p:0.05562251806259155
epoch£º566	 i:8 	 global-step:11328	 l-p:0.05545396730303764
epoch£º566	 i:9 	 global-step:11329	 l-p:0.05549415573477745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[28.4780, 38.5511, 45.5722],
        [28.4780, 38.0921, 44.5157],
        [28.4780, 29.7214, 29.2103],
        [28.4780, 36.7361, 41.4856]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.05560746043920517 
model_pd.l_d.mean(): 0.00016120601503644139 
model_pd.lagr.mean(): 0.05576866492629051 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0644], device='cuda:0')), ('power', tensor([0.3603], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.05560746043920517
epoch£º567	 i:1 	 global-step:11341	 l-p:0.05546864867210388
epoch£º567	 i:2 	 global-step:11342	 l-p:0.05546757951378822
epoch£º567	 i:3 	 global-step:11343	 l-p:0.05545665696263313
epoch£º567	 i:4 	 global-step:11344	 l-p:0.05552353709936142
epoch£º567	 i:5 	 global-step:11345	 l-p:0.05552680790424347
epoch£º567	 i:6 	 global-step:11346	 l-p:0.05549997836351395
epoch£º567	 i:7 	 global-step:11347	 l-p:0.05545543506741524
epoch£º567	 i:8 	 global-step:11348	 l-p:0.05547725036740303
epoch£º567	 i:9 	 global-step:11349	 l-p:0.05551646649837494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3833, 31.4362, 31.4361],
        [28.3833, 28.4381, 28.3880],
        [28.3833, 35.4313, 38.8261],
        [28.3833, 31.6887, 31.8309]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.055504679679870605 
model_pd.l_d.mean(): 0.00010004542855313048 
model_pd.lagr.mean(): 0.0556047260761261 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1416], device='cuda:0')), ('power', tensor([0.1837], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.055504679679870605
epoch£º568	 i:1 	 global-step:11361	 l-p:0.05547454208135605
epoch£º568	 i:2 	 global-step:11362	 l-p:0.0555998869240284
epoch£º568	 i:3 	 global-step:11363	 l-p:0.05546985939145088
epoch£º568	 i:4 	 global-step:11364	 l-p:0.05548612400889397
epoch£º568	 i:5 	 global-step:11365	 l-p:0.055498361587524414
epoch£º568	 i:6 	 global-step:11366	 l-p:0.055495575070381165
epoch£º568	 i:7 	 global-step:11367	 l-p:0.05557513236999512
epoch£º568	 i:8 	 global-step:11368	 l-p:0.0556122288107872
epoch£º568	 i:9 	 global-step:11369	 l-p:0.05552583560347557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2381, 28.2382, 28.2381],
        [28.2381, 28.4498, 28.2790],
        [28.2381, 38.0981, 44.8937],
        [28.2381, 28.2381, 28.2381]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.055577076971530914 
model_pd.l_d.mean(): 8.798194357950706e-06 
model_pd.lagr.mean(): 0.05558587610721588 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1197], device='cuda:0')), ('power', tensor([0.0151], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.055577076971530914
epoch£º569	 i:1 	 global-step:11381	 l-p:0.05558879300951958
epoch£º569	 i:2 	 global-step:11382	 l-p:0.055527009069919586
epoch£º569	 i:3 	 global-step:11383	 l-p:0.055518921464681625
epoch£º569	 i:4 	 global-step:11384	 l-p:0.05553947761654854
epoch£º569	 i:5 	 global-step:11385	 l-p:0.055647220462560654
epoch£º569	 i:6 	 global-step:11386	 l-p:0.05551139637827873
epoch£º569	 i:7 	 global-step:11387	 l-p:0.05552809685468674
epoch£º569	 i:8 	 global-step:11388	 l-p:0.05552908405661583
epoch£º569	 i:9 	 global-step:11389	 l-p:0.05560063570737839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0666, 28.1277, 28.0722],
        [28.0666, 28.0665, 28.0665],
        [28.0666, 30.8749, 30.7678],
        [28.0666, 35.9117, 40.2519]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.05557284876704216 
model_pd.l_d.mean(): -0.00011785150127252564 
model_pd.lagr.mean(): 0.05545499548316002 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1270], device='cuda:0')), ('power', tensor([-0.2178], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.05557284876704216
epoch£º570	 i:1 	 global-step:11401	 l-p:0.055528633296489716
epoch£º570	 i:2 	 global-step:11402	 l-p:0.05553032457828522
epoch£º570	 i:3 	 global-step:11403	 l-p:0.055626656860113144
epoch£º570	 i:4 	 global-step:11404	 l-p:0.05555276572704315
epoch£º570	 i:5 	 global-step:11405	 l-p:0.055556103587150574
epoch£º570	 i:6 	 global-step:11406	 l-p:0.05562753230333328
epoch£º570	 i:7 	 global-step:11407	 l-p:0.055704835802316666
epoch£º570	 i:8 	 global-step:11408	 l-p:0.055649250745773315
epoch£º570	 i:9 	 global-step:11409	 l-p:0.055575065314769745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9037, 29.8401, 29.4162],
        [27.9037, 27.9039, 27.9037],
        [27.9037, 28.0986, 27.9398],
        [27.9037, 27.9341, 27.9056]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.055677611380815506 
model_pd.l_d.mean(): -0.00012528807565104216 
model_pd.lagr.mean(): 0.055552322417497635 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1073], device='cuda:0')), ('power', tensor([-0.3023], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.055677611380815506
epoch£º571	 i:1 	 global-step:11421	 l-p:0.05564991012215614
epoch£º571	 i:2 	 global-step:11422	 l-p:0.05569962039589882
epoch£º571	 i:3 	 global-step:11423	 l-p:0.055591944605112076
epoch£º571	 i:4 	 global-step:11424	 l-p:0.05560891330242157
epoch£º571	 i:5 	 global-step:11425	 l-p:0.0555926114320755
epoch£º571	 i:6 	 global-step:11426	 l-p:0.05559442564845085
epoch£º571	 i:7 	 global-step:11427	 l-p:0.05556882917881012
epoch£º571	 i:8 	 global-step:11428	 l-p:0.05559207871556282
epoch£º571	 i:9 	 global-step:11429	 l-p:0.05564950034022331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7872, 27.7923, 27.7873],
        [27.7872, 37.9445, 45.2383],
        [27.7872, 37.2918, 43.7248],
        [27.7872, 27.8611, 27.7948]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.05567105486989021 
model_pd.l_d.mean(): -8.701455226400867e-05 
model_pd.lagr.mean(): 0.05558403953909874 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0880], device='cuda:0')), ('power', tensor([-0.4030], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.05567105486989021
epoch£º572	 i:1 	 global-step:11441	 l-p:0.055622681975364685
epoch£º572	 i:2 	 global-step:11442	 l-p:0.055613573640584946
epoch£º572	 i:3 	 global-step:11443	 l-p:0.05559607222676277
epoch£º572	 i:4 	 global-step:11444	 l-p:0.055693671107292175
epoch£º572	 i:5 	 global-step:11445	 l-p:0.0556911826133728
epoch£º572	 i:6 	 global-step:11446	 l-p:0.055592332035303116
epoch£º572	 i:7 	 global-step:11447	 l-p:0.05563121661543846
epoch£º572	 i:8 	 global-step:11448	 l-p:0.055621955543756485
epoch£º572	 i:9 	 global-step:11449	 l-p:0.05566837266087532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7519, 28.1479, 27.8668],
        [27.7519, 29.2847, 28.7962],
        [27.7519, 28.3483, 27.9765],
        [27.7519, 27.7532, 27.7519]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.05569691210985184 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05569691210985184 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1237], device='cuda:0')), ('power', tensor([-0.4891], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.05569691210985184
epoch£º573	 i:1 	 global-step:11461	 l-p:0.055693287402391434
epoch£º573	 i:2 	 global-step:11462	 l-p:0.05560644343495369
epoch£º573	 i:3 	 global-step:11463	 l-p:0.055622246116399765
epoch£º573	 i:4 	 global-step:11464	 l-p:0.05561256408691406
epoch£º573	 i:5 	 global-step:11465	 l-p:0.05569702386856079
epoch£º573	 i:6 	 global-step:11466	 l-p:0.05563576519489288
epoch£º573	 i:7 	 global-step:11467	 l-p:0.05557120218873024
epoch£º573	 i:8 	 global-step:11468	 l-p:0.05562455952167511
epoch£º573	 i:9 	 global-step:11469	 l-p:0.05562823265790939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8023, 30.4104, 30.2239],
        [27.8023, 33.6603, 35.9115],
        [27.8023, 27.8023, 27.8023],
        [27.8023, 27.8240, 27.8034]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.05561242625117302 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05561242625117302 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1427], device='cuda:0')), ('power', tensor([-0.4709], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.05561242625117302
epoch£º574	 i:1 	 global-step:11481	 l-p:0.05559244379401207
epoch£º574	 i:2 	 global-step:11482	 l-p:0.05564272776246071
epoch£º574	 i:3 	 global-step:11483	 l-p:0.05566779151558876
epoch£º574	 i:4 	 global-step:11484	 l-p:0.05569741129875183
epoch£º574	 i:5 	 global-step:11485	 l-p:0.055583033710718155
epoch£º574	 i:6 	 global-step:11486	 l-p:0.055691223591566086
epoch£º574	 i:7 	 global-step:11487	 l-p:0.05557680130004883
epoch£º574	 i:8 	 global-step:11488	 l-p:0.0555780790746212
epoch£º574	 i:9 	 global-step:11489	 l-p:0.055597953498363495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8897, 27.9594, 27.8966],
        [27.8897, 29.3200, 28.8208],
        [27.8897, 27.8897, 27.8897],
        [27.8897, 27.9692, 27.8982]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.05558528006076813 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05558528006076813 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1282], device='cuda:0')), ('power', tensor([-0.3256], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.05558528006076813
epoch£º575	 i:1 	 global-step:11501	 l-p:0.055617205798625946
epoch£º575	 i:2 	 global-step:11502	 l-p:0.05555921047925949
epoch£º575	 i:3 	 global-step:11503	 l-p:0.05568063259124756
epoch£º575	 i:4 	 global-step:11504	 l-p:0.05559021979570389
epoch£º575	 i:5 	 global-step:11505	 l-p:0.055538203567266464
epoch£º575	 i:6 	 global-step:11506	 l-p:0.0555446594953537
epoch£º575	 i:7 	 global-step:11507	 l-p:0.05574478209018707
epoch£º575	 i:8 	 global-step:11508	 l-p:0.0555700957775116
epoch£º575	 i:9 	 global-step:11509	 l-p:0.055610693991184235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9902, 27.9918, 27.9902],
        [27.9902, 28.0094, 27.9911],
        [27.9902, 37.3200, 43.4842],
        [27.9902, 32.4416, 33.4175]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.05557022988796234 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05557022988796234 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1341], device='cuda:0')), ('power', tensor([-0.2624], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.05557022988796234
epoch£º576	 i:1 	 global-step:11521	 l-p:0.05554129183292389
epoch£º576	 i:2 	 global-step:11522	 l-p:0.055578913539648056
epoch£º576	 i:3 	 global-step:11523	 l-p:0.055554646998643875
epoch£º576	 i:4 	 global-step:11524	 l-p:0.05569199100136757
epoch£º576	 i:5 	 global-step:11525	 l-p:0.055561475455760956
epoch£º576	 i:6 	 global-step:11526	 l-p:0.05553555488586426
epoch£º576	 i:7 	 global-step:11527	 l-p:0.05566946044564247
epoch£º576	 i:8 	 global-step:11528	 l-p:0.055568769574165344
epoch£º576	 i:9 	 global-step:11529	 l-p:0.05555330589413643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0957, 28.9394, 28.4885],
        [28.0957, 28.0957, 28.0957],
        [28.0957, 34.2318, 36.7188],
        [28.0957, 28.0957, 28.0957]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.055525436997413635 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055525436997413635 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1762], device='cuda:0')), ('power', tensor([-0.1839], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:0.055525436997413635
epoch£º577	 i:1 	 global-step:11541	 l-p:0.05565447360277176
epoch£º577	 i:2 	 global-step:11542	 l-p:0.055520642548799515
epoch£º577	 i:3 	 global-step:11543	 l-p:0.05556410551071167
epoch£º577	 i:4 	 global-step:11544	 l-p:0.055538348853588104
epoch£º577	 i:5 	 global-step:11545	 l-p:0.055600352585315704
epoch£º577	 i:6 	 global-step:11546	 l-p:0.0555441752076149
epoch£º577	 i:7 	 global-step:11547	 l-p:0.05551108717918396
epoch£º577	 i:8 	 global-step:11548	 l-p:0.05562859773635864
epoch£º577	 i:9 	 global-step:11549	 l-p:0.055517781525850296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2033, 28.2032, 28.2032],
        [28.2033, 34.1568, 36.4491],
        [28.2033, 29.9611, 29.4909],
        [28.2033, 31.3976, 31.4871]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.05551513656973839 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05551513656973839 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1734], device='cuda:0')), ('power', tensor([-0.0765], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.05551513656973839
epoch£º578	 i:1 	 global-step:11561	 l-p:0.05549207702279091
epoch£º578	 i:2 	 global-step:11562	 l-p:0.055762555450201035
epoch£º578	 i:3 	 global-step:11563	 l-p:0.05553700402379036
epoch£º578	 i:4 	 global-step:11564	 l-p:0.05549189820885658
epoch£º578	 i:5 	 global-step:11565	 l-p:0.055504340678453445
epoch£º578	 i:6 	 global-step:11566	 l-p:0.05552799627184868
epoch£º578	 i:7 	 global-step:11567	 l-p:0.05555461719632149
epoch£º578	 i:8 	 global-step:11568	 l-p:0.05549432337284088
epoch£º578	 i:9 	 global-step:11569	 l-p:0.05550374835729599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3110, 38.3286, 45.3146],
        [28.3110, 28.8759, 28.5137],
        [28.3110, 31.3556, 31.3554],
        [28.3110, 28.6910, 28.4170]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.055498167872428894 
model_pd.l_d.mean(): 4.6636273509648163e-07 
model_pd.lagr.mean(): 0.0554986335337162 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.4897e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1692], device='cuda:0')), ('power', tensor([0.0355], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.055498167872428894
epoch£º579	 i:1 	 global-step:11581	 l-p:0.05549408495426178
epoch£º579	 i:2 	 global-step:11582	 l-p:0.0556066557765007
epoch£º579	 i:3 	 global-step:11583	 l-p:0.05547882616519928
epoch£º579	 i:4 	 global-step:11584	 l-p:0.05547073110938072
epoch£º579	 i:5 	 global-step:11585	 l-p:0.055501118302345276
epoch£º579	 i:6 	 global-step:11586	 l-p:0.05553603917360306
epoch£º579	 i:7 	 global-step:11587	 l-p:0.05557290464639664
epoch£º579	 i:8 	 global-step:11588	 l-p:0.05552384629845619
epoch£º579	 i:9 	 global-step:11589	 l-p:0.055484838783741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4129, 28.5378, 28.4303],
        [28.4129, 28.4454, 28.4150],
        [28.4129, 33.0697, 34.1698],
        [28.4129, 28.5759, 28.4396]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.05544896051287651 
model_pd.l_d.mean(): 5.108458935865201e-06 
model_pd.lagr.mean(): 0.0554540678858757 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.2841e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2254], device='cuda:0')), ('power', tensor([0.0739], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.05544896051287651
epoch£º580	 i:1 	 global-step:11601	 l-p:0.05545490235090256
epoch£º580	 i:2 	 global-step:11602	 l-p:0.05546058714389801
epoch£º580	 i:3 	 global-step:11603	 l-p:0.05548550561070442
epoch£º580	 i:4 	 global-step:11604	 l-p:0.05556170269846916
epoch£º580	 i:5 	 global-step:11605	 l-p:0.05547484755516052
epoch£º580	 i:6 	 global-step:11606	 l-p:0.0555669404566288
epoch£º580	 i:7 	 global-step:11607	 l-p:0.05552108585834503
epoch£º580	 i:8 	 global-step:11608	 l-p:0.055535223335027695
epoch£º580	 i:9 	 global-step:11609	 l-p:0.055466994643211365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4934, 28.4934, 28.4934],
        [28.4934, 35.8924, 39.6543],
        [28.4934, 31.4219, 31.3505],
        [28.4934, 29.2831, 28.8426]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.05546934902667999 
model_pd.l_d.mean(): 4.2644092900445685e-05 
model_pd.lagr.mean(): 0.055511992424726486 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1482], device='cuda:0')), ('power', tensor([0.2481], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.05546934902667999
epoch£º581	 i:1 	 global-step:11621	 l-p:0.05549056455492973
epoch£º581	 i:2 	 global-step:11622	 l-p:0.055443279445171356
epoch£º581	 i:3 	 global-step:11623	 l-p:0.05550543963909149
epoch£º581	 i:4 	 global-step:11624	 l-p:0.05553443357348442
epoch£º581	 i:5 	 global-step:11625	 l-p:0.05544206500053406
epoch£º581	 i:6 	 global-step:11626	 l-p:0.055449265986680984
epoch£º581	 i:7 	 global-step:11627	 l-p:0.05554493889212608
epoch£º581	 i:8 	 global-step:11628	 l-p:0.0554962232708931
epoch£º581	 i:9 	 global-step:11629	 l-p:0.05547645315527916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5286, 28.8735, 28.6186],
        [28.5286, 38.3236, 44.9695],
        [28.5286, 28.5306, 28.5287],
        [28.5286, 28.5909, 28.5343]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.05560571700334549 
model_pd.l_d.mean(): 0.00011920691031264141 
model_pd.lagr.mean(): 0.05572492256760597 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0815], device='cuda:0')), ('power', tensor([0.3901], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.05560571700334549
epoch£º582	 i:1 	 global-step:11641	 l-p:0.05551925674080849
epoch£º582	 i:2 	 global-step:11642	 l-p:0.05546818673610687
epoch£º582	 i:3 	 global-step:11643	 l-p:0.05544048175215721
epoch£º582	 i:4 	 global-step:11644	 l-p:0.05544641241431236
epoch£º582	 i:5 	 global-step:11645	 l-p:0.05545974522829056
epoch£º582	 i:6 	 global-step:11646	 l-p:0.055540990084409714
epoch£º582	 i:7 	 global-step:11647	 l-p:0.05548011139035225
epoch£º582	 i:8 	 global-step:11648	 l-p:0.0554475337266922
epoch£º582	 i:9 	 global-step:11649	 l-p:0.055428486317396164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4990, 28.7171, 28.5417],
        [28.4990, 35.1184, 38.0417],
        [28.4990, 35.3181, 38.4505],
        [28.4990, 31.9057, 32.1007]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.05545765906572342 
model_pd.l_d.mean(): 0.00011681338219204918 
model_pd.lagr.mean(): 0.05557447299361229 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1667], device='cuda:0')), ('power', tensor([0.2637], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.05545765906572342
epoch£º583	 i:1 	 global-step:11661	 l-p:0.0555141381919384
epoch£º583	 i:2 	 global-step:11662	 l-p:0.05560576170682907
epoch£º583	 i:3 	 global-step:11663	 l-p:0.05546390265226364
epoch£º583	 i:4 	 global-step:11664	 l-p:0.05546550080180168
epoch£º583	 i:5 	 global-step:11665	 l-p:0.05546794831752777
epoch£º583	 i:6 	 global-step:11666	 l-p:0.05547518655657768
epoch£º583	 i:7 	 global-step:11667	 l-p:0.05545558035373688
epoch£º583	 i:8 	 global-step:11668	 l-p:0.055476970970630646
epoch£º583	 i:9 	 global-step:11669	 l-p:0.055579252541065216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3985, 28.5233, 28.4158],
        [28.3985, 28.3985, 28.3985],
        [28.3985, 28.7744, 28.5024],
        [28.3985, 28.8663, 28.5471]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.05546517297625542 
model_pd.l_d.mean(): 5.772797157987952e-05 
model_pd.lagr.mean(): 0.05552290007472038 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1814], device='cuda:0')), ('power', tensor([0.1050], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.05546517297625542
epoch£º584	 i:1 	 global-step:11681	 l-p:0.05564245209097862
epoch£º584	 i:2 	 global-step:11682	 l-p:0.055473823100328445
epoch£º584	 i:3 	 global-step:11683	 l-p:0.05553346499800682
epoch£º584	 i:4 	 global-step:11684	 l-p:0.055512648075819016
epoch£º584	 i:5 	 global-step:11685	 l-p:0.05548815429210663
epoch£º584	 i:6 	 global-step:11686	 l-p:0.055554211139678955
epoch£º584	 i:7 	 global-step:11687	 l-p:0.05552668869495392
epoch£º584	 i:8 	 global-step:11688	 l-p:0.05549227446317673
epoch£º584	 i:9 	 global-step:11689	 l-p:0.05553249269723892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2396, 28.7927, 28.4358],
        [28.2396, 28.2807, 28.2426],
        [28.2396, 28.2479, 28.2399],
        [28.2396, 28.2941, 28.2442]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.055513571947813034 
model_pd.l_d.mean(): -4.413645001477562e-05 
model_pd.lagr.mean(): 0.05546943470835686 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1664], device='cuda:0')), ('power', tensor([-0.0745], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.055513571947813034
epoch£º585	 i:1 	 global-step:11701	 l-p:0.055528685450553894
epoch£º585	 i:2 	 global-step:11702	 l-p:0.05552828684449196
epoch£º585	 i:3 	 global-step:11703	 l-p:0.05556659400463104
epoch£º585	 i:4 	 global-step:11704	 l-p:0.055614251643419266
epoch£º585	 i:5 	 global-step:11705	 l-p:0.0556684210896492
epoch£º585	 i:6 	 global-step:11706	 l-p:0.0555400550365448
epoch£º585	 i:7 	 global-step:11707	 l-p:0.05553197115659714
epoch£º585	 i:8 	 global-step:11708	 l-p:0.0555509477853775
epoch£º585	 i:9 	 global-step:11709	 l-p:0.05553868040442467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0505, 28.0538, 28.0506],
        [28.0505, 37.6495, 44.1468],
        [28.0505, 29.3524, 28.8474],
        [28.0505, 28.2485, 28.0874]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.05556468293070793 
model_pd.l_d.mean(): -0.00012110695388400927 
model_pd.lagr.mean(): 0.05544357746839523 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1339], device='cuda:0')), ('power', tensor([-0.2211], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.05556468293070793
epoch£º586	 i:1 	 global-step:11721	 l-p:0.05565132200717926
epoch£º586	 i:2 	 global-step:11722	 l-p:0.05557072535157204
epoch£º586	 i:3 	 global-step:11723	 l-p:0.05557091534137726
epoch£º586	 i:4 	 global-step:11724	 l-p:0.05557039752602577
epoch£º586	 i:5 	 global-step:11725	 l-p:0.055608443915843964
epoch£º586	 i:6 	 global-step:11726	 l-p:0.05554446205496788
epoch£º586	 i:7 	 global-step:11727	 l-p:0.05564677715301514
epoch£º586	 i:8 	 global-step:11728	 l-p:0.0555860809981823
epoch£º586	 i:9 	 global-step:11729	 l-p:0.055658720433712006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8714, 30.7539, 30.6949],
        [27.8714, 34.0371, 36.5846],
        [27.8714, 33.4368, 35.3984],
        [27.8714, 28.2321, 27.9697]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.05560298636555672 
model_pd.l_d.mean(): -0.00014516279043164104 
model_pd.lagr.mean(): 0.05545782297849655 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1153], device='cuda:0')), ('power', tensor([-0.3546], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.05560298636555672
epoch£º587	 i:1 	 global-step:11741	 l-p:0.05561627447605133
epoch£º587	 i:2 	 global-step:11742	 l-p:0.05572528392076492
epoch£º587	 i:3 	 global-step:11743	 l-p:0.05559515208005905
epoch£º587	 i:4 	 global-step:11744	 l-p:0.05562901869416237
epoch£º587	 i:5 	 global-step:11745	 l-p:0.05558665096759796
epoch£º587	 i:6 	 global-step:11746	 l-p:0.05557990074157715
epoch£º587	 i:7 	 global-step:11747	 l-p:0.055749062448740005
epoch£º587	 i:8 	 global-step:11748	 l-p:0.055633869022130966
epoch£º587	 i:9 	 global-step:11749	 l-p:0.055585745722055435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7478, 27.7478, 27.7478],
        [27.7478, 28.3693, 27.9880],
        [27.7478, 29.8036, 29.4167],
        [27.7478, 28.2572, 27.9213]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.05561140924692154 
model_pd.l_d.mean(): -0.00010120029037352651 
model_pd.lagr.mean(): 0.05551020801067352 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1282], device='cuda:0')), ('power', tensor([-0.5256], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.05561140924692154
epoch£º588	 i:1 	 global-step:11761	 l-p:0.05558380112051964
epoch£º588	 i:2 	 global-step:11762	 l-p:0.05562545731663704
epoch£º588	 i:3 	 global-step:11763	 l-p:0.05563591048121452
epoch£º588	 i:4 	 global-step:11764	 l-p:0.05560019239783287
epoch£º588	 i:5 	 global-step:11765	 l-p:0.055611323565244675
epoch£º588	 i:6 	 global-step:11766	 l-p:0.05572441965341568
epoch£º588	 i:7 	 global-step:11767	 l-p:0.05576685816049576
epoch£º588	 i:8 	 global-step:11768	 l-p:0.05566641315817833
epoch£º588	 i:9 	 global-step:11769	 l-p:0.0556553453207016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7209, 27.8011, 27.7296],
        [27.7209, 27.7250, 27.7210],
        [27.7209, 27.9879, 27.7814],
        [27.7209, 27.7211, 27.7209]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.055632419884204865 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055632419884204865 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1043], device='cuda:0')), ('power', tensor([-0.5129], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.055632419884204865
epoch£º589	 i:1 	 global-step:11781	 l-p:0.055597610771656036
epoch£º589	 i:2 	 global-step:11782	 l-p:0.0557355098426342
epoch£º589	 i:3 	 global-step:11783	 l-p:0.05560467764735222
epoch£º589	 i:4 	 global-step:11784	 l-p:0.055610768496990204
epoch£º589	 i:5 	 global-step:11785	 l-p:0.05569155514240265
epoch£º589	 i:6 	 global-step:11786	 l-p:0.05568460002541542
epoch£º589	 i:7 	 global-step:11787	 l-p:0.05560451000928879
epoch£º589	 i:8 	 global-step:11788	 l-p:0.055607136338949203
epoch£º589	 i:9 	 global-step:11789	 l-p:0.055673398077487946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7830, 27.9889, 27.8225],
        [27.7830, 32.8882, 34.4381],
        [27.7830, 28.1535, 27.8859],
        [27.7830, 27.8365, 27.7875]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.05561180040240288 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05561180040240288 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1321], device='cuda:0')), ('power', tensor([-0.4565], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.05561180040240288
epoch£º590	 i:1 	 global-step:11801	 l-p:0.05561746656894684
epoch£º590	 i:2 	 global-step:11802	 l-p:0.05557488650083542
epoch£º590	 i:3 	 global-step:11803	 l-p:0.05566193535923958
epoch£º590	 i:4 	 global-step:11804	 l-p:0.05564102903008461
epoch£º590	 i:5 	 global-step:11805	 l-p:0.05568000301718712
epoch£º590	 i:6 	 global-step:11806	 l-p:0.05569753050804138
epoch£º590	 i:7 	 global-step:11807	 l-p:0.05559555068612099
epoch£º590	 i:8 	 global-step:11808	 l-p:0.05562235414981842
epoch£º590	 i:9 	 global-step:11809	 l-p:0.055568840354681015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8798, 34.1888, 36.8806],
        [27.8798, 30.7196, 30.6384],
        [27.8798, 27.8803, 27.8798],
        [27.8798, 27.8816, 27.8798]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.055724721401929855 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055724721401929855 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0773], device='cuda:0')), ('power', tensor([-0.2996], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.055724721401929855
epoch£º591	 i:1 	 global-step:11821	 l-p:0.0555640384554863
epoch£º591	 i:2 	 global-step:11822	 l-p:0.05555003136396408
epoch£º591	 i:3 	 global-step:11823	 l-p:0.05560484156012535
epoch£º591	 i:4 	 global-step:11824	 l-p:0.055559191852808
epoch£º591	 i:5 	 global-step:11825	 l-p:0.055585186928510666
epoch£º591	 i:6 	 global-step:11826	 l-p:0.055623121559619904
epoch£º591	 i:7 	 global-step:11827	 l-p:0.055634815245866776
epoch£º591	 i:8 	 global-step:11828	 l-p:0.055583782494068146
epoch£º591	 i:9 	 global-step:11829	 l-p:0.05562363937497139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[27.9892, 29.7624, 29.3012],
        [27.9892, 29.7648, 29.3040],
        [27.9892, 38.1840, 45.4802],
        [27.9892, 30.0613, 29.6701]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.05559053644537926 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05559053644537926 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1522], device='cuda:0')), ('power', tensor([-0.2850], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.05559053644537926
epoch£º592	 i:1 	 global-step:11841	 l-p:0.055554237216711044
epoch£º592	 i:2 	 global-step:11842	 l-p:0.05556050315499306
epoch£º592	 i:3 	 global-step:11843	 l-p:0.05561208724975586
epoch£º592	 i:4 	 global-step:11844	 l-p:0.05564432963728905
epoch£º592	 i:5 	 global-step:11845	 l-p:0.05554124340415001
epoch£º592	 i:6 	 global-step:11846	 l-p:0.05553745478391647
epoch£º592	 i:7 	 global-step:11847	 l-p:0.055558718740940094
epoch£º592	 i:8 	 global-step:11848	 l-p:0.0557052344083786
epoch£º592	 i:9 	 global-step:11849	 l-p:0.05551550164818764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1031, 28.5941, 28.2650],
        [28.1031, 28.9791, 28.5207],
        [28.1031, 28.1033, 28.1031],
        [28.1031, 28.1031, 28.1031]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.055520474910736084 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055520474910736084 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1895], device='cuda:0')), ('power', tensor([-0.1924], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.055520474910736084
epoch£º593	 i:1 	 global-step:11861	 l-p:0.05554664134979248
epoch£º593	 i:2 	 global-step:11862	 l-p:0.05553697794675827
epoch£º593	 i:3 	 global-step:11863	 l-p:0.05553409457206726
epoch£º593	 i:4 	 global-step:11864	 l-p:0.05552069470286369
epoch£º593	 i:5 	 global-step:11865	 l-p:0.0556325726211071
epoch£º593	 i:6 	 global-step:11866	 l-p:0.05560608580708504
epoch£º593	 i:7 	 global-step:11867	 l-p:0.05555231124162674
epoch£º593	 i:8 	 global-step:11868	 l-p:0.055631399154663086
epoch£º593	 i:9 	 global-step:11869	 l-p:0.05550036206841469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2189, 29.0092, 28.5709],
        [28.2189, 28.2191, 28.2189],
        [28.2189, 28.6257, 28.3377],
        [28.2189, 36.0981, 40.4509]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.0555204376578331 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0555204376578331 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.6268e-07], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1419], device='cuda:0')), ('power', tensor([0.0153], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.0555204376578331
epoch£º594	 i:1 	 global-step:11881	 l-p:0.055498719215393066
epoch£º594	 i:2 	 global-step:11882	 l-p:0.055502936244010925
epoch£º594	 i:3 	 global-step:11883	 l-p:0.05559885874390602
epoch£º594	 i:4 	 global-step:11884	 l-p:0.05560396984219551
epoch£º594	 i:5 	 global-step:11885	 l-p:0.05552520975470543
epoch£º594	 i:6 	 global-step:11886	 l-p:0.05547110363841057
epoch£º594	 i:7 	 global-step:11887	 l-p:0.05556662380695343
epoch£º594	 i:8 	 global-step:11888	 l-p:0.05551328882575035
epoch£º594	 i:9 	 global-step:11889	 l-p:0.05554230138659477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3348, 36.2823, 40.6939],
        [28.3348, 30.1338, 29.6670],
        [28.3348, 28.3349, 28.3348],
        [28.3348, 28.3431, 28.3350]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.05548347905278206 
model_pd.l_d.mean(): 8.036426493163162e-07 
model_pd.lagr.mean(): 0.055484283715486526 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.2659e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1809], device='cuda:0')), ('power', tensor([0.0388], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.05548347905278206
epoch£º595	 i:1 	 global-step:11901	 l-p:0.05550259351730347
epoch£º595	 i:2 	 global-step:11902	 l-p:0.05546943470835686
epoch£º595	 i:3 	 global-step:11903	 l-p:0.055478718131780624
epoch£º595	 i:4 	 global-step:11904	 l-p:0.055597443133592606
epoch£º595	 i:5 	 global-step:11905	 l-p:0.05555076524615288
epoch£º595	 i:6 	 global-step:11906	 l-p:0.05545835196971893
epoch£º595	 i:7 	 global-step:11907	 l-p:0.05561421811580658
epoch£º595	 i:8 	 global-step:11908	 l-p:0.055478714406490326
epoch£º595	 i:9 	 global-step:11909	 l-p:0.055479783564805984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4411, 28.6664, 28.4862],
        [28.4411, 28.4451, 28.4412],
        [28.4411, 32.9684, 33.9612],
        [28.4411, 28.4413, 28.4411]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.05546768009662628 
model_pd.l_d.mean(): 1.2979706298210658e-05 
model_pd.lagr.mean(): 0.05548065900802612 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.7099e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1836], device='cuda:0')), ('power', tensor([0.1444], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.05546768009662628
epoch£º596	 i:1 	 global-step:11921	 l-p:0.05558742582798004
epoch£º596	 i:2 	 global-step:11922	 l-p:0.05549674853682518
epoch£º596	 i:3 	 global-step:11923	 l-p:0.05549575760960579
epoch£º596	 i:4 	 global-step:11924	 l-p:0.05547475814819336
epoch£º596	 i:5 	 global-step:11925	 l-p:0.05546491593122482
epoch£º596	 i:6 	 global-step:11926	 l-p:0.05552925541996956
epoch£º596	 i:7 	 global-step:11927	 l-p:0.05544905364513397
epoch£º596	 i:8 	 global-step:11928	 l-p:0.05544959381222725
epoch£º596	 i:9 	 global-step:11929	 l-p:0.05550813674926758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5161, 29.9860, 29.4753],
        [28.5161, 28.6582, 28.5374],
        [28.5161, 28.5163, 28.5161],
        [28.5161, 28.5244, 28.5163]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.055458664894104004 
model_pd.l_d.mean(): 4.400909165269695e-05 
model_pd.lagr.mean(): 0.05550267547369003 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1707], device='cuda:0')), ('power', tensor([0.2136], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.055458664894104004
epoch£º597	 i:1 	 global-step:11941	 l-p:0.05546259880065918
epoch£º597	 i:2 	 global-step:11942	 l-p:0.05543607473373413
epoch£º597	 i:3 	 global-step:11943	 l-p:0.05553723871707916
epoch£º597	 i:4 	 global-step:11944	 l-p:0.055450428277254105
epoch£º597	 i:5 	 global-step:11945	 l-p:0.055476993322372437
epoch£º597	 i:6 	 global-step:11946	 l-p:0.05553942918777466
epoch£º597	 i:7 	 global-step:11947	 l-p:0.05553150177001953
epoch£º597	 i:8 	 global-step:11948	 l-p:0.0554683543741703
epoch£º597	 i:9 	 global-step:11949	 l-p:0.055457670241594315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5361, 28.5400, 28.5361],
        [28.5361, 35.5769, 38.9403],
        [28.5361, 29.6739, 29.1692],
        [28.5361, 37.9414, 44.0859]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.05554107576608658 
model_pd.l_d.mean(): 0.00011550317140063271 
model_pd.lagr.mean(): 0.05565657839179039 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1113], device='cuda:0')), ('power', tensor([0.3320], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.05554107576608658
epoch£º598	 i:1 	 global-step:11961	 l-p:0.05543915554881096
epoch£º598	 i:2 	 global-step:11962	 l-p:0.05562908947467804
epoch£º598	 i:3 	 global-step:11963	 l-p:0.05542956292629242
epoch£º598	 i:4 	 global-step:11964	 l-p:0.05545917525887489
epoch£º598	 i:5 	 global-step:11965	 l-p:0.05545235425233841
epoch£º598	 i:6 	 global-step:11966	 l-p:0.055473554879426956
epoch£º598	 i:7 	 global-step:11967	 l-p:0.05545400455594063
epoch£º598	 i:8 	 global-step:11968	 l-p:0.05546387657523155
epoch£º598	 i:9 	 global-step:11969	 l-p:0.05550041422247887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4827, 28.4827, 28.4827],
        [28.4827, 29.7081, 29.1978],
        [28.4827, 36.4738, 40.9097],
        [28.4827, 28.5023, 28.4836]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.05549599975347519 
model_pd.l_d.mean(): 0.00014379511412698776 
model_pd.lagr.mean(): 0.05563979595899582 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0946], device='cuda:0')), ('power', tensor([0.2971], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.05549599975347519
epoch£º599	 i:1 	 global-step:11981	 l-p:0.05543744936585426
epoch£º599	 i:2 	 global-step:11982	 l-p:0.0556020550429821
epoch£º599	 i:3 	 global-step:11983	 l-p:0.055525340139865875
epoch£º599	 i:4 	 global-step:11984	 l-p:0.055483825504779816
epoch£º599	 i:5 	 global-step:11985	 l-p:0.05547923967242241
epoch£º599	 i:6 	 global-step:11986	 l-p:0.05545146390795708
epoch£º599	 i:7 	 global-step:11987	 l-p:0.05551619082689285
epoch£º599	 i:8 	 global-step:11988	 l-p:0.0554814375936985
epoch£º599	 i:9 	 global-step:11989	 l-p:0.055547721683979034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3529, 34.7141, 37.3927],
        [28.3529, 28.3529, 28.3529],
        [28.3529, 28.3529, 28.3529],
        [28.3529, 28.3529, 28.3529]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.05556781589984894 
model_pd.l_d.mean(): 6.466090417234227e-05 
model_pd.lagr.mean(): 0.05563247576355934 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1555], device='cuda:0')), ('power', tensor([0.1123], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.05556781589984894
epoch£º600	 i:1 	 global-step:12001	 l-p:0.05548292025923729
epoch£º600	 i:2 	 global-step:12002	 l-p:0.05549971014261246
epoch£º600	 i:3 	 global-step:12003	 l-p:0.055512234568595886
epoch£º600	 i:4 	 global-step:12004	 l-p:0.055484838783741
epoch£º600	 i:5 	 global-step:12005	 l-p:0.05550066754221916
epoch£º600	 i:6 	 global-step:12006	 l-p:0.05560456961393356
epoch£º600	 i:7 	 global-step:12007	 l-p:0.05551731586456299
epoch£º600	 i:8 	 global-step:12008	 l-p:0.055492982268333435
epoch£º600	 i:9 	 global-step:12009	 l-p:0.05557984858751297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2780, 33.7311, 35.5387],
        [28.2780, 31.3189, 31.3187],
        [28.2780, 28.3911, 28.2929],
        [28.2780, 28.2811, 28.2781]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.0554959699511528 
model_pd.l_d.mean(): -2.709796535782516e-05 
model_pd.lagr.mean(): 0.05546887218952179 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1889], device='cuda:0')), ('power', tensor([-0.0442], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.0554959699511528
epoch£º601	 i:1 	 global-step:12021	 l-p:0.05552662909030914
epoch£º601	 i:2 	 global-step:12022	 l-p:0.055497005581855774
epoch£º601	 i:3 	 global-step:12023	 l-p:0.05557071417570114
epoch£º601	 i:4 	 global-step:12024	 l-p:0.05559173598885536
epoch£º601	 i:5 	 global-step:12025	 l-p:0.05551794916391373
epoch£º601	 i:6 	 global-step:12026	 l-p:0.055558398365974426
epoch£º601	 i:7 	 global-step:12027	 l-p:0.05553774535655975
epoch£º601	 i:8 	 global-step:12028	 l-p:0.055513646453619
epoch£º601	 i:9 	 global-step:12029	 l-p:0.05559956282377243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1905, 28.1905, 28.1905],
        [28.1905, 34.3219, 36.7916],
        [28.1905, 28.3261, 28.2104],
        [28.1905, 33.8696, 35.8991]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.055525317788124084 
model_pd.l_d.mean(): -4.065205212100409e-05 
model_pd.lagr.mean(): 0.05548466742038727 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1537], device='cuda:0')), ('power', tensor([-0.0666], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.055525317788124084
epoch£º602	 i:1 	 global-step:12041	 l-p:0.05562160536646843
epoch£º602	 i:2 	 global-step:12042	 l-p:0.055604904890060425
epoch£º602	 i:3 	 global-step:12043	 l-p:0.05558835715055466
epoch£º602	 i:4 	 global-step:12044	 l-p:0.05552448332309723
epoch£º602	 i:5 	 global-step:12045	 l-p:0.05553414300084114
epoch£º602	 i:6 	 global-step:12046	 l-p:0.05561714991927147
epoch£º602	 i:7 	 global-step:12047	 l-p:0.05552702397108078
epoch£º602	 i:8 	 global-step:12048	 l-p:0.05552978441119194
epoch£º602	 i:9 	 global-step:12049	 l-p:0.055520351976156235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0997, 29.0677, 28.5913],
        [28.0997, 28.1577, 28.1048],
        [28.0997, 28.1001, 28.0997],
        [28.0997, 28.9755, 28.5172]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.05552341416478157 
model_pd.l_d.mean(): -0.00012769363820552826 
model_pd.lagr.mean(): 0.05539572238922119 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1895], device='cuda:0')), ('power', tensor([-0.2268], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.05552341416478157
epoch£º603	 i:1 	 global-step:12061	 l-p:0.05553442984819412
epoch£º603	 i:2 	 global-step:12062	 l-p:0.05559242144227028
epoch£º603	 i:3 	 global-step:12063	 l-p:0.05559185892343521
epoch£º603	 i:4 	 global-step:12064	 l-p:0.055591244250535965
epoch£º603	 i:5 	 global-step:12065	 l-p:0.05554498732089996
epoch£º603	 i:6 	 global-step:12066	 l-p:0.0555410198867321
epoch£º603	 i:7 	 global-step:12067	 l-p:0.05556011199951172
epoch£º603	 i:8 	 global-step:12068	 l-p:0.05573126673698425
epoch£º603	 i:9 	 global-step:12069	 l-p:0.055566929280757904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0156, 36.1331, 40.8011],
        [28.0156, 28.0156, 28.0156],
        [28.0156, 28.4392, 28.1431],
        [28.0156, 29.9684, 29.5448]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.05555255711078644 
model_pd.l_d.mean(): -0.00013323493476491421 
model_pd.lagr.mean(): 0.05541932210326195 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1555], device='cuda:0')), ('power', tensor([-0.2827], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:0.05555255711078644
epoch£º604	 i:1 	 global-step:12081	 l-p:0.05560395494103432
epoch£º604	 i:2 	 global-step:12082	 l-p:0.055636461824178696
epoch£º604	 i:3 	 global-step:12083	 l-p:0.05560439079999924
epoch£º604	 i:4 	 global-step:12084	 l-p:0.0556286983191967
epoch£º604	 i:5 	 global-step:12085	 l-p:0.0555814653635025
epoch£º604	 i:6 	 global-step:12086	 l-p:0.05568809434771538
epoch£º604	 i:7 	 global-step:12087	 l-p:0.055543120950460434
epoch£º604	 i:8 	 global-step:12088	 l-p:0.05553755536675453
epoch£º604	 i:9 	 global-step:12089	 l-p:0.05556115880608559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9485, 33.1647, 34.7951],
        [27.9485, 28.7658, 28.3227],
        [27.9485, 28.2910, 28.0387],
        [27.9485, 28.2178, 28.0095]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.055573124438524246 
model_pd.l_d.mean(): -0.00011551013449206948 
model_pd.lagr.mean(): 0.05545761436223984 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1511], device='cuda:0')), ('power', tensor([-0.3385], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.055573124438524246
epoch£º605	 i:1 	 global-step:12101	 l-p:0.05555743724107742
epoch£º605	 i:2 	 global-step:12102	 l-p:0.05555958300828934
epoch£º605	 i:3 	 global-step:12103	 l-p:0.05557630583643913
epoch£º605	 i:4 	 global-step:12104	 l-p:0.05565541982650757
epoch£º605	 i:5 	 global-step:12105	 l-p:0.055636510252952576
epoch£º605	 i:6 	 global-step:12106	 l-p:0.05567966029047966
epoch£º605	 i:7 	 global-step:12107	 l-p:0.05560761317610741
epoch£º605	 i:8 	 global-step:12108	 l-p:0.05562042072415352
epoch£º605	 i:9 	 global-step:12109	 l-p:0.05558981001377106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9073, 27.9073, 27.9073],
        [27.9073, 35.7716, 40.1621],
        [27.9073, 29.8432, 29.4190],
        [27.9073, 27.9090, 27.9074]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.05556550249457359 
model_pd.l_d.mean(): -7.269999332493171e-05 
model_pd.lagr.mean(): 0.05549280345439911 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1716], device='cuda:0')), ('power', tensor([-0.3969], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.05556550249457359
epoch£º606	 i:1 	 global-step:12121	 l-p:0.055591557174921036
epoch£º606	 i:2 	 global-step:12122	 l-p:0.05559363588690758
epoch£º606	 i:3 	 global-step:12123	 l-p:0.05558009818196297
epoch£º606	 i:4 	 global-step:12124	 l-p:0.05558602139353752
epoch£º606	 i:5 	 global-step:12125	 l-p:0.05564698576927185
epoch£º606	 i:6 	 global-step:12126	 l-p:0.055570974946022034
epoch£º606	 i:7 	 global-step:12127	 l-p:0.05558136850595474
epoch£º606	 i:8 	 global-step:12128	 l-p:0.05573955923318863
epoch£º606	 i:9 	 global-step:12129	 l-p:0.055655330419540405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9000, 27.9484, 27.9039],
        [27.9000, 30.0992, 29.7503],
        [27.9000, 27.9066, 27.9002],
        [27.9000, 29.6656, 29.2056]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.05557998642325401 
model_pd.l_d.mean(): -4.400271336635342e-06 
model_pd.lagr.mean(): 0.05557558685541153 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1454], device='cuda:0')), ('power', tensor([-0.3690], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.05557998642325401
epoch£º607	 i:1 	 global-step:12141	 l-p:0.055671799927949905
epoch£º607	 i:2 	 global-step:12142	 l-p:0.05557090789079666
epoch£º607	 i:3 	 global-step:12143	 l-p:0.05561302229762077
epoch£º607	 i:4 	 global-step:12144	 l-p:0.05558577552437782
epoch£º607	 i:5 	 global-step:12145	 l-p:0.05557825788855553
epoch£º607	 i:6 	 global-step:12146	 l-p:0.05561438575387001
epoch£º607	 i:7 	 global-step:12147	 l-p:0.055643074214458466
epoch£º607	 i:8 	 global-step:12148	 l-p:0.055577997118234634
epoch£º607	 i:9 	 global-step:12149	 l-p:0.05565747991204262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9260, 27.9331, 27.9262],
        [27.9260, 27.9544, 27.9277],
        [27.9260, 28.3247, 28.0417],
        [27.9260, 35.6073, 39.7832]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.055637940764427185 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055637940764427185 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0854], device='cuda:0')), ('power', tensor([-0.2412], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.055637940764427185
epoch£º608	 i:1 	 global-step:12161	 l-p:0.05556250363588333
epoch£º608	 i:2 	 global-step:12162	 l-p:0.05558348074555397
epoch£º608	 i:3 	 global-step:12163	 l-p:0.05557849258184433
epoch£º608	 i:4 	 global-step:12164	 l-p:0.05557387322187424
epoch£º608	 i:5 	 global-step:12165	 l-p:0.05565476417541504
epoch£º608	 i:6 	 global-step:12166	 l-p:0.05553792044520378
epoch£º608	 i:7 	 global-step:12167	 l-p:0.055628664791584015
epoch£º608	 i:8 	 global-step:12168	 l-p:0.05561211332678795
epoch£º608	 i:9 	 global-step:12169	 l-p:0.05565093457698822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9676, 27.9676, 27.9675],
        [27.9676, 28.0616, 27.9787],
        [27.9676, 35.6609, 39.8434],
        [27.9676, 28.8071, 28.3584]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.055658698081970215 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055658698081970215 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0652], device='cuda:0')), ('power', tensor([-0.1921], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.055658698081970215
epoch£º609	 i:1 	 global-step:12181	 l-p:0.05555180460214615
epoch£º609	 i:2 	 global-step:12182	 l-p:0.055576980113983154
epoch£º609	 i:3 	 global-step:12183	 l-p:0.05559414625167847
epoch£º609	 i:4 	 global-step:12184	 l-p:0.05555131658911705
epoch£º609	 i:5 	 global-step:12185	 l-p:0.05554948374629021
epoch£º609	 i:6 	 global-step:12186	 l-p:0.055663928389549255
epoch£º609	 i:7 	 global-step:12187	 l-p:0.05563889071345329
epoch£º609	 i:8 	 global-step:12188	 l-p:0.0555846281349659
epoch£º609	 i:9 	 global-step:12189	 l-p:0.055557772517204285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0148, 28.8558, 28.4063],
        [28.0148, 28.0164, 28.0148],
        [28.0148, 28.0149, 28.0148],
        [28.0148, 28.2104, 28.0510]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.055562298744916916 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055562298744916916 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1451], device='cuda:0')), ('power', tensor([-0.2663], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.055562298744916916
epoch£º610	 i:1 	 global-step:12201	 l-p:0.05552588403224945
epoch£º610	 i:2 	 global-step:12202	 l-p:0.055641286075115204
epoch£º610	 i:3 	 global-step:12203	 l-p:0.055640820413827896
epoch£º610	 i:4 	 global-step:12204	 l-p:0.055588558316230774
epoch£º610	 i:5 	 global-step:12205	 l-p:0.05553395301103592
epoch£º610	 i:6 	 global-step:12206	 l-p:0.0556211918592453
epoch£º610	 i:7 	 global-step:12207	 l-p:0.05555051565170288
epoch£º610	 i:8 	 global-step:12208	 l-p:0.055521972477436066
epoch£º610	 i:9 	 global-step:12209	 l-p:0.05564046651124954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0640, 30.0834, 29.6750],
        [28.0640, 36.2910, 41.0800],
        [28.0640, 37.9301, 44.7729],
        [28.0640, 28.0692, 28.0642]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.05554281547665596 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554281547665596 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1666], device='cuda:0')), ('power', tensor([-0.2939], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.05554281547665596
epoch£º611	 i:1 	 global-step:12221	 l-p:0.05554317682981491
epoch£º611	 i:2 	 global-step:12222	 l-p:0.055550966411828995
epoch£º611	 i:3 	 global-step:12223	 l-p:0.05560469999909401
epoch£º611	 i:4 	 global-step:12224	 l-p:0.05559982359409332
epoch£º611	 i:5 	 global-step:12225	 l-p:0.05571209266781807
epoch£º611	 i:6 	 global-step:12226	 l-p:0.05551885813474655
epoch£º611	 i:7 	 global-step:12227	 l-p:0.055586352944374084
epoch£º611	 i:8 	 global-step:12228	 l-p:0.055541422218084335
epoch£º611	 i:9 	 global-step:12229	 l-p:0.05552360787987709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1142, 33.3581, 34.9945],
        [28.1142, 28.1143, 28.1142],
        [28.1142, 29.8408, 29.3679],
        [28.1142, 28.1186, 28.1143]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.05561825633049011 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05561825633049011 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1276], device='cuda:0')), ('power', tensor([-0.0968], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.05561825633049011
epoch£º612	 i:1 	 global-step:12241	 l-p:0.05553528666496277
epoch£º612	 i:2 	 global-step:12242	 l-p:0.05553535744547844
epoch£º612	 i:3 	 global-step:12243	 l-p:0.05550846457481384
epoch£º612	 i:4 	 global-step:12244	 l-p:0.05558061599731445
epoch£º612	 i:5 	 global-step:12245	 l-p:0.055568329989910126
epoch£º612	 i:6 	 global-step:12246	 l-p:0.05565638840198517
epoch£º612	 i:7 	 global-step:12247	 l-p:0.05551699176430702
epoch£º612	 i:8 	 global-step:12248	 l-p:0.05552363768219948
epoch£º612	 i:9 	 global-step:12249	 l-p:0.05557628348469734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1648, 28.1654, 28.1647],
        [28.1648, 28.3003, 28.1847],
        [28.1648, 28.1806, 28.1654],
        [28.1648, 28.5176, 28.2590]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.05551997572183609 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05551997572183609 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1707], device='cuda:0')), ('power', tensor([-0.1144], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.05551997572183609
epoch£º613	 i:1 	 global-step:12261	 l-p:0.05552532151341438
epoch£º613	 i:2 	 global-step:12262	 l-p:0.055565040558576584
epoch£º613	 i:3 	 global-step:12263	 l-p:0.05554479360580444
epoch£º613	 i:4 	 global-step:12264	 l-p:0.05550193786621094
epoch£º613	 i:5 	 global-step:12265	 l-p:0.05568521469831467
epoch£º613	 i:6 	 global-step:12266	 l-p:0.05551472678780556
epoch£º613	 i:7 	 global-step:12267	 l-p:0.055566929280757904
epoch£º613	 i:8 	 global-step:12268	 l-p:0.05550500750541687
epoch£º613	 i:9 	 global-step:12269	 l-p:0.055586136877536774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2155, 32.8073, 33.8740],
        [28.2155, 30.6352, 30.3509],
        [28.2155, 28.2324, 28.2163],
        [28.2155, 28.2238, 28.2158]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.05551667511463165 
model_pd.l_d.mean(): -5.301402428159463e-09 
model_pd.lagr.mean(): 0.055516671389341354 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1529], device='cuda:0')), ('power', tensor([-0.0504], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.05551667511463165
epoch£º614	 i:1 	 global-step:12281	 l-p:0.055515725165605545
epoch£º614	 i:2 	 global-step:12282	 l-p:0.05553072690963745
epoch£º614	 i:3 	 global-step:12283	 l-p:0.05557144433259964
epoch£º614	 i:4 	 global-step:12284	 l-p:0.05553760752081871
epoch£º614	 i:5 	 global-step:12285	 l-p:0.05559970811009407
epoch£º614	 i:6 	 global-step:12286	 l-p:0.05549339950084686
epoch£º614	 i:7 	 global-step:12287	 l-p:0.05550043284893036
epoch£º614	 i:8 	 global-step:12288	 l-p:0.055622853338718414
epoch£º614	 i:9 	 global-step:12289	 l-p:0.055522043257951736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2665, 30.9106, 30.7165],
        [28.2665, 30.6169, 30.3047],
        [28.2665, 34.5825, 37.2273],
        [28.2665, 28.2666, 28.2665]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.05561840906739235 
model_pd.l_d.mean(): 6.084421215746261e-07 
model_pd.lagr.mean(): 0.055619016289711 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.2318e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1103], device='cuda:0')), ('power', tensor([0.0684], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.05561840906739235
epoch£º615	 i:1 	 global-step:12301	 l-p:0.05550869181752205
epoch£º615	 i:2 	 global-step:12302	 l-p:0.055545371025800705
epoch£º615	 i:3 	 global-step:12303	 l-p:0.05549017712473869
epoch£º615	 i:4 	 global-step:12304	 l-p:0.05547820031642914
epoch£º615	 i:5 	 global-step:12305	 l-p:0.055477675050497055
epoch£º615	 i:6 	 global-step:12306	 l-p:0.05548259615898132
epoch£º615	 i:7 	 global-step:12307	 l-p:0.055498283356428146
epoch£º615	 i:8 	 global-step:12308	 l-p:0.05565793067216873
epoch£º615	 i:9 	 global-step:12309	 l-p:0.055549781769514084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3162, 28.3264, 28.3166],
        [28.3162, 28.5040, 28.3499],
        [28.3162, 35.6669, 39.4040],
        [28.3162, 32.2675, 32.8364]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.05561939626932144 
model_pd.l_d.mean(): 6.380643753800541e-06 
model_pd.lagr.mean(): 0.05562577769160271 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.1130e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0485], device='cuda:0')), ('power', tensor([0.2074], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.05561939626932144
epoch£º616	 i:1 	 global-step:12321	 l-p:0.05547195300459862
epoch£º616	 i:2 	 global-step:12322	 l-p:0.055639833211898804
epoch£º616	 i:3 	 global-step:12323	 l-p:0.05547770857810974
epoch£º616	 i:4 	 global-step:12324	 l-p:0.055531878024339676
epoch£º616	 i:5 	 global-step:12325	 l-p:0.05549560859799385
epoch£º616	 i:6 	 global-step:12326	 l-p:0.055517349392175674
epoch£º616	 i:7 	 global-step:12327	 l-p:0.05549035966396332
epoch£º616	 i:8 	 global-step:12328	 l-p:0.05547364428639412
epoch£º616	 i:9 	 global-step:12329	 l-p:0.05549173802137375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3613, 28.7368, 28.4651],
        [28.3613, 29.5993, 29.0905],
        [28.3613, 38.5560, 45.7647],
        [28.3613, 28.7288, 28.4615]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.05550350621342659 
model_pd.l_d.mean(): 7.309815373446327e-06 
model_pd.lagr.mean(): 0.05551081523299217 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.1329e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1577], device='cuda:0')), ('power', tensor([0.0955], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.05550350621342659
epoch£º617	 i:1 	 global-step:12341	 l-p:0.05556759610772133
epoch£º617	 i:2 	 global-step:12342	 l-p:0.05553848296403885
epoch£º617	 i:3 	 global-step:12343	 l-p:0.05553208291530609
epoch£º617	 i:4 	 global-step:12344	 l-p:0.05549570918083191
epoch£º617	 i:5 	 global-step:12345	 l-p:0.055467698723077774
epoch£º617	 i:6 	 global-step:12346	 l-p:0.055544979870319366
epoch£º617	 i:7 	 global-step:12347	 l-p:0.05547419935464859
epoch£º617	 i:8 	 global-step:12348	 l-p:0.05551297217607498
epoch£º617	 i:9 	 global-step:12349	 l-p:0.055488672107458115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3967, 30.9660, 30.7333],
        [28.3967, 28.7826, 28.5051],
        [28.3967, 38.4460, 45.4543],
        [28.3967, 30.1586, 29.6836]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.05549295246601105 
model_pd.l_d.mean(): 1.8803359125740826e-05 
model_pd.lagr.mean(): 0.055511754006147385 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1587], device='cuda:0')), ('power', tensor([0.1317], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.05549295246601105
epoch£º618	 i:1 	 global-step:12361	 l-p:0.05545586347579956
epoch£º618	 i:2 	 global-step:12362	 l-p:0.055516403168439865
epoch£º618	 i:3 	 global-step:12363	 l-p:0.05551458150148392
epoch£º618	 i:4 	 global-step:12364	 l-p:0.05552643537521362
epoch£º618	 i:5 	 global-step:12365	 l-p:0.05556303262710571
epoch£º618	 i:6 	 global-step:12366	 l-p:0.05546204373240471
epoch£º618	 i:7 	 global-step:12367	 l-p:0.05545079708099365
epoch£º618	 i:8 	 global-step:12368	 l-p:0.05562460795044899
epoch£º618	 i:9 	 global-step:12369	 l-p:0.05545974522829056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4177, 28.7860, 28.5181],
        [28.4177, 29.2888, 28.8284],
        [28.4177, 29.3338, 28.8636],
        [28.4177, 35.2497, 38.4080]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.055481623858213425 
model_pd.l_d.mean(): 4.3905376514885575e-05 
model_pd.lagr.mean(): 0.0555255301296711 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1567], device='cuda:0')), ('power', tensor([0.1964], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.055481623858213425
epoch£º619	 i:1 	 global-step:12381	 l-p:0.055628690868616104
epoch£º619	 i:2 	 global-step:12382	 l-p:0.055521633476018906
epoch£º619	 i:3 	 global-step:12383	 l-p:0.055524490773677826
epoch£º619	 i:4 	 global-step:12384	 l-p:0.05545089766383171
epoch£º619	 i:5 	 global-step:12385	 l-p:0.05555145442485809
epoch£º619	 i:6 	 global-step:12386	 l-p:0.05549699813127518
epoch£º619	 i:7 	 global-step:12387	 l-p:0.05545825883746147
epoch£º619	 i:8 	 global-step:12388	 l-p:0.05545863136649132
epoch£º619	 i:9 	 global-step:12389	 l-p:0.0554676316678524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4199, 28.5448, 28.4373],
        [28.4199, 29.6236, 29.1155],
        [28.4199, 28.4199, 28.4199],
        [28.4199, 33.9884, 35.8860]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.055451493710279465 
model_pd.l_d.mean(): 4.063641972607002e-05 
model_pd.lagr.mean(): 0.05549212917685509 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2143], device='cuda:0')), ('power', tensor([0.1308], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.055451493710279465
epoch£º620	 i:1 	 global-step:12401	 l-p:0.05547692999243736
epoch£º620	 i:2 	 global-step:12402	 l-p:0.0554506853222847
epoch£º620	 i:3 	 global-step:12403	 l-p:0.05551028996706009
epoch£º620	 i:4 	 global-step:12404	 l-p:0.05549084022641182
epoch£º620	 i:5 	 global-step:12405	 l-p:0.05557293817400932
epoch£º620	 i:6 	 global-step:12406	 l-p:0.05563613772392273
epoch£º620	 i:7 	 global-step:12407	 l-p:0.055472295731306076
epoch£º620	 i:8 	 global-step:12408	 l-p:0.0554623007774353
epoch£º620	 i:9 	 global-step:12409	 l-p:0.05553114786744118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4014, 34.8346, 37.5800],
        [28.4014, 28.4057, 28.4015],
        [28.4014, 28.4014, 28.4014],
        [28.4014, 28.4015, 28.4014]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.05545714125037193 
model_pd.l_d.mean(): 2.1081608792883344e-05 
model_pd.lagr.mean(): 0.05547822266817093 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2125], device='cuda:0')), ('power', tensor([0.0535], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.05545714125037193
epoch£º621	 i:1 	 global-step:12421	 l-p:0.05554632097482681
epoch£º621	 i:2 	 global-step:12422	 l-p:0.05548572912812233
epoch£º621	 i:3 	 global-step:12423	 l-p:0.05552230775356293
epoch£º621	 i:4 	 global-step:12424	 l-p:0.05548296496272087
epoch£º621	 i:5 	 global-step:12425	 l-p:0.05559590086340904
epoch£º621	 i:6 	 global-step:12426	 l-p:0.055534303188323975
epoch£º621	 i:7 	 global-step:12427	 l-p:0.05547235533595085
epoch£º621	 i:8 	 global-step:12428	 l-p:0.05550447851419449
epoch£º621	 i:9 	 global-step:12429	 l-p:0.055509671568870544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3622, 28.3622, 28.3622],
        [28.3622, 30.3407, 29.9116],
        [28.3622, 32.9791, 34.0519],
        [28.3622, 29.7612, 29.2509]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.05546912923455238 
model_pd.l_d.mean(): 2.387701897532679e-05 
model_pd.lagr.mean(): 0.055493004620075226 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1977], device='cuda:0')), ('power', tensor([0.0514], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.05546912923455238
epoch£º622	 i:1 	 global-step:12441	 l-p:0.05566461384296417
epoch£º622	 i:2 	 global-step:12442	 l-p:0.05548877641558647
epoch£º622	 i:3 	 global-step:12443	 l-p:0.05546860024333
epoch£º622	 i:4 	 global-step:12444	 l-p:0.05547165870666504
epoch£º622	 i:5 	 global-step:12445	 l-p:0.055570829659700394
epoch£º622	 i:6 	 global-step:12446	 l-p:0.05549537017941475
epoch£º622	 i:7 	 global-step:12447	 l-p:0.055498044937849045
epoch£º622	 i:8 	 global-step:12448	 l-p:0.05549824237823486
epoch£º622	 i:9 	 global-step:12449	 l-p:0.055583324283361435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3043, 28.4967, 28.3393],
        [28.3043, 29.1546, 28.7002],
        [28.3043, 28.3366, 28.3063],
        [28.3043, 28.3050, 28.3043]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.055519454181194305 
model_pd.l_d.mean(): -4.679507128457772e-06 
model_pd.lagr.mean(): 0.05551477521657944 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1773], device='cuda:0')), ('power', tensor([-0.0092], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.055519454181194305
epoch£º623	 i:1 	 global-step:12461	 l-p:0.05549181252717972
epoch£º623	 i:2 	 global-step:12462	 l-p:0.05559607222676277
epoch£º623	 i:3 	 global-step:12463	 l-p:0.05554058775305748
epoch£º623	 i:4 	 global-step:12464	 l-p:0.05548397824168205
epoch£º623	 i:5 	 global-step:12465	 l-p:0.05547577887773514
epoch£º623	 i:6 	 global-step:12466	 l-p:0.05557463690638542
epoch£º623	 i:7 	 global-step:12467	 l-p:0.055633895099163055
epoch£º623	 i:8 	 global-step:12468	 l-p:0.055527567863464355
epoch£º623	 i:9 	 global-step:12469	 l-p:0.05549611896276474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2331, 28.2757, 28.2363],
        [28.2331, 37.2112, 42.8815],
        [28.2331, 28.2414, 28.2334],
        [28.2331, 28.2331, 28.2331]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.055511560291051865 
model_pd.l_d.mean(): -2.4986053176689893e-05 
model_pd.lagr.mean(): 0.05548657476902008 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1610], device='cuda:0')), ('power', tensor([-0.0477], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.055511560291051865
epoch£º624	 i:1 	 global-step:12481	 l-p:0.05549981817603111
epoch£º624	 i:2 	 global-step:12482	 l-p:0.05560576170682907
epoch£º624	 i:3 	 global-step:12483	 l-p:0.05551079660654068
epoch£º624	 i:4 	 global-step:12484	 l-p:0.055565014481544495
epoch£º624	 i:5 	 global-step:12485	 l-p:0.05553540959954262
epoch£º624	 i:6 	 global-step:12486	 l-p:0.05551961809396744
epoch£º624	 i:7 	 global-step:12487	 l-p:0.05563153326511383
epoch£º624	 i:8 	 global-step:12488	 l-p:0.05557748302817345
epoch£º624	 i:9 	 global-step:12489	 l-p:0.0555361732840538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1556, 28.1556, 28.1556],
        [28.1556, 28.2195, 28.1616],
        [28.1556, 28.1615, 28.1557],
        [28.1556, 31.2675, 31.3136]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.05553777515888214 
model_pd.l_d.mean(): -4.7033503506099805e-05 
model_pd.lagr.mean(): 0.05549074336886406 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1457], device='cuda:0')), ('power', tensor([-0.0939], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.05553777515888214
epoch£º625	 i:1 	 global-step:12501	 l-p:0.055613867938518524
epoch£º625	 i:2 	 global-step:12502	 l-p:0.05556686595082283
epoch£º625	 i:3 	 global-step:12503	 l-p:0.05550147965550423
epoch£º625	 i:4 	 global-step:12504	 l-p:0.05553947761654854
epoch£º625	 i:5 	 global-step:12505	 l-p:0.05559537932276726
epoch£º625	 i:6 	 global-step:12506	 l-p:0.05564136803150177
epoch£º625	 i:7 	 global-step:12507	 l-p:0.05559239909052849
epoch£º625	 i:8 	 global-step:12508	 l-p:0.05552038550376892
epoch£º625	 i:9 	 global-step:12509	 l-p:0.05554305016994476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0804, 28.3218, 28.1313],
        [28.0804, 30.0762, 29.6610],
        [28.0804, 28.0804, 28.0804],
        [28.0804, 37.1493, 42.9642]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.055522479116916656 
model_pd.l_d.mean(): -0.00011186584742972627 
model_pd.lagr.mean(): 0.055410612374544144 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1912], device='cuda:0')), ('power', tensor([-0.2548], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.055522479116916656
epoch£º626	 i:1 	 global-step:12521	 l-p:0.05561445653438568
epoch£º626	 i:2 	 global-step:12522	 l-p:0.05553937330842018
epoch£º626	 i:3 	 global-step:12523	 l-p:0.055618125945329666
epoch£º626	 i:4 	 global-step:12524	 l-p:0.05557309091091156
epoch£º626	 i:5 	 global-step:12525	 l-p:0.055562589317560196
epoch£º626	 i:6 	 global-step:12526	 l-p:0.05566265434026718
epoch£º626	 i:7 	 global-step:12527	 l-p:0.05553538724780083
epoch£º626	 i:8 	 global-step:12528	 l-p:0.05557649955153465
epoch£º626	 i:9 	 global-step:12529	 l-p:0.05559404194355011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0170, 28.6805, 28.2826],
        [28.0170, 31.1555, 31.2252],
        [28.0170, 28.6137, 28.2403],
        [28.0170, 28.1939, 28.0477]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.05554097890853882 
model_pd.l_d.mean(): -0.00011452139733592048 
model_pd.lagr.mean(): 0.0554264560341835 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1776], device='cuda:0')), ('power', tensor([-0.3347], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.05554097890853882
epoch£º627	 i:1 	 global-step:12541	 l-p:0.05560183525085449
epoch£º627	 i:2 	 global-step:12542	 l-p:0.05554492771625519
epoch£º627	 i:3 	 global-step:12543	 l-p:0.055537670850753784
epoch£º627	 i:4 	 global-step:12544	 l-p:0.055712148547172546
epoch£º627	 i:5 	 global-step:12545	 l-p:0.05562601983547211
epoch£º627	 i:6 	 global-step:12546	 l-p:0.055581726133823395
epoch£º627	 i:7 	 global-step:12547	 l-p:0.05555488541722298
epoch£º627	 i:8 	 global-step:12548	 l-p:0.0555659756064415
epoch£º627	 i:9 	 global-step:12549	 l-p:0.05564658343791962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9744, 30.3951, 30.1223],
        [27.9744, 27.9760, 27.9744],
        [27.9744, 27.9786, 27.9745],
        [27.9744, 28.3775, 28.0921]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.05554703250527382 
model_pd.l_d.mean(): -7.889403059380129e-05 
model_pd.lagr.mean(): 0.05546813830733299 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1747], device='cuda:0')), ('power', tensor([-0.3617], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.05554703250527382
epoch£º628	 i:1 	 global-step:12561	 l-p:0.055564574897289276
epoch£º628	 i:2 	 global-step:12562	 l-p:0.055562857538461685
epoch£º628	 i:3 	 global-step:12563	 l-p:0.055580977350473404
epoch£º628	 i:4 	 global-step:12564	 l-p:0.0556640662252903
epoch£º628	 i:5 	 global-step:12565	 l-p:0.055543720722198486
epoch£º628	 i:6 	 global-step:12566	 l-p:0.055605415254831314
epoch£º628	 i:7 	 global-step:12567	 l-p:0.05566681921482086
epoch£º628	 i:8 	 global-step:12568	 l-p:0.055629003793001175
epoch£º628	 i:9 	 global-step:12569	 l-p:0.05561130493879318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9607, 27.9649, 27.9608],
        [27.9607, 29.9040, 29.4799],
        [27.9607, 27.9826, 27.9618],
        [27.9607, 35.6521, 39.8335]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.05553656443953514 
model_pd.l_d.mean(): -3.1993535230867565e-05 
model_pd.lagr.mean(): 0.05550457164645195 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.8731e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2136], device='cuda:0')), ('power', tensor([-0.4051], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.05553656443953514
epoch£º629	 i:1 	 global-step:12581	 l-p:0.055607497692108154
epoch£º629	 i:2 	 global-step:12582	 l-p:0.055624693632125854
epoch£º629	 i:3 	 global-step:12583	 l-p:0.05564780905842781
epoch£º629	 i:4 	 global-step:12584	 l-p:0.05556033179163933
epoch£º629	 i:5 	 global-step:12585	 l-p:0.05556827411055565
epoch£º629	 i:6 	 global-step:12586	 l-p:0.05557827278971672
epoch£º629	 i:7 	 global-step:12587	 l-p:0.05558600649237633
epoch£º629	 i:8 	 global-step:12588	 l-p:0.0556771494448185
epoch£º629	 i:9 	 global-step:12589	 l-p:0.05558617413043976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9805, 27.9806, 27.9804],
        [27.9805, 29.9225, 29.4974],
        [27.9805, 30.5961, 30.4040],
        [27.9805, 27.9804, 27.9805]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.05559847876429558 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05559847876429558 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1365], device='cuda:0')), ('power', tensor([-0.2526], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.05559847876429558
epoch£º630	 i:1 	 global-step:12601	 l-p:0.05559135228395462
epoch£º630	 i:2 	 global-step:12602	 l-p:0.05555843189358711
epoch£º630	 i:3 	 global-step:12603	 l-p:0.05562489852309227
epoch£º630	 i:4 	 global-step:12604	 l-p:0.055572398006916046
epoch£º630	 i:5 	 global-step:12605	 l-p:0.05555630475282669
epoch£º630	 i:6 	 global-step:12606	 l-p:0.05555076524615288
epoch£º630	 i:7 	 global-step:12607	 l-p:0.055604852735996246
epoch£º630	 i:8 	 global-step:12608	 l-p:0.0556303970515728
epoch£º630	 i:9 	 global-step:12609	 l-p:0.05561906099319458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0228, 28.1086, 28.0324],
        [28.0228, 33.2487, 34.8794],
        [28.0228, 28.0228, 28.0228],
        [28.0228, 28.9880, 28.5130]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.055526938289403915 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055526938289403915 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2067], device='cuda:0')), ('power', tensor([-0.3457], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.055526938289403915
epoch£º631	 i:1 	 global-step:12621	 l-p:0.05563734844326973
epoch£º631	 i:2 	 global-step:12622	 l-p:0.0555618554353714
epoch£º631	 i:3 	 global-step:12623	 l-p:0.055652979761362076
epoch£º631	 i:4 	 global-step:12624	 l-p:0.05557765066623688
epoch£º631	 i:5 	 global-step:12625	 l-p:0.055653009563684464
epoch£º631	 i:6 	 global-step:12626	 l-p:0.05554826557636261
epoch£º631	 i:7 	 global-step:12627	 l-p:0.05551965534687042
epoch£º631	 i:8 	 global-step:12628	 l-p:0.05558215081691742
epoch£º631	 i:9 	 global-step:12629	 l-p:0.05554959177970886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0738, 28.3957, 28.1551],
        [28.0738, 28.1609, 28.0836],
        [28.0738, 28.0990, 28.0751],
        [28.0738, 28.0936, 28.0747]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.05563368275761604 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05563368275761604 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1192], device='cuda:0')), ('power', tensor([-0.1503], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.05563368275761604
epoch£º632	 i:1 	 global-step:12641	 l-p:0.055631883442401886
epoch£º632	 i:2 	 global-step:12642	 l-p:0.05552830919623375
epoch£º632	 i:3 	 global-step:12643	 l-p:0.055519722402095795
epoch£º632	 i:4 	 global-step:12644	 l-p:0.05557040497660637
epoch£º632	 i:5 	 global-step:12645	 l-p:0.05552862957119942
epoch£º632	 i:6 	 global-step:12646	 l-p:0.0555514320731163
epoch£º632	 i:7 	 global-step:12647	 l-p:0.055575743317604065
epoch£º632	 i:8 	 global-step:12648	 l-p:0.05552858114242554
epoch£º632	 i:9 	 global-step:12649	 l-p:0.05563187971711159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1278, 28.2008, 28.1352],
        [28.1278, 31.0274, 30.9622],
        [28.1278, 28.9153, 28.4785],
        [28.1278, 38.4155, 45.8034]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.05558371916413307 
model_pd.l_d.mean(): -3.4268573756435217e-08 
model_pd.lagr.mean(): 0.055583685636520386 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1322], device='cuda:0')), ('power', tensor([-0.0953], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.05558371916413307
epoch£º633	 i:1 	 global-step:12661	 l-p:0.05553846061229706
epoch£º633	 i:2 	 global-step:12662	 l-p:0.055527620017528534
epoch£º633	 i:3 	 global-step:12663	 l-p:0.05562185123562813
epoch£º633	 i:4 	 global-step:12664	 l-p:0.05555608123540878
epoch£º633	 i:5 	 global-step:12665	 l-p:0.05552588403224945
epoch£º633	 i:6 	 global-step:12666	 l-p:0.05551866441965103
epoch£º633	 i:7 	 global-step:12667	 l-p:0.0555579848587513
epoch£º633	 i:8 	 global-step:12668	 l-p:0.05550672486424446
epoch£º633	 i:9 	 global-step:12669	 l-p:0.05565028637647629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1830, 28.1830, 28.1830],
        [28.1830, 28.2014, 28.1838],
        [28.1830, 28.8897, 28.4764],
        [28.1830, 28.8015, 28.4188]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.05555004999041557 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555004999041557 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1247], device='cuda:0')), ('power', tensor([-0.0774], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.05555004999041557
epoch£º634	 i:1 	 global-step:12681	 l-p:0.05551096796989441
epoch£º634	 i:2 	 global-step:12682	 l-p:0.05555584281682968
epoch£º634	 i:3 	 global-step:12683	 l-p:0.05552796274423599
epoch£º634	 i:4 	 global-step:12684	 l-p:0.055552225559949875
epoch£º634	 i:5 	 global-step:12685	 l-p:0.05549350008368492
epoch£º634	 i:6 	 global-step:12686	 l-p:0.05558297038078308
epoch£º634	 i:7 	 global-step:12687	 l-p:0.05551587790250778
epoch£º634	 i:8 	 global-step:12688	 l-p:0.05568021163344383
epoch£º634	 i:9 	 global-step:12689	 l-p:0.055503424257040024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2387, 29.4710, 28.9644],
        [28.2387, 29.3361, 28.8398],
        [28.2387, 28.2430, 28.2388],
        [28.2387, 32.5218, 33.3426]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.055500712245702744 
model_pd.l_d.mean(): -5.410428016716651e-08 
model_pd.lagr.mean(): 0.05550065636634827 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.8767e-08], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1735], device='cuda:0')), ('power', tensor([-0.0322], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.055500712245702744
epoch£º635	 i:1 	 global-step:12701	 l-p:0.055711694061756134
epoch£º635	 i:2 	 global-step:12702	 l-p:0.055523861199617386
epoch£º635	 i:3 	 global-step:12703	 l-p:0.05553864687681198
epoch£º635	 i:4 	 global-step:12704	 l-p:0.05552305281162262
epoch£º635	 i:5 	 global-step:12705	 l-p:0.055563993752002716
epoch£º635	 i:6 	 global-step:12706	 l-p:0.0554896742105484
epoch£º635	 i:7 	 global-step:12707	 l-p:0.05551113188266754
epoch£º635	 i:8 	 global-step:12708	 l-p:0.055493392050266266
epoch£º635	 i:9 	 global-step:12709	 l-p:0.05550326406955719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2939, 28.6605, 28.3939],
        [28.2939, 28.3865, 28.3047],
        [28.2939, 33.5781, 35.2302],
        [28.2939, 28.2944, 28.2939]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.05550049617886543 
model_pd.l_d.mean(): -4.356863598786731e-07 
model_pd.lagr.mean(): 0.05550006031990051 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.9131e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1704], device='cuda:0')), ('power', tensor([-0.0399], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.05550049617886543
epoch£º636	 i:1 	 global-step:12721	 l-p:0.05548091232776642
epoch£º636	 i:2 	 global-step:12722	 l-p:0.05553656071424484
epoch£º636	 i:3 	 global-step:12723	 l-p:0.055512912571430206
epoch£º636	 i:4 	 global-step:12724	 l-p:0.05549604445695877
epoch£º636	 i:5 	 global-step:12725	 l-p:0.05557883158326149
epoch£º636	 i:6 	 global-step:12726	 l-p:0.05555835738778114
epoch£º636	 i:7 	 global-step:12727	 l-p:0.05559837818145752
epoch£º636	 i:8 	 global-step:12728	 l-p:0.05551009625196457
epoch£º636	 i:9 	 global-step:12729	 l-p:0.05547512322664261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3474, 30.1085, 29.6348],
        [28.3474, 38.6788, 46.0734],
        [28.3474, 29.3401, 28.8566],
        [28.3474, 28.4355, 28.3573]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.055570825934410095 
model_pd.l_d.mean(): 5.861130375706125e-06 
model_pd.lagr.mean(): 0.055576685816049576 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.3446e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1341], device='cuda:0')), ('power', tensor([0.1241], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.055570825934410095
epoch£º637	 i:1 	 global-step:12741	 l-p:0.055617332458496094
epoch£º637	 i:2 	 global-step:12742	 l-p:0.055511653423309326
epoch£º637	 i:3 	 global-step:12743	 l-p:0.055473651736974716
epoch£º637	 i:4 	 global-step:12744	 l-p:0.05553071200847626
epoch£º637	 i:5 	 global-step:12745	 l-p:0.05546772852540016
epoch£º637	 i:6 	 global-step:12746	 l-p:0.055535297840833664
epoch£º637	 i:7 	 global-step:12747	 l-p:0.055477894842624664
epoch£º637	 i:8 	 global-step:12748	 l-p:0.05547657981514931
epoch£º637	 i:9 	 global-step:12749	 l-p:0.055483121424913406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3933, 28.3946, 28.3933],
        [28.3933, 28.3933, 28.3933],
        [28.3933, 36.2084, 40.4569],
        [28.3933, 37.7272, 43.8114]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.055478230118751526 
model_pd.l_d.mean(): 2.257467531308066e-05 
model_pd.lagr.mean(): 0.055500805377960205 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1557], device='cuda:0')), ('power', tensor([0.2073], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.055478230118751526
epoch£º638	 i:1 	 global-step:12761	 l-p:0.05548468604683876
epoch£º638	 i:2 	 global-step:12762	 l-p:0.05547650158405304
epoch£º638	 i:3 	 global-step:12763	 l-p:0.05556993931531906
epoch£º638	 i:4 	 global-step:12764	 l-p:0.05554848164319992
epoch£º638	 i:5 	 global-step:12765	 l-p:0.05551864579319954
epoch£º638	 i:6 	 global-step:12766	 l-p:0.05546146258711815
epoch£º638	 i:7 	 global-step:12767	 l-p:0.05550629645586014
epoch£º638	 i:8 	 global-step:12768	 l-p:0.055518988519907
epoch£º638	 i:9 	 global-step:12769	 l-p:0.05550055205821991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4248, 38.0201, 44.4310],
        [28.4248, 28.4434, 28.4256],
        [28.4248, 30.5339, 30.1372],
        [28.4248, 30.2732, 29.8132]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.05561993643641472 
model_pd.l_d.mean(): 6.675127224298194e-05 
model_pd.lagr.mean(): 0.05568668618798256 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0186], device='cuda:0')), ('power', tensor([0.3507], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.05561993643641472
epoch£º639	 i:1 	 global-step:12781	 l-p:0.055468253791332245
epoch£º639	 i:2 	 global-step:12782	 l-p:0.05548824369907379
epoch£º639	 i:3 	 global-step:12783	 l-p:0.05545248091220856
epoch£º639	 i:4 	 global-step:12784	 l-p:0.055454399436712265
epoch£º639	 i:5 	 global-step:12785	 l-p:0.05549027398228645
epoch£º639	 i:6 	 global-step:12786	 l-p:0.0554671473801136
epoch£º639	 i:7 	 global-step:12787	 l-p:0.05544402077794075
epoch£º639	 i:8 	 global-step:12788	 l-p:0.05562853813171387
epoch£º639	 i:9 	 global-step:12789	 l-p:0.05550418421626091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4362, 30.4202, 29.9899],
        [28.4362, 31.3575, 31.2856],
        [28.4362, 28.4362, 28.4362],
        [28.4362, 28.5293, 28.4471]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.055492982268333435 
model_pd.l_d.mean(): 7.197025115601718e-05 
model_pd.lagr.mean(): 0.05556495115160942 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1190], device='cuda:0')), ('power', tensor([0.2542], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.055492982268333435
epoch£º640	 i:1 	 global-step:12801	 l-p:0.055486276745796204
epoch£º640	 i:2 	 global-step:12802	 l-p:0.05548791214823723
epoch£º640	 i:3 	 global-step:12803	 l-p:0.05545083433389664
epoch£º640	 i:4 	 global-step:12804	 l-p:0.055469341576099396
epoch£º640	 i:5 	 global-step:12805	 l-p:0.05551384016871452
epoch£º640	 i:6 	 global-step:12806	 l-p:0.055570609867572784
epoch£º640	 i:7 	 global-step:12807	 l-p:0.0554606057703495
epoch£º640	 i:8 	 global-step:12808	 l-p:0.055518101900815964
epoch£º640	 i:9 	 global-step:12809	 l-p:0.05556506663560867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4239, 28.4307, 28.4241],
        [28.4239, 33.9620, 35.8308],
        [28.4239, 28.4240, 28.4239],
        [28.4239, 28.7923, 28.5244]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.05546152591705322 
model_pd.l_d.mean(): 6.146368104964495e-05 
model_pd.lagr.mean(): 0.055522989481687546 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1841], device='cuda:0')), ('power', tensor([0.1632], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.05546152591705322
epoch£º641	 i:1 	 global-step:12821	 l-p:0.055568989366292953
epoch£º641	 i:2 	 global-step:12822	 l-p:0.05551711097359657
epoch£º641	 i:3 	 global-step:12823	 l-p:0.0554829016327858
epoch£º641	 i:4 	 global-step:12824	 l-p:0.05559489130973816
epoch£º641	 i:5 	 global-step:12825	 l-p:0.055471811443567276
epoch£º641	 i:6 	 global-step:12826	 l-p:0.05545875430107117
epoch£º641	 i:7 	 global-step:12827	 l-p:0.055577218532562256
epoch£º641	 i:8 	 global-step:12828	 l-p:0.05545908957719803
epoch£º641	 i:9 	 global-step:12829	 l-p:0.05547065660357475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3865, 28.3876, 28.3865],
        [28.3865, 30.2323, 29.7729],
        [28.3865, 28.3865, 28.3864],
        [28.3865, 38.1303, 44.7413]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.0555732399225235 
model_pd.l_d.mean(): 0.00012079465523129329 
model_pd.lagr.mean(): 0.055694036185741425 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0824], device='cuda:0')), ('power', tensor([0.2637], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:0.0555732399225235
epoch£º642	 i:1 	 global-step:12841	 l-p:0.055467937141656876
epoch£º642	 i:2 	 global-step:12842	 l-p:0.055466268211603165
epoch£º642	 i:3 	 global-step:12843	 l-p:0.055462684482336044
epoch£º642	 i:4 	 global-step:12844	 l-p:0.05547626316547394
epoch£º642	 i:5 	 global-step:12845	 l-p:0.05552174523472786
epoch£º642	 i:6 	 global-step:12846	 l-p:0.05555037409067154
epoch£º642	 i:7 	 global-step:12847	 l-p:0.055628105998039246
epoch£º642	 i:8 	 global-step:12848	 l-p:0.055495698004961014
epoch£º642	 i:9 	 global-step:12849	 l-p:0.05551949515938759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3252, 28.7337, 28.4444],
        [28.3252, 28.3252, 28.3252],
        [28.3252, 32.8390, 33.8324],
        [28.3252, 28.3253, 28.3252]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.055607669055461884 
model_pd.l_d.mean(): 8.739700570004061e-05 
model_pd.lagr.mean(): 0.0556950643658638 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0848], device='cuda:0')), ('power', tensor([0.1695], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.055607669055461884
epoch£º643	 i:1 	 global-step:12861	 l-p:0.055546268820762634
epoch£º643	 i:2 	 global-step:12862	 l-p:0.055472295731306076
epoch£º643	 i:3 	 global-step:12863	 l-p:0.055517714470624924
epoch£º643	 i:4 	 global-step:12864	 l-p:0.055481262505054474
epoch£º643	 i:5 	 global-step:12865	 l-p:0.05569798871874809
epoch£º643	 i:6 	 global-step:12866	 l-p:0.055482421070337296
epoch£º643	 i:7 	 global-step:12867	 l-p:0.05548297241330147
epoch£º643	 i:8 	 global-step:12868	 l-p:0.05551643669605255
epoch£º643	 i:9 	 global-step:12869	 l-p:0.05549808219075203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[28.2460, 30.6684, 30.3838],
        [28.2460, 32.7464, 33.7368],
        [28.2460, 30.1224, 29.6737],
        [28.2460, 28.9166, 28.5148]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.05552156642079353 
model_pd.l_d.mean(): 1.156425423687324e-05 
model_pd.lagr.mean(): 0.05553312972187996 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1450], device='cuda:0')), ('power', tensor([0.0215], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.05552156642079353
epoch£º644	 i:1 	 global-step:12881	 l-p:0.055543992668390274
epoch£º644	 i:2 	 global-step:12882	 l-p:0.055627282708883286
epoch£º644	 i:3 	 global-step:12883	 l-p:0.05550228804349899
epoch£º644	 i:4 	 global-step:12884	 l-p:0.05550112575292587
epoch£º644	 i:5 	 global-step:12885	 l-p:0.055579423904418945
epoch£º644	 i:6 	 global-step:12886	 l-p:0.05550810694694519
epoch£º644	 i:7 	 global-step:12887	 l-p:0.055608730763196945
epoch£º644	 i:8 	 global-step:12888	 l-p:0.05554775521159172
epoch£º644	 i:9 	 global-step:12889	 l-p:0.0555366650223732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1570, 28.5022, 28.2479],
        [28.1570, 29.7354, 29.2416],
        [28.1570, 33.8289, 35.8559],
        [28.1570, 28.1725, 28.1576]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.05555314943194389 
model_pd.l_d.mean(): -3.270098386565223e-05 
model_pd.lagr.mean(): 0.055520448833703995 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1220], device='cuda:0')), ('power', tensor([-0.0630], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.05555314943194389
epoch£º645	 i:1 	 global-step:12901	 l-p:0.05550781264901161
epoch£º645	 i:2 	 global-step:12902	 l-p:0.05561692267656326
epoch£º645	 i:3 	 global-step:12903	 l-p:0.05555443465709686
epoch£º645	 i:4 	 global-step:12904	 l-p:0.05571195110678673
epoch£º645	 i:5 	 global-step:12905	 l-p:0.055559661239385605
epoch£º645	 i:6 	 global-step:12906	 l-p:0.055534686893224716
epoch£º645	 i:7 	 global-step:12907	 l-p:0.05555829778313637
epoch£º645	 i:8 	 global-step:12908	 l-p:0.055542849004268646
epoch£º645	 i:9 	 global-step:12909	 l-p:0.055520329624414444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[28.0697, 33.0334, 34.4273],
        [28.0697, 37.6700, 44.1647],
        [28.0697, 37.9379, 44.7822],
        [28.0697, 29.8101, 29.3408]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.05561915412545204 
model_pd.l_d.mean(): -4.4292966776993126e-05 
model_pd.lagr.mean(): 0.05557486042380333 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0754], device='cuda:0')), ('power', tensor([-0.0973], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.05561915412545204
epoch£º646	 i:1 	 global-step:12921	 l-p:0.0555361844599247
epoch£º646	 i:2 	 global-step:12922	 l-p:0.0555662140250206
epoch£º646	 i:3 	 global-step:12923	 l-p:0.05555446073412895
epoch£º646	 i:4 	 global-step:12924	 l-p:0.055631864815950394
epoch£º646	 i:5 	 global-step:12925	 l-p:0.05554971471428871
epoch£º646	 i:6 	 global-step:12926	 l-p:0.05554039403796196
epoch£º646	 i:7 	 global-step:12927	 l-p:0.05555945634841919
epoch£º646	 i:8 	 global-step:12928	 l-p:0.05553741380572319
epoch£º646	 i:9 	 global-step:12929	 l-p:0.055737145245075226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9951, 27.9950, 27.9951],
        [27.9951, 27.9964, 27.9951],
        [27.9951, 32.3734, 33.2909],
        [27.9951, 27.9966, 27.9951]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.055582065135240555 
model_pd.l_d.mean(): -7.095847104210407e-05 
model_pd.lagr.mean(): 0.05551110580563545 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1033], device='cuda:0')), ('power', tensor([-0.2025], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.055582065135240555
epoch£º647	 i:1 	 global-step:12941	 l-p:0.055544547736644745
epoch£º647	 i:2 	 global-step:12942	 l-p:0.05563671886920929
epoch£º647	 i:3 	 global-step:12943	 l-p:0.0556727834045887
epoch£º647	 i:4 	 global-step:12944	 l-p:0.05554503947496414
epoch£º647	 i:5 	 global-step:12945	 l-p:0.05554461479187012
epoch£º647	 i:6 	 global-step:12946	 l-p:0.055578675121068954
epoch£º647	 i:7 	 global-step:12947	 l-p:0.05557091534137726
epoch£º647	 i:8 	 global-step:12948	 l-p:0.05562911927700043
epoch£º647	 i:9 	 global-step:12949	 l-p:0.05566151812672615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9453, 33.6270, 35.6896],
        [27.9453, 27.9455, 27.9453],
        [27.9453, 28.1526, 27.9852],
        [27.9453, 27.9672, 27.9464]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.05555151030421257 
model_pd.l_d.mean(): -7.271049980772659e-05 
model_pd.lagr.mean(): 0.05547880008816719 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1793], device='cuda:0')), ('power', tensor([-0.3403], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.05555151030421257
epoch£º648	 i:1 	 global-step:12961	 l-p:0.05556535720825195
epoch£º648	 i:2 	 global-step:12962	 l-p:0.055604033172130585
epoch£º648	 i:3 	 global-step:12963	 l-p:0.0556439533829689
epoch£º648	 i:4 	 global-step:12964	 l-p:0.05554870516061783
epoch£º648	 i:5 	 global-step:12965	 l-p:0.055617280304431915
epoch£º648	 i:6 	 global-step:12966	 l-p:0.05565550550818443
epoch£º648	 i:7 	 global-step:12967	 l-p:0.055565137416124344
epoch£º648	 i:8 	 global-step:12968	 l-p:0.055576588958501816
epoch£º648	 i:9 	 global-step:12969	 l-p:0.05571098253130913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9303, 28.1328, 27.9687],
        [27.9303, 27.9443, 27.9309],
        [27.9303, 27.9410, 27.9307],
        [27.9303, 29.7845, 29.3410]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.05560833215713501 
model_pd.l_d.mean(): -1.9548357158782892e-05 
model_pd.lagr.mean(): 0.05558878555893898 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.3077e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1371], device='cuda:0')), ('power', tensor([-0.3285], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.05560833215713501
epoch£º649	 i:1 	 global-step:12981	 l-p:0.05561589077115059
epoch£º649	 i:2 	 global-step:12982	 l-p:0.055649690330028534
epoch£º649	 i:3 	 global-step:12983	 l-p:0.05557616055011749
epoch£º649	 i:4 	 global-step:12984	 l-p:0.05560128390789032
epoch£º649	 i:5 	 global-step:12985	 l-p:0.05554768443107605
epoch£º649	 i:6 	 global-step:12986	 l-p:0.055658720433712006
epoch£º649	 i:7 	 global-step:12987	 l-p:0.05556207522749901
epoch£º649	 i:8 	 global-step:12988	 l-p:0.05562889575958252
epoch£º649	 i:9 	 global-step:12989	 l-p:0.05558396130800247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9546, 28.5677, 28.1884],
        [27.9546, 28.2253, 28.0161],
        [27.9546, 31.8525, 32.4134],
        [27.9546, 29.1659, 28.6651]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.05555424466729164 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555424466729164 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1687], device='cuda:0')), ('power', tensor([-0.3423], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.05555424466729164
epoch£º650	 i:1 	 global-step:13001	 l-p:0.05575158819556236
epoch£º650	 i:2 	 global-step:13002	 l-p:0.05557228997349739
epoch£º650	 i:3 	 global-step:13003	 l-p:0.05562615394592285
epoch£º650	 i:4 	 global-step:13004	 l-p:0.05559711158275604
epoch£º650	 i:5 	 global-step:13005	 l-p:0.05554872006177902
epoch£º650	 i:6 	 global-step:13006	 l-p:0.05558834597468376
epoch£º650	 i:7 	 global-step:13007	 l-p:0.0555354580283165
epoch£º650	 i:8 	 global-step:13008	 l-p:0.05553247407078743
epoch£º650	 i:9 	 global-step:13009	 l-p:0.05564984679222107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0022, 28.0022, 28.0022],
        [28.0022, 37.4748, 43.8196],
        [28.0022, 28.0022, 28.0022],
        [28.0022, 29.5714, 29.0805]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.05556941032409668 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05556941032409668 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1303], device='cuda:0')), ('power', tensor([-0.2307], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.05556941032409668
epoch£º651	 i:1 	 global-step:13021	 l-p:0.055582378059625626
epoch£º651	 i:2 	 global-step:13022	 l-p:0.055557411164045334
epoch£º651	 i:3 	 global-step:13023	 l-p:0.0555553138256073
epoch£º651	 i:4 	 global-step:13024	 l-p:0.055589646100997925
epoch£º651	 i:5 	 global-step:13025	 l-p:0.05557197332382202
epoch£º651	 i:6 	 global-step:13026	 l-p:0.05566488951444626
epoch£º651	 i:7 	 global-step:13027	 l-p:0.0555623434484005
epoch£º651	 i:8 	 global-step:13028	 l-p:0.05553799122571945
epoch£º651	 i:9 	 global-step:13029	 l-p:0.05565594509243965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0582, 28.8349, 28.4017],
        [28.0582, 28.1161, 28.0633],
        [28.0582, 28.0582, 28.0582],
        [28.0582, 28.0642, 28.0583]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.055546216666698456 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055546216666698456 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1630], device='cuda:0')), ('power', tensor([-0.2641], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.055546216666698456
epoch£º652	 i:1 	 global-step:13041	 l-p:0.05553663522005081
epoch£º652	 i:2 	 global-step:13042	 l-p:0.055553171783685684
epoch£º652	 i:3 	 global-step:13043	 l-p:0.055552467703819275
epoch£º652	 i:4 	 global-step:13044	 l-p:0.055583685636520386
epoch£º652	 i:5 	 global-step:13045	 l-p:0.055625852197408676
epoch£º652	 i:6 	 global-step:13046	 l-p:0.055612023919820786
epoch£º652	 i:7 	 global-step:13047	 l-p:0.05559780076146126
epoch£º652	 i:8 	 global-step:13048	 l-p:0.055554285645484924
epoch£º652	 i:9 	 global-step:13049	 l-p:0.05556543543934822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1174, 35.2260, 38.7285],
        [28.1174, 32.9523, 34.2310],
        [28.1174, 35.2911, 38.8649],
        [28.1174, 28.5226, 28.2356]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.05555488169193268 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555488169193268 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1292], device='cuda:0')), ('power', tensor([-0.1445], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.05555488169193268
epoch£º653	 i:1 	 global-step:13061	 l-p:0.05561776086688042
epoch£º653	 i:2 	 global-step:13062	 l-p:0.05557086318731308
epoch£º653	 i:3 	 global-step:13063	 l-p:0.055507734417915344
epoch£º653	 i:4 	 global-step:13064	 l-p:0.055538568645715714
epoch£º653	 i:5 	 global-step:13065	 l-p:0.055504292249679565
epoch£º653	 i:6 	 global-step:13066	 l-p:0.055617425590753555
epoch£º653	 i:7 	 global-step:13067	 l-p:0.055530499666929245
epoch£º653	 i:8 	 global-step:13068	 l-p:0.05552450194954872
epoch£º653	 i:9 	 global-step:13069	 l-p:0.05563732609152794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1778, 35.1257, 38.4444],
        [28.1778, 29.5505, 29.0434],
        [28.1778, 28.1779, 28.1778],
        [28.1778, 29.9066, 29.4323]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.05559047684073448 
model_pd.l_d.mean(): -8.653097682831401e-11 
model_pd.lagr.mean(): 0.05559047684073448 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1270], device='cuda:0')), ('power', tensor([-0.0217], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.05559047684073448
epoch£º654	 i:1 	 global-step:13081	 l-p:0.055571049451828
epoch£º654	 i:2 	 global-step:13082	 l-p:0.055550601333379745
epoch£º654	 i:3 	 global-step:13083	 l-p:0.05551236867904663
epoch£º654	 i:4 	 global-step:13084	 l-p:0.05552799999713898
epoch£º654	 i:5 	 global-step:13085	 l-p:0.05562055855989456
epoch£º654	 i:6 	 global-step:13086	 l-p:0.05556689202785492
epoch£º654	 i:7 	 global-step:13087	 l-p:0.055527083575725555
epoch£º654	 i:8 	 global-step:13088	 l-p:0.055486563593149185
epoch£º654	 i:9 	 global-step:13089	 l-p:0.05552538111805916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[28.2389, 31.1620, 31.1023],
        [28.2389, 32.0519, 32.5302],
        [28.2389, 30.1067, 29.6564],
        [28.2389, 29.2120, 28.7331]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.05550685152411461 
model_pd.l_d.mean(): -4.685389853875677e-09 
model_pd.lagr.mean(): 0.05550684779882431 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1790], device='cuda:0')), ('power', tensor([-0.0339], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.05550685152411461
epoch£º655	 i:1 	 global-step:13101	 l-p:0.05548611655831337
epoch£º655	 i:2 	 global-step:13102	 l-p:0.05551990494132042
epoch£º655	 i:3 	 global-step:13103	 l-p:0.05560969188809395
epoch£º655	 i:4 	 global-step:13104	 l-p:0.05554240942001343
epoch£º655	 i:5 	 global-step:13105	 l-p:0.0555151104927063
epoch£º655	 i:6 	 global-step:13106	 l-p:0.05558266490697861
epoch£º655	 i:7 	 global-step:13107	 l-p:0.0554903969168663
epoch£º655	 i:8 	 global-step:13108	 l-p:0.05549426004290581
epoch£º655	 i:9 	 global-step:13109	 l-p:0.0556059330701828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2998, 28.4068, 28.3134],
        [28.2998, 28.4776, 28.3306],
        [28.2998, 28.3751, 28.3075],
        [28.2998, 33.0291, 34.2004]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.055491846054792404 
model_pd.l_d.mean(): -1.9046638044528663e-07 
model_pd.lagr.mean(): 0.05549165606498718 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.6250e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1767], device='cuda:0')), ('power', tensor([-0.0113], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.055491846054792404
epoch£º656	 i:1 	 global-step:13121	 l-p:0.05549289658665657
epoch£º656	 i:2 	 global-step:13122	 l-p:0.05556633695960045
epoch£º656	 i:3 	 global-step:13123	 l-p:0.055659882724285126
epoch£º656	 i:4 	 global-step:13124	 l-p:0.055481646209955215
epoch£º656	 i:5 	 global-step:13125	 l-p:0.05548640713095665
epoch£º656	 i:6 	 global-step:13126	 l-p:0.055497195571660995
epoch£º656	 i:7 	 global-step:13127	 l-p:0.05546852946281433
epoch£º656	 i:8 	 global-step:13128	 l-p:0.05547458678483963
epoch£º656	 i:9 	 global-step:13129	 l-p:0.0556122325360775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[28.3574, 30.1168, 29.6424],
        [28.3574, 31.1054, 30.9538],
        [28.3574, 34.2965, 36.5546],
        [28.3574, 33.3286, 34.6981]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.05548911914229393 
model_pd.l_d.mean(): 7.2240040935867e-06 
model_pd.lagr.mean(): 0.05549634248018265 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.3449e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1686], device='cuda:0')), ('power', tensor([0.1265], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.05548911914229393
epoch£º657	 i:1 	 global-step:13141	 l-p:0.055483128875494
epoch£º657	 i:2 	 global-step:13142	 l-p:0.055475518107414246
epoch£º657	 i:3 	 global-step:13143	 l-p:0.055483654141426086
epoch£º657	 i:4 	 global-step:13144	 l-p:0.0556076280772686
epoch£º657	 i:5 	 global-step:13145	 l-p:0.05557936429977417
epoch£º657	 i:6 	 global-step:13146	 l-p:0.05557229369878769
epoch£º657	 i:7 	 global-step:13147	 l-p:0.055502116680145264
epoch£º657	 i:8 	 global-step:13148	 l-p:0.05545084923505783
epoch£º657	 i:9 	 global-step:13149	 l-p:0.05547815561294556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4055, 28.4057, 28.4055],
        [28.4055, 31.4497, 31.4436],
        [28.4055, 30.4510, 30.0374],
        [28.4055, 30.6471, 30.2916]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.05549187585711479 
model_pd.l_d.mean(): 1.770912240317557e-05 
model_pd.lagr.mean(): 0.05550958588719368 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1372], device='cuda:0')), ('power', tensor([0.1425], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.05549187585711479
epoch£º658	 i:1 	 global-step:13161	 l-p:0.05545732378959656
epoch£º658	 i:2 	 global-step:13162	 l-p:0.0554938018321991
epoch£º658	 i:3 	 global-step:13163	 l-p:0.05552683770656586
epoch£º658	 i:4 	 global-step:13164	 l-p:0.05553360655903816
epoch£º658	 i:5 	 global-step:13165	 l-p:0.05547211691737175
epoch£º658	 i:6 	 global-step:13166	 l-p:0.05547930672764778
epoch£º658	 i:7 	 global-step:13167	 l-p:0.0555521659553051
epoch£º658	 i:8 	 global-step:13168	 l-p:0.055444810539484024
epoch£º658	 i:9 	 global-step:13169	 l-p:0.05558714270591736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4364, 34.2233, 36.3247],
        [28.4364, 28.4379, 28.4365],
        [28.4364, 31.1077, 30.9170],
        [28.4364, 28.4408, 28.4365]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.05547627434134483 
model_pd.l_d.mean(): 2.3698219592915848e-05 
model_pd.lagr.mean(): 0.05549997091293335 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1889], device='cuda:0')), ('power', tensor([0.1119], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.05547627434134483
epoch£º659	 i:1 	 global-step:13181	 l-p:0.05549299716949463
epoch£º659	 i:2 	 global-step:13182	 l-p:0.05546843260526657
epoch£º659	 i:3 	 global-step:13183	 l-p:0.055453378707170486
epoch£º659	 i:4 	 global-step:13184	 l-p:0.05551069229841232
epoch£º659	 i:5 	 global-step:13185	 l-p:0.05552101880311966
epoch£º659	 i:6 	 global-step:13186	 l-p:0.0555579848587513
epoch£º659	 i:7 	 global-step:13187	 l-p:0.05550112575292587
epoch£º659	 i:8 	 global-step:13188	 l-p:0.0554644949734211
epoch£º659	 i:9 	 global-step:13189	 l-p:0.05555000156164169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4445, 36.2673, 40.5156],
        [28.4445, 29.2918, 28.8368],
        [28.4445, 34.4451, 36.7518],
        [28.4445, 28.4450, 28.4445]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.055521924048662186 
model_pd.l_d.mean(): 9.373296779813245e-05 
model_pd.lagr.mean(): 0.055615656077861786 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1075], device='cuda:0')), ('power', tensor([0.3025], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.055521924048662186
epoch£º660	 i:1 	 global-step:13201	 l-p:0.05551277846097946
epoch£º660	 i:2 	 global-step:13202	 l-p:0.05548958107829094
epoch£º660	 i:3 	 global-step:13203	 l-p:0.055475667119026184
epoch£º660	 i:4 	 global-step:13204	 l-p:0.05551638454198837
epoch£º660	 i:5 	 global-step:13205	 l-p:0.055600833147764206
epoch£º660	 i:6 	 global-step:13206	 l-p:0.05545379966497421
epoch£º660	 i:7 	 global-step:13207	 l-p:0.055457599461078644
epoch£º660	 i:8 	 global-step:13208	 l-p:0.05551444739103317
epoch£º660	 i:9 	 global-step:13209	 l-p:0.05546238645911217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4235, 28.4278, 28.4236],
        [28.4235, 29.1006, 28.6955],
        [28.4235, 28.5651, 28.4447],
        [28.4235, 29.5858, 29.0807]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.05551735684275627 
model_pd.l_d.mean(): 7.662652205908671e-05 
model_pd.lagr.mean(): 0.055593982338905334 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1604], device='cuda:0')), ('power', tensor([0.1889], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.05551735684275627
epoch£º661	 i:1 	 global-step:13221	 l-p:0.055475130677223206
epoch£º661	 i:2 	 global-step:13222	 l-p:0.05545687675476074
epoch£º661	 i:3 	 global-step:13223	 l-p:0.055471271276474
epoch£º661	 i:4 	 global-step:13224	 l-p:0.055543944239616394
epoch£º661	 i:5 	 global-step:13225	 l-p:0.05559655651450157
epoch£º661	 i:6 	 global-step:13226	 l-p:0.055457573384046555
epoch£º661	 i:7 	 global-step:13227	 l-p:0.05549044907093048
epoch£º661	 i:8 	 global-step:13228	 l-p:0.05549092963337898
epoch£º661	 i:9 	 global-step:13229	 l-p:0.0555754154920578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3734, 28.3734, 28.3734],
        [28.3734, 28.3734, 28.3734],
        [28.3734, 28.4869, 28.3883],
        [28.3734, 29.9648, 29.4670]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.05546150729060173 
model_pd.l_d.mean(): 4.1912454662451637e-07 
model_pd.lagr.mean(): 0.05546192824840546 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2096], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.05546150729060173
epoch£º662	 i:1 	 global-step:13241	 l-p:0.05555475503206253
epoch£º662	 i:2 	 global-step:13242	 l-p:0.05567541345953941
epoch£º662	 i:3 	 global-step:13243	 l-p:0.05547448620200157
epoch£º662	 i:4 	 global-step:13244	 l-p:0.05551193654537201
epoch£º662	 i:5 	 global-step:13245	 l-p:0.055507685989141464
epoch£º662	 i:6 	 global-step:13246	 l-p:0.05548359826207161
epoch£º662	 i:7 	 global-step:13247	 l-p:0.05548706278204918
epoch£º662	 i:8 	 global-step:13248	 l-p:0.05550483614206314
epoch£º662	 i:9 	 global-step:13249	 l-p:0.055538520216941833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2989, 35.5213, 39.1196],
        [28.2989, 28.6535, 28.3936],
        [28.2989, 31.0469, 30.8983],
        [28.2989, 28.3631, 28.3049]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.055495284497737885 
model_pd.l_d.mean(): -1.9179846276529133e-05 
model_pd.lagr.mean(): 0.055476102977991104 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1796], device='cuda:0')), ('power', tensor([-0.0360], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.055495284497737885
epoch£º663	 i:1 	 global-step:13261	 l-p:0.05559197813272476
epoch£º663	 i:2 	 global-step:13262	 l-p:0.05549246072769165
epoch£º663	 i:3 	 global-step:13263	 l-p:0.05553031712770462
epoch£º663	 i:4 	 global-step:13264	 l-p:0.055501289665699005
epoch£º663	 i:5 	 global-step:13265	 l-p:0.05554553493857384
epoch£º663	 i:6 	 global-step:13266	 l-p:0.05555049702525139
epoch£º663	 i:7 	 global-step:13267	 l-p:0.0555536262691021
epoch£º663	 i:8 	 global-step:13268	 l-p:0.0556015819311142
epoch£º663	 i:9 	 global-step:13269	 l-p:0.05550681799650192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2074, 28.2076, 28.2074],
        [28.2074, 31.2292, 31.2230],
        [28.2074, 28.2156, 28.2077],
        [28.2074, 29.6551, 29.1500]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.05552314594388008 
model_pd.l_d.mean(): -3.306709186290391e-05 
model_pd.lagr.mean(): 0.05549008026719093 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1477], device='cuda:0')), ('power', tensor([-0.0613], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.05552314594388008
epoch£º664	 i:1 	 global-step:13281	 l-p:0.05560476332902908
epoch£º664	 i:2 	 global-step:13282	 l-p:0.05551080405712128
epoch£º664	 i:3 	 global-step:13283	 l-p:0.05555965378880501
epoch£º664	 i:4 	 global-step:13284	 l-p:0.05563437193632126
epoch£º664	 i:5 	 global-step:13285	 l-p:0.05550326406955719
epoch£º664	 i:6 	 global-step:13286	 l-p:0.05554117262363434
epoch£º664	 i:7 	 global-step:13287	 l-p:0.05557496100664139
epoch£º664	 i:8 	 global-step:13288	 l-p:0.05558596923947334
epoch£º664	 i:9 	 global-step:13289	 l-p:0.055526409298181534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1101, 28.1209, 28.1105],
        [28.1101, 28.1212, 28.1105],
        [28.1101, 31.0189, 30.9594],
        [28.1101, 28.5567, 28.2488]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.055540502071380615 
model_pd.l_d.mean(): -5.578332638833672e-05 
model_pd.lagr.mean(): 0.05548471957445145 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1467], device='cuda:0')), ('power', tensor([-0.1119], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.055540502071380615
epoch£º665	 i:1 	 global-step:13301	 l-p:0.05557568371295929
epoch£º665	 i:2 	 global-step:13302	 l-p:0.05562053248286247
epoch£º665	 i:3 	 global-step:13303	 l-p:0.055553920567035675
epoch£º665	 i:4 	 global-step:13304	 l-p:0.055628638714551926
epoch£º665	 i:5 	 global-step:13305	 l-p:0.05557243153452873
epoch£º665	 i:6 	 global-step:13306	 l-p:0.05559515953063965
epoch£º665	 i:7 	 global-step:13307	 l-p:0.055550381541252136
epoch£º665	 i:8 	 global-step:13308	 l-p:0.055552367120981216
epoch£º665	 i:9 	 global-step:13309	 l-p:0.05557194724678993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0195, 28.0195, 28.0195],
        [28.0195, 33.2254, 34.8386],
        [28.0195, 33.4199, 35.2098],
        [28.0195, 29.3569, 28.8524]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.055684857070446014 
model_pd.l_d.mean(): -5.494881406775676e-05 
model_pd.lagr.mean(): 0.0556299090385437 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0777], device='cuda:0')), ('power', tensor([-0.1339], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.055684857070446014
epoch£º666	 i:1 	 global-step:13321	 l-p:0.0555739589035511
epoch£º666	 i:2 	 global-step:13322	 l-p:0.0555456280708313
epoch£º666	 i:3 	 global-step:13323	 l-p:0.055652208626270294
epoch£º666	 i:4 	 global-step:13324	 l-p:0.05563899874687195
epoch£º666	 i:5 	 global-step:13325	 l-p:0.05557124316692352
epoch£º666	 i:6 	 global-step:13326	 l-p:0.05557550489902496
epoch£º666	 i:7 	 global-step:13327	 l-p:0.05556146427989006
epoch£º666	 i:8 	 global-step:13328	 l-p:0.055573929101228714
epoch£º666	 i:9 	 global-step:13329	 l-p:0.05555528402328491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[27.9494, 37.8331, 44.7251],
        [27.9494, 30.0635, 29.6861],
        [27.9494, 36.1412, 40.9094],
        [27.9494, 31.0372, 31.0828]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.05558308959007263 
model_pd.l_d.mean(): -7.154890772653744e-05 
model_pd.lagr.mean(): 0.05551154166460037 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1307], device='cuda:0')), ('power', tensor([-0.2543], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.05558308959007263
epoch£º667	 i:1 	 global-step:13341	 l-p:0.05561165139079094
epoch£º667	 i:2 	 global-step:13342	 l-p:0.05565182864665985
epoch£º667	 i:3 	 global-step:13343	 l-p:0.05557185411453247
epoch£º667	 i:4 	 global-step:13344	 l-p:0.05567469447851181
epoch£º667	 i:5 	 global-step:13345	 l-p:0.055618856102228165
epoch£º667	 i:6 	 global-step:13346	 l-p:0.05555868148803711
epoch£º667	 i:7 	 global-step:13347	 l-p:0.05561789870262146
epoch£º667	 i:8 	 global-step:13348	 l-p:0.055580154061317444
epoch£º667	 i:9 	 global-step:13349	 l-p:0.05558190122246742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9128, 28.0659, 27.9372],
        [27.9128, 27.9129, 27.9128],
        [27.9128, 29.6792, 29.2190],
        [27.9128, 32.8471, 34.2327]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.055611755698919296 
model_pd.l_d.mean(): -4.028721014037728e-05 
model_pd.lagr.mean(): 0.05557146668434143 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0967], device='cuda:0')), ('power', tensor([-0.3239], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.055611755698919296
epoch£º668	 i:1 	 global-step:13361	 l-p:0.05560268461704254
epoch£º668	 i:2 	 global-step:13362	 l-p:0.05563300475478172
epoch£º668	 i:3 	 global-step:13363	 l-p:0.05563124641776085
epoch£º668	 i:4 	 global-step:13364	 l-p:0.05559134483337402
epoch£º668	 i:5 	 global-step:13365	 l-p:0.05557875335216522
epoch£º668	 i:6 	 global-step:13366	 l-p:0.055562086403369904
epoch£º668	 i:7 	 global-step:13367	 l-p:0.0556182786822319
epoch£º668	 i:8 	 global-step:13368	 l-p:0.055693645030260086
epoch£º668	 i:9 	 global-step:13369	 l-p:0.05556489899754524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9198, 29.1446, 28.6437],
        [27.9198, 27.9201, 27.9198],
        [27.9198, 27.9198, 27.9198],
        [27.9198, 31.1680, 31.3074]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.055624283850193024 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055624283850193024 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1656], device='cuda:0')), ('power', tensor([-0.4029], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.055624283850193024
epoch£º669	 i:1 	 global-step:13381	 l-p:0.055570151656866074
epoch£º669	 i:2 	 global-step:13382	 l-p:0.055601995438337326
epoch£º669	 i:3 	 global-step:13383	 l-p:0.05561855062842369
epoch£º669	 i:4 	 global-step:13384	 l-p:0.055565670132637024
epoch£º669	 i:5 	 global-step:13385	 l-p:0.05561726912856102
epoch£º669	 i:6 	 global-step:13386	 l-p:0.055543527007102966
epoch£º669	 i:7 	 global-step:13387	 l-p:0.05565449967980385
epoch£º669	 i:8 	 global-step:13388	 l-p:0.055641744285821915
epoch£º669	 i:9 	 global-step:13389	 l-p:0.05559682101011276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9632, 27.9632, 27.9632],
        [27.9632, 28.3765, 28.0858],
        [27.9632, 27.9698, 27.9633],
        [27.9632, 27.9632, 27.9632]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.0555570125579834 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0555570125579834 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1558], device='cuda:0')), ('power', tensor([-0.2772], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.0555570125579834
epoch£º670	 i:1 	 global-step:13401	 l-p:0.055614374577999115
epoch£º670	 i:2 	 global-step:13402	 l-p:0.0555741973221302
epoch£º670	 i:3 	 global-step:13403	 l-p:0.05562087893486023
epoch£º670	 i:4 	 global-step:13404	 l-p:0.05571233481168747
epoch£º670	 i:5 	 global-step:13405	 l-p:0.055640581995248795
epoch£º670	 i:6 	 global-step:13406	 l-p:0.05555729568004608
epoch£º670	 i:7 	 global-step:13407	 l-p:0.0555453896522522
epoch£º670	 i:8 	 global-step:13408	 l-p:0.05557296797633171
epoch£º670	 i:9 	 global-step:13409	 l-p:0.05553261563181877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0213, 28.0729, 28.0256],
        [28.0213, 28.0380, 28.0220],
        [28.0213, 29.9745, 29.5508],
        [28.0213, 28.0386, 28.0221]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.05554354190826416 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554354190826416 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1859], device='cuda:0')), ('power', tensor([-0.3024], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.05554354190826416
epoch£º671	 i:1 	 global-step:13421	 l-p:0.055555056780576706
epoch£º671	 i:2 	 global-step:13422	 l-p:0.05556117743253708
epoch£º671	 i:3 	 global-step:13423	 l-p:0.05569617077708244
epoch£º671	 i:4 	 global-step:13424	 l-p:0.055574681609869
epoch£º671	 i:5 	 global-step:13425	 l-p:0.055540017783641815
epoch£º671	 i:6 	 global-step:13426	 l-p:0.05557796359062195
epoch£º671	 i:7 	 global-step:13427	 l-p:0.05554571375250816
epoch£º671	 i:8 	 global-step:13428	 l-p:0.05562886968255043
epoch£º671	 i:9 	 global-step:13429	 l-p:0.0555773563683033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0846, 28.1907, 28.0981],
        [28.0846, 32.6538, 33.7152],
        [28.0846, 28.0846, 28.0846],
        [28.0846, 29.8259, 29.3563]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.055567264556884766 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055567264556884766 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1255], device='cuda:0')), ('power', tensor([-0.1458], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.055567264556884766
epoch£º672	 i:1 	 global-step:13441	 l-p:0.055663857609033585
epoch£º672	 i:2 	 global-step:13442	 l-p:0.055596545338630676
epoch£º672	 i:3 	 global-step:13443	 l-p:0.05551619455218315
epoch£º672	 i:4 	 global-step:13444	 l-p:0.05551724135875702
epoch£º672	 i:5 	 global-step:13445	 l-p:0.05555826053023338
epoch£º672	 i:6 	 global-step:13446	 l-p:0.05552872642874718
epoch£º672	 i:7 	 global-step:13447	 l-p:0.0555415004491806
epoch£º672	 i:8 	 global-step:13448	 l-p:0.05562378838658333
epoch£º672	 i:9 	 global-step:13449	 l-p:0.05555390194058418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1499, 28.1512, 28.1499],
        [28.1499, 28.1499, 28.1499],
        [28.1499, 34.2984, 36.7906],
        [28.1499, 28.2003, 28.1540]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.05555477365851402 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555477365851402 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1641], device='cuda:0')), ('power', tensor([-0.1357], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.05555477365851402
epoch£º673	 i:1 	 global-step:13461	 l-p:0.05551791563630104
epoch£º673	 i:2 	 global-step:13462	 l-p:0.05559776350855827
epoch£º673	 i:3 	 global-step:13463	 l-p:0.05551213026046753
epoch£º673	 i:4 	 global-step:13464	 l-p:0.055518705397844315
epoch£º673	 i:5 	 global-step:13465	 l-p:0.05552450194954872
epoch£º673	 i:6 	 global-step:13466	 l-p:0.05561899393796921
epoch£º673	 i:7 	 global-step:13467	 l-p:0.05562597140669823
epoch£º673	 i:8 	 global-step:13468	 l-p:0.055534932762384415
epoch£º673	 i:9 	 global-step:13469	 l-p:0.0555257685482502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2160, 28.9600, 28.5349],
        [28.2160, 28.8353, 28.4522],
        [28.2160, 29.3402, 28.8416],
        [28.2160, 30.8835, 30.7025]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.055582158267498016 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055582158267498016 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.7375e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0972], device='cuda:0')), ('power', tensor([0.0547], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.055582158267498016
epoch£º674	 i:1 	 global-step:13481	 l-p:0.055498719215393066
epoch£º674	 i:2 	 global-step:13482	 l-p:0.05552290752530098
epoch£º674	 i:3 	 global-step:13483	 l-p:0.05562319979071617
epoch£º674	 i:4 	 global-step:13484	 l-p:0.05551835894584656
epoch£º674	 i:5 	 global-step:13485	 l-p:0.055507954210042953
epoch£º674	 i:6 	 global-step:13486	 l-p:0.05550381913781166
epoch£º674	 i:7 	 global-step:13487	 l-p:0.0555361770093441
epoch£º674	 i:8 	 global-step:13488	 l-p:0.055502668023109436
epoch£º674	 i:9 	 global-step:13489	 l-p:0.055599573999643326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2824, 29.6909, 29.1826],
        [28.2824, 30.6222, 30.3055],
        [28.2824, 28.8852, 28.5080],
        [28.2824, 28.7131, 28.4125]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.05552732199430466 
model_pd.l_d.mean(): -2.5052726115859514e-08 
model_pd.lagr.mean(): 0.05552729591727257 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.4448e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1486], device='cuda:0')), ('power', tensor([-0.0044], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.05552732199430466
epoch£º675	 i:1 	 global-step:13501	 l-p:0.05552907660603523
epoch£º675	 i:2 	 global-step:13502	 l-p:0.055488843470811844
epoch£º675	 i:3 	 global-step:13503	 l-p:0.05554318428039551
epoch£º675	 i:4 	 global-step:13504	 l-p:0.055523376911878586
epoch£º675	 i:5 	 global-step:13505	 l-p:0.05552728474140167
epoch£º675	 i:6 	 global-step:13506	 l-p:0.055491235107183456
epoch£º675	 i:7 	 global-step:13507	 l-p:0.055593814700841904
epoch£º675	 i:8 	 global-step:13508	 l-p:0.0555543415248394
epoch£º675	 i:9 	 global-step:13509	 l-p:0.05548210069537163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3471, 28.3473, 28.3471],
        [28.3471, 31.7474, 31.9491],
        [28.3471, 28.4057, 28.3523],
        [28.3471, 28.3657, 28.3480]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.05549179017543793 
model_pd.l_d.mean(): 1.1408893669795361e-06 
model_pd.lagr.mean(): 0.05549293011426926 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.0336e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1620], device='cuda:0')), ('power', tensor([0.0294], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.05549179017543793
epoch£º676	 i:1 	 global-step:13521	 l-p:0.05546233057975769
epoch£º676	 i:2 	 global-step:13522	 l-p:0.05548548698425293
epoch£º676	 i:3 	 global-step:13523	 l-p:0.055570945143699646
epoch£º676	 i:4 	 global-step:13524	 l-p:0.05547293648123741
epoch£º676	 i:5 	 global-step:13525	 l-p:0.055557794868946075
epoch£º676	 i:6 	 global-step:13526	 l-p:0.055511292070150375
epoch£º676	 i:7 	 global-step:13527	 l-p:0.05554914474487305
epoch£º676	 i:8 	 global-step:13528	 l-p:0.05555879324674606
epoch£º676	 i:9 	 global-step:13529	 l-p:0.05547436699271202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4039, 28.4039, 28.4039],
        [28.4039, 29.0411, 28.6503],
        [28.4039, 30.2762, 29.8217],
        [28.4039, 28.4850, 28.4126]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.05547234043478966 
model_pd.l_d.mean(): 1.2036953194183297e-05 
model_pd.lagr.mean(): 0.05548437684774399 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1814], device='cuda:0')), ('power', tensor([0.1171], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.05547234043478966
epoch£º677	 i:1 	 global-step:13541	 l-p:0.055503103882074356
epoch£º677	 i:2 	 global-step:13542	 l-p:0.05545830726623535
epoch£º677	 i:3 	 global-step:13543	 l-p:0.055566251277923584
epoch£º677	 i:4 	 global-step:13544	 l-p:0.05545404925942421
epoch£º677	 i:5 	 global-step:13545	 l-p:0.05551094934344292
epoch£º677	 i:6 	 global-step:13546	 l-p:0.05552881583571434
epoch£º677	 i:7 	 global-step:13547	 l-p:0.05547712370753288
epoch£º677	 i:8 	 global-step:13548	 l-p:0.05553605034947395
epoch£º677	 i:9 	 global-step:13549	 l-p:0.055526670068502426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4442, 28.6074, 28.4710],
        [28.4442, 28.4514, 28.4444],
        [28.4442, 28.5859, 28.4655],
        [28.4442, 28.6554, 28.4848]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): 0.05550311133265495 
model_pd.l_d.mean(): 4.9052694521378726e-05 
model_pd.lagr.mean(): 0.0555521622300148 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1066], device='cuda:0')), ('power', tensor([0.2560], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:0.05550311133265495
epoch£º678	 i:1 	 global-step:13561	 l-p:0.055605895817279816
epoch£º678	 i:2 	 global-step:13562	 l-p:0.05550085008144379
epoch£º678	 i:3 	 global-step:13563	 l-p:0.055498916655778885
epoch£º678	 i:4 	 global-step:13564	 l-p:0.05546124652028084
epoch£º678	 i:5 	 global-step:13565	 l-p:0.05545974150300026
epoch£º678	 i:6 	 global-step:13566	 l-p:0.055530257523059845
epoch£º678	 i:7 	 global-step:13567	 l-p:0.05544620007276535
epoch£º678	 i:8 	 global-step:13568	 l-p:0.05543681979179382
epoch£º678	 i:9 	 global-step:13569	 l-p:0.05553111061453819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4585, 28.4588, 28.4585],
        [28.4585, 34.4621, 36.7700],
        [28.4585, 28.5948, 28.4785],
        [28.4585, 28.7331, 28.5206]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.055471811443567276 
model_pd.l_d.mean(): 5.302437057252973e-05 
model_pd.lagr.mean(): 0.05552483722567558 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1661], device='cuda:0')), ('power', tensor([0.1796], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.055471811443567276
epoch£º679	 i:1 	 global-step:13581	 l-p:0.05546947196125984
epoch£º679	 i:2 	 global-step:13582	 l-p:0.05546766519546509
epoch£º679	 i:3 	 global-step:13583	 l-p:0.055549755692481995
epoch£º679	 i:4 	 global-step:13584	 l-p:0.05547401309013367
epoch£º679	 i:5 	 global-step:13585	 l-p:0.05549731105566025
epoch£º679	 i:6 	 global-step:13586	 l-p:0.055568378418684006
epoch£º679	 i:7 	 global-step:13587	 l-p:0.05545734614133835
epoch£º679	 i:8 	 global-step:13588	 l-p:0.05547798052430153
epoch£º679	 i:9 	 global-step:13589	 l-p:0.05554041638970375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4409, 28.4729, 28.4429],
        [28.4409, 28.4521, 28.4413],
        [28.4409, 28.6398, 28.4777],
        [28.4409, 36.4630, 40.9425]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.055445026606321335 
model_pd.l_d.mean(): 4.634835568140261e-05 
model_pd.lagr.mean(): 0.0554913766682148 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2213], device='cuda:0')), ('power', tensor([0.1162], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.055445026606321335
epoch£º680	 i:1 	 global-step:13601	 l-p:0.05544646084308624
epoch£º680	 i:2 	 global-step:13602	 l-p:0.05556024610996246
epoch£º680	 i:3 	 global-step:13603	 l-p:0.055486492812633514
epoch£º680	 i:4 	 global-step:13604	 l-p:0.0554901584982872
epoch£º680	 i:5 	 global-step:13605	 l-p:0.05548844113945961
epoch£º680	 i:6 	 global-step:13606	 l-p:0.055651452392339706
epoch£º680	 i:7 	 global-step:13607	 l-p:0.05551258474588394
epoch£º680	 i:8 	 global-step:13608	 l-p:0.055482346564531326
epoch£º680	 i:9 	 global-step:13609	 l-p:0.055476006120443344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3910, 29.9913, 29.4940],
        [28.3910, 37.7462, 43.8578],
        [28.3910, 28.3950, 28.3911],
        [28.3910, 28.3953, 28.3911]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.05548829957842827 
model_pd.l_d.mean(): 4.4492699089460075e-05 
model_pd.lagr.mean(): 0.0555327907204628 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1559], device='cuda:0')), ('power', tensor([0.0915], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.05548829957842827
epoch£º681	 i:1 	 global-step:13621	 l-p:0.05548934265971184
epoch£º681	 i:2 	 global-step:13622	 l-p:0.05559103190898895
epoch£º681	 i:3 	 global-step:13623	 l-p:0.05551061034202576
epoch£º681	 i:4 	 global-step:13624	 l-p:0.05550071969628334
epoch£º681	 i:5 	 global-step:13625	 l-p:0.05548836663365364
epoch£º681	 i:6 	 global-step:13626	 l-p:0.05567922443151474
epoch£º681	 i:7 	 global-step:13627	 l-p:0.05546339973807335
epoch£º681	 i:8 	 global-step:13628	 l-p:0.05547498166561127
epoch£º681	 i:9 	 global-step:13629	 l-p:0.05548232048749924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3110, 28.9145, 28.5369],
        [28.3110, 29.1636, 28.7086],
        [28.3110, 28.3221, 28.3114],
        [28.3110, 28.3846, 28.3185]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.05549289658665657 
model_pd.l_d.mean(): 1.6028589016059414e-05 
model_pd.lagr.mean(): 0.05550892651081085 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1680], device='cuda:0')), ('power', tensor([0.0296], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.05549289658665657
epoch£º682	 i:1 	 global-step:13641	 l-p:0.055479202419519424
epoch£º682	 i:2 	 global-step:13642	 l-p:0.055523235350847244
epoch£º682	 i:3 	 global-step:13643	 l-p:0.055636391043663025
epoch£º682	 i:4 	 global-step:13644	 l-p:0.055521368980407715
epoch£º682	 i:5 	 global-step:13645	 l-p:0.05549975112080574
epoch£º682	 i:6 	 global-step:13646	 l-p:0.05548854544758797
epoch£º682	 i:7 	 global-step:13647	 l-p:0.05552428215742111
epoch£º682	 i:8 	 global-step:13648	 l-p:0.05567675456404686
epoch£º682	 i:9 	 global-step:13649	 l-p:0.05551103875041008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2098, 28.9998, 28.5616],
        [28.2098, 28.3968, 28.2433],
        [28.2098, 28.2098, 28.2098],
        [28.2098, 29.3468, 28.8471]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.05549640208482742 
model_pd.l_d.mean(): -4.398599048727192e-05 
model_pd.lagr.mean(): 0.05545241758227348 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2005], device='cuda:0')), ('power', tensor([-0.0796], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.05549640208482742
epoch£º683	 i:1 	 global-step:13661	 l-p:0.05552975833415985
epoch£º683	 i:2 	 global-step:13662	 l-p:0.05555504560470581
epoch£º683	 i:3 	 global-step:13663	 l-p:0.05559466779232025
epoch£º683	 i:4 	 global-step:13664	 l-p:0.05562395974993706
epoch£º683	 i:5 	 global-step:13665	 l-p:0.05561289191246033
epoch£º683	 i:6 	 global-step:13666	 l-p:0.055517300963401794
epoch£º683	 i:7 	 global-step:13667	 l-p:0.0555587001144886
epoch£º683	 i:8 	 global-step:13668	 l-p:0.05553346499800682
epoch£º683	 i:9 	 global-step:13669	 l-p:0.055548157542943954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1008, 32.5769, 33.5617],
        [28.1008, 28.1008, 28.1008],
        [28.1008, 36.4245, 41.3219],
        [28.1008, 28.1109, 28.1012]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.05554991960525513 
model_pd.l_d.mean(): -8.842116221785545e-05 
model_pd.lagr.mean(): 0.05546149984002113 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1414], device='cuda:0')), ('power', tensor([-0.1731], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.05554991960525513
epoch£º684	 i:1 	 global-step:13681	 l-p:0.05555727332830429
epoch£º684	 i:2 	 global-step:13682	 l-p:0.05553029105067253
epoch£º684	 i:3 	 global-step:13683	 l-p:0.05555133521556854
epoch£º684	 i:4 	 global-step:13684	 l-p:0.05569083243608475
epoch£º684	 i:5 	 global-step:13685	 l-p:0.05560050904750824
epoch£º684	 i:6 	 global-step:13686	 l-p:0.055526044219732285
epoch£º684	 i:7 	 global-step:13687	 l-p:0.05554511025547981
epoch£º684	 i:8 	 global-step:13688	 l-p:0.055633723735809326
epoch£º684	 i:9 	 global-step:13689	 l-p:0.05560688674449921
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9996, 28.4605, 28.1460],
        [27.9996, 37.1895, 43.1738],
        [27.9996, 30.7167, 30.5697],
        [27.9996, 28.0000, 27.9996]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.05555146560072899 
model_pd.l_d.mean(): -0.00011136036482639611 
model_pd.lagr.mean(): 0.05544010549783707 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1595], device='cuda:0')), ('power', tensor([-0.2680], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.05555146560072899
epoch£º685	 i:1 	 global-step:13701	 l-p:0.055541928857564926
epoch£º685	 i:2 	 global-step:13702	 l-p:0.05557746812701225
epoch£º685	 i:3 	 global-step:13703	 l-p:0.055537670850753784
epoch£º685	 i:4 	 global-step:13704	 l-p:0.05569237098097801
epoch£º685	 i:5 	 global-step:13705	 l-p:0.05568228289484978
epoch£º685	 i:6 	 global-step:13706	 l-p:0.055578622967004776
epoch£º685	 i:7 	 global-step:13707	 l-p:0.05559105798602104
epoch£º685	 i:8 	 global-step:13708	 l-p:0.055582351982593536
epoch£º685	 i:9 	 global-step:13709	 l-p:0.055647071450948715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9234, 27.9248, 27.9234],
        [27.9234, 27.9258, 27.9234],
        [27.9234, 28.0803, 27.9488],
        [27.9234, 36.0128, 40.6645]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.05567682534456253 
model_pd.l_d.mean(): -5.5031599913490936e-05 
model_pd.lagr.mean(): 0.05562179535627365 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0357], device='cuda:0')), ('power', tensor([-0.2002], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.05567682534456253
epoch£º686	 i:1 	 global-step:13721	 l-p:0.05557001009583473
epoch£º686	 i:2 	 global-step:13722	 l-p:0.05562660098075867
epoch£º686	 i:3 	 global-step:13723	 l-p:0.05562853813171387
epoch£º686	 i:4 	 global-step:13724	 l-p:0.0555759035050869
epoch£º686	 i:5 	 global-step:13725	 l-p:0.055595483630895615
epoch£º686	 i:6 	 global-step:13726	 l-p:0.055712681263685226
epoch£º686	 i:7 	 global-step:13727	 l-p:0.055576637387275696
epoch£º686	 i:8 	 global-step:13728	 l-p:0.05557205155491829
epoch£º686	 i:9 	 global-step:13729	 l-p:0.0555720254778862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[27.8861, 35.6783, 39.9890],
        [27.8861, 31.6655, 32.1488],
        [27.8861, 29.7290, 29.2846],
        [27.8861, 34.7584, 38.0406]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.05562635883688927 
model_pd.l_d.mean(): -2.864459383999929e-05 
model_pd.lagr.mean(): 0.05559771507978439 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.0949e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0649], device='cuda:0')), ('power', tensor([-0.2738], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.05562635883688927
epoch£º687	 i:1 	 global-step:13741	 l-p:0.0556659922003746
epoch£º687	 i:2 	 global-step:13742	 l-p:0.05561297386884689
epoch£º687	 i:3 	 global-step:13743	 l-p:0.05562499538064003
epoch£º687	 i:4 	 global-step:13744	 l-p:0.05555415526032448
epoch£º687	 i:5 	 global-step:13745	 l-p:0.05562176927924156
epoch£º687	 i:6 	 global-step:13746	 l-p:0.055650029331445694
epoch£º687	 i:7 	 global-step:13747	 l-p:0.055564865469932556
epoch£º687	 i:8 	 global-step:13748	 l-p:0.05556705221533775
epoch£º687	 i:9 	 global-step:13749	 l-p:0.055651720613241196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8980, 27.8980, 27.8980],
        [27.8980, 31.1436, 31.2829],
        [27.8980, 29.1149, 28.6148],
        [27.8980, 27.8983, 27.8980]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.05566076189279556 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05566076189279556 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0870], device='cuda:0')), ('power', tensor([-0.2705], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:0.05566076189279556
epoch£º688	 i:1 	 global-step:13761	 l-p:0.05558500066399574
epoch£º688	 i:2 	 global-step:13762	 l-p:0.055703774094581604
epoch£º688	 i:3 	 global-step:13763	 l-p:0.05555509775876999
epoch£º688	 i:4 	 global-step:13764	 l-p:0.05562474578619003
epoch£º688	 i:5 	 global-step:13765	 l-p:0.05555605888366699
epoch£º688	 i:6 	 global-step:13766	 l-p:0.05557897314429283
epoch£º688	 i:7 	 global-step:13767	 l-p:0.055581074208021164
epoch£º688	 i:8 	 global-step:13768	 l-p:0.055620238184928894
epoch£º688	 i:9 	 global-step:13769	 l-p:0.05560881644487381
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9474, 28.3170, 28.0496],
        [27.9474, 27.9805, 27.9495],
        [27.9474, 27.9476, 27.9474],
        [27.9474, 27.9474, 27.9474]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.05555793642997742 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555793642997742 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1614], device='cuda:0')), ('power', tensor([-0.3467], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.05555793642997742
epoch£º689	 i:1 	 global-step:13781	 l-p:0.0556020587682724
epoch£º689	 i:2 	 global-step:13782	 l-p:0.055602725595235825
epoch£º689	 i:3 	 global-step:13783	 l-p:0.055572524666786194
epoch£º689	 i:4 	 global-step:13784	 l-p:0.05556604266166687
epoch£º689	 i:5 	 global-step:13785	 l-p:0.05556686595082283
epoch£º689	 i:6 	 global-step:13786	 l-p:0.05561215803027153
epoch£º689	 i:7 	 global-step:13787	 l-p:0.055673759430646896
epoch£º689	 i:8 	 global-step:13788	 l-p:0.05554145202040672
epoch£º689	 i:9 	 global-step:13789	 l-p:0.05565933138132095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0110, 28.0113, 28.0110],
        [28.0110, 37.5902, 44.0706],
        [28.0110, 28.0135, 28.0111],
        [28.0110, 28.0110, 28.0110]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.05555059015750885 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555059015750885 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1644], device='cuda:0')), ('power', tensor([-0.2539], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.05555059015750885
epoch£º690	 i:1 	 global-step:13801	 l-p:0.05566323176026344
epoch£º690	 i:2 	 global-step:13802	 l-p:0.05571160092949867
epoch£º690	 i:3 	 global-step:13803	 l-p:0.05555732920765877
epoch£º690	 i:4 	 global-step:13804	 l-p:0.055591125041246414
epoch£º690	 i:5 	 global-step:13805	 l-p:0.05561269819736481
epoch£º690	 i:6 	 global-step:13806	 l-p:0.05554680526256561
epoch£º690	 i:7 	 global-step:13807	 l-p:0.055526942014694214
epoch£º690	 i:8 	 global-step:13808	 l-p:0.05552434176206589
epoch£º690	 i:9 	 global-step:13809	 l-p:0.055532295256853104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0802, 28.0803, 28.0802],
        [28.0802, 28.7453, 28.3465],
        [28.0802, 33.2981, 34.9150],
        [28.0802, 28.0902, 28.0805]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.055619657039642334 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055619657039642334 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1191], device='cuda:0')), ('power', tensor([-0.1159], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.055619657039642334
epoch£º691	 i:1 	 global-step:13821	 l-p:0.055644433945417404
epoch£º691	 i:2 	 global-step:13822	 l-p:0.055622395128011703
epoch£º691	 i:3 	 global-step:13823	 l-p:0.05554133281111717
epoch£º691	 i:4 	 global-step:13824	 l-p:0.05552295967936516
epoch£º691	 i:5 	 global-step:13825	 l-p:0.05552017688751221
epoch£º691	 i:6 	 global-step:13826	 l-p:0.055540457367897034
epoch£º691	 i:7 	 global-step:13827	 l-p:0.05556154623627663
epoch£º691	 i:8 	 global-step:13828	 l-p:0.055536143481731415
epoch£º691	 i:9 	 global-step:13829	 l-p:0.05556230992078781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1511, 28.1524, 28.1511],
        [28.1511, 28.1536, 28.1512],
        [28.1511, 28.5067, 28.2465],
        [28.1511, 28.1511, 28.1511]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.0555085726082325 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0555085726082325 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1952], device='cuda:0')), ('power', tensor([-0.2006], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.0555085726082325
epoch£º692	 i:1 	 global-step:13841	 l-p:0.05553140491247177
epoch£º692	 i:2 	 global-step:13842	 l-p:0.05554388090968132
epoch£º692	 i:3 	 global-step:13843	 l-p:0.05559047684073448
epoch£º692	 i:4 	 global-step:13844	 l-p:0.05555308610200882
epoch£º692	 i:5 	 global-step:13845	 l-p:0.05554664507508278
epoch£º692	 i:6 	 global-step:13846	 l-p:0.055500250309705734
epoch£º692	 i:7 	 global-step:13847	 l-p:0.055554188787937164
epoch£º692	 i:8 	 global-step:13848	 l-p:0.055619221180677414
epoch£º692	 i:9 	 global-step:13849	 l-p:0.05557606741786003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2227, 28.2395, 28.2234],
        [28.2227, 35.5478, 39.2718],
        [28.2227, 37.4892, 43.5238],
        [28.2227, 37.7734, 44.1709]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.055574964731931686 
model_pd.l_d.mean(): 2.0526390187569632e-07 
model_pd.lagr.mean(): 0.0555751696228981 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.2372e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0842], device='cuda:0')), ('power', tensor([0.1060], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.055574964731931686
epoch£º693	 i:1 	 global-step:13861	 l-p:0.055531080812215805
epoch£º693	 i:2 	 global-step:13862	 l-p:0.05549298971891403
epoch£º693	 i:3 	 global-step:13863	 l-p:0.05559724569320679
epoch£º693	 i:4 	 global-step:13864	 l-p:0.05565618351101875
epoch£º693	 i:5 	 global-step:13865	 l-p:0.05548423156142235
epoch£º693	 i:6 	 global-step:13866	 l-p:0.05552930757403374
epoch£º693	 i:7 	 global-step:13867	 l-p:0.0554899200797081
epoch£º693	 i:8 	 global-step:13868	 l-p:0.0555267408490181
epoch£º693	 i:9 	 global-step:13869	 l-p:0.05549446493387222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2945, 28.6536, 28.3911],
        [28.2945, 31.5005, 31.5908],
        [28.2945, 28.2946, 28.2945],
        [28.2945, 28.2944, 28.2945]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.05559418722987175 
model_pd.l_d.mean(): 7.898887588453363e-07 
model_pd.lagr.mean(): 0.055594976991415024 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.2614e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0841], device='cuda:0')), ('power', tensor([0.1155], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.05559418722987175
epoch£º694	 i:1 	 global-step:13881	 l-p:0.055482201278209686
epoch£º694	 i:2 	 global-step:13882	 l-p:0.055487193167209625
epoch£º694	 i:3 	 global-step:13883	 l-p:0.05559197813272476
epoch£º694	 i:4 	 global-step:13884	 l-p:0.05549135059118271
epoch£º694	 i:5 	 global-step:13885	 l-p:0.055536627769470215
epoch£º694	 i:6 	 global-step:13886	 l-p:0.055488403886556625
epoch£º694	 i:7 	 global-step:13887	 l-p:0.05557120218873024
epoch£º694	 i:8 	 global-step:13888	 l-p:0.05550086870789528
epoch£º694	 i:9 	 global-step:13889	 l-p:0.055487897247076035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3638, 30.7107, 30.3931],
        [28.3638, 32.8780, 33.8679],
        [28.3638, 33.0146, 34.1148],
        [28.3638, 28.3651, 28.3638]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.0555710606276989 
model_pd.l_d.mean(): 7.400177310046274e-06 
model_pd.lagr.mean(): 0.055578459054231644 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.4964e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1433], device='cuda:0')), ('power', tensor([0.1571], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.0555710606276989
epoch£º695	 i:1 	 global-step:13901	 l-p:0.055464018136262894
epoch£º695	 i:2 	 global-step:13902	 l-p:0.05552845820784569
epoch£º695	 i:3 	 global-step:13903	 l-p:0.05552342161536217
epoch£º695	 i:4 	 global-step:13904	 l-p:0.05547787621617317
epoch£º695	 i:5 	 global-step:13905	 l-p:0.055496085435152054
epoch£º695	 i:6 	 global-step:13906	 l-p:0.05554725229740143
epoch£º695	 i:7 	 global-step:13907	 l-p:0.055585723370313644
epoch£º695	 i:8 	 global-step:13908	 l-p:0.05544381961226463
epoch£º695	 i:9 	 global-step:13909	 l-p:0.05546127259731293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4225, 28.4638, 28.4254],
        [28.4225, 32.9465, 33.9387],
        [28.4225, 28.6589, 28.4713],
        [28.4225, 37.6070, 43.4967]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.05545530095696449 
model_pd.l_d.mean(): 1.3512176337826531e-05 
model_pd.lagr.mean(): 0.055468812584877014 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2038], device='cuda:0')), ('power', tensor([0.1127], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.05545530095696449
epoch£º696	 i:1 	 global-step:13921	 l-p:0.05547487735748291
epoch£º696	 i:2 	 global-step:13922	 l-p:0.05545603483915329
epoch£º696	 i:3 	 global-step:13923	 l-p:0.05558096244931221
epoch£º696	 i:4 	 global-step:13924	 l-p:0.055539682507514954
epoch£º696	 i:5 	 global-step:13925	 l-p:0.055531397461891174
epoch£º696	 i:6 	 global-step:13926	 l-p:0.055540718138217926
epoch£º696	 i:7 	 global-step:13927	 l-p:0.055469006299972534
epoch£º696	 i:8 	 global-step:13928	 l-p:0.055454280227422714
epoch£º696	 i:9 	 global-step:13929	 l-p:0.0554954931139946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4603, 28.4602, 28.4602],
        [28.4603, 28.4605, 28.4602],
        [28.4603, 35.7259, 39.3459],
        [28.4603, 28.4642, 28.4603]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.055503662675619125 
model_pd.l_d.mean(): 3.516461219987832e-05 
model_pd.lagr.mean(): 0.05553882569074631 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1722], device='cuda:0')), ('power', tensor([0.1616], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.055503662675619125
epoch£º697	 i:1 	 global-step:13941	 l-p:0.055568769574165344
epoch£º697	 i:2 	 global-step:13942	 l-p:0.05546296387910843
epoch£º697	 i:3 	 global-step:13943	 l-p:0.055464763194322586
epoch£º697	 i:4 	 global-step:13944	 l-p:0.05546221137046814
epoch£º697	 i:5 	 global-step:13945	 l-p:0.055467963218688965
epoch£º697	 i:6 	 global-step:13946	 l-p:0.0554540753364563
epoch£º697	 i:7 	 global-step:13947	 l-p:0.05558321997523308
epoch£º697	 i:8 	 global-step:13948	 l-p:0.055494241416454315
epoch£º697	 i:9 	 global-step:13949	 l-p:0.05548541992902756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4680, 28.6561, 28.5016],
        [28.4680, 28.4792, 28.4684],
        [28.4680, 30.0650, 29.5655],
        [28.4680, 31.1596, 30.9764]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.055461835116147995 
model_pd.l_d.mean(): 5.524570951820351e-05 
model_pd.lagr.mean(): 0.055517081171274185 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1840], device='cuda:0')), ('power', tensor([0.1685], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.055461835116147995
epoch£º698	 i:1 	 global-step:13961	 l-p:0.05544692277908325
epoch£º698	 i:2 	 global-step:13962	 l-p:0.05544506758451462
epoch£º698	 i:3 	 global-step:13963	 l-p:0.055542659014463425
epoch£º698	 i:4 	 global-step:13964	 l-p:0.055553462356328964
epoch£º698	 i:5 	 global-step:13965	 l-p:0.05545562505722046
epoch£º698	 i:6 	 global-step:13966	 l-p:0.0554811991751194
epoch£º698	 i:7 	 global-step:13967	 l-p:0.05551658570766449
epoch£º698	 i:8 	 global-step:13968	 l-p:0.05549030750989914
epoch£º698	 i:9 	 global-step:13969	 l-p:0.055570170283317566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4400, 28.4724, 28.4420],
        [28.4400, 28.7965, 28.5352],
        [28.4400, 31.3616, 31.2898],
        [28.4400, 28.4400, 28.4400]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.05547403544187546 
model_pd.l_d.mean(): 7.604802522109821e-05 
model_pd.lagr.mean(): 0.05555008351802826 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1648], device='cuda:0')), ('power', tensor([0.1753], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.05547403544187546
epoch£º699	 i:1 	 global-step:13981	 l-p:0.0555487684905529
epoch£º699	 i:2 	 global-step:13982	 l-p:0.055561866611242294
epoch£º699	 i:3 	 global-step:13983	 l-p:0.05546087026596069
epoch£º699	 i:4 	 global-step:13984	 l-p:0.05560631677508354
epoch£º699	 i:5 	 global-step:13985	 l-p:0.05546851456165314
epoch£º699	 i:6 	 global-step:13986	 l-p:0.055473364889621735
epoch£º699	 i:7 	 global-step:13987	 l-p:0.055494874715805054
epoch£º699	 i:8 	 global-step:13988	 l-p:0.05547518655657768
epoch£º699	 i:9 	 global-step:13989	 l-p:0.05549018085002899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3744, 28.3972, 28.3755],
        [28.3744, 30.6147, 30.2601],
        [28.3744, 30.1373, 29.6630],
        [28.3744, 28.7420, 28.4746]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.05549175292253494 
model_pd.l_d.mean(): 2.9400222047115676e-05 
model_pd.lagr.mean(): 0.055521152913570404 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1797], device='cuda:0')), ('power', tensor([0.0568], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.05549175292253494
epoch£º700	 i:1 	 global-step:14001	 l-p:0.05558441951870918
epoch£º700	 i:2 	 global-step:14002	 l-p:0.05550723895430565
epoch£º700	 i:3 	 global-step:14003	 l-p:0.05548388510942459
epoch£º700	 i:4 	 global-step:14004	 l-p:0.05555281415581703
epoch£º700	 i:5 	 global-step:14005	 l-p:0.05547783896327019
epoch£º700	 i:6 	 global-step:14006	 l-p:0.05549375340342522
epoch£º700	 i:7 	 global-step:14007	 l-p:0.05547498166561127
epoch£º700	 i:8 	 global-step:14008	 l-p:0.0554976612329483
epoch£º700	 i:9 	 global-step:14009	 l-p:0.0556548647582531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2760, 28.3095, 28.2782],
        [28.2760, 28.7417, 28.4239],
        [28.2760, 28.2760, 28.2760],
        [28.2760, 29.5891, 29.0798]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.055526264011859894 
model_pd.l_d.mean(): 1.1929389074794017e-05 
model_pd.lagr.mean(): 0.05553819239139557 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1291], device='cuda:0')), ('power', tensor([0.0213], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.055526264011859894
epoch£º701	 i:1 	 global-step:14021	 l-p:0.05550599843263626
epoch£º701	 i:2 	 global-step:14022	 l-p:0.055498912930488586
epoch£º701	 i:3 	 global-step:14023	 l-p:0.05554242432117462
epoch£º701	 i:4 	 global-step:14024	 l-p:0.05551052838563919
epoch£º701	 i:5 	 global-step:14025	 l-p:0.05556594207882881
epoch£º701	 i:6 	 global-step:14026	 l-p:0.05558069422841072
epoch£º701	 i:7 	 global-step:14027	 l-p:0.055564578622579575
epoch£º701	 i:8 	 global-step:14028	 l-p:0.055509600788354874
epoch£º701	 i:9 	 global-step:14029	 l-p:0.05563579499721527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1588, 28.5115, 28.2530],
        [28.1588, 35.9074, 40.1201],
        [28.1588, 28.4989, 28.2475],
        [28.1588, 28.1588, 28.1588]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.055538833141326904 
model_pd.l_d.mean(): -5.484090797835961e-05 
model_pd.lagr.mean(): 0.055483993142843246 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1376], device='cuda:0')), ('power', tensor([-0.0996], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.055538833141326904
epoch£º702	 i:1 	 global-step:14041	 l-p:0.055545397102832794
epoch£º702	 i:2 	 global-step:14042	 l-p:0.05556263029575348
epoch£º702	 i:3 	 global-step:14043	 l-p:0.0555412657558918
epoch£º702	 i:4 	 global-step:14044	 l-p:0.055567268282175064
epoch£º702	 i:5 	 global-step:14045	 l-p:0.05555737391114235
epoch£º702	 i:6 	 global-step:14046	 l-p:0.05553262680768967
epoch£º702	 i:7 	 global-step:14047	 l-p:0.05551991984248161
epoch£º702	 i:8 	 global-step:14048	 l-p:0.05563104525208473
epoch£º702	 i:9 	 global-step:14049	 l-p:0.05569090694189072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0397, 28.0524, 28.0402],
        [28.0397, 36.3443, 41.2304],
        [28.0397, 28.0976, 28.0448],
        [28.0397, 36.5270, 41.6331]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.05555053427815437 
model_pd.l_d.mean(): -9.595674782758579e-05 
model_pd.lagr.mean(): 0.05545457825064659 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1393], device='cuda:0')), ('power', tensor([-0.1996], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.05555053427815437
epoch£º703	 i:1 	 global-step:14061	 l-p:0.05562165006995201
epoch£º703	 i:2 	 global-step:14062	 l-p:0.05558103322982788
epoch£º703	 i:3 	 global-step:14063	 l-p:0.05553627014160156
epoch£º703	 i:4 	 global-step:14064	 l-p:0.055553436279296875
epoch£º703	 i:5 	 global-step:14065	 l-p:0.05555824190378189
epoch£º703	 i:6 	 global-step:14066	 l-p:0.055643919855356216
epoch£º703	 i:7 	 global-step:14067	 l-p:0.05559255927801132
epoch£º703	 i:8 	 global-step:14068	 l-p:0.0556349940598011
epoch£º703	 i:9 	 global-step:14069	 l-p:0.055648889392614365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9380, 27.9380, 27.9380],
        [27.9380, 28.1702, 27.9859],
        [27.9380, 28.4038, 28.0872],
        [27.9380, 32.0552, 32.7775]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.05554007738828659 
model_pd.l_d.mean(): -0.00015778483066242188 
model_pd.lagr.mean(): 0.055382292717695236 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2154], device='cuda:0')), ('power', tensor([-0.4450], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.05554007738828659
epoch£º704	 i:1 	 global-step:14081	 l-p:0.055587638169527054
epoch£º704	 i:2 	 global-step:14082	 l-p:0.05563213303685188
epoch£º704	 i:3 	 global-step:14083	 l-p:0.055575236678123474
epoch£º704	 i:4 	 global-step:14084	 l-p:0.05568584427237511
epoch£º704	 i:5 	 global-step:14085	 l-p:0.05559905990958214
epoch£º704	 i:6 	 global-step:14086	 l-p:0.05569303408265114
epoch£º704	 i:7 	 global-step:14087	 l-p:0.055563654750585556
epoch£º704	 i:8 	 global-step:14088	 l-p:0.05563656613230705
epoch£º704	 i:9 	 global-step:14089	 l-p:0.05558932572603226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8733, 27.8733, 27.8733],
        [27.8733, 27.8930, 27.8742],
        [27.8733, 32.1512, 33.0018],
        [27.8733, 34.9170, 38.3873]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 0.05560432747006416 
model_pd.l_d.mean(): -5.88342227274552e-05 
model_pd.lagr.mean(): 0.055545493960380554 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1089], device='cuda:0')), ('power', tensor([-0.3174], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:0.05560432747006416
epoch£º705	 i:1 	 global-step:14101	 l-p:0.055672090500593185
epoch£º705	 i:2 	 global-step:14102	 l-p:0.0556124784052372
epoch£º705	 i:3 	 global-step:14103	 l-p:0.05560377612709999
epoch£º705	 i:4 	 global-step:14104	 l-p:0.05556243658065796
epoch£º705	 i:5 	 global-step:14105	 l-p:0.05566529929637909
epoch£º705	 i:6 	 global-step:14106	 l-p:0.055581606924533844
epoch£º705	 i:7 	 global-step:14107	 l-p:0.055705174803733826
epoch£º705	 i:8 	 global-step:14108	 l-p:0.05559271574020386
epoch£º705	 i:9 	 global-step:14109	 l-p:0.055591754615306854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8604, 28.2842, 27.9884],
        [27.8604, 35.6344, 39.9287],
        [27.8604, 28.0442, 27.8933],
        [27.8604, 30.5912, 30.4581]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.05558475852012634 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05558475852012634 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1403], device='cuda:0')), ('power', tensor([-0.3978], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.05558475852012634
epoch£º706	 i:1 	 global-step:14121	 l-p:0.055578649044036865
epoch£º706	 i:2 	 global-step:14122	 l-p:0.05568806454539299
epoch£º706	 i:3 	 global-step:14123	 l-p:0.05558326467871666
epoch£º706	 i:4 	 global-step:14124	 l-p:0.055625710636377335
epoch£º706	 i:5 	 global-step:14125	 l-p:0.0555977039039135
epoch£º706	 i:6 	 global-step:14126	 l-p:0.05566849559545517
epoch£º706	 i:7 	 global-step:14127	 l-p:0.055696334689855576
epoch£º706	 i:8 	 global-step:14128	 l-p:0.05557900294661522
epoch£º706	 i:9 	 global-step:14129	 l-p:0.05556342378258705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9002, 27.9015, 27.9002],
        [27.9002, 27.9003, 27.9002],
        [27.9002, 27.9004, 27.9002],
        [27.9002, 28.5621, 28.1655]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.05555592477321625 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555592477321625 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1903], device='cuda:0')), ('power', tensor([-0.4148], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.05555592477321625
epoch£º707	 i:1 	 global-step:14141	 l-p:0.05557820573449135
epoch£º707	 i:2 	 global-step:14142	 l-p:0.055586423724889755
epoch£º707	 i:3 	 global-step:14143	 l-p:0.05561450868844986
epoch£º707	 i:4 	 global-step:14144	 l-p:0.05561383068561554
epoch£º707	 i:5 	 global-step:14145	 l-p:0.055606354027986526
epoch£º707	 i:6 	 global-step:14146	 l-p:0.055589865893125534
epoch£º707	 i:7 	 global-step:14147	 l-p:0.055669255554676056
epoch£º707	 i:8 	 global-step:14148	 l-p:0.05554932355880737
epoch£º707	 i:9 	 global-step:14149	 l-p:0.05569068714976311
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9640, 28.3747, 28.0854],
        [27.9640, 29.2617, 28.7582],
        [27.9640, 33.8748, 36.1567],
        [27.9640, 28.3803, 28.0881]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.05557532235980034 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05557532235980034 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1396], device='cuda:0')), ('power', tensor([-0.2799], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.05557532235980034
epoch£º708	 i:1 	 global-step:14161	 l-p:0.055568814277648926
epoch£º708	 i:2 	 global-step:14162	 l-p:0.055558595806360245
epoch£º708	 i:3 	 global-step:14163	 l-p:0.055584657937288284
epoch£º708	 i:4 	 global-step:14164	 l-p:0.05564625561237335
epoch£º708	 i:5 	 global-step:14165	 l-p:0.055623725056648254
epoch£º708	 i:6 	 global-step:14166	 l-p:0.05557447299361229
epoch£º708	 i:7 	 global-step:14167	 l-p:0.05554358288645744
epoch£º708	 i:8 	 global-step:14168	 l-p:0.05566792190074921
epoch£º708	 i:9 	 global-step:14169	 l-p:0.05556795001029968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0367, 30.0777, 29.6761],
        [28.0367, 28.2272, 28.0713],
        [28.0367, 37.9464, 44.8528],
        [28.0367, 32.2590, 33.0519]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.05557629466056824 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05557629466056824 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1406], device='cuda:0')), ('power', tensor([-0.2241], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.05557629466056824
epoch£º709	 i:1 	 global-step:14181	 l-p:0.05554496496915817
epoch£º709	 i:2 	 global-step:14182	 l-p:0.05556144565343857
epoch£º709	 i:3 	 global-step:14183	 l-p:0.05556377395987511
epoch£º709	 i:4 	 global-step:14184	 l-p:0.05554676800966263
epoch£º709	 i:5 	 global-step:14185	 l-p:0.05562165752053261
epoch£º709	 i:6 	 global-step:14186	 l-p:0.05552629381418228
epoch£º709	 i:7 	 global-step:14187	 l-p:0.05570903792977333
epoch£º709	 i:8 	 global-step:14188	 l-p:0.05559254437685013
epoch£º709	 i:9 	 global-step:14189	 l-p:0.055513910949230194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1125, 28.1939, 28.1213],
        [28.1125, 30.1106, 29.6950],
        [28.1125, 29.4818, 28.9759],
        [28.1125, 28.2470, 28.1322]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.055537641048431396 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055537641048431396 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1594], device='cuda:0')), ('power', tensor([-0.2177], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.055537641048431396
epoch£º710	 i:1 	 global-step:14201	 l-p:0.05555424466729164
epoch£º710	 i:2 	 global-step:14202	 l-p:0.05559764802455902
epoch£º710	 i:3 	 global-step:14203	 l-p:0.055557575076818466
epoch£º710	 i:4 	 global-step:14204	 l-p:0.05555425211787224
epoch£º710	 i:5 	 global-step:14205	 l-p:0.055565375834703445
epoch£º710	 i:6 	 global-step:14206	 l-p:0.05550218001008034
epoch£º710	 i:7 	 global-step:14207	 l-p:0.055628444999456406
epoch£º710	 i:8 	 global-step:14208	 l-p:0.05551742762327194
epoch£º710	 i:9 	 global-step:14209	 l-p:0.05558374896645546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1895, 28.2399, 28.1936],
        [28.1895, 35.1868, 38.5570],
        [28.1895, 34.5121, 37.1743],
        [28.1895, 28.1895, 28.1895]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.05556773021817207 
model_pd.l_d.mean(): -4.864360914069721e-09 
model_pd.lagr.mean(): 0.055567726492881775 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0868], device='cuda:0')), ('power', tensor([-0.0175], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.05556773021817207
epoch£º711	 i:1 	 global-step:14221	 l-p:0.055510930716991425
epoch£º711	 i:2 	 global-step:14222	 l-p:0.05552985891699791
epoch£º711	 i:3 	 global-step:14223	 l-p:0.055606141686439514
epoch£º711	 i:4 	 global-step:14224	 l-p:0.055553656071424484
epoch£º711	 i:5 	 global-step:14225	 l-p:0.05557333305478096
epoch£º711	 i:6 	 global-step:14226	 l-p:0.05558299273252487
epoch£º711	 i:7 	 global-step:14227	 l-p:0.05549616739153862
epoch£º711	 i:8 	 global-step:14228	 l-p:0.055487584322690964
epoch£º711	 i:9 	 global-step:14229	 l-p:0.05553119629621506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2672, 34.0178, 36.1058],
        [28.2672, 35.4157, 38.9380],
        [28.2672, 28.6827, 28.3900],
        [28.2672, 36.4612, 41.1735]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 0.05551617965102196 
model_pd.l_d.mean(): 9.723087401880548e-08 
model_pd.lagr.mean(): 0.05551627650856972 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.0798e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1299], device='cuda:0')), ('power', tensor([0.1027], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:0.05551617965102196
epoch£º712	 i:1 	 global-step:14241	 l-p:0.05564827844500542
epoch£º712	 i:2 	 global-step:14242	 l-p:0.055498331785202026
epoch£º712	 i:3 	 global-step:14243	 l-p:0.055483706295490265
epoch£º712	 i:4 	 global-step:14244	 l-p:0.055556170642375946
epoch£º712	 i:5 	 global-step:14245	 l-p:0.05548812448978424
epoch£º712	 i:6 	 global-step:14246	 l-p:0.05552728474140167
epoch£º712	 i:7 	 global-step:14247	 l-p:0.055493585765361786
epoch£º712	 i:8 	 global-step:14248	 l-p:0.055484797805547714
epoch£º712	 i:9 	 global-step:14249	 l-p:0.0555848628282547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3434, 28.3753, 28.3453],
        [28.3434, 31.2778, 31.2180],
        [28.3434, 28.3434, 28.3433],
        [28.3434, 32.4134, 33.0657]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.05546634644269943 
model_pd.l_d.mean(): -4.56017062333558e-07 
model_pd.lagr.mean(): 0.05546589195728302 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.8354e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2101], device='cuda:0')), ('power', tensor([-0.0157], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.05546634644269943
epoch£º713	 i:1 	 global-step:14261	 l-p:0.055659569799900055
epoch£º713	 i:2 	 global-step:14262	 l-p:0.05549933388829231
epoch£º713	 i:3 	 global-step:14263	 l-p:0.0554906502366066
epoch£º713	 i:4 	 global-step:14264	 l-p:0.055476561188697815
epoch£º713	 i:5 	 global-step:14265	 l-p:0.055470943450927734
epoch£º713	 i:6 	 global-step:14266	 l-p:0.055559489876031876
epoch£º713	 i:7 	 global-step:14267	 l-p:0.055480580776929855
epoch£º713	 i:8 	 global-step:14268	 l-p:0.05548296868801117
epoch£º713	 i:9 	 global-step:14269	 l-p:0.05554555729031563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4117, 28.4117, 28.4117],
        [28.4117, 34.1933, 36.2927],
        [28.4117, 32.7771, 33.6455],
        [28.4117, 28.4304, 28.4126]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.05546953156590462 
model_pd.l_d.mean(): 1.2765571227646433e-05 
model_pd.lagr.mean(): 0.055482298135757446 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1859], device='cuda:0')), ('power', tensor([0.1360], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.05546953156590462
epoch£º714	 i:1 	 global-step:14281	 l-p:0.0554720014333725
epoch£º714	 i:2 	 global-step:14282	 l-p:0.055503539741039276
epoch£º714	 i:3 	 global-step:14283	 l-p:0.05556115880608559
epoch£º714	 i:4 	 global-step:14284	 l-p:0.05556909367442131
epoch£º714	 i:5 	 global-step:14285	 l-p:0.05548767000436783
epoch£º714	 i:6 	 global-step:14286	 l-p:0.0554710328578949
epoch£º714	 i:7 	 global-step:14287	 l-p:0.055479440838098526
epoch£º714	 i:8 	 global-step:14288	 l-p:0.055549927055835724
epoch£º714	 i:9 	 global-step:14289	 l-p:0.05544565990567207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4606, 28.5164, 28.4654],
        [28.4606, 37.9547, 44.2282],
        [28.4606, 31.3631, 31.2805],
        [28.4606, 28.4606, 28.4606]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.05547400191426277 
model_pd.l_d.mean(): 3.5385754017625004e-05 
model_pd.lagr.mean(): 0.05550938844680786 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1606], device='cuda:0')), ('power', tensor([0.1875], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.05547400191426277
epoch£º715	 i:1 	 global-step:14301	 l-p:0.055550821125507355
epoch£º715	 i:2 	 global-step:14302	 l-p:0.055463556200265884
epoch£º715	 i:3 	 global-step:14303	 l-p:0.05544378608465195
epoch£º715	 i:4 	 global-step:14304	 l-p:0.05545921251177788
epoch£º715	 i:5 	 global-step:14305	 l-p:0.05550361052155495
epoch£º715	 i:6 	 global-step:14306	 l-p:0.05546203628182411
epoch£º715	 i:7 	 global-step:14307	 l-p:0.055509548634290695
epoch£º715	 i:8 	 global-step:14308	 l-p:0.05554898455739021
epoch£º715	 i:9 	 global-step:14309	 l-p:0.055521849542856216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4783, 33.7706, 35.4090],
        [28.4783, 28.4895, 28.4787],
        [28.4783, 28.4783, 28.4783],
        [28.4783, 37.8415, 43.9448]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.05547066405415535 
model_pd.l_d.mean(): 7.05611237208359e-05 
model_pd.lagr.mean(): 0.05554122477769852 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1531], device='cuda:0')), ('power', tensor([0.2341], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.05547066405415535
epoch£º716	 i:1 	 global-step:14321	 l-p:0.055525217205286026
epoch£º716	 i:2 	 global-step:14322	 l-p:0.05553966015577316
epoch£º716	 i:3 	 global-step:14323	 l-p:0.055461302399635315
epoch£º716	 i:4 	 global-step:14324	 l-p:0.05551338940858841
epoch£º716	 i:5 	 global-step:14325	 l-p:0.055484212934970856
epoch£º716	 i:6 	 global-step:14326	 l-p:0.05550096556544304
epoch£º716	 i:7 	 global-step:14327	 l-p:0.0554482527077198
epoch£º716	 i:8 	 global-step:14328	 l-p:0.05546804144978523
epoch£º716	 i:9 	 global-step:14329	 l-p:0.055523838847875595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4575, 29.0701, 28.6882],
        [28.4575, 29.3051, 28.8499],
        [28.4575, 28.4811, 28.4587],
        [28.4575, 36.0095, 39.9483]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.05544601380825043 
model_pd.l_d.mean(): 5.062306081526913e-05 
model_pd.lagr.mean(): 0.055496636778116226 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2100], device='cuda:0')), ('power', tensor([0.1222], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.05544601380825043
epoch£º717	 i:1 	 global-step:14341	 l-p:0.05556831136345863
epoch£º717	 i:2 	 global-step:14342	 l-p:0.05555978789925575
epoch£º717	 i:3 	 global-step:14343	 l-p:0.05545562133193016
epoch£º717	 i:4 	 global-step:14344	 l-p:0.055456288158893585
epoch£º717	 i:5 	 global-step:14345	 l-p:0.055463772267103195
epoch£º717	 i:6 	 global-step:14346	 l-p:0.05551595613360405
epoch£º717	 i:7 	 global-step:14347	 l-p:0.055496785789728165
epoch£º717	 i:8 	 global-step:14348	 l-p:0.05556264892220497
epoch£º717	 i:9 	 global-step:14349	 l-p:0.05549049377441406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[28.3948, 30.1937, 29.7252],
        [28.3948, 29.6344, 29.1249],
        [28.3948, 38.1172, 44.6985],
        [28.3948, 29.3296, 28.8560]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.05545979365706444 
model_pd.l_d.mean(): 2.662221777427476e-05 
model_pd.lagr.mean(): 0.05548641458153725 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2149], device='cuda:0')), ('power', tensor([0.0524], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.05545979365706444
epoch£º718	 i:1 	 global-step:14361	 l-p:0.05548062175512314
epoch£º718	 i:2 	 global-step:14362	 l-p:0.05557534098625183
epoch£º718	 i:3 	 global-step:14363	 l-p:0.055475685745477676
epoch£º718	 i:4 	 global-step:14364	 l-p:0.05548432096838951
epoch£º718	 i:5 	 global-step:14365	 l-p:0.055527254939079285
epoch£º718	 i:6 	 global-step:14366	 l-p:0.05550597235560417
epoch£º718	 i:7 	 global-step:14367	 l-p:0.05563310533761978
epoch£º718	 i:8 	 global-step:14368	 l-p:0.05550018325448036
epoch£º718	 i:9 	 global-step:14369	 l-p:0.05553578585386276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2955, 29.3324, 28.8429],
        [28.2955, 33.8384, 35.7270],
        [28.2955, 35.5170, 39.1148],
        [28.2955, 28.4025, 28.3091]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.055638108402490616 
model_pd.l_d.mean(): 0.0001174779754364863 
model_pd.lagr.mean(): 0.05575558543205261 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0567], device='cuda:0')), ('power', tensor([0.2093], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.055638108402490616
epoch£º719	 i:1 	 global-step:14381	 l-p:0.055515773594379425
epoch£º719	 i:2 	 global-step:14382	 l-p:0.05551067367196083
epoch£º719	 i:3 	 global-step:14383	 l-p:0.055493585765361786
epoch£º719	 i:4 	 global-step:14384	 l-p:0.055594004690647125
epoch£º719	 i:5 	 global-step:14385	 l-p:0.05552227422595024
epoch£º719	 i:6 	 global-step:14386	 l-p:0.055491719394922256
epoch£º719	 i:7 	 global-step:14387	 l-p:0.05552934855222702
epoch£º719	 i:8 	 global-step:14388	 l-p:0.05557738244533539
epoch£º719	 i:9 	 global-step:14389	 l-p:0.055532049387693405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1714, 28.5540, 28.2789],
        [28.1714, 29.9593, 29.4954],
        [28.1714, 32.4152, 33.2122],
        [28.1714, 30.6102, 30.3354]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.05553605780005455 
model_pd.l_d.mean(): -5.2607007091864944e-05 
model_pd.lagr.mean(): 0.05548344925045967 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1587], device='cuda:0')), ('power', tensor([-0.0941], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.05553605780005455
epoch£º720	 i:1 	 global-step:14401	 l-p:0.05558039993047714
epoch£º720	 i:2 	 global-step:14402	 l-p:0.0556027889251709
epoch£º720	 i:3 	 global-step:14403	 l-p:0.05563349649310112
epoch£º720	 i:4 	 global-step:14404	 l-p:0.055572688579559326
epoch£º720	 i:5 	 global-step:14405	 l-p:0.05554439499974251
epoch£º720	 i:6 	 global-step:14406	 l-p:0.055511314421892166
epoch£º720	 i:7 	 global-step:14407	 l-p:0.055544231086969376
epoch£º720	 i:8 	 global-step:14408	 l-p:0.05559245124459267
epoch£º720	 i:9 	 global-step:14409	 l-p:0.05555231124162674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[28.0407, 32.8886, 34.1862],
        [28.0407, 37.6305, 44.1181],
        [28.0407, 30.6623, 30.4698],
        [28.0407, 31.2163, 31.3055]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.055624742060899734 
model_pd.l_d.mean(): -7.377610018011183e-05 
model_pd.lagr.mean(): 0.055550966411828995 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1149], device='cuda:0')), ('power', tensor([-0.1496], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.055624742060899734
epoch£º721	 i:1 	 global-step:14421	 l-p:0.05556069314479828
epoch£º721	 i:2 	 global-step:14422	 l-p:0.05554350093007088
epoch£º721	 i:3 	 global-step:14423	 l-p:0.05561000108718872
epoch£º721	 i:4 	 global-step:14424	 l-p:0.055545613169670105
epoch£º721	 i:5 	 global-step:14425	 l-p:0.05562788248062134
epoch£º721	 i:6 	 global-step:14426	 l-p:0.05561219900846481
epoch£º721	 i:7 	 global-step:14427	 l-p:0.05554460361599922
epoch£º721	 i:8 	 global-step:14428	 l-p:0.055658791214227676
epoch£º721	 i:9 	 global-step:14429	 l-p:0.05560292303562164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9267, 27.9267, 27.9267],
        [27.9267, 27.9374, 27.9271],
        [27.9267, 27.9267, 27.9267],
        [27.9267, 28.1652, 27.9768]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.05559128522872925 
model_pd.l_d.mean(): -0.00011596876720432192 
model_pd.lagr.mean(): 0.05547531694173813 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1162], device='cuda:0')), ('power', tensor([-0.3180], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.05559128522872925
epoch£º722	 i:1 	 global-step:14441	 l-p:0.05569585785269737
epoch£º722	 i:2 	 global-step:14442	 l-p:0.055557698011398315
epoch£º722	 i:3 	 global-step:14443	 l-p:0.05562099441885948
epoch£º722	 i:4 	 global-step:14444	 l-p:0.055657342076301575
epoch£º722	 i:5 	 global-step:14445	 l-p:0.055638380348682404
epoch£º722	 i:6 	 global-step:14446	 l-p:0.05563180893659592
epoch£º722	 i:7 	 global-step:14447	 l-p:0.05556673929095268
epoch£º722	 i:8 	 global-step:14448	 l-p:0.055589139461517334
epoch£º722	 i:9 	 global-step:14449	 l-p:0.0555865652859211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8519, 28.1935, 27.9420],
        [27.8519, 33.3794, 35.3077],
        [27.8519, 27.8526, 27.8519],
        [27.8519, 27.8519, 27.8519]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.05558893084526062 
model_pd.l_d.mean(): -7.800833554938436e-05 
model_pd.lagr.mean(): 0.05551092326641083 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1438], device='cuda:0')), ('power', tensor([-0.4162], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.05558893084526062
epoch£º723	 i:1 	 global-step:14461	 l-p:0.05567515268921852
epoch£º723	 i:2 	 global-step:14462	 l-p:0.05566461384296417
epoch£º723	 i:3 	 global-step:14463	 l-p:0.055637672543525696
epoch£º723	 i:4 	 global-step:14464	 l-p:0.05560370907187462
epoch£º723	 i:5 	 global-step:14465	 l-p:0.05559245124459267
epoch£º723	 i:6 	 global-step:14466	 l-p:0.05562954768538475
epoch£º723	 i:7 	 global-step:14467	 l-p:0.05567225441336632
epoch£º723	 i:8 	 global-step:14468	 l-p:0.055581092834472656
epoch£º723	 i:9 	 global-step:14469	 l-p:0.055595763027668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8358, 27.8476, 27.8363],
        [27.8358, 33.7181, 35.9888],
        [27.8358, 27.8890, 27.8403],
        [27.8358, 29.8613, 29.4627]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.055648915469646454 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055648915469646454 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1075], device='cuda:0')), ('power', tensor([-0.3929], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.055648915469646454
epoch£º724	 i:1 	 global-step:14481	 l-p:0.05559616535902023
epoch£º724	 i:2 	 global-step:14482	 l-p:0.055616531521081924
epoch£º724	 i:3 	 global-step:14483	 l-p:0.05562293902039528
epoch£º724	 i:4 	 global-step:14484	 l-p:0.055640969425439835
epoch£º724	 i:5 	 global-step:14485	 l-p:0.05558282881975174
epoch£º724	 i:6 	 global-step:14486	 l-p:0.05556527152657509
epoch£º724	 i:7 	 global-step:14487	 l-p:0.055591240525245667
epoch£º724	 i:8 	 global-step:14488	 l-p:0.05570681765675545
epoch£º724	 i:9 	 global-step:14489	 l-p:0.055643245577812195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8783, 27.8966, 27.8792],
        [27.8783, 32.6966, 33.9862],
        [27.8783, 32.1037, 32.9132],
        [27.8783, 28.7101, 28.2641]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.055583637207746506 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055583637207746506 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1582], device='cuda:0')), ('power', tensor([-0.4602], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.055583637207746506
epoch£º725	 i:1 	 global-step:14501	 l-p:0.055686015635728836
epoch£º725	 i:2 	 global-step:14502	 l-p:0.05558915436267853
epoch£º725	 i:3 	 global-step:14503	 l-p:0.05556776374578476
epoch£º725	 i:4 	 global-step:14504	 l-p:0.05563211441040039
epoch£º725	 i:5 	 global-step:14505	 l-p:0.05557350441813469
epoch£º725	 i:6 	 global-step:14506	 l-p:0.055565137416124344
epoch£º725	 i:7 	 global-step:14507	 l-p:0.05566343292593956
epoch£º725	 i:8 	 global-step:14508	 l-p:0.055621594190597534
epoch£º725	 i:9 	 global-step:14509	 l-p:0.05561375990509987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[27.9470, 28.7205, 28.2890],
        [27.9470, 30.6869, 30.5534],
        [27.9470, 34.0418, 36.5079],
        [27.9470, 36.1380, 40.9058]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.05566846579313278 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05566846579313278 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0618], device='cuda:0')), ('power', tensor([-0.2134], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.05566846579313278
epoch£º726	 i:1 	 global-step:14521	 l-p:0.05558524653315544
epoch£º726	 i:2 	 global-step:14522	 l-p:0.05558387190103531
epoch£º726	 i:3 	 global-step:14523	 l-p:0.055546730756759644
epoch£º726	 i:4 	 global-step:14524	 l-p:0.05556054040789604
epoch£º726	 i:5 	 global-step:14525	 l-p:0.055601879954338074
epoch£º726	 i:6 	 global-step:14526	 l-p:0.055663179606199265
epoch£º726	 i:7 	 global-step:14527	 l-p:0.055598046630620956
epoch£º726	 i:8 	 global-step:14528	 l-p:0.05554310977458954
epoch£º726	 i:9 	 global-step:14529	 l-p:0.05559096857905388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0250, 28.0250, 28.0250],
        [28.0250, 35.7338, 39.9239],
        [28.0250, 37.6389, 44.1612],
        [28.0250, 28.0250, 28.0250]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.05555158481001854 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05555158481001854 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1551], device='cuda:0')), ('power', tensor([-0.2281], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.05555158481001854
epoch£º727	 i:1 	 global-step:14541	 l-p:0.055536478757858276
epoch£º727	 i:2 	 global-step:14542	 l-p:0.055536527186632156
epoch£º727	 i:3 	 global-step:14543	 l-p:0.05556230992078781
epoch£º727	 i:4 	 global-step:14544	 l-p:0.055554501712322235
epoch£º727	 i:5 	 global-step:14545	 l-p:0.05565258488059044
epoch£º727	 i:6 	 global-step:14546	 l-p:0.05561487376689911
epoch£º727	 i:7 	 global-step:14547	 l-p:0.05556803196668625
epoch£º727	 i:8 	 global-step:14548	 l-p:0.055625561624765396
epoch£º727	 i:9 	 global-step:14549	 l-p:0.05557308718562126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1063, 28.1676, 28.1119],
        [28.1063, 28.1095, 28.1063],
        [28.1063, 32.3397, 33.1347],
        [28.1063, 37.9553, 44.7660]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.055538520216941833 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055538520216941833 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1497], device='cuda:0')), ('power', tensor([-0.1701], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.055538520216941833
epoch£º728	 i:1 	 global-step:14561	 l-p:0.055616091936826706
epoch£º728	 i:2 	 global-step:14562	 l-p:0.05557045713067055
epoch£º728	 i:3 	 global-step:14563	 l-p:0.055540043860673904
epoch£º728	 i:4 	 global-step:14564	 l-p:0.05567912012338638
epoch£º728	 i:5 	 global-step:14565	 l-p:0.05553550273180008
epoch£º728	 i:6 	 global-step:14566	 l-p:0.0555824413895607
epoch£º728	 i:7 	 global-step:14567	 l-p:0.05552958697080612
epoch£º728	 i:8 	 global-step:14568	 l-p:0.055513933300971985
epoch£º728	 i:9 	 global-step:14569	 l-p:0.055500615388154984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1892, 29.1016, 28.6346],
        [28.1892, 28.5956, 28.3078],
        [28.1892, 29.4113, 28.9061],
        [28.1892, 28.1892, 28.1892]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.05559981241822243 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05559981241822243 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.3306e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0640], device='cuda:0')), ('power', tensor([0.0866], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:0.05559981241822243
epoch£º729	 i:1 	 global-step:14581	 l-p:0.05554146692156792
epoch£º729	 i:2 	 global-step:14582	 l-p:0.05572173371911049
epoch£º729	 i:3 	 global-step:14583	 l-p:0.055494122207164764
epoch£º729	 i:4 	 global-step:14584	 l-p:0.0555325411260128
epoch£º729	 i:5 	 global-step:14585	 l-p:0.055499352514743805
epoch£º729	 i:6 	 global-step:14586	 l-p:0.055505670607089996
epoch£º729	 i:7 	 global-step:14587	 l-p:0.055513229221105576
epoch£º729	 i:8 	 global-step:14588	 l-p:0.055520590394735336
epoch£º729	 i:9 	 global-step:14589	 l-p:0.05550708621740341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[28.2723, 33.9523, 35.9724],
        [28.2723, 36.4679, 41.1811],
        [28.2723, 32.6150, 33.4788],
        [28.2723, 32.7712, 33.7576]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.05559451878070831 
model_pd.l_d.mean(): 6.985167999573605e-08 
model_pd.lagr.mean(): 0.055594589561223984 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.8582e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0959], device='cuda:0')), ('power', tensor([0.1477], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.05559451878070831
epoch£º730	 i:1 	 global-step:14601	 l-p:0.05549045279622078
epoch£º730	 i:2 	 global-step:14602	 l-p:0.0555608794093132
epoch£º730	 i:3 	 global-step:14603	 l-p:0.055486951023340225
epoch£º730	 i:4 	 global-step:14604	 l-p:0.055599331855773926
epoch£º730	 i:5 	 global-step:14605	 l-p:0.05552089586853981
epoch£º730	 i:6 	 global-step:14606	 l-p:0.05554572865366936
epoch£º730	 i:7 	 global-step:14607	 l-p:0.0554826483130455
epoch£º730	 i:8 	 global-step:14608	 l-p:0.05549681559205055
epoch£º730	 i:9 	 global-step:14609	 l-p:0.05548769235610962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3541, 37.4893, 43.3311],
        [28.3541, 29.7526, 29.2425],
        [28.3541, 31.2450, 31.1626],
        [28.3541, 28.4033, 28.3580]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.05550074949860573 
model_pd.l_d.mean(): 3.7986628740327433e-06 
model_pd.lagr.mean(): 0.05550454929471016 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.8333e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1370], device='cuda:0')), ('power', tensor([0.1169], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.05550074949860573
epoch£º731	 i:1 	 global-step:14621	 l-p:0.05546073615550995
epoch£º731	 i:2 	 global-step:14622	 l-p:0.05548608675599098
epoch£º731	 i:3 	 global-step:14623	 l-p:0.05553113669157028
epoch£º731	 i:4 	 global-step:14624	 l-p:0.055560726672410965
epoch£º731	 i:5 	 global-step:14625	 l-p:0.05551351606845856
epoch£º731	 i:6 	 global-step:14626	 l-p:0.05552191659808159
epoch£º731	 i:7 	 global-step:14627	 l-p:0.05550998076796532
epoch£º731	 i:8 	 global-step:14628	 l-p:0.05556856840848923
epoch£º731	 i:9 	 global-step:14629	 l-p:0.05545279011130333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4264, 34.4296, 36.7412],
        [28.4264, 28.4366, 28.4268],
        [28.4264, 28.4264, 28.4264],
        [28.4264, 28.4425, 28.4271]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.05554933845996857 
model_pd.l_d.mean(): 2.4860730263753794e-05 
model_pd.lagr.mean(): 0.0555742010474205 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1160], device='cuda:0')), ('power', tensor([0.2403], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:0.05554933845996857
epoch£º732	 i:1 	 global-step:14641	 l-p:0.05549343302845955
epoch£º732	 i:2 	 global-step:14642	 l-p:0.05548772215843201
epoch£º732	 i:3 	 global-step:14643	 l-p:0.055500105023384094
epoch£º732	 i:4 	 global-step:14644	 l-p:0.055578868836164474
epoch£º732	 i:5 	 global-step:14645	 l-p:0.05547528713941574
epoch£º732	 i:6 	 global-step:14646	 l-p:0.05546360835433006
epoch£º732	 i:7 	 global-step:14647	 l-p:0.05545733869075775
epoch£º732	 i:8 	 global-step:14648	 l-p:0.05546058714389801
epoch£º732	 i:9 	 global-step:14649	 l-p:0.055512044578790665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4759, 28.4759, 28.4759],
        [28.4759, 28.5304, 28.4805],
        [28.4759, 28.9097, 28.6070],
        [28.4759, 28.6362, 28.5019]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.055513229221105576 
model_pd.l_d.mean(): 5.620987212751061e-05 
model_pd.lagr.mean(): 0.05556944012641907 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1442], device='cuda:0')), ('power', tensor([0.2729], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.055513229221105576
epoch£º733	 i:1 	 global-step:14661	 l-p:0.055476732552051544
epoch£º733	 i:2 	 global-step:14662	 l-p:0.05546668916940689
epoch£º733	 i:3 	 global-step:14663	 l-p:0.05545831099152565
epoch£º733	 i:4 	 global-step:14664	 l-p:0.0554877407848835
epoch£º733	 i:5 	 global-step:14665	 l-p:0.05544652044773102
epoch£º733	 i:6 	 global-step:14666	 l-p:0.05545184016227722
epoch£º733	 i:7 	 global-step:14667	 l-p:0.05557514727115631
epoch£º733	 i:8 	 global-step:14668	 l-p:0.05554528161883354
epoch£º733	 i:9 	 global-step:14669	 l-p:0.05548817664384842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4890, 28.4943, 28.4891],
        [28.4890, 36.0499, 39.9932],
        [28.4890, 37.6961, 43.6003],
        [28.4890, 32.3549, 32.8497]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.05546415224671364 
model_pd.l_d.mean(): 6.809794285800308e-05 
model_pd.lagr.mean(): 0.05553225055336952 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1691], device='cuda:0')), ('power', tensor([0.2092], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.05546415224671364
epoch£º734	 i:1 	 global-step:14681	 l-p:0.05546838045120239
epoch£º734	 i:2 	 global-step:14682	 l-p:0.05544529855251312
epoch£º734	 i:3 	 global-step:14683	 l-p:0.0554739274084568
epoch£º734	 i:4 	 global-step:14684	 l-p:0.055454324930906296
epoch£º734	 i:5 	 global-step:14685	 l-p:0.055528298020362854
epoch£º734	 i:6 	 global-step:14686	 l-p:0.0554945170879364
epoch£º734	 i:7 	 global-step:14687	 l-p:0.05562888830900192
epoch£º734	 i:8 	 global-step:14688	 l-p:0.05543310567736626
epoch£º734	 i:9 	 global-step:14689	 l-p:0.05553191900253296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4578, 30.9230, 30.6453],
        [28.4578, 28.9267, 28.6068],
        [28.4578, 34.1121, 36.0852],
        [28.4578, 33.7462, 35.3832]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.05550728738307953 
model_pd.l_d.mean(): 9.396926179761067e-05 
model_pd.lagr.mean(): 0.05560125783085823 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1617], device='cuda:0')), ('power', tensor([0.2127], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.05550728738307953
epoch£º735	 i:1 	 global-step:14701	 l-p:0.055457692593336105
epoch£º735	 i:2 	 global-step:14702	 l-p:0.055483922362327576
epoch£º735	 i:3 	 global-step:14703	 l-p:0.05549657717347145
epoch£º735	 i:4 	 global-step:14704	 l-p:0.0555526539683342
epoch£º735	 i:5 	 global-step:14705	 l-p:0.05556824058294296
epoch£º735	 i:6 	 global-step:14706	 l-p:0.05547803267836571
epoch£º735	 i:7 	 global-step:14707	 l-p:0.05550846830010414
epoch£º735	 i:8 	 global-step:14708	 l-p:0.0554635226726532
epoch£º735	 i:9 	 global-step:14709	 l-p:0.05551117658615112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3807, 37.6238, 43.5958],
        [28.3807, 28.7364, 28.4757],
        [28.3807, 31.3679, 31.3331],
        [28.3807, 31.2745, 31.1921]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.055485352873802185 
model_pd.l_d.mean(): 8.322267240146175e-05 
model_pd.lagr.mean(): 0.055568575859069824 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1693], device='cuda:0')), ('power', tensor([0.1564], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.055485352873802185
epoch£º736	 i:1 	 global-step:14721	 l-p:0.055467117577791214
epoch£º736	 i:2 	 global-step:14722	 l-p:0.055484142154455185
epoch£º736	 i:3 	 global-step:14723	 l-p:0.055596932768821716
epoch£º736	 i:4 	 global-step:14724	 l-p:0.05561854690313339
epoch£º736	 i:5 	 global-step:14725	 l-p:0.055487461388111115
epoch£º736	 i:6 	 global-step:14726	 l-p:0.05548560991883278
epoch£º736	 i:7 	 global-step:14727	 l-p:0.05549727380275726
epoch£º736	 i:8 	 global-step:14728	 l-p:0.05558666214346886
epoch£º736	 i:9 	 global-step:14729	 l-p:0.055513009428977966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2642, 28.4262, 28.2907],
        [28.2642, 38.1337, 44.9357],
        [28.2642, 33.6970, 35.4877],
        [28.2642, 37.6692, 43.8711]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.05549704283475876 
model_pd.l_d.mean(): -4.229527621646412e-05 
model_pd.lagr.mean(): 0.05545474588871002 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1904], device='cuda:0')), ('power', tensor([-0.0736], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.05549704283475876
epoch£º737	 i:1 	 global-step:14741	 l-p:0.05554667487740517
epoch£º737	 i:2 	 global-step:14742	 l-p:0.05548759177327156
epoch£º737	 i:3 	 global-step:14743	 l-p:0.055591411888599396
epoch£º737	 i:4 	 global-step:14744	 l-p:0.05551394447684288
epoch£º737	 i:5 	 global-step:14745	 l-p:0.05560920760035515
epoch£º737	 i:6 	 global-step:14746	 l-p:0.05553007498383522
epoch£º737	 i:7 	 global-step:14747	 l-p:0.05565190315246582
epoch£º737	 i:8 	 global-step:14748	 l-p:0.05552391707897186
epoch£º737	 i:9 	 global-step:14749	 l-p:0.05553283542394638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1254, 28.1805, 28.1302],
        [28.1254, 28.1362, 28.1258],
        [28.1254, 28.1285, 28.1255],
        [28.1254, 28.1255, 28.1254]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.055547378957271576 
model_pd.l_d.mean(): -6.921244494151324e-05 
model_pd.lagr.mean(): 0.05547816678881645 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1322], device='cuda:0')), ('power', tensor([-0.1250], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.055547378957271576
epoch£º738	 i:1 	 global-step:14761	 l-p:0.05556334927678108
epoch£º738	 i:2 	 global-step:14762	 l-p:0.055719487369060516
epoch£º738	 i:3 	 global-step:14763	 l-p:0.05558254197239876
epoch£º738	 i:4 	 global-step:14764	 l-p:0.05558484047651291
epoch£º738	 i:5 	 global-step:14765	 l-p:0.05557091906666756
epoch£º738	 i:6 	 global-step:14766	 l-p:0.055560678243637085
epoch£º738	 i:7 	 global-step:14767	 l-p:0.05552797019481659
epoch£º738	 i:8 	 global-step:14768	 l-p:0.05554882809519768
epoch£º738	 i:9 	 global-step:14769	 l-p:0.05556798353791237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9866, 27.9866, 27.9866],
        [27.9866, 27.9866, 27.9866],
        [27.9866, 33.8480, 36.0787],
        [27.9866, 28.8288, 28.3793]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.05562084540724754 
model_pd.l_d.mean(): -8.675899152876809e-05 
model_pd.lagr.mean(): 0.055534087121486664 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1034], device='cuda:0')), ('power', tensor([-0.1876], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.05562084540724754
epoch£º739	 i:1 	 global-step:14781	 l-p:0.05562619864940643
epoch£º739	 i:2 	 global-step:14782	 l-p:0.05556193366646767
epoch£º739	 i:3 	 global-step:14783	 l-p:0.055620867758989334
epoch£º739	 i:4 	 global-step:14784	 l-p:0.05561055615544319
epoch£º739	 i:5 	 global-step:14785	 l-p:0.05558779090642929
epoch£º739	 i:6 	 global-step:14786	 l-p:0.055600766092538834
epoch£º739	 i:7 	 global-step:14787	 l-p:0.05556627735495567
epoch£º739	 i:8 	 global-step:14788	 l-p:0.055669959634542465
epoch£º739	 i:9 	 global-step:14789	 l-p:0.05557940527796745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8737, 27.8737, 27.8737],
        [27.8737, 27.9235, 27.8778],
        [27.8737, 29.8160, 29.3946],
        [27.8737, 31.6345, 32.1060]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.05569502338767052 
model_pd.l_d.mean(): -8.487404556944966e-05 
model_pd.lagr.mean(): 0.05561015009880066 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0854], device='cuda:0')), ('power', tensor([-0.2766], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.05569502338767052
epoch£º740	 i:1 	 global-step:14801	 l-p:0.055577728897333145
epoch£º740	 i:2 	 global-step:14802	 l-p:0.055638331919908524
epoch£º740	 i:3 	 global-step:14803	 l-p:0.0556180477142334
epoch£º740	 i:4 	 global-step:14804	 l-p:0.05566791817545891
epoch£º740	 i:5 	 global-step:14805	 l-p:0.05557418614625931
epoch£º740	 i:6 	 global-step:14806	 l-p:0.055697422474622726
epoch£º740	 i:7 	 global-step:14807	 l-p:0.05558674782514572
epoch£º740	 i:8 	 global-step:14808	 l-p:0.055599749088287354
epoch£º740	 i:9 	 global-step:14809	 l-p:0.05558157339692116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8129, 32.3354, 33.3858],
        [27.8129, 37.3267, 43.7659],
        [27.8129, 32.4564, 33.6060],
        [27.8129, 27.8129, 27.8129]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.05560639128088951 
model_pd.l_d.mean(): -4.4813641579821706e-05 
model_pd.lagr.mean(): 0.055561576038599014 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.4700e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1232], device='cuda:0')), ('power', tensor([-0.4233], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.05560639128088951
epoch£º741	 i:1 	 global-step:14821	 l-p:0.0556132048368454
epoch£º741	 i:2 	 global-step:14822	 l-p:0.05570840835571289
epoch£º741	 i:3 	 global-step:14823	 l-p:0.05557914450764656
epoch£º741	 i:4 	 global-step:14824	 l-p:0.055675752460956573
epoch£º741	 i:5 	 global-step:14825	 l-p:0.05559948459267616
epoch£º741	 i:6 	 global-step:14826	 l-p:0.055625561624765396
epoch£º741	 i:7 	 global-step:14827	 l-p:0.05558625981211662
epoch£º741	 i:8 	 global-step:14828	 l-p:0.05561921000480652
epoch£º741	 i:9 	 global-step:14829	 l-p:0.05568632856011391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8231, 37.3644, 43.8371],
        [27.8231, 28.6015, 28.1697],
        [27.8231, 27.8403, 27.8238],
        [27.8231, 30.1343, 29.8271]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.055636364966630936 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055636364966630936 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0721], device='cuda:0')), ('power', tensor([-0.3590], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.055636364966630936
epoch£º742	 i:1 	 global-step:14841	 l-p:0.0555836483836174
epoch£º742	 i:2 	 global-step:14842	 l-p:0.05561311915516853
epoch£º742	 i:3 	 global-step:14843	 l-p:0.0557519905269146
epoch£º742	 i:4 	 global-step:14844	 l-p:0.055577654391527176
epoch£º742	 i:5 	 global-step:14845	 l-p:0.055702656507492065
epoch£º742	 i:6 	 global-step:14846	 l-p:0.05560802295804024
epoch£º742	 i:7 	 global-step:14847	 l-p:0.05557166412472725
epoch£º742	 i:8 	 global-step:14848	 l-p:0.05556584149599075
epoch£º742	 i:9 	 global-step:14849	 l-p:0.055612221360206604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8834, 27.8836, 27.8834],
        [27.8834, 28.2199, 27.9712],
        [27.8834, 33.9636, 36.4236],
        [27.8834, 32.3168, 33.2886]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.05562444403767586 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05562444403767586 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0911], device='cuda:0')), ('power', tensor([-0.3269], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.05562444403767586
epoch£º743	 i:1 	 global-step:14861	 l-p:0.055600471794605255
epoch£º743	 i:2 	 global-step:14862	 l-p:0.055582646280527115
epoch£º743	 i:3 	 global-step:14863	 l-p:0.05557167902588844
epoch£º743	 i:4 	 global-step:14864	 l-p:0.0556933730840683
epoch£º743	 i:5 	 global-step:14865	 l-p:0.05561605840921402
epoch£º743	 i:6 	 global-step:14866	 l-p:0.05554939806461334
epoch£º743	 i:7 	 global-step:14867	 l-p:0.05558329448103905
epoch£º743	 i:8 	 global-step:14868	 l-p:0.05565967410802841
epoch£º743	 i:9 	 global-step:14869	 l-p:0.055594511330127716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9619, 28.4282, 28.1112],
        [27.9619, 28.8181, 28.3656],
        [27.9619, 28.4186, 28.1063],
        [27.9619, 28.1472, 27.9952]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.05554405227303505 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554405227303505 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1969], device='cuda:0')), ('power', tensor([-0.4086], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.05554405227303505
epoch£º744	 i:1 	 global-step:14881	 l-p:0.0556081086397171
epoch£º744	 i:2 	 global-step:14882	 l-p:0.05556920915842056
epoch£º744	 i:3 	 global-step:14883	 l-p:0.05563421919941902
epoch£º744	 i:4 	 global-step:14884	 l-p:0.05555625259876251
epoch£º744	 i:5 	 global-step:14885	 l-p:0.05559415742754936
epoch£º744	 i:6 	 global-step:14886	 l-p:0.055535946041345596
epoch£º744	 i:7 	 global-step:14887	 l-p:0.055555690079927444
epoch£º744	 i:8 	 global-step:14888	 l-p:0.055565498769283295
epoch£º744	 i:9 	 global-step:14889	 l-p:0.05574030801653862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0470, 28.2047, 28.0726],
        [28.0470, 28.0472, 28.0470],
        [28.0470, 37.8372, 44.5842],
        [28.0470, 33.0064, 34.3991]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.05560849979519844 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05560849979519844 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0942], device='cuda:0')), ('power', tensor([-0.1427], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.05560849979519844
epoch£º745	 i:1 	 global-step:14901	 l-p:0.055561088025569916
epoch£º745	 i:2 	 global-step:14902	 l-p:0.05553334206342697
epoch£º745	 i:3 	 global-step:14903	 l-p:0.055542752146720886
epoch£º745	 i:4 	 global-step:14904	 l-p:0.05554121732711792
epoch£º745	 i:5 	 global-step:14905	 l-p:0.0557124987244606
epoch£º745	 i:6 	 global-step:14906	 l-p:0.05562032014131546
epoch£º745	 i:7 	 global-step:14907	 l-p:0.05557067692279816
epoch£º745	 i:8 	 global-step:14908	 l-p:0.05551227554678917
epoch£º745	 i:9 	 global-step:14909	 l-p:0.05552142858505249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1349, 28.1635, 28.1366],
        [28.1349, 28.1773, 28.1380],
        [28.1349, 35.1180, 38.4812],
        [28.1349, 28.1349, 28.1349]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.05563477799296379 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05563477799296379 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.2700e-08], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0937], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.05563477799296379
epoch£º746	 i:1 	 global-step:14921	 l-p:0.05551137030124664
epoch£º746	 i:2 	 global-step:14922	 l-p:0.055604271590709686
epoch£º746	 i:3 	 global-step:14923	 l-p:0.0555153451859951
epoch£º746	 i:4 	 global-step:14924	 l-p:0.055519070476293564
epoch£º746	 i:5 	 global-step:14925	 l-p:0.0555071160197258
epoch£º746	 i:6 	 global-step:14926	 l-p:0.05556280538439751
epoch£º746	 i:7 	 global-step:14927	 l-p:0.055549755692481995
epoch£º746	 i:8 	 global-step:14928	 l-p:0.055569007992744446
epoch£º746	 i:9 	 global-step:14929	 l-p:0.05556793510913849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2237, 28.4958, 28.2853],
        [28.2237, 28.4108, 28.2572],
        [28.2237, 34.7862, 37.6902],
        [28.2237, 34.9734, 38.0738]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.05555464327335358 
model_pd.l_d.mean(): 1.1814587530523113e-08 
model_pd.lagr.mean(): 0.05555465444922447 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.9549e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1328], device='cuda:0')), ('power', tensor([0.0316], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.05555464327335358
epoch£º747	 i:1 	 global-step:14941	 l-p:0.05551379919052124
epoch£º747	 i:2 	 global-step:14942	 l-p:0.05551211163401604
epoch£º747	 i:3 	 global-step:14943	 l-p:0.05557194724678993
epoch£º747	 i:4 	 global-step:14944	 l-p:0.05548504739999771
epoch£º747	 i:5 	 global-step:14945	 l-p:0.0556054562330246
epoch£º747	 i:6 	 global-step:14946	 l-p:0.0555056594312191
epoch£º747	 i:7 	 global-step:14947	 l-p:0.055574964731931686
epoch£º747	 i:8 	 global-step:14948	 l-p:0.055548522621393204
epoch£º747	 i:9 	 global-step:14949	 l-p:0.05548688769340515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3125, 29.8301, 29.3271],
        [28.3125, 38.0297, 44.6225],
        [28.3125, 28.9637, 28.5683],
        [28.3125, 30.0776, 29.6055]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.05548280104994774 
model_pd.l_d.mean(): 1.0520164295257928e-07 
model_pd.lagr.mean(): 0.055482905358076096 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.6565e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1925], device='cuda:0')), ('power', tensor([0.0065], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.05548280104994774
epoch£º748	 i:1 	 global-step:14961	 l-p:0.055515311658382416
epoch£º748	 i:2 	 global-step:14962	 l-p:0.05553603917360306
epoch£º748	 i:3 	 global-step:14963	 l-p:0.05546848103404045
epoch£º748	 i:4 	 global-step:14964	 l-p:0.055617574602365494
epoch£º748	 i:5 	 global-step:14965	 l-p:0.055526260286569595
epoch£º748	 i:6 	 global-step:14966	 l-p:0.05550944060087204
epoch£º748	 i:7 	 global-step:14967	 l-p:0.05550006777048111
epoch£º748	 i:8 	 global-step:14968	 l-p:0.055484332144260406
epoch£º748	 i:9 	 global-step:14969	 l-p:0.055540937930345535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3959, 37.8674, 44.1259],
        [28.3959, 28.5033, 28.4096],
        [28.3959, 28.5244, 28.4141],
        [28.3959, 31.9553, 32.2529]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.05550262704491615 
model_pd.l_d.mean(): 1.232646536664106e-05 
model_pd.lagr.mean(): 0.05551495403051376 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.7787e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1202], device='cuda:0')), ('power', tensor([0.1791], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:0.05550262704491615
epoch£º749	 i:1 	 global-step:14981	 l-p:0.05546185374259949
epoch£º749	 i:2 	 global-step:14982	 l-p:0.05548637732863426
epoch£º749	 i:3 	 global-step:14983	 l-p:0.05547989904880524
epoch£º749	 i:4 	 global-step:14984	 l-p:0.055452536791563034
epoch£º749	 i:5 	 global-step:14985	 l-p:0.05566445738077164
epoch£º749	 i:6 	 global-step:14986	 l-p:0.05547568202018738
epoch£º749	 i:7 	 global-step:14987	 l-p:0.05544757843017578
epoch£º749	 i:8 	 global-step:14988	 l-p:0.05549881234765053
epoch£º749	 i:9 	 global-step:14989	 l-p:0.055556103587150574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4615, 34.1825, 36.2182],
        [28.4615, 37.6592, 43.5574],
        [28.4615, 28.4804, 28.4623],
        [28.4615, 28.4851, 28.4627]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.05547149479389191 
model_pd.l_d.mean(): 2.8904416467412375e-05 
model_pd.lagr.mean(): 0.05550039932131767 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1589], device='cuda:0')), ('power', tensor([0.1812], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.05547149479389191
epoch£º750	 i:1 	 global-step:15001	 l-p:0.05551918223500252
epoch£º750	 i:2 	 global-step:15002	 l-p:0.05546175688505173
epoch£º750	 i:3 	 global-step:15003	 l-p:0.05550770461559296
epoch£º750	 i:4 	 global-step:15004	 l-p:0.0554741695523262
epoch£º750	 i:5 	 global-step:15005	 l-p:0.05553709715604782
epoch£º750	 i:6 	 global-step:15006	 l-p:0.05549447238445282
epoch£º750	 i:7 	 global-step:15007	 l-p:0.055542800575494766
epoch£º750	 i:8 	 global-step:15008	 l-p:0.05545548349618912
epoch£º750	 i:9 	 global-step:15009	 l-p:0.05545719340443611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4936, 29.1725, 28.7663],
        [28.4936, 34.4669, 36.7407],
        [28.4936, 28.4936, 28.4936],
        [28.4936, 29.7377, 29.2264]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.055446334183216095 
model_pd.l_d.mean(): 6.517219298984855e-05 
model_pd.lagr.mean(): 0.055511508136987686 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1877], device='cuda:0')), ('power', tensor([0.2361], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.055446334183216095
epoch£º751	 i:1 	 global-step:15021	 l-p:0.05543571338057518
epoch£º751	 i:2 	 global-step:15022	 l-p:0.055471885949373245
epoch£º751	 i:3 	 global-step:15023	 l-p:0.05546533316373825
epoch£º751	 i:4 	 global-step:15024	 l-p:0.055515486747026443
epoch£º751	 i:5 	 global-step:15025	 l-p:0.05556873232126236
epoch£º751	 i:6 	 global-step:15026	 l-p:0.05553103983402252
epoch£º751	 i:7 	 global-step:15027	 l-p:0.055463507771492004
epoch£º751	 i:8 	 global-step:15028	 l-p:0.055508051067590714
epoch£º751	 i:9 	 global-step:15029	 l-p:0.05549122765660286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4798, 28.4798, 28.4798],
        [28.4798, 28.6810, 28.5173],
        [28.4798, 29.2779, 28.8352],
        [28.4798, 28.4985, 28.4807]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.05553564056754112 
model_pd.l_d.mean(): 0.00011992538929916918 
model_pd.lagr.mean(): 0.05565556511282921 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1004], device='cuda:0')), ('power', tensor([0.3009], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.05553564056754112
epoch£º752	 i:1 	 global-step:15041	 l-p:0.05548038333654404
epoch£º752	 i:2 	 global-step:15042	 l-p:0.05553667992353439
epoch£º752	 i:3 	 global-step:15043	 l-p:0.0554700568318367
epoch£º752	 i:4 	 global-step:15044	 l-p:0.05546187236905098
epoch£º752	 i:5 	 global-step:15045	 l-p:0.05544981360435486
epoch£º752	 i:6 	 global-step:15046	 l-p:0.055557746440172195
epoch£º752	 i:7 	 global-step:15047	 l-p:0.05551011115312576
epoch£º752	 i:8 	 global-step:15048	 l-p:0.05544007569551468
epoch£º752	 i:9 	 global-step:15049	 l-p:0.05552845820784569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4147, 28.4229, 28.4150],
        [28.4147, 28.4149, 28.4147],
        [28.4147, 29.2610, 28.8066],
        [28.4147, 30.4338, 30.0129]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.05546802282333374 
model_pd.l_d.mean(): 8.383300882996991e-05 
model_pd.lagr.mean(): 0.05555185675621033 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1812], device='cuda:0')), ('power', tensor([0.1667], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.05546802282333374
epoch£º753	 i:1 	 global-step:15061	 l-p:0.05544926971197128
epoch£º753	 i:2 	 global-step:15062	 l-p:0.05552094802260399
epoch£º753	 i:3 	 global-step:15063	 l-p:0.05548443645238876
epoch£º753	 i:4 	 global-step:15064	 l-p:0.055464036762714386
epoch£º753	 i:5 	 global-step:15065	 l-p:0.05562033876776695
epoch£º753	 i:6 	 global-step:15066	 l-p:0.055597782135009766
epoch£º753	 i:7 	 global-step:15067	 l-p:0.05548880621790886
epoch£º753	 i:8 	 global-step:15068	 l-p:0.055481135845184326
epoch£º753	 i:9 	 global-step:15069	 l-p:0.05557241290807724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3038, 28.3273, 28.3050],
        [28.3038, 28.3213, 28.3046],
        [28.3038, 35.7777, 39.6543],
        [28.3038, 28.3045, 28.3038]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.055497922003269196 
model_pd.l_d.mean(): 5.720222361560445e-06 
model_pd.lagr.mean(): 0.05550364404916763 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1683], device='cuda:0')), ('power', tensor([0.0101], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.055497922003269196
epoch£º754	 i:1 	 global-step:15081	 l-p:0.05551318824291229
epoch£º754	 i:2 	 global-step:15082	 l-p:0.05558932200074196
epoch£º754	 i:3 	 global-step:15083	 l-p:0.0554991215467453
epoch£º754	 i:4 	 global-step:15084	 l-p:0.055486228317022324
epoch£º754	 i:5 	 global-step:15085	 l-p:0.05555998533964157
epoch£º754	 i:6 	 global-step:15086	 l-p:0.05559173598885536
epoch£º754	 i:7 	 global-step:15087	 l-p:0.05553760752081871
epoch£º754	 i:8 	 global-step:15088	 l-p:0.05552645027637482
epoch£º754	 i:9 	 global-step:15089	 l-p:0.055602483451366425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1625, 29.2844, 28.7868],
        [28.1625, 28.3667, 28.2012],
        [28.1625, 33.7550, 35.7062],
        [28.1625, 30.3837, 30.0313]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.055551715195178986 
model_pd.l_d.mean(): -5.4350948630599305e-05 
model_pd.lagr.mean(): 0.055497363209724426 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1205], device='cuda:0')), ('power', tensor([-0.0966], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.055551715195178986
epoch£º755	 i:1 	 global-step:15101	 l-p:0.05562072992324829
epoch£º755	 i:2 	 global-step:15102	 l-p:0.05551859736442566
epoch£º755	 i:3 	 global-step:15103	 l-p:0.05555944889783859
epoch£º755	 i:4 	 global-step:15104	 l-p:0.05559881404042244
epoch£º755	 i:5 	 global-step:15105	 l-p:0.05557672306895256
epoch£º755	 i:6 	 global-step:15106	 l-p:0.05552370846271515
epoch£º755	 i:7 	 global-step:15107	 l-p:0.05555998533964157
epoch£º755	 i:8 	 global-step:15108	 l-p:0.055670689791440964
epoch£º755	 i:9 	 global-step:15109	 l-p:0.055525608360767365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0138, 33.2640, 34.9177],
        [28.0138, 28.0138, 28.0138],
        [28.0138, 31.2736, 31.4135],
        [28.0138, 30.3300, 30.0163]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.055551089346408844 
model_pd.l_d.mean(): -0.00014011826715432107 
model_pd.lagr.mean(): 0.055410970002412796 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1719], device='cuda:0')), ('power', tensor([-0.2873], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.055551089346408844
epoch£º756	 i:1 	 global-step:15121	 l-p:0.05556080862879753
epoch£º756	 i:2 	 global-step:15122	 l-p:0.055624231696128845
epoch£º756	 i:3 	 global-step:15123	 l-p:0.055608395487070084
epoch£º756	 i:4 	 global-step:15124	 l-p:0.05556357279419899
epoch£º756	 i:5 	 global-step:15125	 l-p:0.05554705485701561
epoch£º756	 i:6 	 global-step:15126	 l-p:0.05573306977748871
epoch£º756	 i:7 	 global-step:15127	 l-p:0.05565907806158066
epoch£º756	 i:8 	 global-step:15128	 l-p:0.05559070408344269
epoch£º756	 i:9 	 global-step:15129	 l-p:0.055563170462846756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8869, 32.0854, 32.8737],
        [27.8869, 33.4217, 35.3526],
        [27.8869, 28.7164, 28.2709],
        [27.8869, 27.8869, 27.8869]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.05573229491710663 
model_pd.l_d.mean(): -8.805729157757014e-05 
model_pd.lagr.mean(): 0.055644236505031586 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0557], device='cuda:0')), ('power', tensor([-0.2570], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.05573229491710663
epoch£º757	 i:1 	 global-step:15141	 l-p:0.05556461215019226
epoch£º757	 i:2 	 global-step:15142	 l-p:0.055748745799064636
epoch£º757	 i:3 	 global-step:15143	 l-p:0.05559494346380234
epoch£º757	 i:4 	 global-step:15144	 l-p:0.05559748411178589
epoch£º757	 i:5 	 global-step:15145	 l-p:0.05559196695685387
epoch£º757	 i:6 	 global-step:15146	 l-p:0.05559736117720604
epoch£º757	 i:7 	 global-step:15147	 l-p:0.05559868738055229
epoch£º757	 i:8 	 global-step:15148	 l-p:0.05557534098625183
epoch£º757	 i:9 	 global-step:15149	 l-p:0.05562178045511246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8106, 29.2418, 28.7444],
        [27.8106, 28.9175, 28.4264],
        [27.8106, 28.1815, 27.9137],
        [27.8106, 30.2162, 29.9450]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.055582139641046524 
model_pd.l_d.mean(): -7.398078741971403e-05 
model_pd.lagr.mean(): 0.05550815910100937 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1730], device='cuda:0')), ('power', tensor([-0.5111], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.055582139641046524
epoch£º758	 i:1 	 global-step:15161	 l-p:0.05573189631104469
epoch£º758	 i:2 	 global-step:15162	 l-p:0.05560443922877312
epoch£º758	 i:3 	 global-step:15163	 l-p:0.05562324449419975
epoch£º758	 i:4 	 global-step:15164	 l-p:0.05559636652469635
epoch£º758	 i:5 	 global-step:15165	 l-p:0.055620066821575165
epoch£º758	 i:6 	 global-step:15166	 l-p:0.05569755285978317
epoch£º758	 i:7 	 global-step:15167	 l-p:0.05560915917158127
epoch£º758	 i:8 	 global-step:15168	 l-p:0.05556999891996384
epoch£º758	 i:9 	 global-step:15169	 l-p:0.0556831918656826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8082, 27.8083, 27.8082],
        [27.8082, 27.8082, 27.8081],
        [27.8082, 33.1486, 34.9084],
        [27.8082, 27.8082, 27.8081]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.0556546114385128 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0556546114385128 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1169], device='cuda:0')), ('power', tensor([-0.4637], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.0556546114385128
epoch£º759	 i:1 	 global-step:15181	 l-p:0.055650949478149414
epoch£º759	 i:2 	 global-step:15182	 l-p:0.055607154965400696
epoch£º759	 i:3 	 global-step:15183	 l-p:0.0556679330766201
epoch£º759	 i:4 	 global-step:15184	 l-p:0.05559627711772919
epoch£º759	 i:5 	 global-step:15185	 l-p:0.05557658150792122
epoch£º759	 i:6 	 global-step:15186	 l-p:0.055603332817554474
epoch£º759	 i:7 	 global-step:15187	 l-p:0.05557025969028473
epoch£º759	 i:8 	 global-step:15188	 l-p:0.055644452571868896
epoch£º759	 i:9 	 global-step:15189	 l-p:0.0556863471865654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8662, 28.0191, 27.8906],
        [27.8662, 29.2953, 28.7965],
        [27.8662, 31.0439, 31.1457],
        [27.8662, 28.2268, 27.9645]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.055681318044662476 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055681318044662476 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0686], device='cuda:0')), ('power', tensor([-0.2564], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:0.055681318044662476
epoch£º760	 i:1 	 global-step:15201	 l-p:0.05575022101402283
epoch£º760	 i:2 	 global-step:15202	 l-p:0.05564252287149429
epoch£º760	 i:3 	 global-step:15203	 l-p:0.055600717663764954
epoch£º760	 i:4 	 global-step:15204	 l-p:0.055569130927324295
epoch£º760	 i:5 	 global-step:15205	 l-p:0.055575016885995865
epoch£º760	 i:6 	 global-step:15206	 l-p:0.05555496737360954
epoch£º760	 i:7 	 global-step:15207	 l-p:0.05559011176228523
epoch£º760	 i:8 	 global-step:15208	 l-p:0.055580023676157
epoch£º760	 i:9 	 global-step:15209	 l-p:0.055565498769283295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9481, 28.0220, 27.9557],
        [27.9481, 32.1846, 32.9963],
        [27.9481, 29.7186, 29.2581],
        [27.9481, 34.0502, 36.5233]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.055731676518917084 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055731676518917084 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0341], device='cuda:0')), ('power', tensor([-0.0945], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.055731676518917084
epoch£º761	 i:1 	 global-step:15221	 l-p:0.055555760860443115
epoch£º761	 i:2 	 global-step:15222	 l-p:0.055566031485795975
epoch£º761	 i:3 	 global-step:15223	 l-p:0.055544450879096985
epoch£º761	 i:4 	 global-step:15224	 l-p:0.05554972589015961
epoch£º761	 i:5 	 global-step:15225	 l-p:0.05567263811826706
epoch£º761	 i:6 	 global-step:15226	 l-p:0.05557699874043465
epoch£º761	 i:7 	 global-step:15227	 l-p:0.055597588419914246
epoch£º761	 i:8 	 global-step:15228	 l-p:0.05556327849626541
epoch£º761	 i:9 	 global-step:15229	 l-p:0.05557040125131607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0381, 29.5878, 29.0940],
        [28.0381, 29.7850, 29.3177],
        [28.0381, 31.7704, 32.2094],
        [28.0381, 28.0381, 28.0381]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.05554058775305748 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05554058775305748 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1783], device='cuda:0')), ('power', tensor([-0.2480], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.05554058775305748
epoch£º762	 i:1 	 global-step:15241	 l-p:0.05554553493857384
epoch£º762	 i:2 	 global-step:15242	 l-p:0.05553936958312988
epoch£º762	 i:3 	 global-step:15243	 l-p:0.05555078387260437
epoch£º762	 i:4 	 global-step:15244	 l-p:0.05554083734750748
epoch£º762	 i:5 	 global-step:15245	 l-p:0.05565804988145828
epoch£º762	 i:6 	 global-step:15246	 l-p:0.05557172745466232
epoch£º762	 i:7 	 global-step:15247	 l-p:0.05557318031787872
epoch£º762	 i:8 	 global-step:15248	 l-p:0.05555932968854904
epoch£º762	 i:9 	 global-step:15249	 l-p:0.055657923221588135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1310, 30.8900, 30.7556],
        [28.1310, 28.1536, 28.1321],
        [28.1310, 28.3296, 28.1680],
        [28.1310, 36.4497, 41.3355]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.05561770498752594 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05561770498752594 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1020], device='cuda:0')), ('power', tensor([-0.0706], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.05561770498752594
epoch£º763	 i:1 	 global-step:15261	 l-p:0.055581942200660706
epoch£º763	 i:2 	 global-step:15262	 l-p:0.0555616095662117
epoch£º763	 i:3 	 global-step:15263	 l-p:0.055508095771074295
epoch£º763	 i:4 	 global-step:15264	 l-p:0.055514413863420486
epoch£º763	 i:5 	 global-step:15265	 l-p:0.05560074374079704
epoch£º763	 i:6 	 global-step:15266	 l-p:0.05551189184188843
epoch£º763	 i:7 	 global-step:15267	 l-p:0.055606093257665634
epoch£º763	 i:8 	 global-step:15268	 l-p:0.05550627037882805
epoch£º763	 i:9 	 global-step:15269	 l-p:0.05553566664457321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2256, 28.6288, 28.3426],
        [28.2256, 28.2411, 28.2262],
        [28.2256, 32.0366, 32.5146],
        [28.2256, 30.0130, 29.5474]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.05549461394548416 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05549461394548416 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1908], device='cuda:0')), ('power', tensor([-0.0690], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.05549461394548416
epoch£º764	 i:1 	 global-step:15281	 l-p:0.055642422288656235
epoch£º764	 i:2 	 global-step:15282	 l-p:0.055488865822553635
epoch£º764	 i:3 	 global-step:15283	 l-p:0.05551006272435188
epoch£º764	 i:4 	 global-step:15284	 l-p:0.05550006777048111
epoch£º764	 i:5 	 global-step:15285	 l-p:0.055573705583810806
epoch£º764	 i:6 	 global-step:15286	 l-p:0.055556152015924454
epoch£º764	 i:7 	 global-step:15287	 l-p:0.05548343434929848
epoch£º764	 i:8 	 global-step:15288	 l-p:0.05556782707571983
epoch£º764	 i:9 	 global-step:15289	 l-p:0.055532827973365784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3200, 28.9237, 28.5460],
        [28.3200, 28.3455, 28.3214],
        [28.3200, 28.3206, 28.3200],
        [28.3200, 29.0304, 28.6150]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.0556466281414032 
model_pd.l_d.mean(): 3.3938870274141664e-06 
model_pd.lagr.mean(): 0.0556500218808651 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.6282e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0640], device='cuda:0')), ('power', tensor([0.2282], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.0556466281414032
epoch£º765	 i:1 	 global-step:15301	 l-p:0.05552391707897186
epoch£º765	 i:2 	 global-step:15302	 l-p:0.055481918156147
epoch£º765	 i:3 	 global-step:15303	 l-p:0.05549119785428047
epoch£º765	 i:4 	 global-step:15304	 l-p:0.055506862699985504
epoch£º765	 i:5 	 global-step:15305	 l-p:0.05547546595335007
epoch£º765	 i:6 	 global-step:15306	 l-p:0.05545775592327118
epoch£º765	 i:7 	 global-step:15307	 l-p:0.05550253763794899
epoch£º765	 i:8 	 global-step:15308	 l-p:0.055528342723846436
epoch£º765	 i:9 	 global-step:15309	 l-p:0.05554705858230591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4078, 37.7380, 43.8143],
        [28.4078, 31.0945, 30.9122],
        [28.4078, 28.6441, 28.4565],
        [28.4078, 35.9459, 39.8774]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.05547276511788368 
model_pd.l_d.mean(): 9.32224702410167e-06 
model_pd.lagr.mean(): 0.055482085794210434 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.8796e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1729], device='cuda:0')), ('power', tensor([0.1288], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.05547276511788368
epoch£º766	 i:1 	 global-step:15321	 l-p:0.05551740899682045
epoch£º766	 i:2 	 global-step:15322	 l-p:0.05546554923057556
epoch£º766	 i:3 	 global-step:15323	 l-p:0.05557054281234741
epoch£º766	 i:4 	 global-step:15324	 l-p:0.05555754527449608
epoch£º766	 i:5 	 global-step:15325	 l-p:0.05544518306851387
epoch£º766	 i:6 	 global-step:15326	 l-p:0.05549875274300575
epoch£º766	 i:7 	 global-step:15327	 l-p:0.0554720014333725
epoch£º766	 i:8 	 global-step:15328	 l-p:0.055456195026636124
epoch£º766	 i:9 	 global-step:15329	 l-p:0.05554397031664848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4750, 28.8018, 28.5575],
        [28.4750, 28.4753, 28.4750],
        [28.4750, 28.4750, 28.4750],
        [28.4750, 37.1005, 42.2903]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.05544421076774597 
model_pd.l_d.mean(): 2.458646849845536e-05 
model_pd.lagr.mean(): 0.05546879768371582 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2068], device='cuda:0')), ('power', tensor([0.1450], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.05544421076774597
epoch£º767	 i:1 	 global-step:15341	 l-p:0.05549671873450279
epoch£º767	 i:2 	 global-step:15342	 l-p:0.05553155764937401
epoch£º767	 i:3 	 global-step:15343	 l-p:0.05543787032365799
epoch£º767	 i:4 	 global-step:15344	 l-p:0.05552966147661209
epoch£º767	 i:5 	 global-step:15345	 l-p:0.05543915927410126
epoch£º767	 i:6 	 global-step:15346	 l-p:0.05551654100418091
epoch£º767	 i:7 	 global-step:15347	 l-p:0.05556163191795349
epoch£º767	 i:8 	 global-step:15348	 l-p:0.05547844246029854
epoch£º767	 i:9 	 global-step:15349	 l-p:0.05545972287654877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5052, 31.2743, 31.1248],
        [28.5052, 38.8967, 46.3346],
        [28.5052, 28.7424, 28.5542],
        [28.5052, 34.4810, 36.7558]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.05553796514868736 
model_pd.l_d.mean(): 9.296383359469473e-05 
model_pd.lagr.mean(): 0.05563092976808548 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1441], device='cuda:0')), ('power', tensor([0.3179], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.05553796514868736
epoch£º768	 i:1 	 global-step:15361	 l-p:0.05545420199632645
epoch£º768	 i:2 	 global-step:15362	 l-p:0.05554235354065895
epoch£º768	 i:3 	 global-step:15363	 l-p:0.055448345839977264
epoch£º768	 i:4 	 global-step:15364	 l-p:0.05543876066803932
epoch£º768	 i:5 	 global-step:15365	 l-p:0.05557140335440636
epoch£º768	 i:6 	 global-step:15366	 l-p:0.05544570833444595
epoch£º768	 i:7 	 global-step:15367	 l-p:0.05546939745545387
epoch£º768	 i:8 	 global-step:15368	 l-p:0.055481281131505966
epoch£º768	 i:9 	 global-step:15369	 l-p:0.0554894283413887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4847, 31.8895, 32.0844],
        [28.4847, 34.9377, 37.6916],
        [28.4847, 34.8565, 37.5276],
        [28.4847, 28.5469, 28.4904]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.055476754903793335 
model_pd.l_d.mean(): 9.834639786276966e-05 
model_pd.lagr.mean(): 0.05557510256767273 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1574], device='cuda:0')), ('power', tensor([0.2344], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.055476754903793335
epoch£º769	 i:1 	 global-step:15381	 l-p:0.05544910207390785
epoch£º769	 i:2 	 global-step:15382	 l-p:0.05546785891056061
epoch£º769	 i:3 	 global-step:15383	 l-p:0.05549323931336403
epoch£º769	 i:4 	 global-step:15384	 l-p:0.055486831814050674
epoch£º769	 i:5 	 global-step:15385	 l-p:0.05546870455145836
epoch£º769	 i:6 	 global-step:15386	 l-p:0.05560540035367012
epoch£º769	 i:7 	 global-step:15387	 l-p:0.055455394089221954
epoch£º769	 i:8 	 global-step:15388	 l-p:0.055535733699798584
epoch£º769	 i:9 	 global-step:15389	 l-p:0.05553262680768967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4079, 28.4081, 28.4079],
        [28.4079, 34.6085, 37.1179],
        [28.4079, 32.0779, 32.4466],
        [28.4079, 29.0813, 28.6775]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): 0.05563890188932419 
model_pd.l_d.mean(): 0.00017444277182221413 
model_pd.lagr.mean(): 0.055813346058130264 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0401], device='cuda:0')), ('power', tensor([0.3331], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:0.05563890188932419
epoch£º770	 i:1 	 global-step:15401	 l-p:0.055466726422309875
epoch£º770	 i:2 	 global-step:15402	 l-p:0.05548453330993652
epoch£º770	 i:3 	 global-step:15403	 l-p:0.05550963431596756
epoch£º770	 i:4 	 global-step:15404	 l-p:0.055468738079071045
epoch£º770	 i:5 	 global-step:15405	 l-p:0.05547092482447624
epoch£º770	 i:6 	 global-step:15406	 l-p:0.05551997944712639
epoch£º770	 i:7 	 global-step:15407	 l-p:0.0554875023663044
epoch£º770	 i:8 	 global-step:15408	 l-p:0.05563971400260925
epoch£º770	 i:9 	 global-step:15409	 l-p:0.055487584322690964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2809, 28.2812, 28.2809],
        [28.2809, 28.4861, 28.3198],
        [28.2809, 28.3004, 28.2818],
        [28.2809, 31.4845, 31.5743]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.05548754706978798 
model_pd.l_d.mean(): -4.3956824811175466e-05 
model_pd.lagr.mean(): 0.05544358864426613 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1948], device='cuda:0')), ('power', tensor([-0.0760], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.05548754706978798
epoch£º771	 i:1 	 global-step:15421	 l-p:0.05551717057824135
epoch£º771	 i:2 	 global-step:15422	 l-p:0.05553726479411125
epoch£º771	 i:3 	 global-step:15423	 l-p:0.055486056953668594
epoch£º771	 i:4 	 global-step:15424	 l-p:0.05549606308341026
epoch£º771	 i:5 	 global-step:15425	 l-p:0.05552356690168381
epoch£º771	 i:6 	 global-step:15426	 l-p:0.055667780339717865
epoch£º771	 i:7 	 global-step:15427	 l-p:0.055621955543756485
epoch£º771	 i:8 	 global-step:15428	 l-p:0.055609893053770065
epoch£º771	 i:9 	 global-step:15429	 l-p:0.05551936477422714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1245, 28.3209, 28.1608],
        [28.1245, 28.4999, 28.2288],
        [28.1245, 37.1821, 42.9740],
        [28.1245, 28.3285, 28.1631]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.0555160716176033 
model_pd.l_d.mean(): -0.00010860864858841524 
model_pd.lagr.mean(): 0.05540746450424194 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1854], device='cuda:0')), ('power', tensor([-0.1934], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.0555160716176033
epoch£º772	 i:1 	 global-step:15441	 l-p:0.05556435510516167
epoch£º772	 i:2 	 global-step:15442	 l-p:0.055633533746004105
epoch£º772	 i:3 	 global-step:15443	 l-p:0.055525653064250946
epoch£º772	 i:4 	 global-step:15444	 l-p:0.055533550679683685
epoch£º772	 i:5 	 global-step:15445	 l-p:0.055660609155893326
epoch£º772	 i:6 	 global-step:15446	 l-p:0.05565312132239342
epoch£º772	 i:7 	 global-step:15447	 l-p:0.05554712563753128
epoch£º772	 i:8 	 global-step:15448	 l-p:0.055577076971530914
epoch£º772	 i:9 	 global-step:15449	 l-p:0.05558420717716217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9667, 27.9667, 27.9667],
        [27.9667, 28.1423, 27.9972],
        [27.9667, 30.1729, 29.8235],
        [27.9667, 30.0822, 29.7045]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.055535878986120224 
model_pd.l_d.mean(): -0.00017951188783627003 
model_pd.lagr.mean(): 0.05535636842250824 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2128], device='cuda:0')), ('power', tensor([-0.3857], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.055535878986120224
epoch£º773	 i:1 	 global-step:15461	 l-p:0.0557461641728878
epoch£º773	 i:2 	 global-step:15462	 l-p:0.05568858981132507
epoch£º773	 i:3 	 global-step:15463	 l-p:0.055614471435546875
epoch£º773	 i:4 	 global-step:15464	 l-p:0.055551107972860336
epoch£º773	 i:5 	 global-step:15465	 l-p:0.0556119941174984
epoch£º773	 i:6 	 global-step:15466	 l-p:0.055562131106853485
epoch£º773	 i:7 	 global-step:15467	 l-p:0.05557287111878395
epoch£º773	 i:8 	 global-step:15468	 l-p:0.055632397532463074
epoch£º773	 i:9 	 global-step:15469	 l-p:0.05558482185006142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8393, 28.3840, 28.0324],
        [27.8393, 28.0283, 27.8737],
        [27.8393, 30.4678, 30.2886],
        [27.8393, 27.9245, 27.8488]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.055575959384441376 
model_pd.l_d.mean(): -0.00013407132064457983 
model_pd.lagr.mean(): 0.05544188991189003 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1720], device='cuda:0')), ('power', tensor([-0.4521], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.055575959384441376
epoch£º774	 i:1 	 global-step:15481	 l-p:0.05570129677653313
epoch£º774	 i:2 	 global-step:15482	 l-p:0.05561628192663193
epoch£º774	 i:3 	 global-step:15483	 l-p:0.055591847747564316
epoch£º774	 i:4 	 global-step:15484	 l-p:0.055601805448532104
epoch£º774	 i:5 	 global-step:15485	 l-p:0.055721357464790344
epoch£º774	 i:6 	 global-step:15486	 l-p:0.05566500499844551
epoch£º774	 i:7 	 global-step:15487	 l-p:0.055612847208976746
epoch£º774	 i:8 	 global-step:15488	 l-p:0.05560681223869324
epoch£º774	 i:9 	 global-step:15489	 l-p:0.05562296882271767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7748, 30.1098, 29.8134],
        [27.7748, 29.6098, 29.1673],
        [27.7748, 27.7748, 27.7748],
        [27.7748, 35.1366, 38.9753]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.05571891739964485 
model_pd.l_d.mean(): -2.24713276111288e-05 
model_pd.lagr.mean(): 0.05569644644856453 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.2288e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.9888], device='cuda:0')), ('power', tensor([-0.2922], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.05571891739964485
epoch£º775	 i:1 	 global-step:15501	 l-p:0.055640000849962234
epoch£º775	 i:2 	 global-step:15502	 l-p:0.05557772144675255
epoch£º775	 i:3 	 global-step:15503	 l-p:0.05574110895395279
epoch£º775	 i:4 	 global-step:15504	 l-p:0.05558661371469498
epoch£º775	 i:5 	 global-step:15505	 l-p:0.05566083639860153
epoch£º775	 i:6 	 global-step:15506	 l-p:0.05563057214021683
epoch£º775	 i:7 	 global-step:15507	 l-p:0.05559093505144119
epoch£º775	 i:8 	 global-step:15508	 l-p:0.05558948218822479
epoch£º775	 i:9 	 global-step:15509	 l-p:0.05563630908727646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[27.7945, 28.9984, 28.5006],
        [27.7945, 29.7667, 29.3554],
        [27.7945, 29.6310, 29.1881],
        [27.7945, 36.9223, 42.8711]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.05559133365750313 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05559133365750313 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1567], device='cuda:0')), ('power', tensor([-0.4669], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.05559133365750313
epoch£º776	 i:1 	 global-step:15521	 l-p:0.05564635992050171
epoch£º776	 i:2 	 global-step:15522	 l-p:0.05570872500538826
epoch£º776	 i:3 	 global-step:15523	 l-p:0.05561867728829384
epoch£º776	 i:4 	 global-step:15524	 l-p:0.05561766028404236
epoch£º776	 i:5 	 global-step:15525	 l-p:0.055586475878953934
epoch£º776	 i:6 	 global-step:15526	 l-p:0.055630069226026535
epoch£º776	 i:7 	 global-step:15527	 l-p:0.05558962747454643
epoch£º776	 i:8 	 global-step:15528	 l-p:0.055602267384529114
epoch£º776	 i:9 	 global-step:15529	 l-p:0.05568162351846695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8659, 29.5743, 29.1055],
        [27.8659, 28.3083, 28.0032],
        [27.8659, 29.7986, 29.3751],
        [27.8659, 30.7368, 30.6722]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.05557975545525551 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05557975545525551 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1575], device='cuda:0')), ('power', tensor([-0.4261], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.05557975545525551
epoch£º777	 i:1 	 global-step:15541	 l-p:0.05556619539856911
epoch£º777	 i:2 	 global-step:15542	 l-p:0.055789392441511154
epoch£º777	 i:3 	 global-step:15543	 l-p:0.05564434453845024
epoch£º777	 i:4 	 global-step:15544	 l-p:0.05555402487516403
epoch£º777	 i:5 	 global-step:15545	 l-p:0.05556883662939072
epoch£º777	 i:6 	 global-step:15546	 l-p:0.05559413135051727
epoch£º777	 i:7 	 global-step:15547	 l-p:0.055652469396591187
epoch£º777	 i:8 	 global-step:15548	 l-p:0.055579908192157745
epoch£º777	 i:9 	 global-step:15549	 l-p:0.05557252839207649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9561, 27.9599, 27.9561],
        [27.9561, 28.3608, 28.0745],
        [27.9561, 28.0300, 27.9636],
        [27.9561, 27.9844, 27.9577]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.055669382214546204 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055669382214546204 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0255], device='cuda:0')), ('power', tensor([-0.1490], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.055669382214546204
epoch£º778	 i:1 	 global-step:15561	 l-p:0.055556610226631165
epoch£º778	 i:2 	 global-step:15562	 l-p:0.055707067251205444
epoch£º778	 i:3 	 global-step:15563	 l-p:0.055544164031744
epoch£º778	 i:4 	 global-step:15564	 l-p:0.05561777949333191
epoch£º778	 i:5 	 global-step:15565	 l-p:0.05558560788631439
epoch£º778	 i:6 	 global-step:15566	 l-p:0.055549487471580505
epoch£º778	 i:7 	 global-step:15567	 l-p:0.0555511936545372
epoch£º778	 i:8 	 global-step:15568	 l-p:0.05559103935956955
epoch£º778	 i:9 	 global-step:15569	 l-p:0.055533263832330704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0530, 29.0804, 28.5953],
        [28.0530, 31.2920, 31.4171],
        [28.0530, 28.0535, 28.0530],
        [28.0530, 28.4592, 28.1719]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.055548571050167084 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055548571050167084 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1519], device='cuda:0')), ('power', tensor([-0.2357], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.055548571050167084
epoch£º779	 i:1 	 global-step:15581	 l-p:0.05560874938964844
epoch£º779	 i:2 	 global-step:15582	 l-p:0.05561821907758713
epoch£º779	 i:3 	 global-step:15583	 l-p:0.05555546656250954
epoch£º779	 i:4 	 global-step:15584	 l-p:0.05552542954683304
epoch£º779	 i:5 	 global-step:15585	 l-p:0.05555726960301399
epoch£º779	 i:6 	 global-step:15586	 l-p:0.05552113801240921
epoch£º779	 i:7 	 global-step:15587	 l-p:0.05554959177970886
epoch£º779	 i:8 	 global-step:15588	 l-p:0.05563271790742874
epoch£º779	 i:9 	 global-step:15589	 l-p:0.055583856999874115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1521, 28.1742, 28.1532],
        [28.1521, 28.2337, 28.1610],
        [28.1521, 36.0635, 40.4654],
        [28.1521, 36.0117, 40.3535]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.05568189173936844 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05568189173936844 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.8846e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0694], device='cuda:0')), ('power', tensor([0.0577], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.05568189173936844
epoch£º780	 i:1 	 global-step:15601	 l-p:0.055512093007564545
epoch£º780	 i:2 	 global-step:15602	 l-p:0.05549536272883415
epoch£º780	 i:3 	 global-step:15603	 l-p:0.055645015090703964
epoch£º780	 i:4 	 global-step:15604	 l-p:0.055514268577098846
epoch£º780	 i:5 	 global-step:15605	 l-p:0.05549955740571022
epoch£º780	 i:6 	 global-step:15606	 l-p:0.05553967505693436
epoch£º780	 i:7 	 global-step:15607	 l-p:0.0555771179497242
epoch£º780	 i:8 	 global-step:15608	 l-p:0.05555173382163048
epoch£º780	 i:9 	 global-step:15609	 l-p:0.05547912418842316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2524, 36.1756, 40.5735],
        [28.2524, 28.2524, 28.2524],
        [28.2524, 28.2778, 28.2537],
        [28.2524, 28.2635, 28.2528]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.05550495535135269 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05550495535135269 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1636], device='cuda:0')), ('power', tensor([-0.0016], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.05550495535135269
epoch£º781	 i:1 	 global-step:15621	 l-p:0.055482033640146255
epoch£º781	 i:2 	 global-step:15622	 l-p:0.05567840486764908
epoch£º781	 i:3 	 global-step:15623	 l-p:0.05549651011824608
epoch£º781	 i:4 	 global-step:15624	 l-p:0.055483024567365646
epoch£º781	 i:5 	 global-step:15625	 l-p:0.05560401454567909
epoch£º781	 i:6 	 global-step:15626	 l-p:0.055559754371643066
epoch£º781	 i:7 	 global-step:15627	 l-p:0.05548032745718956
epoch£º781	 i:8 	 global-step:15628	 l-p:0.05552813410758972
epoch£º781	 i:9 	 global-step:15629	 l-p:0.05547349154949188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3514, 28.4323, 28.3601],
        [28.3514, 35.4853, 38.9784],
        [28.3514, 28.6767, 28.4335],
        [28.3514, 28.6995, 28.4431]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.05552322417497635 
model_pd.l_d.mean(): 1.505433147030999e-06 
model_pd.lagr.mean(): 0.05552472919225693 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.2860e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1686], device='cuda:0')), ('power', tensor([0.0495], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:0.05552322417497635
epoch£º782	 i:1 	 global-step:15641	 l-p:0.05546571686863899
epoch£º782	 i:2 	 global-step:15642	 l-p:0.05549073964357376
epoch£º782	 i:3 	 global-step:15643	 l-p:0.05549651384353638
epoch£º782	 i:4 	 global-step:15644	 l-p:0.05553378164768219
epoch£º782	 i:5 	 global-step:15645	 l-p:0.05551760643720627
epoch£º782	 i:6 	 global-step:15646	 l-p:0.05555359646677971
epoch£º782	 i:7 	 global-step:15647	 l-p:0.05547742545604706
epoch£º782	 i:8 	 global-step:15648	 l-p:0.055494144558906555
epoch£º782	 i:9 	 global-step:15649	 l-p:0.05554389953613281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4394, 34.7125, 37.2901],
        [28.4394, 30.4875, 30.0734],
        [28.4394, 34.3649, 36.5993],
        [28.4394, 28.6528, 28.4807]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.05546261742711067 
model_pd.l_d.mean(): 1.2066279850841966e-05 
model_pd.lagr.mean(): 0.05547468364238739 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1995], device='cuda:0')), ('power', tensor([0.1164], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.05546261742711067
epoch£º783	 i:1 	 global-step:15661	 l-p:0.055498551577329636
epoch£º783	 i:2 	 global-step:15662	 l-p:0.05562417209148407
epoch£º783	 i:3 	 global-step:15663	 l-p:0.05544840171933174
epoch£º783	 i:4 	 global-step:15664	 l-p:0.05554043501615524
epoch£º783	 i:5 	 global-step:15665	 l-p:0.055469807237386703
epoch£º783	 i:6 	 global-step:15666	 l-p:0.05545594543218613
epoch£º783	 i:7 	 global-step:15667	 l-p:0.05544723942875862
epoch£º783	 i:8 	 global-step:15668	 l-p:0.055547814816236496
epoch£º783	 i:9 	 global-step:15669	 l-p:0.05544690415263176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4984, 34.7612, 37.3205],
        [28.4984, 28.4984, 28.4984],
        [28.4984, 29.0673, 28.7026],
        [28.4984, 29.1243, 28.7371]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.05547225847840309 
model_pd.l_d.mean(): 6.0683360061375424e-05 
model_pd.lagr.mean(): 0.055532943457365036 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1347], device='cuda:0')), ('power', tensor([0.2820], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.05547225847840309
epoch£º784	 i:1 	 global-step:15681	 l-p:0.05568099394440651
epoch£º784	 i:2 	 global-step:15682	 l-p:0.0554518960416317
epoch£º784	 i:3 	 global-step:15683	 l-p:0.05544085428118706
epoch£º784	 i:4 	 global-step:15684	 l-p:0.05547964945435524
epoch£º784	 i:5 	 global-step:15685	 l-p:0.05542709678411484
epoch£º784	 i:6 	 global-step:15686	 l-p:0.05547608435153961
epoch£º784	 i:7 	 global-step:15687	 l-p:0.05544349551200867
epoch£º784	 i:8 	 global-step:15688	 l-p:0.055478792637586594
epoch£º784	 i:9 	 global-step:15689	 l-p:0.05551266670227051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.5098, 28.5098, 28.5098],
        [28.5098, 28.5285, 28.5107],
        [28.5098, 32.6616, 33.3594],
        [28.5098, 29.6597, 29.1544]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.05550951883196831 
model_pd.l_d.mean(): 8.580675057601184e-05 
model_pd.lagr.mean(): 0.05559532716870308 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1727], device='cuda:0')), ('power', tensor([0.2480], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.05550951883196831
epoch£º785	 i:1 	 global-step:15701	 l-p:0.05543434992432594
epoch£º785	 i:2 	 global-step:15702	 l-p:0.05545263737440109
epoch£º785	 i:3 	 global-step:15703	 l-p:0.05551419034600258
epoch£º785	 i:4 	 global-step:15704	 l-p:0.05546826869249344
epoch£º785	 i:5 	 global-step:15705	 l-p:0.05544354394078255
epoch£º785	 i:6 	 global-step:15706	 l-p:0.055607784539461136
epoch£º785	 i:7 	 global-step:15707	 l-p:0.05546694993972778
epoch£º785	 i:8 	 global-step:15708	 l-p:0.0554875023663044
epoch£º785	 i:9 	 global-step:15709	 l-p:0.05550742149353027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4637, 28.4637, 28.4637],
        [28.4637, 38.2108, 44.8089],
        [28.4637, 35.0745, 37.9938],
        [28.4637, 28.7904, 28.5462]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.0554528571665287 
model_pd.l_d.mean(): 6.561321788467467e-05 
model_pd.lagr.mean(): 0.05551847070455551 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1966], device='cuda:0')), ('power', tensor([0.1396], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.0554528571665287
epoch£º786	 i:1 	 global-step:15721	 l-p:0.0554514154791832
epoch£º786	 i:2 	 global-step:15722	 l-p:0.055478423833847046
epoch£º786	 i:3 	 global-step:15723	 l-p:0.055520351976156235
epoch£º786	 i:4 	 global-step:15724	 l-p:0.05557376891374588
epoch£º786	 i:5 	 global-step:15725	 l-p:0.0555325411260128
epoch£º786	 i:6 	 global-step:15726	 l-p:0.05549019202589989
epoch£º786	 i:7 	 global-step:15727	 l-p:0.05546876788139343
epoch£º786	 i:8 	 global-step:15728	 l-p:0.055459506809711456
epoch£º786	 i:9 	 global-step:15729	 l-p:0.05561160296201706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3589, 34.5288, 37.0141],
        [28.3589, 28.5715, 28.4000],
        [28.3589, 28.4398, 28.3676],
        [28.3589, 28.7758, 28.4821]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.055558450520038605 
model_pd.l_d.mean(): 0.00011817282211268321 
model_pd.lagr.mean(): 0.055676624178886414 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.0773], device='cuda:0')), ('power', tensor([0.2120], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.055558450520038605
epoch£º787	 i:1 	 global-step:15741	 l-p:0.05551515892148018
epoch£º787	 i:2 	 global-step:15742	 l-p:0.05548705905675888
epoch£º787	 i:3 	 global-step:15743	 l-p:0.055489227175712585
epoch£º787	 i:4 	 global-step:15744	 l-p:0.05554778128862381
epoch£º787	 i:5 	 global-step:15745	 l-p:0.05560588836669922
epoch£º787	 i:6 	 global-step:15746	 l-p:0.055510587990283966
epoch£º787	 i:7 	 global-step:15747	 l-p:0.055491141974925995
epoch£º787	 i:8 	 global-step:15748	 l-p:0.05549662560224533
epoch£º787	 i:9 	 global-step:15749	 l-p:0.055596064776182175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2073, 30.5405, 30.2246],
        [28.2073, 28.2073, 28.2073],
        [28.2073, 30.8454, 30.6518],
        [28.2073, 35.6400, 39.4865]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.05550587922334671 
model_pd.l_d.mean(): -7.580019882880151e-05 
model_pd.lagr.mean(): 0.055430080741643906 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1832], device='cuda:0')), ('power', tensor([-0.1303], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.05550587922334671
epoch£º788	 i:1 	 global-step:15761	 l-p:0.055523041635751724
epoch£º788	 i:2 	 global-step:15762	 l-p:0.05566640570759773
epoch£º788	 i:3 	 global-step:15763	 l-p:0.055553264915943146
epoch£º788	 i:4 	 global-step:15764	 l-p:0.055520765483379364
epoch£º788	 i:5 	 global-step:15765	 l-p:0.055628251284360886
epoch£º788	 i:6 	 global-step:15766	 l-p:0.055516526103019714
epoch£º788	 i:7 	 global-step:15767	 l-p:0.05553344637155533
epoch£º788	 i:8 	 global-step:15768	 l-p:0.05552694573998451
epoch£º788	 i:9 	 global-step:15769	 l-p:0.05565878748893738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0358, 28.2525, 28.0785],
        [28.0358, 30.1143, 29.7232],
        [28.0358, 28.0358, 28.0358],
        [28.0358, 28.0358, 28.0358]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.055558282881975174 
model_pd.l_d.mean(): -0.00013319369463715702 
model_pd.lagr.mean(): 0.05542508885264397 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1437], device='cuda:0')), ('power', tensor([-0.2540], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.055558282881975174
epoch£º789	 i:1 	 global-step:15781	 l-p:0.0556289404630661
epoch£º789	 i:2 	 global-step:15782	 l-p:0.05554342269897461
epoch£º789	 i:3 	 global-step:15783	 l-p:0.055646855384111404
epoch£º789	 i:4 	 global-step:15784	 l-p:0.055557217448949814
epoch£º789	 i:5 	 global-step:15785	 l-p:0.055649563670158386
epoch£º789	 i:6 	 global-step:15786	 l-p:0.05564947426319122
epoch£º789	 i:7 	 global-step:15787	 l-p:0.05557604506611824
epoch£º789	 i:8 	 global-step:15788	 l-p:0.05558279901742935
epoch£º789	 i:9 	 global-step:15789	 l-p:0.05558903515338898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8794, 28.2480, 27.9813],
        [27.8794, 27.8794, 27.8794],
        [27.8794, 34.1088, 36.7195],
        [27.8794, 29.9456, 29.5567]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.05555831640958786 
model_pd.l_d.mean(): -0.0001726019399939105 
model_pd.lagr.mean(): 0.05538571625947952 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1947], device='cuda:0')), ('power', tensor([-0.4496], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.05555831640958786
epoch£º790	 i:1 	 global-step:15801	 l-p:0.055579107254743576
epoch£º790	 i:2 	 global-step:15802	 l-p:0.0555749386548996
epoch£º790	 i:3 	 global-step:15803	 l-p:0.055595166981220245
epoch£º790	 i:4 	 global-step:15804	 l-p:0.05558709055185318
epoch£º790	 i:5 	 global-step:15805	 l-p:0.05560651049017906
epoch£º790	 i:6 	 global-step:15806	 l-p:0.05578961223363876
epoch£º790	 i:7 	 global-step:15807	 l-p:0.05566854029893875
epoch£º790	 i:8 	 global-step:15808	 l-p:0.055637042969465256
epoch£º790	 i:9 	 global-step:15809	 l-p:0.05567144230008125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7770, 29.6123, 29.1697],
        [27.7770, 37.5969, 44.4440],
        [27.7770, 28.3683, 27.9983],
        [27.7770, 29.1557, 28.6569]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.055599063634872437 
model_pd.l_d.mean(): -9.945409692591056e-05 
model_pd.lagr.mean(): 0.0554996095597744 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1634], device='cuda:0')), ('power', tensor([-0.5655], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.055599063634872437
epoch£º791	 i:1 	 global-step:15821	 l-p:0.05562926083803177
epoch£º791	 i:2 	 global-step:15822	 l-p:0.05564279481768608
epoch£º791	 i:3 	 global-step:15823	 l-p:0.05568542331457138
epoch£º791	 i:4 	 global-step:15824	 l-p:0.05569358170032501
epoch£º791	 i:5 	 global-step:15825	 l-p:0.055599868297576904
epoch£º791	 i:6 	 global-step:15826	 l-p:0.055613622069358826
epoch£º791	 i:7 	 global-step:15827	 l-p:0.05559402331709862
epoch£º791	 i:8 	 global-step:15828	 l-p:0.055757880210876465
epoch£º791	 i:9 	 global-step:15829	 l-p:0.05558902770280838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7611, 31.0744, 31.2636],
        [27.7611, 33.0921, 34.8487],
        [27.7611, 28.1015, 27.8508],
        [27.7611, 28.5866, 28.1433]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.05559082701802254 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05559082701802254 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1738], device='cuda:0')), ('power', tensor([-0.5641], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.05559082701802254
epoch£º792	 i:1 	 global-step:15841	 l-p:0.05566031485795975
epoch£º792	 i:2 	 global-step:15842	 l-p:0.055596645921468735
epoch£º792	 i:3 	 global-step:15843	 l-p:0.05563782528042793
epoch£º792	 i:4 	 global-step:15844	 l-p:0.055621013045310974
epoch£º792	 i:5 	 global-step:15845	 l-p:0.0555870346724987
epoch£º792	 i:6 	 global-step:15846	 l-p:0.055735502392053604
epoch£º792	 i:7 	 global-step:15847	 l-p:0.055731892585754395
epoch£º792	 i:8 	 global-step:15848	 l-p:0.05561159923672676
epoch£º792	 i:9 	 global-step:15849	 l-p:0.055585768073797226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8202, 27.8203, 27.8202],
        [27.8202, 27.8311, 27.8206],
        [27.8202, 27.8203, 27.8202],
        [27.8202, 28.0387, 27.8638]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.055583350360393524 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055583350360393524 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1798], device='cuda:0')), ('power', tensor([-0.5381], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.055583350360393524
epoch£º793	 i:1 	 global-step:15861	 l-p:0.05579539015889168
epoch£º793	 i:2 	 global-step:15862	 l-p:0.05558900162577629
epoch£º793	 i:3 	 global-step:15863	 l-p:0.05557142198085785
epoch£º793	 i:4 	 global-step:15864	 l-p:0.05567038431763649
epoch£º793	 i:5 	 global-step:15865	 l-p:0.05556866526603699
epoch£º793	 i:6 	 global-step:15866	 l-p:0.05559394881129265
epoch£º793	 i:7 	 global-step:15867	 l-p:0.05558721348643303
epoch£º793	 i:8 	 global-step:15868	 l-p:0.05560491979122162
epoch£º793	 i:9 	 global-step:15869	 l-p:0.05563505366444588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9098, 30.7527, 30.6715],
        [27.9098, 28.0036, 27.9209],
        [27.9098, 30.5288, 30.3417],
        [27.9098, 27.9098, 27.9098]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.0555935837328434 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0555935837328434 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1171], device='cuda:0')), ('power', tensor([-0.3304], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.0555935837328434
epoch£º794	 i:1 	 global-step:15881	 l-p:0.055629927664995193
epoch£º794	 i:2 	 global-step:15882	 l-p:0.05555707961320877
epoch£º794	 i:3 	 global-step:15883	 l-p:0.05556811764836311
epoch£º794	 i:4 	 global-step:15884	 l-p:0.05568700656294823
epoch£º794	 i:5 	 global-step:15885	 l-p:0.055553339421749115
epoch£º794	 i:6 	 global-step:15886	 l-p:0.055566687136888504
epoch£º794	 i:7 	 global-step:15887	 l-p:0.05562039092183113
epoch£º794	 i:8 	 global-step:15888	 l-p:0.05557247996330261
epoch£º794	 i:9 	 global-step:15889	 l-p:0.05565030872821808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0099, 28.0099, 28.0099],
        [28.0099, 32.3907, 33.3087],
        [28.0099, 28.2056, 28.0462],
        [28.0099, 28.1956, 28.0432]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.05556604266166687 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05556604266166687 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1387], device='cuda:0')), ('power', tensor([-0.2834], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.05556604266166687
epoch£º795	 i:1 	 global-step:15901	 l-p:0.05557557940483093
epoch£º795	 i:2 	 global-step:15902	 l-p:0.055545058101415634
epoch£º795	 i:3 	 global-step:15903	 l-p:0.05561879649758339
epoch£º795	 i:4 	 global-step:15904	 l-p:0.05556893348693848
epoch£º795	 i:5 	 global-step:15905	 l-p:0.05561050772666931
epoch£º795	 i:6 	 global-step:15906	 l-p:0.05561227723956108
epoch£º795	 i:7 	 global-step:15907	 l-p:0.055545609444379807
epoch£º795	 i:8 	 global-step:15908	 l-p:0.055624622851610184
epoch£º795	 i:9 	 global-step:15909	 l-p:0.05551837012171745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1141, 28.3558, 28.1651],
        [28.1141, 34.3942, 37.0238],
        [28.1141, 28.8805, 28.4497],
        [28.1141, 28.1143, 28.1141]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.0555768758058548 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0555768758058548 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1285], device='cuda:0')), ('power', tensor([-0.1139], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.0555768758058548
epoch£º796	 i:1 	 global-step:15921	 l-p:0.05561329424381256
epoch£º796	 i:2 	 global-step:15922	 l-p:0.05561992526054382
epoch£º796	 i:3 	 global-step:15923	 l-p:0.05554017052054405
epoch£º796	 i:4 	 global-step:15924	 l-p:0.05559297278523445
epoch£º796	 i:5 	 global-step:15925	 l-p:0.05551246181130409
epoch£º796	 i:6 	 global-step:15926	 l-p:0.05554836988449097
epoch£º796	 i:7 	 global-step:15927	 l-p:0.055523842573165894
epoch£º796	 i:8 	 global-step:15928	 l-p:0.05553208291530609
epoch£º796	 i:9 	 global-step:15929	 l-p:0.05550903081893921
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2201, 28.6888, 28.3697],
        [28.2201, 28.2906, 28.2270],
        [28.2201, 28.2225, 28.2201],
        [28.2201, 28.2200, 28.2200]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.05551265552639961 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05551265552639961 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1628], device='cuda:0')), ('power', tensor([-0.0584], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.05551265552639961
epoch£º797	 i:1 	 global-step:15941	 l-p:0.05549255758523941
epoch£º797	 i:2 	 global-step:15942	 l-p:0.05548088997602463
epoch£º797	 i:3 	 global-step:15943	 l-p:0.05553887411952019
epoch£º797	 i:4 	 global-step:15944	 l-p:0.05558997020125389
epoch£º797	 i:5 	 global-step:15945	 l-p:0.05553538352251053
epoch£º797	 i:6 	 global-step:15946	 l-p:0.05555004999041557
epoch£º797	 i:7 	 global-step:15947	 l-p:0.055501118302345276
epoch£º797	 i:8 	 global-step:15948	 l-p:0.05553561449050903
epoch£º797	 i:9 	 global-step:15949	 l-p:0.05561339110136032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3253, 35.4891, 39.0190],
        [28.3253, 28.3442, 28.3261],
        [28.3253, 29.1099, 28.6723],
        [28.3253, 37.3339, 43.0236]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.05554977431893349 
model_pd.l_d.mean(): 2.0989446056773886e-06 
model_pd.lagr.mean(): 0.05555187165737152 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.0351e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.1577], device='cuda:0')), ('power', tensor([0.0796], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.05554977431893349
epoch£º798	 i:1 	 global-step:15961	 l-p:0.055556897073984146
epoch£º798	 i:2 	 global-step:15962	 l-p:0.05547535791993141
epoch£º798	 i:3 	 global-step:15963	 l-p:0.05547542870044708
epoch£º798	 i:4 	 global-step:15964	 l-p:0.055633410811424255
epoch£º798	 i:5 	 global-step:15965	 l-p:0.05546365678310394
epoch£º798	 i:6 	 global-step:15966	 l-p:0.05551578849554062
epoch£º798	 i:7 	 global-step:15967	 l-p:0.0554688423871994
epoch£º798	 i:8 	 global-step:15968	 l-p:0.055517785251140594
epoch£º798	 i:9 	 global-step:15969	 l-p:0.055485814809799194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4218, 28.6205, 28.4586],
        [28.4218, 28.4271, 28.4219],
        [28.4218, 28.4218, 28.4218],
        [28.4218, 28.4327, 28.4222]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.055446404963731766 
model_pd.l_d.mean(): 4.4893067752127536e-06 
model_pd.lagr.mean(): 0.05545089393854141 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.1041e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-3.2265], device='cuda:0')), ('power', tensor([0.0507], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.055446404963731766
epoch£º799	 i:1 	 global-step:15981	 l-p:0.05548099800944328
epoch£º799	 i:2 	 global-step:15982	 l-p:0.05549027770757675
epoch£º799	 i:3 	 global-step:15983	 l-p:0.05550212785601616
epoch£º799	 i:4 	 global-step:15984	 l-p:0.055460937321186066
epoch£º799	 i:5 	 global-step:15985	 l-p:0.05557146295905113
epoch£º799	 i:6 	 global-step:15986	 l-p:0.055447280406951904
epoch£º799	 i:7 	 global-step:15987	 l-p:0.05549497529864311
epoch£º799	 i:8 	 global-step:15988	 l-p:0.0554511584341526
epoch£º799	 i:9 	 global-step:15989	 l-p:0.05562122166156769
