
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.12033862620592117 
model_pd.l_d.mean(): -24.47382926940918 
model_pd.lagr.mean(): -24.353490829467773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0805], device='cuda:0')), ('power', tensor([-24.5543], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.12033862620592117
epoch£º0	 i:1 	 global-step:1	 l-p:0.15623173117637634
epoch£º0	 i:2 	 global-step:2	 l-p:0.15599572658538818
epoch£º0	 i:3 	 global-step:3	 l-p:0.13998126983642578
epoch£º0	 i:4 	 global-step:4	 l-p:-0.8872278928756714
epoch£º0	 i:5 	 global-step:5	 l-p:0.11750897765159607
epoch£º0	 i:6 	 global-step:6	 l-p:0.13981249928474426
epoch£º0	 i:7 	 global-step:7	 l-p:0.11974195390939713
epoch£º0	 i:8 	 global-step:8	 l-p:0.03877580910921097
epoch£º0	 i:9 	 global-step:9	 l-p:0.11709308624267578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2256, 3.2260, 3.2256],
        [3.2256, 3.4164, 3.3781],
        [3.2256, 3.2803, 3.2454],
        [3.2256, 4.1137, 4.7313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.10776599496603012 
model_pd.l_d.mean(): -23.923336029052734 
model_pd.lagr.mean(): -23.815570831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2747], device='cuda:0')), ('power', tensor([-23.6487], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.10776599496603012
epoch£º1	 i:1 	 global-step:21	 l-p:0.1131468340754509
epoch£º1	 i:2 	 global-step:22	 l-p:0.10735069215297699
epoch£º1	 i:3 	 global-step:23	 l-p:0.09871793538331985
epoch£º1	 i:4 	 global-step:24	 l-p:0.1287594437599182
epoch£º1	 i:5 	 global-step:25	 l-p:0.11391725391149521
epoch£º1	 i:6 	 global-step:26	 l-p:0.12586139142513275
epoch£º1	 i:7 	 global-step:27	 l-p:0.1297922134399414
epoch£º1	 i:8 	 global-step:28	 l-p:0.1225266084074974
epoch£º1	 i:9 	 global-step:29	 l-p:0.09531988203525543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9208, 3.4956, 3.7844],
        [2.9208, 2.9750, 2.9420],
        [2.9208, 3.3585, 3.5046],
        [2.9208, 2.9663, 2.9367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.1364690661430359 
model_pd.l_d.mean(): -24.47289276123047 
model_pd.lagr.mean(): -24.336423873901367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1897], device='cuda:0')), ('power', tensor([-24.2832], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.1364690661430359
epoch£º2	 i:1 	 global-step:41	 l-p:0.18635177612304688
epoch£º2	 i:2 	 global-step:42	 l-p:0.13883858919143677
epoch£º2	 i:3 	 global-step:43	 l-p:0.12673039734363556
epoch£º2	 i:4 	 global-step:44	 l-p:0.13562938570976257
epoch£º2	 i:5 	 global-step:45	 l-p:0.21877653896808624
epoch£º2	 i:6 	 global-step:46	 l-p:0.11812800914049149
epoch£º2	 i:7 	 global-step:47	 l-p:0.2314431518316269
epoch£º2	 i:8 	 global-step:48	 l-p:0.1351613849401474
epoch£º2	 i:9 	 global-step:49	 l-p:0.1271619349718094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8749, 3.3876, 3.6167],
        [2.8749, 2.8999, 2.8809],
        [2.8749, 2.9266, 2.8948],
        [2.8749, 3.6070, 4.0994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.13393445312976837 
model_pd.l_d.mean(): -23.677711486816406 
model_pd.lagr.mean(): -23.543777465820312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0696], device='cuda:0')), ('power', tensor([-23.6081], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.13393445312976837
epoch£º3	 i:1 	 global-step:61	 l-p:0.15742982923984528
epoch£º3	 i:2 	 global-step:62	 l-p:0.11244472116231918
epoch£º3	 i:3 	 global-step:63	 l-p:0.12329031527042389
epoch£º3	 i:4 	 global-step:64	 l-p:0.14087186753749847
epoch£º3	 i:5 	 global-step:65	 l-p:0.11345460265874863
epoch£º3	 i:6 	 global-step:66	 l-p:0.11638367176055908
epoch£º3	 i:7 	 global-step:67	 l-p:0.12700121104717255
epoch£º3	 i:8 	 global-step:68	 l-p:0.12586621940135956
epoch£º3	 i:9 	 global-step:69	 l-p:0.13128747045993805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9794, 3.3690, 3.4682],
        [2.9794, 3.4537, 3.6308],
        [2.9794, 3.7974, 4.3829],
        [2.9794, 3.0279, 2.9969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.11796686053276062 
model_pd.l_d.mean(): -23.869792938232422 
model_pd.lagr.mean(): -23.7518253326416 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1574], device='cuda:0')), ('power', tensor([-23.7124], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.11796686053276062
epoch£º4	 i:1 	 global-step:81	 l-p:0.12162087857723236
epoch£º4	 i:2 	 global-step:82	 l-p:0.12294977903366089
epoch£º4	 i:3 	 global-step:83	 l-p:0.11910766363143921
epoch£º4	 i:4 	 global-step:84	 l-p:0.14310906827449799
epoch£º4	 i:5 	 global-step:85	 l-p:0.12753444910049438
epoch£º4	 i:6 	 global-step:86	 l-p:0.1403995156288147
epoch£º4	 i:7 	 global-step:87	 l-p:0.12259810417890549
epoch£º4	 i:8 	 global-step:88	 l-p:0.11314065754413605
epoch£º4	 i:9 	 global-step:89	 l-p:0.1515340507030487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9345, 2.9346, 2.9345],
        [2.9345, 2.9380, 2.9347],
        [2.9345, 2.9881, 2.9556],
        [2.9345, 3.0113, 2.9725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.15390758216381073 
model_pd.l_d.mean(): -24.3869571685791 
model_pd.lagr.mean(): -24.233049392700195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1706], device='cuda:0')), ('power', tensor([-24.2164], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.15390758216381073
epoch£º5	 i:1 	 global-step:101	 l-p:0.11670948565006256
epoch£º5	 i:2 	 global-step:102	 l-p:0.11712370812892914
epoch£º5	 i:3 	 global-step:103	 l-p:0.12532098591327667
epoch£º5	 i:4 	 global-step:104	 l-p:0.1520797461271286
epoch£º5	 i:5 	 global-step:105	 l-p:0.1284780353307724
epoch£º5	 i:6 	 global-step:106	 l-p:0.11871396750211716
epoch£º5	 i:7 	 global-step:107	 l-p:0.12135493010282516
epoch£º5	 i:8 	 global-step:108	 l-p:0.12177778035402298
epoch£º5	 i:9 	 global-step:109	 l-p:0.11665178835391998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9792, 2.9945, 2.9819],
        [2.9792, 2.9792, 2.9792],
        [2.9792, 3.7402, 4.2534],
        [2.9792, 2.9819, 2.9794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.12990255653858185 
model_pd.l_d.mean(): -24.6672420501709 
model_pd.lagr.mean(): -24.53734016418457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2477], device='cuda:0')), ('power', tensor([-24.4195], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.12990255653858185
epoch£º6	 i:1 	 global-step:121	 l-p:0.11832033097743988
epoch£º6	 i:2 	 global-step:122	 l-p:0.1448458433151245
epoch£º6	 i:3 	 global-step:123	 l-p:0.11133634299039841
epoch£º6	 i:4 	 global-step:124	 l-p:0.16962413489818573
epoch£º6	 i:5 	 global-step:125	 l-p:0.1231846809387207
epoch£º6	 i:6 	 global-step:126	 l-p:0.12460335344076157
epoch£º6	 i:7 	 global-step:127	 l-p:0.14342078566551208
epoch£º6	 i:8 	 global-step:128	 l-p:0.11829575896263123
epoch£º6	 i:9 	 global-step:129	 l-p:0.12945161759853363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0274, 3.3849, 3.4576],
        [3.0274, 3.0620, 3.0376],
        [3.0274, 3.0275, 3.0274],
        [3.0274, 3.0391, 3.0292]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.12662450969219208 
model_pd.l_d.mean(): -24.579347610473633 
model_pd.lagr.mean(): -24.452722549438477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2524], device='cuda:0')), ('power', tensor([-24.3270], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.12662450969219208
epoch£º7	 i:1 	 global-step:141	 l-p:0.1374436318874359
epoch£º7	 i:2 	 global-step:142	 l-p:0.12060949951410294
epoch£º7	 i:3 	 global-step:143	 l-p:0.12067034095525742
epoch£º7	 i:4 	 global-step:144	 l-p:0.12406407296657562
epoch£º7	 i:5 	 global-step:145	 l-p:0.13588574528694153
epoch£º7	 i:6 	 global-step:146	 l-p:0.1710631102323532
epoch£º7	 i:7 	 global-step:147	 l-p:0.14493328332901
epoch£º7	 i:8 	 global-step:148	 l-p:0.11248449236154556
epoch£º7	 i:9 	 global-step:149	 l-p:0.1109699234366417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9928, 2.9928, 2.9928],
        [2.9928, 2.9928, 2.9928],
        [2.9928, 3.4413, 3.5989],
        [2.9928, 3.0733, 3.0341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.13157464563846588 
model_pd.l_d.mean(): -23.752622604370117 
model_pd.lagr.mean(): -23.621047973632812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1528], device='cuda:0')), ('power', tensor([-23.5999], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.13157464563846588
epoch£º8	 i:1 	 global-step:161	 l-p:0.12204194813966751
epoch£º8	 i:2 	 global-step:162	 l-p:0.12351909279823303
epoch£º8	 i:3 	 global-step:163	 l-p:0.12446316331624985
epoch£º8	 i:4 	 global-step:164	 l-p:0.08886846154928207
epoch£º8	 i:5 	 global-step:165	 l-p:0.12393789738416672
epoch£º8	 i:6 	 global-step:166	 l-p:0.10921342670917511
epoch£º8	 i:7 	 global-step:167	 l-p:0.17442958056926727
epoch£º8	 i:8 	 global-step:168	 l-p:0.12596100568771362
epoch£º8	 i:9 	 global-step:169	 l-p:0.12539006769657135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9972, 2.9982, 2.9972],
        [2.9972, 2.9982, 2.9972],
        [2.9972, 3.0766, 3.0378],
        [2.9972, 3.0083, 2.9988]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.15287275612354279 
model_pd.l_d.mean(): -23.232685089111328 
model_pd.lagr.mean(): -23.07981300354004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1244], device='cuda:0')), ('power', tensor([-23.1083], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.15287275612354279
epoch£º9	 i:1 	 global-step:181	 l-p:0.1255163699388504
epoch£º9	 i:2 	 global-step:182	 l-p:0.10874105244874954
epoch£º9	 i:3 	 global-step:183	 l-p:0.13128480315208435
epoch£º9	 i:4 	 global-step:184	 l-p:0.1130293607711792
epoch£º9	 i:5 	 global-step:185	 l-p:0.11586610227823257
epoch£º9	 i:6 	 global-step:186	 l-p:0.12223752588033676
epoch£º9	 i:7 	 global-step:187	 l-p:0.12883161008358002
epoch£º9	 i:8 	 global-step:188	 l-p:0.13497959077358246
epoch£º9	 i:9 	 global-step:189	 l-p:0.12585896253585815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8590, 3.4821, 3.8534],
        [2.8590, 3.2840, 3.4384],
        [2.8590, 2.8615, 2.8591],
        [2.8590, 2.8590, 2.8590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.1417713314294815 
model_pd.l_d.mean(): -24.468774795532227 
model_pd.lagr.mean(): -24.327003479003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1494], device='cuda:0')), ('power', tensor([-24.3194], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.1417713314294815
epoch£º10	 i:1 	 global-step:201	 l-p:0.27795320749282837
epoch£º10	 i:2 	 global-step:202	 l-p:0.2713924050331116
epoch£º10	 i:3 	 global-step:203	 l-p:0.12900996208190918
epoch£º10	 i:4 	 global-step:204	 l-p:0.13719326257705688
epoch£º10	 i:5 	 global-step:205	 l-p:0.1340589076280594
epoch£º10	 i:6 	 global-step:206	 l-p:-0.1982746124267578
epoch£º10	 i:7 	 global-step:207	 l-p:0.12843100726604462
epoch£º10	 i:8 	 global-step:208	 l-p:0.1281825751066208
epoch£º10	 i:9 	 global-step:209	 l-p:0.1311969757080078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9005, 3.1688, 3.1924],
        [2.9005, 3.5784, 4.0108],
        [2.9005, 3.2684, 3.3674],
        [2.9005, 2.9124, 2.9024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.12280432134866714 
model_pd.l_d.mean(): -24.40215492248535 
model_pd.lagr.mean(): -24.27935028076172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1618], device='cuda:0')), ('power', tensor([-24.2403], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.12280432134866714
epoch£º11	 i:1 	 global-step:221	 l-p:0.12713170051574707
epoch£º11	 i:2 	 global-step:222	 l-p:0.1626727283000946
epoch£º11	 i:3 	 global-step:223	 l-p:0.14571838080883026
epoch£º11	 i:4 	 global-step:224	 l-p:0.1261797547340393
epoch£º11	 i:5 	 global-step:225	 l-p:0.12158330529928207
epoch£º11	 i:6 	 global-step:226	 l-p:0.12595520913600922
epoch£º11	 i:7 	 global-step:227	 l-p:0.12492869794368744
epoch£º11	 i:8 	 global-step:228	 l-p:0.2059948742389679
epoch£º11	 i:9 	 global-step:229	 l-p:0.12344183027744293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9478, 3.1523, 3.1387],
        [2.9478, 2.9663, 2.9517],
        [2.9478, 2.9635, 2.9508],
        [2.9478, 3.0340, 2.9958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.15631158649921417 
model_pd.l_d.mean(): -24.006811141967773 
model_pd.lagr.mean(): -23.850500106811523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1227], device='cuda:0')), ('power', tensor([-23.8841], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.15631158649921417
epoch£º12	 i:1 	 global-step:241	 l-p:0.12310847640037537
epoch£º12	 i:2 	 global-step:242	 l-p:0.12110184133052826
epoch£º12	 i:3 	 global-step:243	 l-p:0.11956682801246643
epoch£º12	 i:4 	 global-step:244	 l-p:0.12897735834121704
epoch£º12	 i:5 	 global-step:245	 l-p:0.12799595296382904
epoch£º12	 i:6 	 global-step:246	 l-p:0.12067285180091858
epoch£º12	 i:7 	 global-step:247	 l-p:0.10315204411745071
epoch£º12	 i:8 	 global-step:248	 l-p:0.1312023252248764
epoch£º12	 i:9 	 global-step:249	 l-p:0.19106444716453552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9148, 3.4976, 3.8158],
        [2.9148, 2.9148, 2.9148],
        [2.9148, 2.9165, 2.9149],
        [2.9148, 3.1142, 3.1005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.14103835821151733 
model_pd.l_d.mean(): -23.718503952026367 
model_pd.lagr.mean(): -23.577465057373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1180], device='cuda:0')), ('power', tensor([-23.6005], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.14103835821151733
epoch£º13	 i:1 	 global-step:261	 l-p:0.13420352339744568
epoch£º13	 i:2 	 global-step:262	 l-p:0.1168648898601532
epoch£º13	 i:3 	 global-step:263	 l-p:0.12089221924543381
epoch£º13	 i:4 	 global-step:264	 l-p:0.12489593029022217
epoch£º13	 i:5 	 global-step:265	 l-p:0.1215994730591774
epoch£º13	 i:6 	 global-step:266	 l-p:0.19180449843406677
epoch£º13	 i:7 	 global-step:267	 l-p:1.0636742115020752
epoch£º13	 i:8 	 global-step:268	 l-p:0.12438525259494781
epoch£º13	 i:9 	 global-step:269	 l-p:0.1548273265361786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9322, 2.9322, 2.9322],
        [2.9322, 3.3924, 3.5776],
        [2.9322, 2.9493, 2.9357],
        [2.9322, 3.3582, 3.5101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.11936374753713608 
model_pd.l_d.mean(): -23.689414978027344 
model_pd.lagr.mean(): -23.570051193237305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1388], device='cuda:0')), ('power', tensor([-23.5507], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.11936374753713608
epoch£º14	 i:1 	 global-step:281	 l-p:0.11038558930158615
epoch£º14	 i:2 	 global-step:282	 l-p:0.11817453801631927
epoch£º14	 i:3 	 global-step:283	 l-p:0.13677112758159637
epoch£º14	 i:4 	 global-step:284	 l-p:0.12743031978607178
epoch£º14	 i:5 	 global-step:285	 l-p:0.10242357105016708
epoch£º14	 i:6 	 global-step:286	 l-p:0.1315450817346573
epoch£º14	 i:7 	 global-step:287	 l-p:0.13079676032066345
epoch£º14	 i:8 	 global-step:288	 l-p:0.1463743895292282
epoch£º14	 i:9 	 global-step:289	 l-p:0.12189868837594986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0010, 3.0171, 3.0041],
        [3.0010, 3.0029, 3.0011],
        [3.0010, 3.0912, 3.0527],
        [3.0010, 3.0064, 3.0015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.12690649926662445 
model_pd.l_d.mean(): -24.621623992919922 
model_pd.lagr.mean(): -24.49471664428711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2454], device='cuda:0')), ('power', tensor([-24.3762], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.12690649926662445
epoch£º15	 i:1 	 global-step:301	 l-p:0.11763698607683182
epoch£º15	 i:2 	 global-step:302	 l-p:0.13112850487232208
epoch£º15	 i:3 	 global-step:303	 l-p:0.12149753421545029
epoch£º15	 i:4 	 global-step:304	 l-p:0.12885168194770813
epoch£º15	 i:5 	 global-step:305	 l-p:0.1250174194574356
epoch£º15	 i:6 	 global-step:306	 l-p:0.25387462973594666
epoch£º15	 i:7 	 global-step:307	 l-p:0.4735964834690094
epoch£º15	 i:8 	 global-step:308	 l-p:0.10564842820167542
epoch£º15	 i:9 	 global-step:309	 l-p:0.12762552499771118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9427, 2.9437, 2.9428],
        [2.9427, 2.9556, 2.9450],
        [2.9427, 2.9429, 2.9428],
        [2.9427, 3.1216, 3.1002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.11556152254343033 
model_pd.l_d.mean(): -23.902481079101562 
model_pd.lagr.mean(): -23.78691864013672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1697], device='cuda:0')), ('power', tensor([-23.7327], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.11556152254343033
epoch£º16	 i:1 	 global-step:321	 l-p:0.1354447901248932
epoch£º16	 i:2 	 global-step:322	 l-p:0.11594163626432419
epoch£º16	 i:3 	 global-step:323	 l-p:0.13988500833511353
epoch£º16	 i:4 	 global-step:324	 l-p:0.13526782393455505
epoch£º16	 i:5 	 global-step:325	 l-p:0.13661819696426392
epoch£º16	 i:6 	 global-step:326	 l-p:0.11316755414009094
epoch£º16	 i:7 	 global-step:327	 l-p:0.14193421602249146
epoch£º16	 i:8 	 global-step:328	 l-p:0.12258408963680267
epoch£º16	 i:9 	 global-step:329	 l-p:0.13692548871040344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0113, 3.4904, 3.6881],
        [3.0113, 3.1705, 3.1406],
        [3.0113, 3.0114, 3.0113],
        [3.0113, 3.4840, 3.6753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.14988002181053162 
model_pd.l_d.mean(): -24.433488845825195 
model_pd.lagr.mean(): -24.28360939025879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2195], device='cuda:0')), ('power', tensor([-24.2140], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.14988002181053162
epoch£º17	 i:1 	 global-step:341	 l-p:0.12653081119060516
epoch£º17	 i:2 	 global-step:342	 l-p:0.1132335364818573
epoch£º17	 i:3 	 global-step:343	 l-p:0.16084223985671997
epoch£º17	 i:4 	 global-step:344	 l-p:0.12554989755153656
epoch£º17	 i:5 	 global-step:345	 l-p:0.120246022939682
epoch£º17	 i:6 	 global-step:346	 l-p:0.1299246847629547
epoch£º17	 i:7 	 global-step:347	 l-p:0.09848646074533463
epoch£º17	 i:8 	 global-step:348	 l-p:0.1197749525308609
epoch£º17	 i:9 	 global-step:349	 l-p:0.11750993132591248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9300, 2.9537, 2.9361],
        [2.9300, 2.9302, 2.9300],
        [2.9300, 2.9854, 2.9543],
        [2.9300, 2.9361, 2.9307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.14307355880737305 
model_pd.l_d.mean(): -24.365388870239258 
model_pd.lagr.mean(): -24.222314834594727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1637], device='cuda:0')), ('power', tensor([-24.2017], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.14307355880737305
epoch£º18	 i:1 	 global-step:361	 l-p:0.11653202027082443
epoch£º18	 i:2 	 global-step:362	 l-p:0.13438019156455994
epoch£º18	 i:3 	 global-step:363	 l-p:0.13054805994033813
epoch£º18	 i:4 	 global-step:364	 l-p:0.18547195196151733
epoch£º18	 i:5 	 global-step:365	 l-p:0.12787361443042755
epoch£º18	 i:6 	 global-step:366	 l-p:0.1285320520401001
epoch£º18	 i:7 	 global-step:367	 l-p:-0.2818983495235443
epoch£º18	 i:8 	 global-step:368	 l-p:0.19581924378871918
epoch£º18	 i:9 	 global-step:369	 l-p:0.11609324812889099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9407, 3.0260, 2.9899],
        [2.9407, 2.9551, 2.9434],
        [2.9407, 2.9407, 2.9407],
        [2.9407, 3.0424, 3.0059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.11729913204908371 
model_pd.l_d.mean(): -24.645662307739258 
model_pd.lagr.mean(): -24.528362274169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2265], device='cuda:0')), ('power', tensor([-24.4192], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.11729913204908371
epoch£º19	 i:1 	 global-step:381	 l-p:0.1262056827545166
epoch£º19	 i:2 	 global-step:382	 l-p:0.12346973270177841
epoch£º19	 i:3 	 global-step:383	 l-p:0.11278007179498672
epoch£º19	 i:4 	 global-step:384	 l-p:0.14407211542129517
epoch£º19	 i:5 	 global-step:385	 l-p:0.12392109632492065
epoch£º19	 i:6 	 global-step:386	 l-p:0.1392073631286621
epoch£º19	 i:7 	 global-step:387	 l-p:0.1262422502040863
epoch£º19	 i:8 	 global-step:388	 l-p:0.12353645265102386
epoch£º19	 i:9 	 global-step:389	 l-p:0.11911078542470932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0376, 3.0389, 3.0377],
        [3.0376, 3.2503, 3.2417],
        [3.0376, 3.0582, 3.0424],
        [3.0376, 3.0674, 3.0463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.12829633057117462 
model_pd.l_d.mean(): -24.605546951293945 
model_pd.lagr.mean(): -24.477251052856445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2572], device='cuda:0')), ('power', tensor([-24.3483], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.12829633057117462
epoch£º20	 i:1 	 global-step:401	 l-p:0.10410021245479584
epoch£º20	 i:2 	 global-step:402	 l-p:0.14255735278129578
epoch£º20	 i:3 	 global-step:403	 l-p:0.16295677423477173
epoch£º20	 i:4 	 global-step:404	 l-p:0.11994276940822601
epoch£º20	 i:5 	 global-step:405	 l-p:0.1310141235589981
epoch£º20	 i:6 	 global-step:406	 l-p:0.12068392336368561
epoch£º20	 i:7 	 global-step:407	 l-p:0.13087689876556396
epoch£º20	 i:8 	 global-step:408	 l-p:0.10940740257501602
epoch£º20	 i:9 	 global-step:409	 l-p:0.1246616318821907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0364, 3.7686, 4.2559],
        [3.0364, 3.0662, 3.0451],
        [3.0364, 3.0364, 3.0364],
        [3.0364, 3.0852, 3.0559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.13975799083709717 
model_pd.l_d.mean(): -24.517566680908203 
model_pd.lagr.mean(): -24.377809524536133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2387], device='cuda:0')), ('power', tensor([-24.2788], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.13975799083709717
epoch£º21	 i:1 	 global-step:421	 l-p:0.13057762384414673
epoch£º21	 i:2 	 global-step:422	 l-p:0.15675055980682373
epoch£º21	 i:3 	 global-step:423	 l-p:0.12489831447601318
epoch£º21	 i:4 	 global-step:424	 l-p:0.12295713275671005
epoch£º21	 i:5 	 global-step:425	 l-p:0.13267959654331207
epoch£º21	 i:6 	 global-step:426	 l-p:0.10930100828409195
epoch£º21	 i:7 	 global-step:427	 l-p:0.2656280994415283
epoch£º21	 i:8 	 global-step:428	 l-p:0.13384941220283508
epoch£º21	 i:9 	 global-step:429	 l-p:0.11670801043510437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8592, 2.9044, 2.8775],
        [2.8592, 2.9935, 2.9645],
        [2.8592, 2.8602, 2.8592],
        [2.8592, 2.8732, 2.8619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.13467785716056824 
model_pd.l_d.mean(): -24.633989334106445 
model_pd.lagr.mean(): -24.499311447143555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1765], device='cuda:0')), ('power', tensor([-24.4575], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.13467785716056824
epoch£º22	 i:1 	 global-step:441	 l-p:0.3502938449382782
epoch£º22	 i:2 	 global-step:442	 l-p:0.12440413981676102
epoch£º22	 i:3 	 global-step:443	 l-p:0.1215953454375267
epoch£º22	 i:4 	 global-step:444	 l-p:0.11990214884281158
epoch£º22	 i:5 	 global-step:445	 l-p:0.13978587090969086
epoch£º22	 i:6 	 global-step:446	 l-p:0.11654534935951233
epoch£º22	 i:7 	 global-step:447	 l-p:0.1337697058916092
epoch£º22	 i:8 	 global-step:448	 l-p:0.16347850859165192
epoch£º22	 i:9 	 global-step:449	 l-p:0.135588601231575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0562, 3.7872, 4.2710],
        [3.0562, 3.3304, 3.3576],
        [3.0562, 3.0565, 3.0562],
        [3.0562, 3.5246, 3.7129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.15203212201595306 
model_pd.l_d.mean(): -24.350074768066406 
model_pd.lagr.mean(): -24.198041915893555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2216], device='cuda:0')), ('power', tensor([-24.1285], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.15203212201595306
epoch£º23	 i:1 	 global-step:461	 l-p:0.11735938489437103
epoch£º23	 i:2 	 global-step:462	 l-p:0.12082427740097046
epoch£º23	 i:3 	 global-step:463	 l-p:0.11810994148254395
epoch£º23	 i:4 	 global-step:464	 l-p:0.08387692272663116
epoch£º23	 i:5 	 global-step:465	 l-p:0.1237030029296875
epoch£º23	 i:6 	 global-step:466	 l-p:0.13255640864372253
epoch£º23	 i:7 	 global-step:467	 l-p:0.13958396017551422
epoch£º23	 i:8 	 global-step:468	 l-p:0.10313732177019119
epoch£º23	 i:9 	 global-step:469	 l-p:0.11679337918758392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9603, 3.1786, 3.1795],
        [2.9603, 3.0967, 3.0660],
        [2.9603, 2.9979, 2.9736],
        [2.9603, 2.9603, 2.9603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.12279924005270004 
model_pd.l_d.mean(): -24.87738609313965 
model_pd.lagr.mean(): -24.754587173461914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2720], device='cuda:0')), ('power', tensor([-24.6054], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.12279924005270004
epoch£º24	 i:1 	 global-step:481	 l-p:0.1187128871679306
epoch£º24	 i:2 	 global-step:482	 l-p:-0.020899228751659393
epoch£º24	 i:3 	 global-step:483	 l-p:0.12346753478050232
epoch£º24	 i:4 	 global-step:484	 l-p:0.29900169372558594
epoch£º24	 i:5 	 global-step:485	 l-p:0.2373250126838684
epoch£º24	 i:6 	 global-step:486	 l-p:0.129246324300766
epoch£º24	 i:7 	 global-step:487	 l-p:0.1239209696650505
epoch£º24	 i:8 	 global-step:488	 l-p:0.12913592159748077
epoch£º24	 i:9 	 global-step:489	 l-p:0.13085173070430756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9063, 2.9063, 2.9063],
        [2.9063, 2.9063, 2.9063],
        [2.9063, 2.9063, 2.9063],
        [2.9063, 3.4503, 3.7413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.15218807756900787 
model_pd.l_d.mean(): -23.528156280517578 
model_pd.lagr.mean(): -23.37596893310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0510], device='cuda:0')), ('power', tensor([-23.4771], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.15218807756900787
epoch£º25	 i:1 	 global-step:501	 l-p:0.12046436220407486
epoch£º25	 i:2 	 global-step:502	 l-p:0.129293292760849
epoch£º25	 i:3 	 global-step:503	 l-p:0.14473295211791992
epoch£º25	 i:4 	 global-step:504	 l-p:0.13739822804927826
epoch£º25	 i:5 	 global-step:505	 l-p:0.13324981927871704
epoch£º25	 i:6 	 global-step:506	 l-p:0.12369439750909805
epoch£º25	 i:7 	 global-step:507	 l-p:0.15496934950351715
epoch£º25	 i:8 	 global-step:508	 l-p:0.08770211786031723
epoch£º25	 i:9 	 global-step:509	 l-p:0.48636993765830994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9170, 3.0045, 2.9702],
        [2.9170, 3.2026, 3.2500],
        [2.9170, 2.9436, 2.9248],
        [2.9170, 3.1619, 3.1819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.1303320825099945 
model_pd.l_d.mean(): -24.670934677124023 
model_pd.lagr.mean(): -24.54060173034668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2055], device='cuda:0')), ('power', tensor([-24.4654], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.1303320825099945
epoch£º26	 i:1 	 global-step:521	 l-p:0.15270845592021942
epoch£º26	 i:2 	 global-step:522	 l-p:0.16262592375278473
epoch£º26	 i:3 	 global-step:523	 l-p:0.1254439502954483
epoch£º26	 i:4 	 global-step:524	 l-p:0.09349606186151505
epoch£º26	 i:5 	 global-step:525	 l-p:0.0976860374212265
epoch£º26	 i:6 	 global-step:526	 l-p:0.11310289800167084
epoch£º26	 i:7 	 global-step:527	 l-p:0.12524284422397614
epoch£º26	 i:8 	 global-step:528	 l-p:0.10310813784599304
epoch£º26	 i:9 	 global-step:529	 l-p:0.11992134153842926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2036, 3.2636, 3.2303],
        [3.2036, 3.3791, 3.3520],
        [3.2036, 3.2036, 3.2036],
        [3.2036, 3.2048, 3.2036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.12188392132520676 
model_pd.l_d.mean(): -23.54332160949707 
model_pd.lagr.mean(): -23.421438217163086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2250], device='cuda:0')), ('power', tensor([-23.3183], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.12188392132520676
epoch£º27	 i:1 	 global-step:541	 l-p:0.11012132465839386
epoch£º27	 i:2 	 global-step:542	 l-p:0.12006454169750214
epoch£º27	 i:3 	 global-step:543	 l-p:0.11957333236932755
epoch£º27	 i:4 	 global-step:544	 l-p:0.1105918288230896
epoch£º27	 i:5 	 global-step:545	 l-p:0.16153563559055328
epoch£º27	 i:6 	 global-step:546	 l-p:0.12854699790477753
epoch£º27	 i:7 	 global-step:547	 l-p:0.12890146672725677
epoch£º27	 i:8 	 global-step:548	 l-p:0.13725493848323822
epoch£º27	 i:9 	 global-step:549	 l-p:0.12163373827934265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8057, 2.8912, 2.8592],
        [2.8057, 3.0718, 3.1152],
        [2.8057, 2.8258, 2.8109],
        [2.8057, 3.4107, 3.7926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.14044612646102905 
model_pd.l_d.mean(): -24.327695846557617 
model_pd.lagr.mean(): -24.1872501373291 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0969], device='cuda:0')), ('power', tensor([-24.2308], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.14044612646102905
epoch£º28	 i:1 	 global-step:561	 l-p:0.13287131488323212
epoch£º28	 i:2 	 global-step:562	 l-p:0.10583320260047913
epoch£º28	 i:3 	 global-step:563	 l-p:0.12348193675279617
epoch£º28	 i:4 	 global-step:564	 l-p:0.14924539625644684
epoch£º28	 i:5 	 global-step:565	 l-p:0.07956023514270782
epoch£º28	 i:6 	 global-step:566	 l-p:0.13248032331466675
epoch£º28	 i:7 	 global-step:567	 l-p:0.12305060774087906
epoch£º28	 i:8 	 global-step:568	 l-p:0.1357582062482834
epoch£º28	 i:9 	 global-step:569	 l-p:0.12487442046403885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9677, 2.9677, 2.9677],
        [2.9677, 3.0593, 3.0246],
        [2.9677, 3.2699, 3.3272],
        [2.9677, 2.9696, 2.9678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.12438825517892838 
model_pd.l_d.mean(): -24.222307205200195 
model_pd.lagr.mean(): -24.097919464111328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1759], device='cuda:0')), ('power', tensor([-24.0464], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.12438825517892838
epoch£º29	 i:1 	 global-step:581	 l-p:0.21102091670036316
epoch£º29	 i:2 	 global-step:582	 l-p:0.12353599071502686
epoch£º29	 i:3 	 global-step:583	 l-p:0.12793923914432526
epoch£º29	 i:4 	 global-step:584	 l-p:0.1252475082874298
epoch£º29	 i:5 	 global-step:585	 l-p:0.11973990499973297
epoch£º29	 i:6 	 global-step:586	 l-p:0.13149718940258026
epoch£º29	 i:7 	 global-step:587	 l-p:0.11312513053417206
epoch£º29	 i:8 	 global-step:588	 l-p:0.1505698710680008
epoch£º29	 i:9 	 global-step:589	 l-p:0.11365433782339096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9741, 2.9748, 2.9741],
        [2.9741, 3.6209, 4.0240],
        [2.9741, 3.0729, 3.0385],
        [2.9741, 3.5588, 3.8892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.12082121521234512 
model_pd.l_d.mean(): -23.81825828552246 
model_pd.lagr.mean(): -23.697437286376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1081], device='cuda:0')), ('power', tensor([-23.7102], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.12082121521234512
epoch£º30	 i:1 	 global-step:601	 l-p:0.12565113604068756
epoch£º30	 i:2 	 global-step:602	 l-p:0.16256186366081238
epoch£º30	 i:3 	 global-step:603	 l-p:0.12768127024173737
epoch£º30	 i:4 	 global-step:604	 l-p:0.11769962310791016
epoch£º30	 i:5 	 global-step:605	 l-p:0.12675145268440247
epoch£º30	 i:6 	 global-step:606	 l-p:0.11391475051641464
epoch£º30	 i:7 	 global-step:607	 l-p:0.125
epoch£º30	 i:8 	 global-step:608	 l-p:0.16687771677970886
epoch£º30	 i:9 	 global-step:609	 l-p:0.155210480093956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9548, 2.9548, 2.9548],
        [2.9548, 2.9584, 2.9551],
        [2.9548, 3.6292, 4.0704],
        [2.9548, 2.9795, 2.9618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.21108253300189972 
model_pd.l_d.mean(): -23.41962432861328 
model_pd.lagr.mean(): -23.208541870117188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0602], device='cuda:0')), ('power', tensor([-23.3595], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.21108253300189972
epoch£º31	 i:1 	 global-step:621	 l-p:0.1161208525300026
epoch£º31	 i:2 	 global-step:622	 l-p:0.10369282215833664
epoch£º31	 i:3 	 global-step:623	 l-p:0.11449535191059113
epoch£º31	 i:4 	 global-step:624	 l-p:0.11182095110416412
epoch£º31	 i:5 	 global-step:625	 l-p:0.12066641449928284
epoch£º31	 i:6 	 global-step:626	 l-p:0.1351182460784912
epoch£º31	 i:7 	 global-step:627	 l-p:0.14539144933223724
epoch£º31	 i:8 	 global-step:628	 l-p:0.12210622429847717
epoch£º31	 i:9 	 global-step:629	 l-p:0.1275656819343567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9342, 3.6431, 4.1323],
        [2.9342, 3.3845, 3.5786],
        [2.9342, 2.9590, 2.9413],
        [2.9342, 2.9348, 2.9342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.1246531680226326 
model_pd.l_d.mean(): -24.362937927246094 
model_pd.lagr.mean(): -24.238285064697266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1860], device='cuda:0')), ('power', tensor([-24.1769], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.1246531680226326
epoch£º32	 i:1 	 global-step:641	 l-p:0.33485549688339233
epoch£º32	 i:2 	 global-step:642	 l-p:0.15194661915302277
epoch£º32	 i:3 	 global-step:643	 l-p:0.06221332401037216
epoch£º32	 i:4 	 global-step:644	 l-p:0.13472294807434082
epoch£º32	 i:5 	 global-step:645	 l-p:0.11904136091470718
epoch£º32	 i:6 	 global-step:646	 l-p:0.12298884242773056
epoch£º32	 i:7 	 global-step:647	 l-p:0.14570599794387817
epoch£º32	 i:8 	 global-step:648	 l-p:1.215549111366272
epoch£º32	 i:9 	 global-step:649	 l-p:0.1322127878665924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8895, 2.8896, 2.8895],
        [2.8895, 2.9951, 2.9637],
        [2.8895, 2.8897, 2.8895],
        [2.8895, 2.9952, 2.9638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.1401502639055252 
model_pd.l_d.mean(): -24.56951904296875 
model_pd.lagr.mean(): -24.42936897277832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1710], device='cuda:0')), ('power', tensor([-24.3986], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.1401502639055252
epoch£º33	 i:1 	 global-step:661	 l-p:0.12513704597949982
epoch£º33	 i:2 	 global-step:662	 l-p:0.1194908618927002
epoch£º33	 i:3 	 global-step:663	 l-p:0.25322553515434265
epoch£º33	 i:4 	 global-step:664	 l-p:0.051622770726680756
epoch£º33	 i:5 	 global-step:665	 l-p:0.12352189421653748
epoch£º33	 i:6 	 global-step:666	 l-p:0.12894417345523834
epoch£º33	 i:7 	 global-step:667	 l-p:0.11112502962350845
epoch£º33	 i:8 	 global-step:668	 l-p:0.13831856846809387
epoch£º33	 i:9 	 global-step:669	 l-p:0.12432245165109634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9878, 3.2715, 3.3170],
        [2.9878, 2.9879, 2.9878],
        [2.9878, 3.1157, 3.0855],
        [2.9878, 3.0120, 2.9946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.17166826128959656 
model_pd.l_d.mean(): -24.573394775390625 
model_pd.lagr.mean(): -24.40172576904297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2193], device='cuda:0')), ('power', tensor([-24.3541], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.17166826128959656
epoch£º34	 i:1 	 global-step:681	 l-p:0.10424782335758209
epoch£º34	 i:2 	 global-step:682	 l-p:0.13574348390102386
epoch£º34	 i:3 	 global-step:683	 l-p:0.13302575051784515
epoch£º34	 i:4 	 global-step:684	 l-p:0.131809264421463
epoch£º34	 i:5 	 global-step:685	 l-p:0.12094543129205704
epoch£º34	 i:6 	 global-step:686	 l-p:0.1245659813284874
epoch£º34	 i:7 	 global-step:687	 l-p:0.09512342512607574
epoch£º34	 i:8 	 global-step:688	 l-p:0.12092683464288712
epoch£º34	 i:9 	 global-step:689	 l-p:0.11693821847438812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9914, 3.2006, 3.2006],
        [2.9914, 2.9914, 2.9914],
        [2.9914, 3.0491, 3.0190],
        [2.9914, 3.6543, 4.0779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.1279446929693222 
model_pd.l_d.mean(): -24.320758819580078 
model_pd.lagr.mean(): -24.192813873291016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1763], device='cuda:0')), ('power', tensor([-24.1444], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.1279446929693222
epoch£º35	 i:1 	 global-step:701	 l-p:0.14931617677211761
epoch£º35	 i:2 	 global-step:702	 l-p:0.12969054281711578
epoch£º35	 i:3 	 global-step:703	 l-p:0.1277688890695572
epoch£º35	 i:4 	 global-step:704	 l-p:0.12301898747682571
epoch£º35	 i:5 	 global-step:705	 l-p:0.12387531250715256
epoch£º35	 i:6 	 global-step:706	 l-p:0.12679076194763184
epoch£º35	 i:7 	 global-step:707	 l-p:0.13742217421531677
epoch£º35	 i:8 	 global-step:708	 l-p:0.059561342000961304
epoch£º35	 i:9 	 global-step:709	 l-p:0.12808650732040405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7988, 2.7988, 2.7988],
        [2.7988, 2.7988, 2.7988],
        [2.7988, 3.0689, 3.1211],
        [2.7988, 2.8162, 2.8031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.12952078878879547 
model_pd.l_d.mean(): -23.906330108642578 
model_pd.lagr.mean(): -23.776809692382812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0662], device='cuda:0')), ('power', tensor([-23.8402], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.12952078878879547
epoch£º36	 i:1 	 global-step:721	 l-p:0.12708725035190582
epoch£º36	 i:2 	 global-step:722	 l-p:0.0923033282160759
epoch£º36	 i:3 	 global-step:723	 l-p:0.13209037482738495
epoch£º36	 i:4 	 global-step:724	 l-p:0.11937002092599869
epoch£º36	 i:5 	 global-step:725	 l-p:0.133168563246727
epoch£º36	 i:6 	 global-step:726	 l-p:0.1590290367603302
epoch£º36	 i:7 	 global-step:727	 l-p:0.14357389509677887
epoch£º36	 i:8 	 global-step:728	 l-p:0.12451571226119995
epoch£º36	 i:9 	 global-step:729	 l-p:0.11837299168109894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0535, 3.0561, 3.0537],
        [3.0535, 3.7127, 4.1222],
        [3.0535, 3.2404, 3.2278],
        [3.0535, 3.2909, 3.3037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.13758860528469086 
model_pd.l_d.mean(): -23.65366554260254 
model_pd.lagr.mean(): -23.516077041625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1377], device='cuda:0')), ('power', tensor([-23.5160], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.13758860528469086
epoch£º37	 i:1 	 global-step:741	 l-p:0.11593116819858551
epoch£º37	 i:2 	 global-step:742	 l-p:0.12038609385490417
epoch£º37	 i:3 	 global-step:743	 l-p:0.12280005216598511
epoch£º37	 i:4 	 global-step:744	 l-p:0.129675030708313
epoch£º37	 i:5 	 global-step:745	 l-p:0.10974372923374176
epoch£º37	 i:6 	 global-step:746	 l-p:0.1165899857878685
epoch£º37	 i:7 	 global-step:747	 l-p:-0.2512945234775543
epoch£º37	 i:8 	 global-step:748	 l-p:0.26167091727256775
epoch£º37	 i:9 	 global-step:749	 l-p:0.13284359872341156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9112, 2.9558, 2.9302],
        [2.9112, 2.9372, 2.9192],
        [2.9112, 2.9210, 2.9129],
        [2.9112, 2.9360, 2.9186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.13127626478672028 
model_pd.l_d.mean(): -24.321744918823242 
model_pd.lagr.mean(): -24.190467834472656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1395], device='cuda:0')), ('power', tensor([-24.1823], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.13127626478672028
epoch£º38	 i:1 	 global-step:761	 l-p:0.28927913308143616
epoch£º38	 i:2 	 global-step:762	 l-p:0.13294173777103424
epoch£º38	 i:3 	 global-step:763	 l-p:0.5448369383811951
epoch£º38	 i:4 	 global-step:764	 l-p:0.11332124471664429
epoch£º38	 i:5 	 global-step:765	 l-p:0.1352272629737854
epoch£º38	 i:6 	 global-step:766	 l-p:0.11019294708967209
epoch£º38	 i:7 	 global-step:767	 l-p:0.1272750198841095
epoch£º38	 i:8 	 global-step:768	 l-p:0.1451662927865982
epoch£º38	 i:9 	 global-step:769	 l-p:0.1270657628774643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9818, 2.9818, 2.9818],
        [2.9818, 3.4987, 3.7609],
        [2.9818, 2.9818, 2.9817],
        [2.9818, 2.9826, 2.9818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.12517312169075012 
model_pd.l_d.mean(): -24.772436141967773 
model_pd.lagr.mean(): -24.647262573242188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2550], device='cuda:0')), ('power', tensor([-24.5175], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.12517312169075012
epoch£º39	 i:1 	 global-step:781	 l-p:0.11805738508701324
epoch£º39	 i:2 	 global-step:782	 l-p:0.12203399091959
epoch£º39	 i:3 	 global-step:783	 l-p:0.13483980298042297
epoch£º39	 i:4 	 global-step:784	 l-p:-481.707763671875
epoch£º39	 i:5 	 global-step:785	 l-p:0.12788806855678558
epoch£º39	 i:6 	 global-step:786	 l-p:-0.034205030649900436
epoch£º39	 i:7 	 global-step:787	 l-p:0.12625575065612793
epoch£º39	 i:8 	 global-step:788	 l-p:0.08308897912502289
epoch£º39	 i:9 	 global-step:789	 l-p:0.12485811859369278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9485, 2.9485, 2.9485],
        [2.9485, 2.9556, 2.9495],
        [2.9485, 2.9939, 2.9679],
        [2.9485, 2.9513, 2.9487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.13539282977581024 
model_pd.l_d.mean(): -24.694971084594727 
model_pd.lagr.mean(): -24.55957794189453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2202], device='cuda:0')), ('power', tensor([-24.4747], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.13539282977581024
epoch£º40	 i:1 	 global-step:801	 l-p:0.24341052770614624
epoch£º40	 i:2 	 global-step:802	 l-p:0.12118066847324371
epoch£º40	 i:3 	 global-step:803	 l-p:0.14151425659656525
epoch£º40	 i:4 	 global-step:804	 l-p:0.13657619059085846
epoch£º40	 i:5 	 global-step:805	 l-p:0.12085972726345062
epoch£º40	 i:6 	 global-step:806	 l-p:0.096098393201828
epoch£º40	 i:7 	 global-step:807	 l-p:0.11968532204627991
epoch£º40	 i:8 	 global-step:808	 l-p:0.12925487756729126
epoch£º40	 i:9 	 global-step:809	 l-p:0.10018227994441986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0560, 3.0571, 3.0561],
        [3.0560, 3.0562, 3.0560],
        [3.0560, 3.1369, 3.1034],
        [3.0560, 3.1056, 3.0777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.10605788230895996 
model_pd.l_d.mean(): -24.620223999023438 
model_pd.lagr.mean(): -24.5141658782959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2630], device='cuda:0')), ('power', tensor([-24.3572], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.10605788230895996
epoch£º41	 i:1 	 global-step:821	 l-p:0.14551331102848053
epoch£º41	 i:2 	 global-step:822	 l-p:0.15013571083545685
epoch£º41	 i:3 	 global-step:823	 l-p:0.11596595495939255
epoch£º41	 i:4 	 global-step:824	 l-p:0.12240877002477646
epoch£º41	 i:5 	 global-step:825	 l-p:0.12733928859233856
epoch£º41	 i:6 	 global-step:826	 l-p:0.11628379672765732
epoch£º41	 i:7 	 global-step:827	 l-p:0.35361701250076294
epoch£º41	 i:8 	 global-step:828	 l-p:0.12281030416488647
epoch£º41	 i:9 	 global-step:829	 l-p:0.12627653777599335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9164, 3.0344, 3.0064],
        [2.9164, 2.9164, 2.9164],
        [2.9164, 3.5627, 3.9860],
        [2.9164, 2.9223, 2.9171]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.130851149559021 
model_pd.l_d.mean(): -23.897947311401367 
model_pd.lagr.mean(): -23.7670955657959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1219], device='cuda:0')), ('power', tensor([-23.7760], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.130851149559021
epoch£º42	 i:1 	 global-step:841	 l-p:0.0263911671936512
epoch£º42	 i:2 	 global-step:842	 l-p:0.11898058652877808
epoch£º42	 i:3 	 global-step:843	 l-p:0.24314868450164795
epoch£º42	 i:4 	 global-step:844	 l-p:0.17082224786281586
epoch£º42	 i:5 	 global-step:845	 l-p:0.1360771209001541
epoch£º42	 i:6 	 global-step:846	 l-p:0.11771583557128906
epoch£º42	 i:7 	 global-step:847	 l-p:0.12049321830272675
epoch£º42	 i:8 	 global-step:848	 l-p:0.1292138546705246
epoch£º42	 i:9 	 global-step:849	 l-p:0.13025100529193878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[2.9742, 3.5678, 3.9191],
        [2.9742, 3.2261, 3.2561],
        [2.9742, 3.1077, 3.0817],
        [2.9742, 3.5362, 3.8517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.15409478545188904 
model_pd.l_d.mean(): -24.327896118164062 
model_pd.lagr.mean(): -24.17380142211914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1702], device='cuda:0')), ('power', tensor([-24.1577], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.15409478545188904
epoch£º43	 i:1 	 global-step:861	 l-p:0.13073976337909698
epoch£º43	 i:2 	 global-step:862	 l-p:0.18701201677322388
epoch£º43	 i:3 	 global-step:863	 l-p:0.15396544337272644
epoch£º43	 i:4 	 global-step:864	 l-p:0.11735153943300247
epoch£º43	 i:5 	 global-step:865	 l-p:0.1153608113527298
epoch£º43	 i:6 	 global-step:866	 l-p:0.09755735099315643
epoch£º43	 i:7 	 global-step:867	 l-p:0.11123302578926086
epoch£º43	 i:8 	 global-step:868	 l-p:0.1270367056131363
epoch£º43	 i:9 	 global-step:869	 l-p:0.12540452182292938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0165, 3.0695, 3.0412],
        [3.0165, 3.0193, 3.0168],
        [3.0165, 3.0174, 3.0166],
        [3.0165, 3.0166, 3.0165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.11346250027418137 
model_pd.l_d.mean(): -24.60232162475586 
model_pd.lagr.mean(): -24.488859176635742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2443], device='cuda:0')), ('power', tensor([-24.3580], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.11346250027418137
epoch£º44	 i:1 	 global-step:881	 l-p:0.1299351453781128
epoch£º44	 i:2 	 global-step:882	 l-p:0.12316734343767166
epoch£º44	 i:3 	 global-step:883	 l-p:-1.9286589622497559
epoch£º44	 i:4 	 global-step:884	 l-p:0.15026503801345825
epoch£º44	 i:5 	 global-step:885	 l-p:0.04191112518310547
epoch£º44	 i:6 	 global-step:886	 l-p:0.11989423632621765
epoch£º44	 i:7 	 global-step:887	 l-p:0.12328772991895676
epoch£º44	 i:8 	 global-step:888	 l-p:0.12733514606952667
epoch£º44	 i:9 	 global-step:889	 l-p:0.14750167727470398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9214, 2.9214, 2.9214],
        [2.9214, 2.9218, 2.9214],
        [2.9214, 3.0163, 2.9858],
        [2.9214, 2.9214, 2.9214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.13517259061336517 
model_pd.l_d.mean(): -24.83953094482422 
model_pd.lagr.mean(): -24.70435905456543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2353], device='cuda:0')), ('power', tensor([-24.6042], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.13517259061336517
epoch£º45	 i:1 	 global-step:901	 l-p:0.1320086121559143
epoch£º45	 i:2 	 global-step:902	 l-p:0.14448875188827515
epoch£º45	 i:3 	 global-step:903	 l-p:0.12817014753818512
epoch£º45	 i:4 	 global-step:904	 l-p:0.2950863838195801
epoch£º45	 i:5 	 global-step:905	 l-p:0.11697549372911453
epoch£º45	 i:6 	 global-step:906	 l-p:0.3010203242301941
epoch£º45	 i:7 	 global-step:907	 l-p:0.15038281679153442
epoch£º45	 i:8 	 global-step:908	 l-p:0.11036533117294312
epoch£º45	 i:9 	 global-step:909	 l-p:0.12628121674060822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0967, 3.0984, 3.0968],
        [3.0967, 3.1077, 3.0987],
        [3.0967, 3.0967, 3.0967],
        [3.0967, 3.1074, 3.0986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.11612698435783386 
model_pd.l_d.mean(): -24.218006134033203 
model_pd.lagr.mean(): -24.101879119873047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2205], device='cuda:0')), ('power', tensor([-23.9975], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.11612698435783386
epoch£º46	 i:1 	 global-step:921	 l-p:0.1191265657544136
epoch£º46	 i:2 	 global-step:922	 l-p:0.11911427974700928
epoch£º46	 i:3 	 global-step:923	 l-p:0.12368594855070114
epoch£º46	 i:4 	 global-step:924	 l-p:0.11489999294281006
epoch£º46	 i:5 	 global-step:925	 l-p:0.1574190855026245
epoch£º46	 i:6 	 global-step:926	 l-p:0.12172213941812515
epoch£º46	 i:7 	 global-step:927	 l-p:0.12205706536769867
epoch£º46	 i:8 	 global-step:928	 l-p:0.12521526217460632
epoch£º46	 i:9 	 global-step:929	 l-p:0.11722198128700256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8918, 2.9124, 2.8976],
        [2.8918, 2.9322, 2.9087],
        [2.8918, 2.8924, 2.8918],
        [2.8918, 3.0319, 3.0117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.1315413862466812 
model_pd.l_d.mean(): -23.652252197265625 
model_pd.lagr.mean(): -23.520709991455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1064], device='cuda:0')), ('power', tensor([-23.5459], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.1315413862466812
epoch£º47	 i:1 	 global-step:941	 l-p:0.14049573242664337
epoch£º47	 i:2 	 global-step:942	 l-p:0.12910842895507812
epoch£º47	 i:3 	 global-step:943	 l-p:0.13868367671966553
epoch£º47	 i:4 	 global-step:944	 l-p:0.09963806718587875
epoch£º47	 i:5 	 global-step:945	 l-p:0.13494791090488434
epoch£º47	 i:6 	 global-step:946	 l-p:0.1341131031513214
epoch£º47	 i:7 	 global-step:947	 l-p:0.14209040999412537
epoch£º47	 i:8 	 global-step:948	 l-p:0.09323393553495407
epoch£º47	 i:9 	 global-step:949	 l-p:0.1352517455816269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8862, 3.0251, 3.0051],
        [2.8862, 2.9569, 2.9276],
        [2.8862, 2.8862, 2.8862],
        [2.8862, 2.9779, 2.9485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.12742610275745392 
model_pd.l_d.mean(): -24.84054946899414 
model_pd.lagr.mean(): -24.713123321533203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2145], device='cuda:0')), ('power', tensor([-24.6261], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.12742610275745392
epoch£º48	 i:1 	 global-step:961	 l-p:0.11234816908836365
epoch£º48	 i:2 	 global-step:962	 l-p:0.6564611792564392
epoch£º48	 i:3 	 global-step:963	 l-p:0.15164977312088013
epoch£º48	 i:4 	 global-step:964	 l-p:0.12503348290920258
epoch£º48	 i:5 	 global-step:965	 l-p:0.12650154531002045
epoch£º48	 i:6 	 global-step:966	 l-p:0.12483363598585129
epoch£º48	 i:7 	 global-step:967	 l-p:0.11934873461723328
epoch£º48	 i:8 	 global-step:968	 l-p:0.14234718680381775
epoch£º48	 i:9 	 global-step:969	 l-p:0.11560632288455963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0388, 3.0520, 3.0415],
        [3.0388, 3.0393, 3.0388],
        [3.0388, 3.1039, 3.0734],
        [3.0388, 3.0411, 3.0390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.12284975498914719 
model_pd.l_d.mean(): -24.826427459716797 
model_pd.lagr.mean(): -24.703577041625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2892], device='cuda:0')), ('power', tensor([-24.5372], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.12284975498914719
epoch£º49	 i:1 	 global-step:981	 l-p:0.11166879534721375
epoch£º49	 i:2 	 global-step:982	 l-p:0.13146311044692993
epoch£º49	 i:3 	 global-step:983	 l-p:0.12093241512775421
epoch£º49	 i:4 	 global-step:984	 l-p:0.05600733309984207
epoch£º49	 i:5 	 global-step:985	 l-p:0.1355327069759369
epoch£º49	 i:6 	 global-step:986	 l-p:0.08891300857067108
epoch£º49	 i:7 	 global-step:987	 l-p:0.008255929686129093
epoch£º49	 i:8 	 global-step:988	 l-p:0.1331195831298828
epoch£º49	 i:9 	 global-step:989	 l-p:0.13296490907669067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9718, 3.3547, 3.4930],
        [2.9718, 2.9718, 2.9718],
        [2.9718, 3.1061, 3.0823],
        [2.9718, 2.9724, 2.9718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.16047969460487366 
model_pd.l_d.mean(): -24.431407928466797 
model_pd.lagr.mean(): -24.27092742919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1830], device='cuda:0')), ('power', tensor([-24.2484], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.16047969460487366
epoch£º50	 i:1 	 global-step:1001	 l-p:0.119376540184021
epoch£º50	 i:2 	 global-step:1002	 l-p:0.1041802391409874
epoch£º50	 i:3 	 global-step:1003	 l-p:0.12392127513885498
epoch£º50	 i:4 	 global-step:1004	 l-p:0.1365177035331726
epoch£º50	 i:5 	 global-step:1005	 l-p:0.13572902977466583
epoch£º50	 i:6 	 global-step:1006	 l-p:0.12250921130180359
epoch£º50	 i:7 	 global-step:1007	 l-p:0.12202468514442444
epoch£º50	 i:8 	 global-step:1008	 l-p:0.1175924688577652
epoch£º50	 i:9 	 global-step:1009	 l-p:0.1161777600646019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0283, 3.0283, 3.0283],
        [3.0283, 3.2954, 3.3357],
        [3.0283, 3.6580, 4.0477],
        [3.0283, 3.0283, 3.0283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.14154987037181854 
model_pd.l_d.mean(): -23.747682571411133 
model_pd.lagr.mean(): -23.60613250732422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1486], device='cuda:0')), ('power', tensor([-23.5991], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.14154987037181854
epoch£º51	 i:1 	 global-step:1021	 l-p:0.10051970928907394
epoch£º51	 i:2 	 global-step:1022	 l-p:0.16419793665409088
epoch£º51	 i:3 	 global-step:1023	 l-p:0.12818112969398499
epoch£º51	 i:4 	 global-step:1024	 l-p:0.11606456339359283
epoch£º51	 i:5 	 global-step:1025	 l-p:0.1208881139755249
epoch£º51	 i:6 	 global-step:1026	 l-p:-0.10620435327291489
epoch£º51	 i:7 	 global-step:1027	 l-p:0.2828768193721771
epoch£º51	 i:8 	 global-step:1028	 l-p:0.13058167695999146
epoch£º51	 i:9 	 global-step:1029	 l-p:0.12933209538459778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9282, 3.1855, 3.2272],
        [2.9282, 3.1074, 3.1029],
        [2.9282, 2.9462, 2.9329],
        [2.9282, 2.9282, 2.9282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.11968500167131424 
model_pd.l_d.mean(): -24.69013786315918 
model_pd.lagr.mean(): -24.570453643798828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2236], device='cuda:0')), ('power', tensor([-24.4665], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.11968500167131424
epoch£º52	 i:1 	 global-step:1041	 l-p:0.13374003767967224
epoch£º52	 i:2 	 global-step:1042	 l-p:-0.012882988899946213
epoch£º52	 i:3 	 global-step:1043	 l-p:0.14196956157684326
epoch£º52	 i:4 	 global-step:1044	 l-p:0.11782265454530716
epoch£º52	 i:5 	 global-step:1045	 l-p:0.1643252670764923
epoch£º52	 i:6 	 global-step:1046	 l-p:0.07715213298797607
epoch£º52	 i:7 	 global-step:1047	 l-p:0.13460224866867065
epoch£º52	 i:8 	 global-step:1048	 l-p:0.12188971042633057
epoch£º52	 i:9 	 global-step:1049	 l-p:0.12642744183540344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0063, 3.0093, 3.0066],
        [3.0063, 3.0063, 3.0063],
        [3.0063, 3.0065, 3.0063],
        [3.0063, 3.0065, 3.0063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.12711036205291748 
model_pd.l_d.mean(): -23.967838287353516 
model_pd.lagr.mean(): -23.840728759765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1776], device='cuda:0')), ('power', tensor([-23.7902], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.12711036205291748
epoch£º53	 i:1 	 global-step:1061	 l-p:0.14850597083568573
epoch£º53	 i:2 	 global-step:1062	 l-p:0.12798576056957245
epoch£º53	 i:3 	 global-step:1063	 l-p:0.11835577338933945
epoch£º53	 i:4 	 global-step:1064	 l-p:0.10150259733200073
epoch£º53	 i:5 	 global-step:1065	 l-p:0.130198135972023
epoch£º53	 i:6 	 global-step:1066	 l-p:0.12129754573106766
epoch£º53	 i:7 	 global-step:1067	 l-p:0.10931073874235153
epoch£º53	 i:8 	 global-step:1068	 l-p:0.11456925421953201
epoch£º53	 i:9 	 global-step:1069	 l-p:0.11403564363718033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9894, 2.9894, 2.9894],
        [2.9894, 3.2940, 3.3665],
        [2.9894, 3.3185, 3.4107],
        [2.9894, 3.0282, 3.0051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.13167504966259003 
model_pd.l_d.mean(): -24.711862564086914 
model_pd.lagr.mean(): -24.58018684387207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2415], device='cuda:0')), ('power', tensor([-24.4704], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.13167504966259003
epoch£º54	 i:1 	 global-step:1081	 l-p:0.12989507615566254
epoch£º54	 i:2 	 global-step:1082	 l-p:0.11067939549684525
epoch£º54	 i:3 	 global-step:1083	 l-p:0.23005646467208862
epoch£º54	 i:4 	 global-step:1084	 l-p:0.11718251556158066
epoch£º54	 i:5 	 global-step:1085	 l-p:0.11403251439332962
epoch£º54	 i:6 	 global-step:1086	 l-p:0.13762252032756805
epoch£º54	 i:7 	 global-step:1087	 l-p:0.13262853026390076
epoch£º54	 i:8 	 global-step:1088	 l-p:0.168572798371315
epoch£º54	 i:9 	 global-step:1089	 l-p:0.13167496025562286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[2.7959, 3.0851, 3.1640],
        [2.7959, 3.2211, 3.4257],
        [2.7959, 2.9557, 2.9498],
        [2.7959, 3.0891, 3.1713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.157211035490036 
model_pd.l_d.mean(): -24.42010498046875 
model_pd.lagr.mean(): -24.262893676757812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0944], device='cuda:0')), ('power', tensor([-24.3257], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.157211035490036
epoch£º55	 i:1 	 global-step:1101	 l-p:0.12540128827095032
epoch£º55	 i:2 	 global-step:1102	 l-p:0.1194329708814621
epoch£º55	 i:3 	 global-step:1103	 l-p:0.12084421515464783
epoch£º55	 i:4 	 global-step:1104	 l-p:0.055678967386484146
epoch£º55	 i:5 	 global-step:1105	 l-p:0.17606908082962036
epoch£º55	 i:6 	 global-step:1106	 l-p:0.1503773033618927
epoch£º55	 i:7 	 global-step:1107	 l-p:0.12990707159042358
epoch£º55	 i:8 	 global-step:1108	 l-p:0.11895038187503815
epoch£º55	 i:9 	 global-step:1109	 l-p:0.10497928410768509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1490, 3.8088, 4.2159],
        [3.1490, 3.1490, 3.1490],
        [3.1490, 3.2525, 3.2197],
        [3.1490, 3.1800, 3.1595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.15173666179180145 
model_pd.l_d.mean(): -24.349294662475586 
model_pd.lagr.mean(): -24.19755744934082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2458], device='cuda:0')), ('power', tensor([-24.1035], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.15173666179180145
epoch£º56	 i:1 	 global-step:1121	 l-p:0.09301929920911789
epoch£º56	 i:2 	 global-step:1122	 l-p:0.11307864636182785
epoch£º56	 i:3 	 global-step:1123	 l-p:0.1201118752360344
epoch£º56	 i:4 	 global-step:1124	 l-p:0.12015577405691147
epoch£º56	 i:5 	 global-step:1125	 l-p:0.1167997494339943
epoch£º56	 i:6 	 global-step:1126	 l-p:0.12484046816825867
epoch£º56	 i:7 	 global-step:1127	 l-p:0.1903427392244339
epoch£º56	 i:8 	 global-step:1128	 l-p:0.1301678717136383
epoch£º56	 i:9 	 global-step:1129	 l-p:0.12507952749729156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8905, 3.3148, 3.5072],
        [2.8905, 3.3707, 3.6204],
        [2.8905, 2.8905, 2.8905],
        [2.8905, 2.9125, 2.8972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.1361641138792038 
model_pd.l_d.mean(): -24.55730438232422 
model_pd.lagr.mean(): -24.421140670776367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1569], device='cuda:0')), ('power', tensor([-24.4004], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.1361641138792038
epoch£º57	 i:1 	 global-step:1141	 l-p:0.09792762249708176
epoch£º57	 i:2 	 global-step:1142	 l-p:0.13077682256698608
epoch£º57	 i:3 	 global-step:1143	 l-p:0.1399296671152115
epoch£º57	 i:4 	 global-step:1144	 l-p:0.08071330934762955
epoch£º57	 i:5 	 global-step:1145	 l-p:0.14239290356636047
epoch£º57	 i:6 	 global-step:1146	 l-p:0.14012005925178528
epoch£º57	 i:7 	 global-step:1147	 l-p:0.13274706900119781
epoch£º57	 i:8 	 global-step:1148	 l-p:0.10609076172113419
epoch£º57	 i:9 	 global-step:1149	 l-p:0.14181779325008392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9059, 3.0208, 2.9962],
        [2.9059, 2.9834, 2.9547],
        [2.9059, 3.4509, 3.7694],
        [2.9059, 2.9059, 2.9059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.14222778379917145 
model_pd.l_d.mean(): -24.73885154724121 
model_pd.lagr.mean(): -24.59662437438965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2030], device='cuda:0')), ('power', tensor([-24.5359], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.14222778379917145
epoch£º58	 i:1 	 global-step:1161	 l-p:0.11599794775247574
epoch£º58	 i:2 	 global-step:1162	 l-p:0.129231259226799
epoch£º58	 i:3 	 global-step:1163	 l-p:0.12451959401369095
epoch£º58	 i:4 	 global-step:1164	 l-p:0.13307303190231323
epoch£º58	 i:5 	 global-step:1165	 l-p:0.16786329448223114
epoch£º58	 i:6 	 global-step:1166	 l-p:0.2982388138771057
epoch£º58	 i:7 	 global-step:1167	 l-p:0.11410500854253769
epoch£º58	 i:8 	 global-step:1168	 l-p:0.12024110555648804
epoch£º58	 i:9 	 global-step:1169	 l-p:0.12889325618743896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0687, 3.2140, 3.1924],
        [3.0687, 3.0694, 3.0687],
        [3.0687, 3.0687, 3.0687],
        [3.0687, 3.1342, 3.1043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.11652688682079315 
model_pd.l_d.mean(): -23.869665145874023 
model_pd.lagr.mean(): -23.753137588500977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1951], device='cuda:0')), ('power', tensor([-23.6746], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.11652688682079315
epoch£º59	 i:1 	 global-step:1181	 l-p:0.14244705438613892
epoch£º59	 i:2 	 global-step:1182	 l-p:0.12006552517414093
epoch£º59	 i:3 	 global-step:1183	 l-p:0.1366651952266693
epoch£º59	 i:4 	 global-step:1184	 l-p:0.10999387502670288
epoch£º59	 i:5 	 global-step:1185	 l-p:0.1613338440656662
epoch£º59	 i:6 	 global-step:1186	 l-p:0.11235589534044266
epoch£º59	 i:7 	 global-step:1187	 l-p:0.1288195699453354
epoch£º59	 i:8 	 global-step:1188	 l-p:0.14050541818141937
epoch£º59	 i:9 	 global-step:1189	 l-p:0.11311210691928864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9193, 2.9541, 2.9331],
        [2.9193, 2.9193, 2.9193],
        [2.9193, 2.9198, 2.9193],
        [2.9193, 3.0446, 3.0226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.12392694503068924 
model_pd.l_d.mean(): -23.483911514282227 
model_pd.lagr.mean(): -23.3599853515625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0353], device='cuda:0')), ('power', tensor([-23.4486], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.12392694503068924
epoch£º60	 i:1 	 global-step:1201	 l-p:0.12637601792812347
epoch£º60	 i:2 	 global-step:1202	 l-p:0.1500146985054016
epoch£º60	 i:3 	 global-step:1203	 l-p:0.0783049687743187
epoch£º60	 i:4 	 global-step:1204	 l-p:0.1353919804096222
epoch£º60	 i:5 	 global-step:1205	 l-p:0.09083489328622818
epoch£º60	 i:6 	 global-step:1206	 l-p:0.13037839531898499
epoch£º60	 i:7 	 global-step:1207	 l-p:0.1292605698108673
epoch£º60	 i:8 	 global-step:1208	 l-p:0.11182170361280441
epoch£º60	 i:9 	 global-step:1209	 l-p:0.05981873720884323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9015, 2.9015, 2.9015],
        [2.9015, 3.1516, 3.1948],
        [2.9015, 2.9034, 2.9017],
        [2.9015, 3.1919, 3.2653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.11566255241632462 
model_pd.l_d.mean(): -24.03676414489746 
model_pd.lagr.mean(): -23.92110252380371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1265], device='cuda:0')), ('power', tensor([-23.9103], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.11566255241632462
epoch£º61	 i:1 	 global-step:1221	 l-p:0.12332256138324738
epoch£º61	 i:2 	 global-step:1222	 l-p:0.13035988807678223
epoch£º61	 i:3 	 global-step:1223	 l-p:0.13341931998729706
epoch£º61	 i:4 	 global-step:1224	 l-p:0.13607937097549438
epoch£º61	 i:5 	 global-step:1225	 l-p:0.05135779827833176
epoch£º61	 i:6 	 global-step:1226	 l-p:0.12237724661827087
epoch£º61	 i:7 	 global-step:1227	 l-p:0.08268944919109344
epoch£º61	 i:8 	 global-step:1228	 l-p:0.0749221071600914
epoch£º61	 i:9 	 global-step:1229	 l-p:0.14232690632343292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9704, 2.9804, 2.9723],
        [2.9704, 3.3800, 3.5514],
        [2.9704, 3.0163, 2.9915],
        [2.9704, 3.0489, 3.0196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.13434500992298126 
model_pd.l_d.mean(): -24.361364364624023 
model_pd.lagr.mean(): -24.227020263671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1618], device='cuda:0')), ('power', tensor([-24.1996], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.13434500992298126
epoch£º62	 i:1 	 global-step:1241	 l-p:0.11780821532011032
epoch£º62	 i:2 	 global-step:1242	 l-p:0.1313435286283493
epoch£º62	 i:3 	 global-step:1243	 l-p:0.1365668773651123
epoch£º62	 i:4 	 global-step:1244	 l-p:0.10828626155853271
epoch£º62	 i:5 	 global-step:1245	 l-p:0.12497222423553467
epoch£º62	 i:6 	 global-step:1246	 l-p:0.13064418733119965
epoch£º62	 i:7 	 global-step:1247	 l-p:0.11713625490665436
epoch£º62	 i:8 	 global-step:1248	 l-p:0.1493571698665619
epoch£º62	 i:9 	 global-step:1249	 l-p:0.13020800054073334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0456, 3.0464, 3.0456],
        [3.0456, 3.6742, 4.0680],
        [3.0456, 3.6767, 4.0733],
        [3.0456, 3.0456, 3.0456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.1035153716802597 
model_pd.l_d.mean(): -24.405420303344727 
model_pd.lagr.mean(): -24.301904678344727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2134], device='cuda:0')), ('power', tensor([-24.1920], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.1035153716802597
epoch£º63	 i:1 	 global-step:1261	 l-p:0.12775106728076935
epoch£º63	 i:2 	 global-step:1262	 l-p:0.12643665075302124
epoch£º63	 i:3 	 global-step:1263	 l-p:0.12265276908874512
epoch£º63	 i:4 	 global-step:1264	 l-p:0.18028654158115387
epoch£º63	 i:5 	 global-step:1265	 l-p:0.19840840995311737
epoch£º63	 i:6 	 global-step:1266	 l-p:0.11753390729427338
epoch£º63	 i:7 	 global-step:1267	 l-p:0.14801426231861115
epoch£º63	 i:8 	 global-step:1268	 l-p:0.167050302028656
epoch£º63	 i:9 	 global-step:1269	 l-p:0.13599036633968353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7223, 2.7226, 2.7223],
        [2.7223, 2.7223, 2.7223],
        [2.7223, 2.7286, 2.7233],
        [2.7223, 2.7223, 2.7223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.8209096193313599 
model_pd.l_d.mean(): -24.318326950073242 
model_pd.lagr.mean(): -23.497417449951172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0548], device='cuda:0')), ('power', tensor([-24.2636], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.8209096193313599
epoch£º64	 i:1 	 global-step:1281	 l-p:0.12551438808441162
epoch£º64	 i:2 	 global-step:1282	 l-p:0.09558689594268799
epoch£º64	 i:3 	 global-step:1283	 l-p:0.12257789075374603
epoch£º64	 i:4 	 global-step:1284	 l-p:0.13413667678833008
epoch£º64	 i:5 	 global-step:1285	 l-p:0.11829377710819244
epoch£º64	 i:6 	 global-step:1286	 l-p:0.12536807358264923
epoch£º64	 i:7 	 global-step:1287	 l-p:0.15088357031345367
epoch£º64	 i:8 	 global-step:1288	 l-p:0.12742215394973755
epoch£º64	 i:9 	 global-step:1289	 l-p:0.10401587933301926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8455, 2.8456, 2.8455],
        [2.8455, 2.8455, 2.8455],
        [2.8455, 3.2715, 3.4770],
        [2.8455, 2.8801, 2.8597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.10220573097467422 
model_pd.l_d.mean(): -24.422119140625 
model_pd.lagr.mean(): -24.319913864135742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1154], device='cuda:0')), ('power', tensor([-24.3067], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.10220573097467422
epoch£º65	 i:1 	 global-step:1301	 l-p:0.1382584571838379
epoch£º65	 i:2 	 global-step:1302	 l-p:0.08979944884777069
epoch£º65	 i:3 	 global-step:1303	 l-p:0.14515821635723114
epoch£º65	 i:4 	 global-step:1304	 l-p:0.13287335634231567
epoch£º65	 i:5 	 global-step:1305	 l-p:0.12427212297916412
epoch£º65	 i:6 	 global-step:1306	 l-p:0.15135237574577332
epoch£º65	 i:7 	 global-step:1307	 l-p:0.17420047521591187
epoch£º65	 i:8 	 global-step:1308	 l-p:0.12204434722661972
epoch£º65	 i:9 	 global-step:1309	 l-p:0.10977265983819962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0912, 3.0912, 3.0912],
        [3.0912, 3.5780, 3.8121],
        [3.0912, 3.3321, 3.3575],
        [3.0912, 3.0995, 3.0926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.15134450793266296 
model_pd.l_d.mean(): -24.589914321899414 
model_pd.lagr.mean(): -24.438570022583008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2649], device='cuda:0')), ('power', tensor([-24.3251], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.15134450793266296
epoch£º66	 i:1 	 global-step:1321	 l-p:0.1175978034734726
epoch£º66	 i:2 	 global-step:1322	 l-p:0.1289534717798233
epoch£º66	 i:3 	 global-step:1323	 l-p:0.1176384910941124
epoch£º66	 i:4 	 global-step:1324	 l-p:0.1180526539683342
epoch£º66	 i:5 	 global-step:1325	 l-p:0.09931319206953049
epoch£º66	 i:6 	 global-step:1326	 l-p:0.12106933444738388
epoch£º66	 i:7 	 global-step:1327	 l-p:0.15795612335205078
epoch£º66	 i:8 	 global-step:1328	 l-p:0.10998120903968811
epoch£º66	 i:9 	 global-step:1329	 l-p:0.13763099908828735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9473, 2.9473, 2.9473],
        [2.9473, 2.9473, 2.9473],
        [2.9473, 3.3590, 3.5382],
        [2.9473, 3.2605, 3.3511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.12228350341320038 
model_pd.l_d.mean(): -24.38375473022461 
model_pd.lagr.mean(): -24.261470794677734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1526], device='cuda:0')), ('power', tensor([-24.2312], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.12228350341320038
epoch£º67	 i:1 	 global-step:1341	 l-p:0.13330210745334625
epoch£º67	 i:2 	 global-step:1342	 l-p:0.13690927624702454
epoch£º67	 i:3 	 global-step:1343	 l-p:0.12439824640750885
epoch£º67	 i:4 	 global-step:1344	 l-p:0.1369129717350006
epoch£º67	 i:5 	 global-step:1345	 l-p:0.12473182380199432
epoch£º67	 i:6 	 global-step:1346	 l-p:0.10875090211629868
epoch£º67	 i:7 	 global-step:1347	 l-p:0.1553948074579239
epoch£º67	 i:8 	 global-step:1348	 l-p:0.14418786764144897
epoch£º67	 i:9 	 global-step:1349	 l-p:-0.38793525099754333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7636, 3.2547, 3.5426],
        [2.7636, 3.2804, 3.5970],
        [2.7636, 2.7655, 2.7637],
        [2.7636, 3.1580, 3.3447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.21442393958568573 
model_pd.l_d.mean(): -23.941598892211914 
model_pd.lagr.mean(): -23.727174758911133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0110], device='cuda:0')), ('power', tensor([-23.9306], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.21442393958568573
epoch£º68	 i:1 	 global-step:1361	 l-p:0.07419025897979736
epoch£º68	 i:2 	 global-step:1362	 l-p:0.1372322291135788
epoch£º68	 i:3 	 global-step:1363	 l-p:0.12467139959335327
epoch£º68	 i:4 	 global-step:1364	 l-p:0.10637213289737701
epoch£º68	 i:5 	 global-step:1365	 l-p:0.11625167727470398
epoch£º68	 i:6 	 global-step:1366	 l-p:0.12232226133346558
epoch£º68	 i:7 	 global-step:1367	 l-p:0.12131286412477493
epoch£º68	 i:8 	 global-step:1368	 l-p:0.1301904320716858
epoch£º68	 i:9 	 global-step:1369	 l-p:0.17062467336654663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0529, 3.1121, 3.0841],
        [3.0529, 3.5483, 3.7984],
        [3.0529, 3.1357, 3.1058],
        [3.0529, 3.0529, 3.0529]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.10062292963266373 
model_pd.l_d.mean(): -23.712142944335938 
model_pd.lagr.mean(): -23.611520767211914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1277], device='cuda:0')), ('power', tensor([-23.5845], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.10062292963266373
epoch£º69	 i:1 	 global-step:1381	 l-p:0.1303262710571289
epoch£º69	 i:2 	 global-step:1382	 l-p:0.12275935709476471
epoch£º69	 i:3 	 global-step:1383	 l-p:0.11495286971330643
epoch£º69	 i:4 	 global-step:1384	 l-p:0.13761115074157715
epoch£º69	 i:5 	 global-step:1385	 l-p:0.15050311386585236
epoch£º69	 i:6 	 global-step:1386	 l-p:0.1553122103214264
epoch£º69	 i:7 	 global-step:1387	 l-p:0.11213760077953339
epoch£º69	 i:8 	 global-step:1388	 l-p:0.12287738174200058
epoch£º69	 i:9 	 global-step:1389	 l-p:0.125407874584198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0455, 3.6835, 4.0923],
        [3.0455, 3.0536, 3.0469],
        [3.0455, 3.0455, 3.0455],
        [3.0455, 3.6371, 3.9929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.2160193920135498 
model_pd.l_d.mean(): -23.42571449279785 
model_pd.lagr.mean(): -23.20969581604004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0974], device='cuda:0')), ('power', tensor([-23.3283], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.2160193920135498
epoch£º70	 i:1 	 global-step:1401	 l-p:0.13381491601467133
epoch£º70	 i:2 	 global-step:1402	 l-p:0.1663702130317688
epoch£º70	 i:3 	 global-step:1403	 l-p:0.11314299702644348
epoch£º70	 i:4 	 global-step:1404	 l-p:0.13497190177440643
epoch£º70	 i:5 	 global-step:1405	 l-p:0.11665027588605881
epoch£º70	 i:6 	 global-step:1406	 l-p:0.1303705871105194
epoch£º70	 i:7 	 global-step:1407	 l-p:0.14095447957515717
epoch£º70	 i:8 	 global-step:1408	 l-p:0.12728005647659302
epoch£º70	 i:9 	 global-step:1409	 l-p:0.13503029942512512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6611, 2.7071, 2.6849],
        [2.6611, 2.6615, 2.6611],
        [2.6611, 2.6611, 2.6611],
        [2.6611, 2.8496, 2.8733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.10824310779571533 
model_pd.l_d.mean(): -24.596532821655273 
model_pd.lagr.mean(): -24.48828887939453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0436], device='cuda:0')), ('power', tensor([-24.5529], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.10824310779571533
epoch£º71	 i:1 	 global-step:1421	 l-p:0.11374049633741379
epoch£º71	 i:2 	 global-step:1422	 l-p:0.14871706068515778
epoch£º71	 i:3 	 global-step:1423	 l-p:0.13960275053977966
epoch£º71	 i:4 	 global-step:1424	 l-p:0.12988916039466858
epoch£º71	 i:5 	 global-step:1425	 l-p:0.1113116443157196
epoch£º71	 i:6 	 global-step:1426	 l-p:0.14586932957172394
epoch£º71	 i:7 	 global-step:1427	 l-p:0.1585731953382492
epoch£º71	 i:8 	 global-step:1428	 l-p:0.1271318644285202
epoch£º71	 i:9 	 global-step:1429	 l-p:0.13428445160388947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9176, 2.9176, 2.9176],
        [2.9176, 2.9176, 2.9176],
        [2.9176, 2.9186, 2.9176],
        [2.9176, 2.9176, 2.9176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.029698681086301804 
model_pd.l_d.mean(): -24.050254821777344 
model_pd.lagr.mean(): -24.02055549621582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1502], device='cuda:0')), ('power', tensor([-23.9001], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.029698681086301804
epoch£º72	 i:1 	 global-step:1441	 l-p:0.12385294586420059
epoch£º72	 i:2 	 global-step:1442	 l-p:0.1280389428138733
epoch£º72	 i:3 	 global-step:1443	 l-p:-0.2591506242752075
epoch£º72	 i:4 	 global-step:1444	 l-p:0.1113542690873146
epoch£º72	 i:5 	 global-step:1445	 l-p:0.1232912689447403
epoch£º72	 i:6 	 global-step:1446	 l-p:0.12086354941129684
epoch£º72	 i:7 	 global-step:1447	 l-p:0.12484698742628098
epoch£º72	 i:8 	 global-step:1448	 l-p:0.17300422489643097
epoch£º72	 i:9 	 global-step:1449	 l-p:0.12262050062417984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9777, 2.9777, 2.9777],
        [2.9777, 3.0270, 3.0018],
        [2.9777, 3.3854, 3.5605],
        [2.9777, 3.0506, 3.0223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.12585951387882233 
model_pd.l_d.mean(): -24.10529136657715 
model_pd.lagr.mean(): -23.97943115234375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1868], device='cuda:0')), ('power', tensor([-23.9185], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.12585951387882233
epoch£º73	 i:1 	 global-step:1461	 l-p:0.05425291880965233
epoch£º73	 i:2 	 global-step:1462	 l-p:-0.7566161751747131
epoch£º73	 i:3 	 global-step:1463	 l-p:0.13084356486797333
epoch£º73	 i:4 	 global-step:1464	 l-p:0.12553565204143524
epoch£º73	 i:5 	 global-step:1465	 l-p:0.15453092753887177
epoch£º73	 i:6 	 global-step:1466	 l-p:0.11451154202222824
epoch£º73	 i:7 	 global-step:1467	 l-p:0.11490190029144287
epoch£º73	 i:8 	 global-step:1468	 l-p:0.12844634056091309
epoch£º73	 i:9 	 global-step:1469	 l-p:0.11667930334806442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0249, 3.0296, 3.0255],
        [3.0249, 3.1607, 3.1410],
        [3.0249, 3.3251, 3.4014],
        [3.0249, 3.0997, 3.0708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.2784261405467987 
model_pd.l_d.mean(): -24.267925262451172 
model_pd.lagr.mean(): -23.989500045776367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1694], device='cuda:0')), ('power', tensor([-24.0985], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.2784261405467987
epoch£º74	 i:1 	 global-step:1481	 l-p:0.11979833245277405
epoch£º74	 i:2 	 global-step:1482	 l-p:0.11191215366125107
epoch£º74	 i:3 	 global-step:1483	 l-p:0.12954452633857727
epoch£º74	 i:4 	 global-step:1484	 l-p:0.13386653363704681
epoch£º74	 i:5 	 global-step:1485	 l-p:0.11448276042938232
epoch£º74	 i:6 	 global-step:1486	 l-p:0.13198742270469666
epoch£º74	 i:7 	 global-step:1487	 l-p:0.14187273383140564
epoch£º74	 i:8 	 global-step:1488	 l-p:0.12393533438444138
epoch£º74	 i:9 	 global-step:1489	 l-p:0.11437774449586868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7775, 2.8723, 2.8497],
        [2.7775, 2.8315, 2.8071],
        [2.7775, 2.7775, 2.7775],
        [2.7775, 3.0479, 3.1240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.10888510197401047 
model_pd.l_d.mean(): -24.47740364074707 
model_pd.lagr.mean(): -24.368518829345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0886], device='cuda:0')), ('power', tensor([-24.3888], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.10888510197401047
epoch£º75	 i:1 	 global-step:1501	 l-p:0.14121881127357483
epoch£º75	 i:2 	 global-step:1502	 l-p:0.16485735774040222
epoch£º75	 i:3 	 global-step:1503	 l-p:0.13308687508106232
epoch£º75	 i:4 	 global-step:1504	 l-p:0.13340654969215393
epoch£º75	 i:5 	 global-step:1505	 l-p:0.1253799945116043
epoch£º75	 i:6 	 global-step:1506	 l-p:0.12037280946969986
epoch£º75	 i:7 	 global-step:1507	 l-p:0.10498303174972534
epoch£º75	 i:8 	 global-step:1508	 l-p:2.0972306728363037
epoch£º75	 i:9 	 global-step:1509	 l-p:0.12616980075836182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8470, 2.8470, 2.8470],
        [2.8470, 2.8470, 2.8470],
        [2.8470, 3.1144, 3.1822],
        [2.8470, 2.9114, 2.8853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.18316476047039032 
model_pd.l_d.mean(): -24.12023162841797 
model_pd.lagr.mean(): -23.93706703186035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0759], device='cuda:0')), ('power', tensor([-24.0444], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.18316476047039032
epoch£º76	 i:1 	 global-step:1521	 l-p:0.12003091722726822
epoch£º76	 i:2 	 global-step:1522	 l-p:0.16856074333190918
epoch£º76	 i:3 	 global-step:1523	 l-p:0.12506718933582306
epoch£º76	 i:4 	 global-step:1524	 l-p:0.09739771485328674
epoch£º76	 i:5 	 global-step:1525	 l-p:0.14572568237781525
epoch£º76	 i:6 	 global-step:1526	 l-p:0.10763168334960938
epoch£º76	 i:7 	 global-step:1527	 l-p:0.12361446768045425
epoch£º76	 i:8 	 global-step:1528	 l-p:0.10590800642967224
epoch£º76	 i:9 	 global-step:1529	 l-p:0.10377082228660583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1624, 3.1672, 3.1630],
        [3.1624, 3.1625, 3.1624],
        [3.1624, 3.1751, 3.1651],
        [3.1624, 3.1958, 3.1748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.13442544639110565 
model_pd.l_d.mean(): -24.59550666809082 
model_pd.lagr.mean(): -24.46108055114746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2929], device='cuda:0')), ('power', tensor([-24.3026], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.13442544639110565
epoch£º77	 i:1 	 global-step:1541	 l-p:0.10548298805952072
epoch£º77	 i:2 	 global-step:1542	 l-p:0.13317367434501648
epoch£º77	 i:3 	 global-step:1543	 l-p:0.12777814269065857
epoch£º77	 i:4 	 global-step:1544	 l-p:0.1284528225660324
epoch£º77	 i:5 	 global-step:1545	 l-p:0.15326008200645447
epoch£º77	 i:6 	 global-step:1546	 l-p:0.14467783272266388
epoch£º77	 i:7 	 global-step:1547	 l-p:0.1261034607887268
epoch£º77	 i:8 	 global-step:1548	 l-p:0.11657575517892838
epoch£º77	 i:9 	 global-step:1549	 l-p:0.1323409378528595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6943, 2.8727, 2.8902],
        [2.6943, 2.9836, 3.0852],
        [2.6943, 2.7130, 2.6996],
        [2.6943, 2.7024, 2.6957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.10606180876493454 
model_pd.l_d.mean(): -24.379934310913086 
model_pd.lagr.mean(): -24.27387237548828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0230], device='cuda:0')), ('power', tensor([-24.3570], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.10606180876493454
epoch£º78	 i:1 	 global-step:1561	 l-p:0.15114188194274902
epoch£º78	 i:2 	 global-step:1562	 l-p:0.13461653888225555
epoch£º78	 i:3 	 global-step:1563	 l-p:0.14280587434768677
epoch£º78	 i:4 	 global-step:1564	 l-p:0.12598241865634918
epoch£º78	 i:5 	 global-step:1565	 l-p:0.15153397619724274
epoch£º78	 i:6 	 global-step:1566	 l-p:0.121805839240551
epoch£º78	 i:7 	 global-step:1567	 l-p:0.15775257349014282
epoch£º78	 i:8 	 global-step:1568	 l-p:0.1271027773618698
epoch£º78	 i:9 	 global-step:1569	 l-p:0.1259898543357849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0437, 3.3052, 3.3534],
        [3.0437, 3.0437, 3.0437],
        [3.0437, 3.0440, 3.0437],
        [3.0437, 3.0446, 3.0437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.13145367801189423 
model_pd.l_d.mean(): -24.610074996948242 
model_pd.lagr.mean(): -24.478620529174805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2338], device='cuda:0')), ('power', tensor([-24.3763], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.13145367801189423
epoch£º79	 i:1 	 global-step:1581	 l-p:0.13118450343608856
epoch£º79	 i:2 	 global-step:1582	 l-p:0.108906589448452
epoch£º79	 i:3 	 global-step:1583	 l-p:0.11406220495700836
epoch£º79	 i:4 	 global-step:1584	 l-p:0.13893990218639374
epoch£º79	 i:5 	 global-step:1585	 l-p:-0.19870957732200623
epoch£º79	 i:6 	 global-step:1586	 l-p:0.10611497610807419
epoch£º79	 i:7 	 global-step:1587	 l-p:0.05076881870627403
epoch£º79	 i:8 	 global-step:1588	 l-p:0.14572620391845703
epoch£º79	 i:9 	 global-step:1589	 l-p:0.07882063090801239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9891, 3.5849, 3.9597],
        [2.9891, 2.9926, 2.9895],
        [2.9891, 2.9891, 2.9891],
        [2.9891, 3.3808, 3.5432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.1252242475748062 
model_pd.l_d.mean(): -23.97532844543457 
model_pd.lagr.mean(): -23.8501033782959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1565], device='cuda:0')), ('power', tensor([-23.8189], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.1252242475748062
epoch£º80	 i:1 	 global-step:1601	 l-p:0.6505107879638672
epoch£º80	 i:2 	 global-step:1602	 l-p:0.1510309875011444
epoch£º80	 i:3 	 global-step:1603	 l-p:0.13098092377185822
epoch£º80	 i:4 	 global-step:1604	 l-p:0.11108259111642838
epoch£º80	 i:5 	 global-step:1605	 l-p:0.11595555394887924
epoch£º80	 i:6 	 global-step:1606	 l-p:0.10099973529577255
epoch£º80	 i:7 	 global-step:1607	 l-p:0.11373548209667206
epoch£º80	 i:8 	 global-step:1608	 l-p:0.10976604372262955
epoch£º80	 i:9 	 global-step:1609	 l-p:0.12481604516506195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0323, 3.0335, 3.0324],
        [3.0323, 3.0323, 3.0323],
        [3.0323, 3.4350, 3.6032],
        [3.0323, 3.0504, 3.0372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.32919439673423767 
model_pd.l_d.mean(): -24.50673484802246 
model_pd.lagr.mean(): -24.177539825439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2106], device='cuda:0')), ('power', tensor([-24.2961], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.32919439673423767
epoch£º81	 i:1 	 global-step:1621	 l-p:0.12565255165100098
epoch£º81	 i:2 	 global-step:1622	 l-p:0.10986263304948807
epoch£º81	 i:3 	 global-step:1623	 l-p:0.08823160082101822
epoch£º81	 i:4 	 global-step:1624	 l-p:0.1259230524301529
epoch£º81	 i:5 	 global-step:1625	 l-p:0.13769952952861786
epoch£º81	 i:6 	 global-step:1626	 l-p:0.14107875525951385
epoch£º81	 i:7 	 global-step:1627	 l-p:0.5565584897994995
epoch£º81	 i:8 	 global-step:1628	 l-p:0.14247137308120728
epoch£º81	 i:9 	 global-step:1629	 l-p:0.13723790645599365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8504, 2.8639, 2.8535],
        [2.8504, 3.0778, 3.1178],
        [2.8504, 2.9032, 2.8782],
        [2.8504, 3.2553, 3.4483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.12431540340185165 
model_pd.l_d.mean(): -24.557010650634766 
model_pd.lagr.mean(): -24.432695388793945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1278], device='cuda:0')), ('power', tensor([-24.4292], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.12431540340185165
epoch£º82	 i:1 	 global-step:1641	 l-p:0.17352069914340973
epoch£º82	 i:2 	 global-step:1642	 l-p:0.12707188725471497
epoch£º82	 i:3 	 global-step:1643	 l-p:0.14125394821166992
epoch£º82	 i:4 	 global-step:1644	 l-p:0.09599382430315018
epoch£º82	 i:5 	 global-step:1645	 l-p:0.14684560894966125
epoch£º82	 i:6 	 global-step:1646	 l-p:0.10313358157873154
epoch£º82	 i:7 	 global-step:1647	 l-p:0.12819664180278778
epoch£º82	 i:8 	 global-step:1648	 l-p:0.11693321913480759
epoch£º82	 i:9 	 global-step:1649	 l-p:1.6211045980453491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9991, 3.3626, 3.4993],
        [2.9991, 3.0682, 3.0401],
        [2.9991, 2.9992, 2.9991],
        [2.9991, 3.1749, 3.1747]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.12201375514268875 
model_pd.l_d.mean(): -24.052127838134766 
model_pd.lagr.mean(): -23.93011474609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1481], device='cuda:0')), ('power', tensor([-23.9041], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.12201375514268875
epoch£º83	 i:1 	 global-step:1661	 l-p:0.1333654522895813
epoch£º83	 i:2 	 global-step:1662	 l-p:0.1211455687880516
epoch£º83	 i:3 	 global-step:1663	 l-p:0.19257083535194397
epoch£º83	 i:4 	 global-step:1664	 l-p:0.12419246137142181
epoch£º83	 i:5 	 global-step:1665	 l-p:0.12725798785686493
epoch£º83	 i:6 	 global-step:1666	 l-p:0.11747100204229355
epoch£º83	 i:7 	 global-step:1667	 l-p:0.12243017554283142
epoch£º83	 i:8 	 global-step:1668	 l-p:0.07773316651582718
epoch£º83	 i:9 	 global-step:1669	 l-p:0.12336572259664536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8916, 2.8916, 2.8916],
        [2.8916, 2.8928, 2.8917],
        [2.8916, 2.9062, 2.8950],
        [2.8916, 2.9516, 2.9252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.1435999870300293 
model_pd.l_d.mean(): -24.560340881347656 
model_pd.lagr.mean(): -24.41674041748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1427], device='cuda:0')), ('power', tensor([-24.4176], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.1435999870300293
epoch£º84	 i:1 	 global-step:1681	 l-p:0.13152363896369934
epoch£º84	 i:2 	 global-step:1682	 l-p:0.1189735159277916
epoch£º84	 i:3 	 global-step:1683	 l-p:0.12169801443815231
epoch£º84	 i:4 	 global-step:1684	 l-p:0.11786560714244843
epoch£º84	 i:5 	 global-step:1685	 l-p:0.13718317449092865
epoch£º84	 i:6 	 global-step:1686	 l-p:0.12951971590518951
epoch£º84	 i:7 	 global-step:1687	 l-p:0.1344415843486786
epoch£º84	 i:8 	 global-step:1688	 l-p:0.10893070697784424
epoch£º84	 i:9 	 global-step:1689	 l-p:0.6045556664466858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[2.8173, 3.0601, 3.1154],
        [2.8173, 2.9531, 2.9432],
        [2.8173, 3.1052, 3.1963],
        [2.8173, 2.9027, 2.8780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.14026518166065216 
model_pd.l_d.mean(): -24.42180633544922 
model_pd.lagr.mean(): -24.28154182434082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0820], device='cuda:0')), ('power', tensor([-24.3398], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.14026518166065216
epoch£º85	 i:1 	 global-step:1701	 l-p:0.1683054268360138
epoch£º85	 i:2 	 global-step:1702	 l-p:0.13246452808380127
epoch£º85	 i:3 	 global-step:1703	 l-p:0.09770847111940384
epoch£º85	 i:4 	 global-step:1704	 l-p:0.1253223717212677
epoch£º85	 i:5 	 global-step:1705	 l-p:0.1328192949295044
epoch£º85	 i:6 	 global-step:1706	 l-p:0.14860333502292633
epoch£º85	 i:7 	 global-step:1707	 l-p:-0.6309241056442261
epoch£º85	 i:8 	 global-step:1708	 l-p:0.1267315000295639
epoch£º85	 i:9 	 global-step:1709	 l-p:-0.02301020547747612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0273, 3.3663, 3.4801],
        [3.0273, 3.0290, 3.0275],
        [3.0273, 3.0273, 3.0273],
        [3.0273, 3.1253, 3.0982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.13620559871196747 
model_pd.l_d.mean(): -23.09429359436035 
model_pd.lagr.mean(): -22.958087921142578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0004], device='cuda:0')), ('power', tensor([-23.0939], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.13620559871196747
epoch£º86	 i:1 	 global-step:1721	 l-p:0.11045455932617188
epoch£º86	 i:2 	 global-step:1722	 l-p:0.10901486873626709
epoch£º86	 i:3 	 global-step:1723	 l-p:0.12953943014144897
epoch£º86	 i:4 	 global-step:1724	 l-p:0.14645656943321228
epoch£º86	 i:5 	 global-step:1725	 l-p:0.1918964385986328
epoch£º86	 i:6 	 global-step:1726	 l-p:0.11333808302879333
epoch£º86	 i:7 	 global-step:1727	 l-p:0.12698888778686523
epoch£º86	 i:8 	 global-step:1728	 l-p:0.11994773894548416
epoch£º86	 i:9 	 global-step:1729	 l-p:0.1278926283121109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9705, 2.9911, 2.9762],
        [2.9705, 2.9711, 2.9705],
        [2.9705, 2.9705, 2.9705],
        [2.9705, 3.1432, 3.1437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): -0.23208005726337433 
model_pd.l_d.mean(): -24.21833038330078 
model_pd.lagr.mean(): -24.450410842895508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1184], device='cuda:0')), ('power', tensor([-24.0999], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:-0.23208005726337433
epoch£º87	 i:1 	 global-step:1741	 l-p:0.12203118950128555
epoch£º87	 i:2 	 global-step:1742	 l-p:0.1299775093793869
epoch£º87	 i:3 	 global-step:1743	 l-p:0.1275455504655838
epoch£º87	 i:4 	 global-step:1744	 l-p:0.13069656491279602
epoch£º87	 i:5 	 global-step:1745	 l-p:0.14661650359630585
epoch£º87	 i:6 	 global-step:1746	 l-p:0.1258104294538498
epoch£º87	 i:7 	 global-step:1747	 l-p:0.10784183442592621
epoch£º87	 i:8 	 global-step:1748	 l-p:-0.11338550597429276
epoch£º87	 i:9 	 global-step:1749	 l-p:0.13259296119213104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7928, 2.7928, 2.7928],
        [2.7928, 2.8604, 2.8343],
        [2.7928, 2.8087, 2.7961],
        [2.7928, 2.7931, 2.7928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.14525195956230164 
model_pd.l_d.mean(): -24.7810115814209 
model_pd.lagr.mean(): -24.635759353637695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1407], device='cuda:0')), ('power', tensor([-24.6404], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.14525195956230164
epoch£º88	 i:1 	 global-step:1761	 l-p:0.22579026222229004
epoch£º88	 i:2 	 global-step:1762	 l-p:0.11039768159389496
epoch£º88	 i:3 	 global-step:1763	 l-p:0.13409316539764404
epoch£º88	 i:4 	 global-step:1764	 l-p:0.144917830824852
epoch£º88	 i:5 	 global-step:1765	 l-p:0.10917186737060547
epoch£º88	 i:6 	 global-step:1766	 l-p:0.12608733773231506
epoch£º88	 i:7 	 global-step:1767	 l-p:0.12424920499324799
epoch£º88	 i:8 	 global-step:1768	 l-p:0.117535300552845
epoch£º88	 i:9 	 global-step:1769	 l-p:0.12476901710033417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9460, 3.3288, 3.4919],
        [2.9460, 2.9497, 2.9463],
        [2.9460, 2.9513, 2.9466],
        [2.9460, 2.9816, 2.9598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.048165950924158096 
model_pd.l_d.mean(): -24.07863426208496 
model_pd.lagr.mean(): -24.030467987060547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0823], device='cuda:0')), ('power', tensor([-23.9963], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.048165950924158096
epoch£º89	 i:1 	 global-step:1781	 l-p:0.14060752093791962
epoch£º89	 i:2 	 global-step:1782	 l-p:0.12656204402446747
epoch£º89	 i:3 	 global-step:1783	 l-p:0.12014884501695633
epoch£º89	 i:4 	 global-step:1784	 l-p:0.03345455229282379
epoch£º89	 i:5 	 global-step:1785	 l-p:0.12703336775302887
epoch£º89	 i:6 	 global-step:1786	 l-p:0.1115855798125267
epoch£º89	 i:7 	 global-step:1787	 l-p:0.12862497568130493
epoch£º89	 i:8 	 global-step:1788	 l-p:0.1714276224374771
epoch£º89	 i:9 	 global-step:1789	 l-p:0.11885793507099152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0460, 3.5001, 3.7204],
        [3.0460, 3.0460, 3.0460],
        [3.0460, 3.0460, 3.0460],
        [3.0460, 3.0529, 3.0470]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.12576013803482056 
model_pd.l_d.mean(): -24.130338668823242 
model_pd.lagr.mean(): -24.00457763671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1930], device='cuda:0')), ('power', tensor([-23.9373], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.12576013803482056
epoch£º90	 i:1 	 global-step:1801	 l-p:-0.5726122260093689
epoch£º90	 i:2 	 global-step:1802	 l-p:0.12422327697277069
epoch£º90	 i:3 	 global-step:1803	 l-p:0.19036918878555298
epoch£º90	 i:4 	 global-step:1804	 l-p:0.12785997986793518
epoch£º90	 i:5 	 global-step:1805	 l-p:0.11263057589530945
epoch£º90	 i:6 	 global-step:1806	 l-p:0.1301252543926239
epoch£º90	 i:7 	 global-step:1807	 l-p:0.13720616698265076
epoch£º90	 i:8 	 global-step:1808	 l-p:0.16301442682743073
epoch£º90	 i:9 	 global-step:1809	 l-p:0.13342635333538055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8422, 2.8422, 2.8422],
        [2.8422, 2.8422, 2.8422],
        [2.8422, 2.8426, 2.8422],
        [2.8422, 3.0964, 3.1603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.12078002840280533 
model_pd.l_d.mean(): -24.137584686279297 
model_pd.lagr.mean(): -24.016803741455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1249], device='cuda:0')), ('power', tensor([-24.0127], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.12078002840280533
epoch£º91	 i:1 	 global-step:1821	 l-p:0.13878028094768524
epoch£º91	 i:2 	 global-step:1822	 l-p:0.0921778455376625
epoch£º91	 i:3 	 global-step:1823	 l-p:0.1289866417646408
epoch£º91	 i:4 	 global-step:1824	 l-p:0.14114420115947723
epoch£º91	 i:5 	 global-step:1825	 l-p:0.11639320105314255
epoch£º91	 i:6 	 global-step:1826	 l-p:0.1370978206396103
epoch£º91	 i:7 	 global-step:1827	 l-p:0.15588995814323425
epoch£º91	 i:8 	 global-step:1828	 l-p:0.16738803684711456
epoch£º91	 i:9 	 global-step:1829	 l-p:0.17810603976249695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9189, 2.9189, 2.9189],
        [2.9189, 3.3065, 3.4788],
        [2.9189, 2.9479, 2.9282],
        [2.9189, 2.9276, 2.9200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.15111574530601501 
model_pd.l_d.mean(): -24.208189010620117 
model_pd.lagr.mean(): -24.05707359313965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0823], device='cuda:0')), ('power', tensor([-24.1259], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.15111574530601501
epoch£º92	 i:1 	 global-step:1841	 l-p:0.12263248860836029
epoch£º92	 i:2 	 global-step:1842	 l-p:0.12498646229505539
epoch£º92	 i:3 	 global-step:1843	 l-p:0.11391429603099823
epoch£º92	 i:4 	 global-step:1844	 l-p:0.14776712656021118
epoch£º92	 i:5 	 global-step:1845	 l-p:0.12253870069980621
epoch£º92	 i:6 	 global-step:1846	 l-p:0.12502209842205048
epoch£º92	 i:7 	 global-step:1847	 l-p:0.08915846794843674
epoch£º92	 i:8 	 global-step:1848	 l-p:0.06244318187236786
epoch£º92	 i:9 	 global-step:1849	 l-p:0.12514151632785797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9083, 2.9083, 2.9083],
        [2.9083, 3.0041, 2.9790],
        [2.9083, 3.0163, 2.9941],
        [2.9083, 2.9730, 2.9452]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.13759703934192657 
model_pd.l_d.mean(): -24.275304794311523 
model_pd.lagr.mean(): -24.13770866394043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1280], device='cuda:0')), ('power', tensor([-24.1473], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.13759703934192657
epoch£º93	 i:1 	 global-step:1861	 l-p:0.1407002955675125
epoch£º93	 i:2 	 global-step:1862	 l-p:0.12613588571548462
epoch£º93	 i:3 	 global-step:1863	 l-p:0.12012334913015366
epoch£º93	 i:4 	 global-step:1864	 l-p:0.3420940637588501
epoch£º93	 i:5 	 global-step:1865	 l-p:0.13097167015075684
epoch£º93	 i:6 	 global-step:1866	 l-p:0.1373828649520874
epoch£º93	 i:7 	 global-step:1867	 l-p:0.12844569981098175
epoch£º93	 i:8 	 global-step:1868	 l-p:0.11359812319278717
epoch£º93	 i:9 	 global-step:1869	 l-p:0.09428637474775314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8675, 3.0179, 3.0135],
        [2.8675, 2.8675, 2.8675],
        [2.8675, 2.8716, 2.8674],
        [2.8675, 2.8682, 2.8674]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.12972398102283478 
model_pd.l_d.mean(): -24.809707641601562 
model_pd.lagr.mean(): -24.679983139038086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1800], device='cuda:0')), ('power', tensor([-24.6297], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.12972398102283478
epoch£º94	 i:1 	 global-step:1881	 l-p:0.15099819004535675
epoch£º94	 i:2 	 global-step:1882	 l-p:0.15866422653198242
epoch£º94	 i:3 	 global-step:1883	 l-p:0.12988588213920593
epoch£º94	 i:4 	 global-step:1884	 l-p:0.10670605301856995
epoch£º94	 i:5 	 global-step:1885	 l-p:0.0535857193171978
epoch£º94	 i:6 	 global-step:1886	 l-p:0.12788380682468414
epoch£º94	 i:7 	 global-step:1887	 l-p:0.4385342001914978
epoch£º94	 i:8 	 global-step:1888	 l-p:0.11483994871377945
epoch£º94	 i:9 	 global-step:1889	 l-p:0.2103109061717987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1285, 3.1285, 3.1285],
        [3.1285, 3.1285, 3.1285],
        [3.1285, 3.1286, 3.1285],
        [3.1285, 3.1307, 3.1286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.14488893747329712 
model_pd.l_d.mean(): -24.043872833251953 
model_pd.lagr.mean(): -23.898983001708984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1911], device='cuda:0')), ('power', tensor([-23.8527], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.14488893747329712
epoch£º95	 i:1 	 global-step:1901	 l-p:0.10944114625453949
epoch£º95	 i:2 	 global-step:1902	 l-p:0.10990773886442184
epoch£º95	 i:3 	 global-step:1903	 l-p:0.09978587180376053
epoch£º95	 i:4 	 global-step:1904	 l-p:0.11711393296718597
epoch£º95	 i:5 	 global-step:1905	 l-p:0.11855136603116989
epoch£º95	 i:6 	 global-step:1906	 l-p:0.14104968309402466
epoch£º95	 i:7 	 global-step:1907	 l-p:0.14939148724079132
epoch£º95	 i:8 	 global-step:1908	 l-p:0.12831436097621918
epoch£º95	 i:9 	 global-step:1909	 l-p:0.12329883128404617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0090, 3.4759, 3.7161],
        [3.0090, 3.0090, 3.0090],
        [3.0090, 3.2843, 3.3521],
        [3.0090, 3.0090, 3.0090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.23356521129608154 
model_pd.l_d.mean(): -23.598560333251953 
model_pd.lagr.mean(): -23.3649959564209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0387], device='cuda:0')), ('power', tensor([-23.5598], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.23356521129608154
epoch£º96	 i:1 	 global-step:1921	 l-p:0.1129501685500145
epoch£º96	 i:2 	 global-step:1922	 l-p:0.04368457570672035
epoch£º96	 i:3 	 global-step:1923	 l-p:0.13503804802894592
epoch£º96	 i:4 	 global-step:1924	 l-p:0.13292522728443146
epoch£º96	 i:5 	 global-step:1925	 l-p:0.13269160687923431
epoch£º96	 i:6 	 global-step:1926	 l-p:0.1399257630109787
epoch£º96	 i:7 	 global-step:1927	 l-p:0.12959611415863037
epoch£º96	 i:8 	 global-step:1928	 l-p:0.12466148287057877
epoch£º96	 i:9 	 global-step:1929	 l-p:0.12712131440639496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7654, 2.8693, 2.8493],
        [2.7654, 3.0065, 3.0692],
        [2.7654, 2.7654, 2.7654],
        [2.7654, 3.1943, 3.4282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.13275933265686035 
model_pd.l_d.mean(): -24.6270809173584 
model_pd.lagr.mean(): -24.494321823120117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0960], device='cuda:0')), ('power', tensor([-24.5311], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.13275933265686035
epoch£º97	 i:1 	 global-step:1941	 l-p:0.13949282467365265
epoch£º97	 i:2 	 global-step:1942	 l-p:-0.21328355371952057
epoch£º97	 i:3 	 global-step:1943	 l-p:0.15616486966609955
epoch£º97	 i:4 	 global-step:1944	 l-p:0.1293240785598755
epoch£º97	 i:5 	 global-step:1945	 l-p:0.201007679104805
epoch£º97	 i:6 	 global-step:1946	 l-p:0.09690914303064346
epoch£º97	 i:7 	 global-step:1947	 l-p:0.11834075301885605
epoch£º97	 i:8 	 global-step:1948	 l-p:0.125865176320076
epoch£º97	 i:9 	 global-step:1949	 l-p:0.13786721229553223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9551, 3.0549, 3.0292],
        [2.9551, 2.9551, 2.9551],
        [2.9551, 2.9589, 2.9549],
        [2.9551, 3.1513, 3.1683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.1402026265859604 
model_pd.l_d.mean(): -24.560239791870117 
model_pd.lagr.mean(): -24.42003631591797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1680], device='cuda:0')), ('power', tensor([-24.3923], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.1402026265859604
epoch£º98	 i:1 	 global-step:1961	 l-p:-0.19097648561000824
epoch£º98	 i:2 	 global-step:1962	 l-p:0.11745034903287888
epoch£º98	 i:3 	 global-step:1963	 l-p:0.15573590993881226
epoch£º98	 i:4 	 global-step:1964	 l-p:0.11782373487949371
epoch£º98	 i:5 	 global-step:1965	 l-p:0.12045463174581528
epoch£º98	 i:6 	 global-step:1966	 l-p:0.10147549211978912
epoch£º98	 i:7 	 global-step:1967	 l-p:0.10243994742631912
epoch£º98	 i:8 	 global-step:1968	 l-p:0.14244195818901062
epoch£º98	 i:9 	 global-step:1969	 l-p:0.12452132254838943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1213, 3.1213, 3.1213],
        [3.1213, 3.1213, 3.1213],
        [3.1213, 3.2718, 3.2574],
        [3.1213, 3.1213, 3.1213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.11655909568071365 
model_pd.l_d.mean(): -24.154645919799805 
model_pd.lagr.mean(): -24.0380859375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2483], device='cuda:0')), ('power', tensor([-23.9064], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.11655909568071365
epoch£º99	 i:1 	 global-step:1981	 l-p:0.13249382376670837
epoch£º99	 i:2 	 global-step:1982	 l-p:0.1310890018939972
epoch£º99	 i:3 	 global-step:1983	 l-p:0.09976339340209961
epoch£º99	 i:4 	 global-step:1984	 l-p:0.17619983851909637
epoch£º99	 i:5 	 global-step:1985	 l-p:0.12253198027610779
epoch£º99	 i:6 	 global-step:1986	 l-p:0.12428400665521622
epoch£º99	 i:7 	 global-step:1987	 l-p:0.1505647599697113
epoch£º99	 i:8 	 global-step:1988	 l-p:1.1628954410552979
epoch£º99	 i:9 	 global-step:1989	 l-p:0.12634943425655365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8292, 2.8291, 2.8292],
        [2.8292, 2.9507, 2.9350],
        [2.8292, 2.8358, 2.8274],
        [2.8292, 2.8292, 2.8292]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.13131356239318848 
model_pd.l_d.mean(): -24.82989501953125 
model_pd.lagr.mean(): -24.69858169555664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1613], device='cuda:0')), ('power', tensor([-24.6686], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.13131356239318848
epoch£º100	 i:1 	 global-step:2001	 l-p:0.1202576532959938
epoch£º100	 i:2 	 global-step:2002	 l-p:0.11586234718561172
epoch£º100	 i:3 	 global-step:2003	 l-p:0.5767890214920044
epoch£º100	 i:4 	 global-step:2004	 l-p:0.15169090032577515
epoch£º100	 i:5 	 global-step:2005	 l-p:0.13580234348773956
epoch£º100	 i:6 	 global-step:2006	 l-p:0.12912411987781525
epoch£º100	 i:7 	 global-step:2007	 l-p:0.10376710444688797
epoch£º100	 i:8 	 global-step:2008	 l-p:0.15132054686546326
epoch£º100	 i:9 	 global-step:2009	 l-p:0.12237744778394699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0211, 3.0277, 3.0209],
        [3.0211, 3.0211, 3.0211],
        [3.0211, 3.0211, 3.0211],
        [3.0211, 3.0211, 3.0211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.572529137134552 
model_pd.l_d.mean(): -24.472471237182617 
model_pd.lagr.mean(): -23.89994239807129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2046], device='cuda:0')), ('power', tensor([-24.2679], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.572529137134552
epoch£º101	 i:1 	 global-step:2021	 l-p:0.12453410774469376
epoch£º101	 i:2 	 global-step:2022	 l-p:0.14072869718074799
epoch£º101	 i:3 	 global-step:2023	 l-p:0.12189284712076187
epoch£º101	 i:4 	 global-step:2024	 l-p:0.1223454624414444
epoch£º101	 i:5 	 global-step:2025	 l-p:0.11591697484254837
epoch£º101	 i:6 	 global-step:2026	 l-p:0.10503879189491272
epoch£º101	 i:7 	 global-step:2027	 l-p:-0.0709594339132309
epoch£º101	 i:8 	 global-step:2028	 l-p:0.11428843438625336
epoch£º101	 i:9 	 global-step:2029	 l-p:0.12157237529754639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[2.9343, 3.1484, 3.1791],
        [2.9343, 3.1505, 3.1825],
        [2.9343, 3.3788, 3.6079],
        [2.9343, 2.9587, 2.9379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.1255701184272766 
model_pd.l_d.mean(): -24.47217559814453 
model_pd.lagr.mean(): -24.34660530090332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1405], device='cuda:0')), ('power', tensor([-24.3317], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.1255701184272766
epoch£º102	 i:1 	 global-step:2041	 l-p:0.10189451277256012
epoch£º102	 i:2 	 global-step:2042	 l-p:0.14429442584514618
epoch£º102	 i:3 	 global-step:2043	 l-p:0.12063125520944595
epoch£º102	 i:4 	 global-step:2044	 l-p:0.131593719124794
epoch£º102	 i:5 	 global-step:2045	 l-p:0.14398743212223053
epoch£º102	 i:6 	 global-step:2046	 l-p:0.12764841318130493
epoch£º102	 i:7 	 global-step:2047	 l-p:0.11898167431354523
epoch£º102	 i:8 	 global-step:2048	 l-p:0.12601710855960846
epoch£º102	 i:9 	 global-step:2049	 l-p:0.09663675725460052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7490, 2.7515, 2.7440],
        [2.7490, 2.8042, 2.7734],
        [2.7490, 2.7508, 2.7442],
        [2.7490, 2.7479, 2.7487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.24221205711364746 
model_pd.l_d.mean(): -24.384445190429688 
model_pd.lagr.mean(): -24.14223289489746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0315], device='cuda:0')), ('power', tensor([-24.3530], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.24221205711364746
epoch£º103	 i:1 	 global-step:2061	 l-p:0.11941508948802948
epoch£º103	 i:2 	 global-step:2062	 l-p:0.12980607151985168
epoch£º103	 i:3 	 global-step:2063	 l-p:0.131573885679245
epoch£º103	 i:4 	 global-step:2064	 l-p:0.12646594643592834
epoch£º103	 i:5 	 global-step:2065	 l-p:0.1421627551317215
epoch£º103	 i:6 	 global-step:2066	 l-p:0.13333472609519958
epoch£º103	 i:7 	 global-step:2067	 l-p:0.17826537787914276
epoch£º103	 i:8 	 global-step:2068	 l-p:0.11830224096775055
epoch£º103	 i:9 	 global-step:2069	 l-p:0.03686003014445305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0513, 3.0513, 3.0513],
        [3.0513, 3.0739, 3.0544],
        [3.0513, 3.0562, 3.0505],
        [3.0513, 3.3854, 3.4998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.13336887955665588 
model_pd.l_d.mean(): -24.78611183166504 
model_pd.lagr.mean(): -24.652742385864258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2682], device='cuda:0')), ('power', tensor([-24.5180], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.13336887955665588
epoch£º104	 i:1 	 global-step:2081	 l-p:0.17522042989730835
epoch£º104	 i:2 	 global-step:2082	 l-p:0.11880740523338318
epoch£º104	 i:3 	 global-step:2083	 l-p:0.11715693026781082
epoch£º104	 i:4 	 global-step:2084	 l-p:0.12476309388875961
epoch£º104	 i:5 	 global-step:2085	 l-p:0.11399812251329422
epoch£º104	 i:6 	 global-step:2086	 l-p:0.12074805796146393
epoch£º104	 i:7 	 global-step:2087	 l-p:0.09904763847589493
epoch£º104	 i:8 	 global-step:2088	 l-p:0.12270084023475647
epoch£º104	 i:9 	 global-step:2089	 l-p:-0.11545886844396591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0210, 3.0210, 3.0210],
        [3.0210, 3.0210, 3.0210],
        [3.0210, 3.0575, 3.0308],
        [3.0210, 3.1731, 3.1635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.12071389704942703 
model_pd.l_d.mean(): -24.58979606628418 
model_pd.lagr.mean(): -24.46908187866211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2126], device='cuda:0')), ('power', tensor([-24.3772], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.12071389704942703
epoch£º105	 i:1 	 global-step:2101	 l-p:0.12819162011146545
epoch£º105	 i:2 	 global-step:2102	 l-p:0.11494888365268707
epoch£º105	 i:3 	 global-step:2103	 l-p:0.12768834829330444
epoch£º105	 i:4 	 global-step:2104	 l-p:0.01065373420715332
epoch£º105	 i:5 	 global-step:2105	 l-p:0.19169895350933075
epoch£º105	 i:6 	 global-step:2106	 l-p:0.13211455941200256
epoch£º105	 i:7 	 global-step:2107	 l-p:0.13038723170757294
epoch£º105	 i:8 	 global-step:2108	 l-p:0.12467408925294876
epoch£º105	 i:9 	 global-step:2109	 l-p:0.10657823085784912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8375, 2.8371, 2.8375],
        [2.8375, 3.0228, 3.0414],
        [2.8375, 2.8361, 2.8369],
        [2.8375, 3.1088, 3.1913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.15079352259635925 
model_pd.l_d.mean(): -23.710304260253906 
model_pd.lagr.mean(): -23.559511184692383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0072], device='cuda:0')), ('power', tensor([-23.7031], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.15079352259635925
epoch£º106	 i:1 	 global-step:2121	 l-p:0.15532493591308594
epoch£º106	 i:2 	 global-step:2122	 l-p:0.1275123804807663
epoch£º106	 i:3 	 global-step:2123	 l-p:0.12814918160438538
epoch£º106	 i:4 	 global-step:2124	 l-p:0.11779329925775528
epoch£º106	 i:5 	 global-step:2125	 l-p:0.11994241178035736
epoch£º106	 i:6 	 global-step:2126	 l-p:0.6412326097488403
epoch£º106	 i:7 	 global-step:2127	 l-p:0.15847685933113098
epoch£º106	 i:8 	 global-step:2128	 l-p:0.1038285419344902
epoch£º106	 i:9 	 global-step:2129	 l-p:0.12623678147792816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2039, 3.2039, 3.2039],
        [3.2039, 3.2040, 3.2039],
        [3.2039, 3.6346, 3.8206],
        [3.2039, 3.5369, 3.6375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.09142675995826721 
model_pd.l_d.mean(): -24.10835838317871 
model_pd.lagr.mean(): -24.016931533813477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2385], device='cuda:0')), ('power', tensor([-23.8698], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.09142675995826721
epoch£º107	 i:1 	 global-step:2141	 l-p:0.13073356449604034
epoch£º107	 i:2 	 global-step:2142	 l-p:0.12205840647220612
epoch£º107	 i:3 	 global-step:2143	 l-p:0.12774623930454254
epoch£º107	 i:4 	 global-step:2144	 l-p:0.11489228904247284
epoch£º107	 i:5 	 global-step:2145	 l-p:0.12529781460762024
epoch£º107	 i:6 	 global-step:2146	 l-p:0.12623871862888336
epoch£º107	 i:7 	 global-step:2147	 l-p:0.09383943676948547
epoch£º107	 i:8 	 global-step:2148	 l-p:0.1754072904586792
epoch£º107	 i:9 	 global-step:2149	 l-p:0.12970678508281708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6804, 2.6785, 2.6800],
        [2.6804, 2.6804, 2.6804],
        [2.6804, 2.6763, 2.6714],
        [2.6804, 2.6804, 2.6804]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.1198360025882721 
model_pd.l_d.mean(): -23.891490936279297 
model_pd.lagr.mean(): -23.77165412902832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0186], device='cuda:0')), ('power', tensor([-23.9101], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.1198360025882721
epoch£º108	 i:1 	 global-step:2161	 l-p:0.1411588191986084
epoch£º108	 i:2 	 global-step:2162	 l-p:0.14255546033382416
epoch£º108	 i:3 	 global-step:2163	 l-p:0.1268976628780365
epoch£º108	 i:4 	 global-step:2164	 l-p:0.19226354360580444
epoch£º108	 i:5 	 global-step:2165	 l-p:0.12869617342948914
epoch£º108	 i:6 	 global-step:2166	 l-p:0.13959014415740967
epoch£º108	 i:7 	 global-step:2167	 l-p:0.2506164014339447
epoch£º108	 i:8 	 global-step:2168	 l-p:0.11698287725448608
epoch£º108	 i:9 	 global-step:2169	 l-p:0.8219120502471924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9226, 2.9248, 2.9182],
        [2.9226, 2.9224, 2.9226],
        [2.9226, 3.3676, 3.5996],
        [2.9226, 2.9226, 2.9226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.15996797382831573 
model_pd.l_d.mean(): -24.452436447143555 
model_pd.lagr.mean(): -24.292469024658203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1358], device='cuda:0')), ('power', tensor([-24.3167], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.15996797382831573
epoch£º109	 i:1 	 global-step:2181	 l-p:0.3833472430706024
epoch£º109	 i:2 	 global-step:2182	 l-p:0.15778447687625885
epoch£º109	 i:3 	 global-step:2183	 l-p:0.11439555138349533
epoch£º109	 i:4 	 global-step:2184	 l-p:0.0593012236058712
epoch£º109	 i:5 	 global-step:2185	 l-p:0.10237030684947968
epoch£º109	 i:6 	 global-step:2186	 l-p:0.1969917267560959
epoch£º109	 i:7 	 global-step:2187	 l-p:0.10395693778991699
epoch£º109	 i:8 	 global-step:2188	 l-p:0.10920967161655426
epoch£º109	 i:9 	 global-step:2189	 l-p:0.10565370321273804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7785, 3.7786, 3.7785],
        [3.7785, 3.7785, 3.7785],
        [3.7785, 3.7787, 3.7785],
        [3.7785, 3.7841, 3.7791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.10262743383646011 
model_pd.l_d.mean(): -24.54203987121582 
model_pd.lagr.mean(): -24.43941307067871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5600], device='cuda:0')), ('power', tensor([-23.9820], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.10262743383646011
epoch£º110	 i:1 	 global-step:2201	 l-p:0.10382278263568878
epoch£º110	 i:2 	 global-step:2202	 l-p:0.11597460508346558
epoch£º110	 i:3 	 global-step:2203	 l-p:0.0867556631565094
epoch£º110	 i:4 	 global-step:2204	 l-p:0.10194063931703568
epoch£º110	 i:5 	 global-step:2205	 l-p:0.07425146549940109
epoch£º110	 i:6 	 global-step:2206	 l-p:0.11037347465753555
epoch£º110	 i:7 	 global-step:2207	 l-p:0.10350187122821808
epoch£º110	 i:8 	 global-step:2208	 l-p:0.12893107533454895
epoch£º110	 i:9 	 global-step:2209	 l-p:-0.178409606218338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9234, 2.9228, 2.9233],
        [2.9234, 3.1164, 3.1348],
        [2.9234, 3.0499, 3.0323],
        [2.9234, 3.1979, 3.2755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.13230708241462708 
model_pd.l_d.mean(): -24.475862503051758 
model_pd.lagr.mean(): -24.343555450439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1341], device='cuda:0')), ('power', tensor([-24.3417], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.13230708241462708
epoch£º111	 i:1 	 global-step:2221	 l-p:0.635819673538208
epoch£º111	 i:2 	 global-step:2222	 l-p:0.13607804477214813
epoch£º111	 i:3 	 global-step:2223	 l-p:0.1419871747493744
epoch£º111	 i:4 	 global-step:2224	 l-p:0.565406858921051
epoch£º111	 i:5 	 global-step:2225	 l-p:0.11607466638088226
epoch£º111	 i:6 	 global-step:2226	 l-p:0.061146412044763565
epoch£º111	 i:7 	 global-step:2227	 l-p:0.1884400099515915
epoch£º111	 i:8 	 global-step:2228	 l-p:0.09040768444538116
epoch£º111	 i:9 	 global-step:2229	 l-p:0.14108841121196747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8896, 2.8892, 2.8859],
        [2.8896, 2.8890, 2.8860],
        [2.8896, 2.8896, 2.8896],
        [2.8896, 2.8879, 2.8887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.2230018526315689 
model_pd.l_d.mean(): -24.732940673828125 
model_pd.lagr.mean(): -24.509939193725586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1689], device='cuda:0')), ('power', tensor([-24.5641], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.2230018526315689
epoch£º112	 i:1 	 global-step:2241	 l-p:0.1214761734008789
epoch£º112	 i:2 	 global-step:2242	 l-p:0.1304589807987213
epoch£º112	 i:3 	 global-step:2243	 l-p:0.24493062496185303
epoch£º112	 i:4 	 global-step:2244	 l-p:0.11717767268419266
epoch£º112	 i:5 	 global-step:2245	 l-p:0.12249593436717987
epoch£º112	 i:6 	 global-step:2246	 l-p:0.1307731419801712
epoch£º112	 i:7 	 global-step:2247	 l-p:0.12318447977304459
epoch£º112	 i:8 	 global-step:2248	 l-p:0.11709310859441757
epoch£º112	 i:9 	 global-step:2249	 l-p:0.09242985397577286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1370, 3.1370, 3.1370],
        [3.1370, 3.3771, 3.4139],
        [3.1370, 3.1422, 3.1355],
        [3.1370, 3.1370, 3.1370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.16014625132083893 
model_pd.l_d.mean(): -24.177494049072266 
model_pd.lagr.mean(): -24.01734733581543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2171], device='cuda:0')), ('power', tensor([-23.9604], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.16014625132083893
epoch£º113	 i:1 	 global-step:2261	 l-p:0.15239392220973969
epoch£º113	 i:2 	 global-step:2262	 l-p:0.15107175707817078
epoch£º113	 i:3 	 global-step:2263	 l-p:0.12618336081504822
epoch£º113	 i:4 	 global-step:2264	 l-p:0.11027530580759048
epoch£º113	 i:5 	 global-step:2265	 l-p:0.1334773153066635
epoch£º113	 i:6 	 global-step:2266	 l-p:0.12020023167133331
epoch£º113	 i:7 	 global-step:2267	 l-p:0.11725013703107834
epoch£º113	 i:8 	 global-step:2268	 l-p:0.16495615243911743
epoch£º113	 i:9 	 global-step:2269	 l-p:0.12610937654972076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9211, 2.9211, 2.9211],
        [2.9211, 2.9209, 2.9211],
        [2.9211, 2.9210, 2.9211],
        [2.9211, 3.2008, 3.2833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.14208635687828064 
model_pd.l_d.mean(): -24.22909927368164 
model_pd.lagr.mean(): -24.087013244628906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0882], device='cuda:0')), ('power', tensor([-24.1409], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.14208635687828064
epoch£º114	 i:1 	 global-step:2281	 l-p:0.1072334423661232
epoch£º114	 i:2 	 global-step:2282	 l-p:0.10866834223270416
epoch£º114	 i:3 	 global-step:2283	 l-p:0.12712450325489044
epoch£º114	 i:4 	 global-step:2284	 l-p:0.1403719037771225
epoch£º114	 i:5 	 global-step:2285	 l-p:0.1281755268573761
epoch£º114	 i:6 	 global-step:2286	 l-p:0.11202514171600342
epoch£º114	 i:7 	 global-step:2287	 l-p:0.15080788731575012
epoch£º114	 i:8 	 global-step:2288	 l-p:0.12820689380168915
epoch£º114	 i:9 	 global-step:2289	 l-p:0.25250500440597534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8604, 2.8604, 2.8604],
        [2.8604, 2.8604, 2.8604],
        [2.8604, 2.8604, 2.8604],
        [2.8604, 2.8760, 2.8524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.0980571061372757 
model_pd.l_d.mean(): -24.584142684936523 
model_pd.lagr.mean(): -24.486085891723633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1237], device='cuda:0')), ('power', tensor([-24.4604], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.0980571061372757
epoch£º115	 i:1 	 global-step:2301	 l-p:0.14270754158496857
epoch£º115	 i:2 	 global-step:2302	 l-p:0.10616108030080795
epoch£º115	 i:3 	 global-step:2303	 l-p:0.12122863531112671
epoch£º115	 i:4 	 global-step:2304	 l-p:0.12570519745349884
epoch£º115	 i:5 	 global-step:2305	 l-p:0.13792605698108673
epoch£º115	 i:6 	 global-step:2306	 l-p:0.11989659070968628
epoch£º115	 i:7 	 global-step:2307	 l-p:0.18646277487277985
epoch£º115	 i:8 	 global-step:2308	 l-p:0.15090015530586243
epoch£º115	 i:9 	 global-step:2309	 l-p:0.1274038404226303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9511, 2.9512, 2.9455],
        [2.9511, 2.9487, 2.9488],
        [2.9511, 2.9511, 2.9511],
        [2.9511, 2.9723, 2.9467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.09974851459264755 
model_pd.l_d.mean(): -24.31393051147461 
model_pd.lagr.mean(): -24.214181900024414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1281], device='cuda:0')), ('power', tensor([-24.1858], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.09974851459264755
epoch£º116	 i:1 	 global-step:2321	 l-p:0.11666350811719894
epoch£º116	 i:2 	 global-step:2322	 l-p:0.12404780834913254
epoch£º116	 i:3 	 global-step:2323	 l-p:0.1400420367717743
epoch£º116	 i:4 	 global-step:2324	 l-p:0.13321524858474731
epoch£º116	 i:5 	 global-step:2325	 l-p:0.1316905915737152
epoch£º116	 i:6 	 global-step:2326	 l-p:0.13045459985733032
epoch£º116	 i:7 	 global-step:2327	 l-p:0.12495318800210953
epoch£º116	 i:8 	 global-step:2328	 l-p:0.1662791222333908
epoch£º116	 i:9 	 global-step:2329	 l-p:0.08886030316352844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7450, 2.7447, 2.7450],
        [2.7450, 2.7378, 2.7386],
        [2.7450, 2.7450, 2.7450],
        [2.7450, 2.7450, 2.7450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.10842136293649673 
model_pd.l_d.mean(): -24.896278381347656 
model_pd.lagr.mean(): -24.787857055664062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1346], device='cuda:0')), ('power', tensor([-24.7617], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.10842136293649673
epoch£º117	 i:1 	 global-step:2341	 l-p:0.13064122200012207
epoch£º117	 i:2 	 global-step:2342	 l-p:0.13096210360527039
epoch£º117	 i:3 	 global-step:2343	 l-p:0.20773471891880035
epoch£º117	 i:4 	 global-step:2344	 l-p:0.13326336443424225
epoch£º117	 i:5 	 global-step:2345	 l-p:0.34217512607574463
epoch£º117	 i:6 	 global-step:2346	 l-p:0.1440575271844864
epoch£º117	 i:7 	 global-step:2347	 l-p:-3.3381221294403076
epoch£º117	 i:8 	 global-step:2348	 l-p:0.12686002254486084
epoch£º117	 i:9 	 global-step:2349	 l-p:0.11649370193481445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0481, 3.5117, 3.7479],
        [3.0481, 3.1071, 3.0714],
        [3.0481, 3.0463, 3.0471],
        [3.0481, 3.0481, 3.0481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.12673121690750122 
model_pd.l_d.mean(): -24.2885684967041 
model_pd.lagr.mean(): -24.161836624145508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1645], device='cuda:0')), ('power', tensor([-24.1241], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.12673121690750122
epoch£º118	 i:1 	 global-step:2361	 l-p:0.11485841125249863
epoch£º118	 i:2 	 global-step:2362	 l-p:0.11741328239440918
epoch£º118	 i:3 	 global-step:2363	 l-p:0.12565526366233826
epoch£º118	 i:4 	 global-step:2364	 l-p:0.17787224054336548
epoch£º118	 i:5 	 global-step:2365	 l-p:0.12139654904603958
epoch£º118	 i:6 	 global-step:2366	 l-p:0.4388779103755951
epoch£º118	 i:7 	 global-step:2367	 l-p:0.12069808691740036
epoch£º118	 i:8 	 global-step:2368	 l-p:0.12576474249362946
epoch£º118	 i:9 	 global-step:2369	 l-p:0.12218010425567627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0243, 3.0447, 3.0188],
        [3.0243, 3.0230, 3.0240],
        [3.0243, 3.1598, 3.1419],
        [3.0243, 3.2170, 3.2300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.12373937666416168 
model_pd.l_d.mean(): -24.785255432128906 
model_pd.lagr.mean(): -24.661516189575195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2419], device='cuda:0')), ('power', tensor([-24.5433], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.12373937666416168
epoch£º119	 i:1 	 global-step:2381	 l-p:0.14713647961616516
epoch£º119	 i:2 	 global-step:2382	 l-p:0.11866644769906998
epoch£º119	 i:3 	 global-step:2383	 l-p:-0.14041085541248322
epoch£º119	 i:4 	 global-step:2384	 l-p:0.1569688469171524
epoch£º119	 i:5 	 global-step:2385	 l-p:0.16813667118549347
epoch£º119	 i:6 	 global-step:2386	 l-p:0.11496700346469879
epoch£º119	 i:7 	 global-step:2387	 l-p:0.12894220650196075
epoch£º119	 i:8 	 global-step:2388	 l-p:0.1220981702208519
epoch£º119	 i:9 	 global-step:2389	 l-p:0.11672838032245636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8973, 2.8973, 2.8973],
        [2.8973, 2.8972, 2.8973],
        [2.8973, 2.8970, 2.8973],
        [2.8973, 2.9628, 2.9256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.1129225343465805 
model_pd.l_d.mean(): -24.605632781982422 
model_pd.lagr.mean(): -24.49271011352539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1372], device='cuda:0')), ('power', tensor([-24.4685], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.1129225343465805
epoch£º120	 i:1 	 global-step:2401	 l-p:0.16484759747982025
epoch£º120	 i:2 	 global-step:2402	 l-p:0.16285789012908936
epoch£º120	 i:3 	 global-step:2403	 l-p:0.11803027242422104
epoch£º120	 i:4 	 global-step:2404	 l-p:0.11603846400976181
epoch£º120	 i:5 	 global-step:2405	 l-p:0.10921653360128403
epoch£º120	 i:6 	 global-step:2406	 l-p:0.045420076698064804
epoch£º120	 i:7 	 global-step:2407	 l-p:0.2637261152267456
epoch£º120	 i:8 	 global-step:2408	 l-p:0.11908289790153503
epoch£º120	 i:9 	 global-step:2409	 l-p:0.12340481579303741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1245, 3.1486, 3.1213],
        [3.1245, 3.1224, 3.1229],
        [3.1245, 3.1245, 3.1245],
        [3.1245, 3.1244, 3.1245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.11577284336090088 
model_pd.l_d.mean(): -24.227510452270508 
model_pd.lagr.mean(): -24.111738204956055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2558], device='cuda:0')), ('power', tensor([-23.9717], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.11577284336090088
epoch£º121	 i:1 	 global-step:2421	 l-p:0.1310129165649414
epoch£º121	 i:2 	 global-step:2422	 l-p:0.08616229891777039
epoch£º121	 i:3 	 global-step:2423	 l-p:-0.004167094361037016
epoch£º121	 i:4 	 global-step:2424	 l-p:0.1296319216489792
epoch£º121	 i:5 	 global-step:2425	 l-p:0.1367175281047821
epoch£º121	 i:6 	 global-step:2426	 l-p:0.12693816423416138
epoch£º121	 i:7 	 global-step:2427	 l-p:0.115193210542202
epoch£º121	 i:8 	 global-step:2428	 l-p:0.1274641752243042
epoch£º121	 i:9 	 global-step:2429	 l-p:0.07643341273069382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7625, 2.7560, 2.7599],
        [2.7625, 2.7595, 2.7619],
        [2.7625, 2.7625, 2.7625],
        [2.7625, 2.7603, 2.7622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.11627791821956635 
model_pd.l_d.mean(): -23.792259216308594 
model_pd.lagr.mean(): -23.675981521606445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0052], device='cuda:0')), ('power', tensor([-23.7870], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.11627791821956635
epoch£º122	 i:1 	 global-step:2441	 l-p:0.1552085429430008
epoch£º122	 i:2 	 global-step:2442	 l-p:0.09125687181949615
epoch£º122	 i:3 	 global-step:2443	 l-p:0.1246136724948883
epoch£º122	 i:4 	 global-step:2444	 l-p:0.129879891872406
epoch£º122	 i:5 	 global-step:2445	 l-p:1.10999596118927
epoch£º122	 i:6 	 global-step:2446	 l-p:0.12796007096767426
epoch£º122	 i:7 	 global-step:2447	 l-p:0.12411878257989883
epoch£º122	 i:8 	 global-step:2448	 l-p:0.19444110989570618
epoch£º122	 i:9 	 global-step:2449	 l-p:0.15337716042995453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8563, 2.8561, 2.8563],
        [2.8563, 2.8478, 2.8494],
        [2.8563, 2.8544, 2.8560],
        [2.8563, 2.8563, 2.8563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.14530082046985626 
model_pd.l_d.mean(): -24.714012145996094 
model_pd.lagr.mean(): -24.56871223449707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1372], device='cuda:0')), ('power', tensor([-24.5768], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.14530082046985626
epoch£º123	 i:1 	 global-step:2461	 l-p:0.15740269422531128
epoch£º123	 i:2 	 global-step:2462	 l-p:0.13091835379600525
epoch£º123	 i:3 	 global-step:2463	 l-p:-0.08578202873468399
epoch£º123	 i:4 	 global-step:2464	 l-p:0.11664208769798279
epoch£º123	 i:5 	 global-step:2465	 l-p:0.12509526312351227
epoch£º123	 i:6 	 global-step:2466	 l-p:0.11934472620487213
epoch£º123	 i:7 	 global-step:2467	 l-p:0.033863700926303864
epoch£º123	 i:8 	 global-step:2468	 l-p:0.12828326225280762
epoch£º123	 i:9 	 global-step:2469	 l-p:0.179709792137146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1666, 3.1665, 3.1666],
        [3.1666, 3.4128, 3.4529],
        [3.1666, 3.1638, 3.1639],
        [3.1666, 3.1644, 3.1622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.12296827882528305 
model_pd.l_d.mean(): -24.464008331298828 
model_pd.lagr.mean(): -24.341039657592773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2600], device='cuda:0')), ('power', tensor([-24.2040], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.12296827882528305
epoch£º124	 i:1 	 global-step:2481	 l-p:0.12717756628990173
epoch£º124	 i:2 	 global-step:2482	 l-p:0.11307290196418762
epoch£º124	 i:3 	 global-step:2483	 l-p:0.1219964548945427
epoch£º124	 i:4 	 global-step:2484	 l-p:-1.364463448524475
epoch£º124	 i:5 	 global-step:2485	 l-p:0.12474727630615234
epoch£º124	 i:6 	 global-step:2486	 l-p:0.12432345747947693
epoch£º124	 i:7 	 global-step:2487	 l-p:0.04683952406048775
epoch£º124	 i:8 	 global-step:2488	 l-p:0.11672752350568771
epoch£º124	 i:9 	 global-step:2489	 l-p:0.07942905277013779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9653, 2.9620, 2.9644],
        [2.9653, 3.4043, 3.6254],
        [2.9653, 2.9613, 2.9641],
        [2.9653, 3.0271, 2.9861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.11800561845302582 
model_pd.l_d.mean(): -24.017629623413086 
model_pd.lagr.mean(): -23.89962387084961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1348], device='cuda:0')), ('power', tensor([-23.8829], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.11800561845302582
epoch£º125	 i:1 	 global-step:2501	 l-p:0.12529920041561127
epoch£º125	 i:2 	 global-step:2502	 l-p:0.1638004332780838
epoch£º125	 i:3 	 global-step:2503	 l-p:0.138205423951149
epoch£º125	 i:4 	 global-step:2504	 l-p:0.11033378541469574
epoch£º125	 i:5 	 global-step:2505	 l-p:0.1301770657300949
epoch£º125	 i:6 	 global-step:2506	 l-p:0.1082785502076149
epoch£º125	 i:7 	 global-step:2507	 l-p:0.10005753487348557
epoch£º125	 i:8 	 global-step:2508	 l-p:0.1522407829761505
epoch£º125	 i:9 	 global-step:2509	 l-p:0.12430022656917572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0188, 3.2083, 3.2190],
        [3.0188, 3.1031, 3.0650],
        [3.0188, 3.0188, 3.0188],
        [3.0188, 3.0171, 3.0185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.11494920402765274 
model_pd.l_d.mean(): -24.768108367919922 
model_pd.lagr.mean(): -24.653160095214844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2455], device='cuda:0')), ('power', tensor([-24.5226], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.11494920402765274
epoch£º126	 i:1 	 global-step:2521	 l-p:0.12739764153957367
epoch£º126	 i:2 	 global-step:2522	 l-p:0.08743507415056229
epoch£º126	 i:3 	 global-step:2523	 l-p:0.16880212724208832
epoch£º126	 i:4 	 global-step:2524	 l-p:0.13826848566532135
epoch£º126	 i:5 	 global-step:2525	 l-p:0.07920675724744797
epoch£º126	 i:6 	 global-step:2526	 l-p:0.11635906994342804
epoch£º126	 i:7 	 global-step:2527	 l-p:0.11614990234375
epoch£º126	 i:8 	 global-step:2528	 l-p:0.14944499731063843
epoch£º126	 i:9 	 global-step:2529	 l-p:0.09685848653316498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[3.0189, 3.0143, 3.0027],
        [3.0189, 3.0115, 3.0075],
        [3.0189, 3.0273, 2.9995],
        [3.0189, 3.2574, 3.3015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.12308282405138016 
model_pd.l_d.mean(): -24.323837280273438 
model_pd.lagr.mean(): -24.200754165649414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1780], device='cuda:0')), ('power', tensor([-24.1458], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.12308282405138016
epoch£º127	 i:1 	 global-step:2541	 l-p:0.1386394053697586
epoch£º127	 i:2 	 global-step:2542	 l-p:0.10657012462615967
epoch£º127	 i:3 	 global-step:2543	 l-p:0.1338942050933838
epoch£º127	 i:4 	 global-step:2544	 l-p:0.13432525098323822
epoch£º127	 i:5 	 global-step:2545	 l-p:0.14931705594062805
epoch£º127	 i:6 	 global-step:2546	 l-p:0.13765329122543335
epoch£º127	 i:7 	 global-step:2547	 l-p:0.11512331664562225
epoch£º127	 i:8 	 global-step:2548	 l-p:0.1375170797109604
epoch£º127	 i:9 	 global-step:2549	 l-p:0.1395200937986374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7687, 2.8025, 2.7544],
        [2.7687, 2.7524, 2.7351],
        [2.7687, 2.7675, 2.7686],
        [2.7687, 2.7684, 2.7687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.11124011874198914 
model_pd.l_d.mean(): -24.326435089111328 
model_pd.lagr.mean(): -24.215194702148438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0246], device='cuda:0')), ('power', tensor([-24.3018], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.11124011874198914
epoch£º128	 i:1 	 global-step:2561	 l-p:0.11703761667013168
epoch£º128	 i:2 	 global-step:2562	 l-p:0.13440099358558655
epoch£º128	 i:3 	 global-step:2563	 l-p:0.12552481889724731
epoch£º128	 i:4 	 global-step:2564	 l-p:0.1302524358034134
epoch£º128	 i:5 	 global-step:2565	 l-p:0.11422325670719147
epoch£º128	 i:6 	 global-step:2566	 l-p:0.20229530334472656
epoch£º128	 i:7 	 global-step:2567	 l-p:0.019072609022259712
epoch£º128	 i:8 	 global-step:2568	 l-p:0.14722023904323578
epoch£º128	 i:9 	 global-step:2569	 l-p:0.1346728354692459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9320, 2.9316, 2.9320],
        [2.9320, 2.9262, 2.9302],
        [2.9320, 2.9320, 2.9320],
        [2.9320, 2.9208, 2.9085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.13220593333244324 
model_pd.l_d.mean(): -24.427467346191406 
model_pd.lagr.mean(): -24.29526138305664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1057], device='cuda:0')), ('power', tensor([-24.3218], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.13220593333244324
epoch£º129	 i:1 	 global-step:2581	 l-p:0.2099815458059311
epoch£º129	 i:2 	 global-step:2582	 l-p:0.1204342171549797
epoch£º129	 i:3 	 global-step:2583	 l-p:0.13650529086589813
epoch£º129	 i:4 	 global-step:2584	 l-p:0.03747698664665222
epoch£º129	 i:5 	 global-step:2585	 l-p:0.11718805879354477
epoch£º129	 i:6 	 global-step:2586	 l-p:0.12044750154018402
epoch£º129	 i:7 	 global-step:2587	 l-p:0.12391871958971024
epoch£º129	 i:8 	 global-step:2588	 l-p:0.1167166605591774
epoch£º129	 i:9 	 global-step:2589	 l-p:0.1406758576631546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8967, 2.8823, 2.8696],
        [2.8967, 2.8967, 2.8967],
        [2.8967, 2.8967, 2.8967],
        [2.8967, 2.8962, 2.8967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.14597393572330475 
model_pd.l_d.mean(): -23.878990173339844 
model_pd.lagr.mean(): -23.733016967773438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0714], device='cuda:0')), ('power', tensor([-23.8076], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.14597393572330475
epoch£º130	 i:1 	 global-step:2601	 l-p:0.1250772625207901
epoch£º130	 i:2 	 global-step:2602	 l-p:0.12067508697509766
epoch£º130	 i:3 	 global-step:2603	 l-p:0.13631799817085266
epoch£º130	 i:4 	 global-step:2604	 l-p:0.011633691377937794
epoch£º130	 i:5 	 global-step:2605	 l-p:-0.10234752297401428
epoch£º130	 i:6 	 global-step:2606	 l-p:0.10272615402936935
epoch£º130	 i:7 	 global-step:2607	 l-p:0.14079904556274414
epoch£º130	 i:8 	 global-step:2608	 l-p:0.21301256120204926
epoch£º130	 i:9 	 global-step:2609	 l-p:0.1470186710357666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7716, 2.7716, 2.7716],
        [2.7716, 2.7515, 2.7548],
        [2.7716, 2.7715, 2.7716],
        [2.7716, 2.7716, 2.7716]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.1406201869249344 
model_pd.l_d.mean(): -24.34691047668457 
model_pd.lagr.mean(): -24.20629119873047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0347], device='cuda:0')), ('power', tensor([-24.3123], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.1406201869249344
epoch£º131	 i:1 	 global-step:2621	 l-p:0.12854772806167603
epoch£º131	 i:2 	 global-step:2622	 l-p:0.13328370451927185
epoch£º131	 i:3 	 global-step:2623	 l-p:0.11401374638080597
epoch£º131	 i:4 	 global-step:2624	 l-p:0.17192785441875458
epoch£º131	 i:5 	 global-step:2625	 l-p:0.1239987164735794
epoch£º131	 i:6 	 global-step:2626	 l-p:0.2006353735923767
epoch£º131	 i:7 	 global-step:2627	 l-p:0.11254028230905533
epoch£º131	 i:8 	 global-step:2628	 l-p:0.14452968537807465
epoch£º131	 i:9 	 global-step:2629	 l-p:0.1263173669576645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0482, 3.0482, 3.0482],
        [3.0482, 3.3944, 3.5209],
        [3.0482, 3.0482, 3.0482],
        [3.0482, 3.1072, 3.0595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.12733598053455353 
model_pd.l_d.mean(): -24.03252410888672 
model_pd.lagr.mean(): -23.905187606811523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1330], device='cuda:0')), ('power', tensor([-23.8995], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.12733598053455353
epoch£º132	 i:1 	 global-step:2641	 l-p:0.12961284816265106
epoch£º132	 i:2 	 global-step:2642	 l-p:0.1847696602344513
epoch£º132	 i:3 	 global-step:2643	 l-p:0.11544593423604965
epoch£º132	 i:4 	 global-step:2644	 l-p:0.13180778920650482
epoch£º132	 i:5 	 global-step:2645	 l-p:0.1546710878610611
epoch£º132	 i:6 	 global-step:2646	 l-p:0.11066501587629318
epoch£º132	 i:7 	 global-step:2647	 l-p:0.10844532400369644
epoch£º132	 i:8 	 global-step:2648	 l-p:0.18448908627033234
epoch£º132	 i:9 	 global-step:2649	 l-p:0.11493761092424393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1471, 3.1471, 3.1471],
        [3.1471, 3.1471, 3.1471],
        [3.1471, 3.1779, 3.1333],
        [3.1471, 3.1383, 3.1285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.11420867592096329 
model_pd.l_d.mean(): -23.79889678955078 
model_pd.lagr.mean(): -23.684688568115234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1764], device='cuda:0')), ('power', tensor([-23.6225], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.11420867592096329
epoch£º133	 i:1 	 global-step:2661	 l-p:0.11099840700626373
epoch£º133	 i:2 	 global-step:2662	 l-p:0.1271134465932846
epoch£º133	 i:3 	 global-step:2663	 l-p:0.13524851202964783
epoch£º133	 i:4 	 global-step:2664	 l-p:0.10920753329992294
epoch£º133	 i:5 	 global-step:2665	 l-p:0.1140560731291771
epoch£º133	 i:6 	 global-step:2666	 l-p:0.10359165817499161
epoch£º133	 i:7 	 global-step:2667	 l-p:0.47574132680892944
epoch£º133	 i:8 	 global-step:2668	 l-p:0.1398063451051712
epoch£º133	 i:9 	 global-step:2669	 l-p:0.09661998599767685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7569, 2.7393, 2.6962],
        [2.7569, 2.7324, 2.7367],
        [2.7569, 2.7488, 2.7548],
        [2.7569, 2.7553, 2.7568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13669544458389282 
model_pd.l_d.mean(): -24.670852661132812 
model_pd.lagr.mean(): -24.534156799316406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0823], device='cuda:0')), ('power', tensor([-24.5886], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.13669544458389282
epoch£º134	 i:1 	 global-step:2681	 l-p:0.18503202497959137
epoch£º134	 i:2 	 global-step:2682	 l-p:0.11639858037233353
epoch£º134	 i:3 	 global-step:2683	 l-p:0.09809685498476028
epoch£º134	 i:4 	 global-step:2684	 l-p:0.13978368043899536
epoch£º134	 i:5 	 global-step:2685	 l-p:0.12608011066913605
epoch£º134	 i:6 	 global-step:2686	 l-p:0.01875966414809227
epoch£º134	 i:7 	 global-step:2687	 l-p:0.1582927405834198
epoch£º134	 i:8 	 global-step:2688	 l-p:0.14192907512187958
epoch£º134	 i:9 	 global-step:2689	 l-p:0.1064593493938446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9671, 2.9671, 2.9671],
        [2.9671, 3.2871, 3.3972],
        [2.9671, 2.9475, 2.9380],
        [2.9671, 3.1993, 3.2401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.1365756094455719 
model_pd.l_d.mean(): -23.8669376373291 
model_pd.lagr.mean(): -23.730361938476562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0626], device='cuda:0')), ('power', tensor([-23.8043], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.1365756094455719
epoch£º135	 i:1 	 global-step:2701	 l-p:0.1341220587491989
epoch£º135	 i:2 	 global-step:2702	 l-p:0.1285228282213211
epoch£º135	 i:3 	 global-step:2703	 l-p:0.1267523169517517
epoch£º135	 i:4 	 global-step:2704	 l-p:0.09812547266483307
epoch£º135	 i:5 	 global-step:2705	 l-p:0.14717568457126617
epoch£º135	 i:6 	 global-step:2706	 l-p:0.21329043805599213
epoch£º135	 i:7 	 global-step:2707	 l-p:0.06619726866483688
epoch£º135	 i:8 	 global-step:2708	 l-p:0.07417508959770203
epoch£º135	 i:9 	 global-step:2709	 l-p:0.12320461124181747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9546, 2.9545, 2.9546],
        [2.9546, 2.9331, 2.9192],
        [2.9546, 2.9541, 2.9546],
        [2.9546, 2.9351, 2.9138]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.12467086315155029 
model_pd.l_d.mean(): -24.944290161132812 
model_pd.lagr.mean(): -24.81962013244629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2370], device='cuda:0')), ('power', tensor([-24.7073], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.12467086315155029
epoch£º136	 i:1 	 global-step:2721	 l-p:0.1267358809709549
epoch£º136	 i:2 	 global-step:2722	 l-p:0.1794179230928421
epoch£º136	 i:3 	 global-step:2723	 l-p:0.10666945576667786
epoch£º136	 i:4 	 global-step:2724	 l-p:0.13790899515151978
epoch£º136	 i:5 	 global-step:2725	 l-p:0.14075571298599243
epoch£º136	 i:6 	 global-step:2726	 l-p:0.14105866849422455
epoch£º136	 i:7 	 global-step:2727	 l-p:0.10410591214895248
epoch£º136	 i:8 	 global-step:2728	 l-p:0.1266215443611145
epoch£º136	 i:9 	 global-step:2729	 l-p:0.0431835912168026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9008, 2.9008, 2.9008],
        [2.9008, 2.8923, 2.8985],
        [2.9008, 2.9008, 2.9008],
        [2.9008, 3.1427, 3.1946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.13329662382602692 
model_pd.l_d.mean(): -24.803447723388672 
model_pd.lagr.mean(): -24.670150756835938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1705], device='cuda:0')), ('power', tensor([-24.6329], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.13329662382602692
epoch£º137	 i:1 	 global-step:2741	 l-p:0.12414272129535675
epoch£º137	 i:2 	 global-step:2742	 l-p:0.17801642417907715
epoch£º137	 i:3 	 global-step:2743	 l-p:0.1655901074409485
epoch£º137	 i:4 	 global-step:2744	 l-p:0.11208547651767731
epoch£º137	 i:5 	 global-step:2745	 l-p:0.052460867911577225
epoch£º137	 i:6 	 global-step:2746	 l-p:0.10846411436796188
epoch£º137	 i:7 	 global-step:2747	 l-p:0.14717309176921844
epoch£º137	 i:8 	 global-step:2748	 l-p:0.10013879835605621
epoch£º137	 i:9 	 global-step:2749	 l-p:0.08473344892263412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0889, 3.0717, 3.0731],
        [3.0889, 3.0889, 3.0889],
        [3.0889, 3.0889, 3.0889],
        [3.0889, 3.0889, 3.0889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): -0.01162160374224186 
model_pd.l_d.mean(): -24.759721755981445 
model_pd.lagr.mean(): -24.771343231201172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2610], device='cuda:0')), ('power', tensor([-24.4987], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:-0.01162160374224186
epoch£º138	 i:1 	 global-step:2761	 l-p:0.39872899651527405
epoch£º138	 i:2 	 global-step:2762	 l-p:0.11417482048273087
epoch£º138	 i:3 	 global-step:2763	 l-p:0.1236535832285881
epoch£º138	 i:4 	 global-step:2764	 l-p:0.11510255187749863
epoch£º138	 i:5 	 global-step:2765	 l-p:0.10994413495063782
epoch£º138	 i:6 	 global-step:2766	 l-p:0.11364171653985977
epoch£º138	 i:7 	 global-step:2767	 l-p:0.11757668852806091
epoch£º138	 i:8 	 global-step:2768	 l-p:0.1288854330778122
epoch£º138	 i:9 	 global-step:2769	 l-p:0.12218284606933594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9850, 3.2463, 3.3065],
        [2.9850, 3.3372, 3.4717],
        [2.9850, 2.9850, 2.9850],
        [2.9850, 2.9835, 2.9849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.1515086591243744 
model_pd.l_d.mean(): -23.87332534790039 
model_pd.lagr.mean(): -23.721817016601562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0958], device='cuda:0')), ('power', tensor([-23.7775], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.1515086591243744
epoch£º139	 i:1 	 global-step:2781	 l-p:0.1269349753856659
epoch£º139	 i:2 	 global-step:2782	 l-p:0.11947285383939743
epoch£º139	 i:3 	 global-step:2783	 l-p:0.07392886281013489
epoch£º139	 i:4 	 global-step:2784	 l-p:0.23128756880760193
epoch£º139	 i:5 	 global-step:2785	 l-p:0.08075813949108124
epoch£º139	 i:6 	 global-step:2786	 l-p:0.1736554354429245
epoch£º139	 i:7 	 global-step:2787	 l-p:0.09383568912744522
epoch£º139	 i:8 	 global-step:2788	 l-p:0.1317881941795349
epoch£º139	 i:9 	 global-step:2789	 l-p:-0.08378072828054428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8027, 3.0312, 3.0794],
        [2.8027, 2.7848, 2.7945],
        [2.8027, 2.8601, 2.8042],
        [2.8027, 3.0396, 3.0942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.13964585959911346 
model_pd.l_d.mean(): -24.5351619720459 
model_pd.lagr.mean(): -24.39551544189453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0579], device='cuda:0')), ('power', tensor([-24.4772], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.13964585959911346
epoch£º140	 i:1 	 global-step:2801	 l-p:0.10268177092075348
epoch£º140	 i:2 	 global-step:2802	 l-p:-1.7637673616409302
epoch£º140	 i:3 	 global-step:2803	 l-p:0.13652583956718445
epoch£º140	 i:4 	 global-step:2804	 l-p:0.1256469041109085
epoch£º140	 i:5 	 global-step:2805	 l-p:0.1305682212114334
epoch£º140	 i:6 	 global-step:2806	 l-p:0.14312082529067993
epoch£º140	 i:7 	 global-step:2807	 l-p:0.11475768685340881
epoch£º140	 i:8 	 global-step:2808	 l-p:-0.8101113438606262
epoch£º140	 i:9 	 global-step:2809	 l-p:0.11061982810497284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9778, 2.9777, 2.9778],
        [2.9778, 2.9777, 2.9778],
        [2.9778, 2.9778, 2.9778],
        [2.9778, 2.9511, 2.9416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.09790141880512238 
model_pd.l_d.mean(): -24.547958374023438 
model_pd.lagr.mean(): -24.450056076049805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1611], device='cuda:0')), ('power', tensor([-24.3868], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.09790141880512238
epoch£º141	 i:1 	 global-step:2821	 l-p:0.12356225401163101
epoch£º141	 i:2 	 global-step:2822	 l-p:0.12618418037891388
epoch£º141	 i:3 	 global-step:2823	 l-p:0.13092194497585297
epoch£º141	 i:4 	 global-step:2824	 l-p:0.13217224180698395
epoch£º141	 i:5 	 global-step:2825	 l-p:0.10607880353927612
epoch£º141	 i:6 	 global-step:2826	 l-p:-0.14521263539791107
epoch£º141	 i:7 	 global-step:2827	 l-p:0.14390258491039276
epoch£º141	 i:8 	 global-step:2828	 l-p:0.23095670342445374
epoch£º141	 i:9 	 global-step:2829	 l-p:0.11846509575843811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8578, 2.8246, 2.8234],
        [2.8578, 2.8222, 2.8004],
        [2.8578, 2.8219, 2.8053],
        [2.8578, 2.8574, 2.8578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.13724051415920258 
model_pd.l_d.mean(): -24.60365104675293 
model_pd.lagr.mean(): -24.46640968322754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1082], device='cuda:0')), ('power', tensor([-24.4955], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.13724051415920258
epoch£º142	 i:1 	 global-step:2841	 l-p:0.12977692484855652
epoch£º142	 i:2 	 global-step:2842	 l-p:0.18046355247497559
epoch£º142	 i:3 	 global-step:2843	 l-p:0.1228753998875618
epoch£º142	 i:4 	 global-step:2844	 l-p:0.11743558943271637
epoch£º142	 i:5 	 global-step:2845	 l-p:0.06499787420034409
epoch£º142	 i:6 	 global-step:2846	 l-p:0.17274333536624908
epoch£º142	 i:7 	 global-step:2847	 l-p:-0.0686568021774292
epoch£º142	 i:8 	 global-step:2848	 l-p:0.09199438244104385
epoch£º142	 i:9 	 global-step:2849	 l-p:0.1275479793548584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1062, 3.1062, 3.1062],
        [3.1062, 3.1062, 3.1062],
        [3.1062, 3.1062, 3.1062],
        [3.1062, 3.2646, 3.2467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.06873159855604172 
model_pd.l_d.mean(): -23.976831436157227 
model_pd.lagr.mean(): -23.908100128173828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1625], device='cuda:0')), ('power', tensor([-23.8144], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.06873159855604172
epoch£º143	 i:1 	 global-step:2861	 l-p:0.1251160204410553
epoch£º143	 i:2 	 global-step:2862	 l-p:0.12906725704669952
epoch£º143	 i:3 	 global-step:2863	 l-p:0.11805804818868637
epoch£º143	 i:4 	 global-step:2864	 l-p:0.1259295791387558
epoch£º143	 i:5 	 global-step:2865	 l-p:0.014872088097035885
epoch£º143	 i:6 	 global-step:2866	 l-p:0.13402734696865082
epoch£º143	 i:7 	 global-step:2867	 l-p:0.12221717834472656
epoch£º143	 i:8 	 global-step:2868	 l-p:0.11460839211940765
epoch£º143	 i:9 	 global-step:2869	 l-p:0.1284445971250534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8740, 3.1967, 3.3126],
        [2.8740, 2.9257, 2.8633],
        [2.8740, 2.8392, 2.8394],
        [2.8740, 2.8694, 2.8734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.15891772508621216 
model_pd.l_d.mean(): -24.286359786987305 
model_pd.lagr.mean(): -24.12744140625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1166], device='cuda:0')), ('power', tensor([-24.1697], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.15891772508621216
epoch£º144	 i:1 	 global-step:2881	 l-p:0.11403612047433853
epoch£º144	 i:2 	 global-step:2882	 l-p:0.13009919226169586
epoch£º144	 i:3 	 global-step:2883	 l-p:0.12851816415786743
epoch£º144	 i:4 	 global-step:2884	 l-p:0.013773273676633835
epoch£º144	 i:5 	 global-step:2885	 l-p:0.20036479830741882
epoch£º144	 i:6 	 global-step:2886	 l-p:0.13622784614562988
epoch£º144	 i:7 	 global-step:2887	 l-p:0.08747798949480057
epoch£º144	 i:8 	 global-step:2888	 l-p:0.11692412942647934
epoch£º144	 i:9 	 global-step:2889	 l-p:0.13093656301498413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9353, 3.0368, 2.9928],
        [2.9353, 3.0744, 3.0507],
        [2.9353, 2.9290, 2.9341],
        [2.9353, 3.1930, 3.2514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.1362486183643341 
model_pd.l_d.mean(): -24.725088119506836 
model_pd.lagr.mean(): -24.588838577270508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1603], device='cuda:0')), ('power', tensor([-24.5648], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.1362486183643341
epoch£º145	 i:1 	 global-step:2901	 l-p:0.12814106047153473
epoch£º145	 i:2 	 global-step:2902	 l-p:0.13297700881958008
epoch£º145	 i:3 	 global-step:2903	 l-p:0.11121627688407898
epoch£º145	 i:4 	 global-step:2904	 l-p:0.12379345297813416
epoch£º145	 i:5 	 global-step:2905	 l-p:0.04645845666527748
epoch£º145	 i:6 	 global-step:2906	 l-p:0.3932575583457947
epoch£º145	 i:7 	 global-step:2907	 l-p:0.11710914969444275
epoch£º145	 i:8 	 global-step:2908	 l-p:0.13207997381687164
epoch£º145	 i:9 	 global-step:2909	 l-p:0.12133987993001938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0596, 3.0596, 3.0596],
        [3.0596, 3.0429, 3.0518],
        [3.0596, 3.0418, 2.9955],
        [3.0596, 3.0352, 3.0414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.11519593000411987 
model_pd.l_d.mean(): -24.30136489868164 
model_pd.lagr.mean(): -24.186168670654297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1770], device='cuda:0')), ('power', tensor([-24.1244], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.11519593000411987
epoch£º146	 i:1 	 global-step:2921	 l-p:0.08493319153785706
epoch£º146	 i:2 	 global-step:2922	 l-p:0.11131533980369568
epoch£º146	 i:3 	 global-step:2923	 l-p:0.16812562942504883
epoch£º146	 i:4 	 global-step:2924	 l-p:0.1316165179014206
epoch£º146	 i:5 	 global-step:2925	 l-p:0.11327457427978516
epoch£º146	 i:6 	 global-step:2926	 l-p:0.124664306640625
epoch£º146	 i:7 	 global-step:2927	 l-p:0.1415647715330124
epoch£º146	 i:8 	 global-step:2928	 l-p:0.4557250738143921
epoch£º146	 i:9 	 global-step:2929	 l-p:0.10102968662977219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9603, 2.9217, 2.8967],
        [2.9603, 2.9602, 2.9603],
        [2.9603, 2.9358, 2.9459],
        [2.9603, 2.9300, 2.8822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.13674093782901764 
model_pd.l_d.mean(): -24.85491371154785 
model_pd.lagr.mean(): -24.718172073364258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2111], device='cuda:0')), ('power', tensor([-24.6438], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.13674093782901764
epoch£º147	 i:1 	 global-step:2941	 l-p:0.052922286093235016
epoch£º147	 i:2 	 global-step:2942	 l-p:0.16131900250911713
epoch£º147	 i:3 	 global-step:2943	 l-p:0.11466958373785019
epoch£º147	 i:4 	 global-step:2944	 l-p:0.12429160624742508
epoch£º147	 i:5 	 global-step:2945	 l-p:0.15307438373565674
epoch£º147	 i:6 	 global-step:2946	 l-p:0.09246640652418137
epoch£º147	 i:7 	 global-step:2947	 l-p:0.14065928757190704
epoch£º147	 i:8 	 global-step:2948	 l-p:0.11565671861171722
epoch£º147	 i:9 	 global-step:2949	 l-p:0.11259731650352478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8630, 2.8477, 2.8580],
        [2.8630, 2.8549, 2.8614],
        [2.8630, 3.0951, 3.1372],
        [2.8630, 2.8630, 2.8630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.12937000393867493 
model_pd.l_d.mean(): -24.76402473449707 
model_pd.lagr.mean(): -24.634654998779297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1402], device='cuda:0')), ('power', tensor([-24.6238], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.12937000393867493
epoch£º148	 i:1 	 global-step:2961	 l-p:0.1600024402141571
epoch£º148	 i:2 	 global-step:2962	 l-p:0.292830228805542
epoch£º148	 i:3 	 global-step:2963	 l-p:0.05646342784166336
epoch£º148	 i:4 	 global-step:2964	 l-p:0.14798752963542938
epoch£º148	 i:5 	 global-step:2965	 l-p:0.10681413114070892
epoch£º148	 i:6 	 global-step:2966	 l-p:0.12292129546403885
epoch£º148	 i:7 	 global-step:2967	 l-p:0.12092950195074081
epoch£º148	 i:8 	 global-step:2968	 l-p:0.15871867537498474
epoch£º148	 i:9 	 global-step:2969	 l-p:0.08177946507930756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8968, 2.8627, 2.8722],
        [2.8968, 2.8757, 2.8877],
        [2.8968, 3.1283, 3.1669],
        [2.8968, 2.8971, 2.8197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.10941094905138016 
model_pd.l_d.mean(): -23.902700424194336 
model_pd.lagr.mean(): -23.793289184570312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0196], device='cuda:0')), ('power', tensor([-23.8831], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.10941094905138016
epoch£º149	 i:1 	 global-step:2981	 l-p:0.14252707362174988
epoch£º149	 i:2 	 global-step:2982	 l-p:0.12968847155570984
epoch£º149	 i:3 	 global-step:2983	 l-p:0.1223045289516449
epoch£º149	 i:4 	 global-step:2984	 l-p:0.13888439536094666
epoch£º149	 i:5 	 global-step:2985	 l-p:0.12895548343658447
epoch£º149	 i:6 	 global-step:2986	 l-p:0.1089775413274765
epoch£º149	 i:7 	 global-step:2987	 l-p:0.09191737323999405
epoch£º149	 i:8 	 global-step:2988	 l-p:-0.03216636925935745
epoch£º149	 i:9 	 global-step:2989	 l-p:0.12028474360704422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8316, 2.7827, 2.7224],
        [2.8316, 2.8070, 2.7268],
        [2.8316, 2.7819, 2.7231],
        [2.8316, 2.8294, 2.8314]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.1224982813000679 
model_pd.l_d.mean(): -23.881912231445312 
model_pd.lagr.mean(): -23.759414672851562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0183], device='cuda:0')), ('power', tensor([-23.8636], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.1224982813000679
epoch£º150	 i:1 	 global-step:3001	 l-p:0.13703161478042603
epoch£º150	 i:2 	 global-step:3002	 l-p:0.12819252908229828
epoch£º150	 i:3 	 global-step:3003	 l-p:0.08773171901702881
epoch£º150	 i:4 	 global-step:3004	 l-p:0.12716837227344513
epoch£º150	 i:5 	 global-step:3005	 l-p:0.11934329569339752
epoch£º150	 i:6 	 global-step:3006	 l-p:0.15331178903579712
epoch£º150	 i:7 	 global-step:3007	 l-p:0.11819883435964584
epoch£º150	 i:8 	 global-step:3008	 l-p:0.13177980482578278
epoch£º150	 i:9 	 global-step:3009	 l-p:0.11359968781471252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1861, 3.1907, 3.1238],
        [3.1861, 3.1861, 3.1861],
        [3.1861, 3.3604, 3.3433],
        [3.1861, 3.2305, 3.1604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.1194547638297081 
model_pd.l_d.mean(): -24.777925491333008 
model_pd.lagr.mean(): -24.658470153808594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3014], device='cuda:0')), ('power', tensor([-24.4766], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.1194547638297081
epoch£º151	 i:1 	 global-step:3021	 l-p:0.08327250927686691
epoch£º151	 i:2 	 global-step:3022	 l-p:0.12814556062221527
epoch£º151	 i:3 	 global-step:3023	 l-p:0.1107337549328804
epoch£º151	 i:4 	 global-step:3024	 l-p:0.12036409974098206
epoch£º151	 i:5 	 global-step:3025	 l-p:0.12463544309139252
epoch£º151	 i:6 	 global-step:3026	 l-p:1.8015638589859009
epoch£º151	 i:7 	 global-step:3027	 l-p:0.10184092074632645
epoch£º151	 i:8 	 global-step:3028	 l-p:0.13734370470046997
epoch£º151	 i:9 	 global-step:3029	 l-p:0.12664645910263062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[2.7346, 2.6712, 2.6636],
        [2.7346, 2.8529, 2.8207],
        [2.7346, 2.6662, 2.6438],
        [2.7346, 2.6666, 2.6461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): -0.013931474648416042 
model_pd.l_d.mean(): -24.437522888183594 
model_pd.lagr.mean(): -24.451454162597656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0184], device='cuda:0')), ('power', tensor([-24.4559], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:-0.013931474648416042
epoch£º152	 i:1 	 global-step:3041	 l-p:0.14107774198055267
epoch£º152	 i:2 	 global-step:3042	 l-p:0.12336414307355881
epoch£º152	 i:3 	 global-step:3043	 l-p:-0.12272971868515015
epoch£º152	 i:4 	 global-step:3044	 l-p:0.1447743922472
epoch£º152	 i:5 	 global-step:3045	 l-p:0.13739259541034698
epoch£º152	 i:6 	 global-step:3046	 l-p:0.14809481799602509
epoch£º152	 i:7 	 global-step:3047	 l-p:0.13176950812339783
epoch£º152	 i:8 	 global-step:3048	 l-p:0.14296050369739532
epoch£º152	 i:9 	 global-step:3049	 l-p:0.11511456221342087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7660, 2.7615, 2.7655],
        [2.7660, 2.7659, 2.7660],
        [2.7660, 2.7551, 2.7638],
        [2.7660, 2.7660, 2.7660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.13458575308322906 
model_pd.l_d.mean(): -24.24484634399414 
model_pd.lagr.mean(): -24.110260009765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0515], device='cuda:0')), ('power', tensor([-24.1933], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.13458575308322906
epoch£º153	 i:1 	 global-step:3061	 l-p:0.11530910432338715
epoch£º153	 i:2 	 global-step:3062	 l-p:0.163874551653862
epoch£º153	 i:3 	 global-step:3063	 l-p:0.1073332354426384
epoch£º153	 i:4 	 global-step:3064	 l-p:0.14869651198387146
epoch£º153	 i:5 	 global-step:3065	 l-p:0.16824497282505035
epoch£º153	 i:6 	 global-step:3066	 l-p:0.1827736794948578
epoch£º153	 i:7 	 global-step:3067	 l-p:0.11238235235214233
epoch£º153	 i:8 	 global-step:3068	 l-p:0.10033942759037018
epoch£º153	 i:9 	 global-step:3069	 l-p:0.11891590058803558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0407, 3.0992, 3.0252],
        [3.0407, 2.9906, 2.9730],
        [3.0407, 3.0213, 3.0334],
        [3.0407, 3.0360, 3.0402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.13865576684474945 
model_pd.l_d.mean(): -23.917938232421875 
model_pd.lagr.mean(): -23.779281616210938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1058], device='cuda:0')), ('power', tensor([-23.8121], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.13865576684474945
epoch£º154	 i:1 	 global-step:3081	 l-p:0.10927135497331619
epoch£º154	 i:2 	 global-step:3082	 l-p:0.1303989291191101
epoch£º154	 i:3 	 global-step:3083	 l-p:0.11866581439971924
epoch£º154	 i:4 	 global-step:3084	 l-p:0.11688865721225739
epoch£º154	 i:5 	 global-step:3085	 l-p:0.11715450137853622
epoch£º154	 i:6 	 global-step:3086	 l-p:0.1231016144156456
epoch£º154	 i:7 	 global-step:3087	 l-p:0.10149749368429184
epoch£º154	 i:8 	 global-step:3088	 l-p:0.12204490602016449
epoch£º154	 i:9 	 global-step:3089	 l-p:0.17836803197860718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6764, 2.6523, 2.5518],
        [2.6764, 2.6291, 2.5267],
        [2.6764, 2.6391, 2.6577],
        [2.6764, 2.5911, 2.5203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.14385147392749786 
model_pd.l_d.mean(): -24.521394729614258 
model_pd.lagr.mean(): -24.37754249572754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0020], device='cuda:0')), ('power', tensor([-24.5234], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.14385147392749786
epoch£º155	 i:1 	 global-step:3101	 l-p:0.1970621794462204
epoch£º155	 i:2 	 global-step:3102	 l-p:0.05930149927735329
epoch£º155	 i:3 	 global-step:3103	 l-p:0.1299286186695099
epoch£º155	 i:4 	 global-step:3104	 l-p:0.056598857045173645
epoch£º155	 i:5 	 global-step:3105	 l-p:0.1424608826637268
epoch£º155	 i:6 	 global-step:3106	 l-p:0.10722985863685608
epoch£º155	 i:7 	 global-step:3107	 l-p:0.13136236369609833
epoch£º155	 i:8 	 global-step:3108	 l-p:0.1253858357667923
epoch£º155	 i:9 	 global-step:3109	 l-p:0.27285534143447876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[2.7111, 2.8724, 2.8634],
        [2.7111, 2.8821, 2.8798],
        [2.7111, 2.6318, 2.6174],
        [2.7111, 2.8710, 2.8611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.08024314790964127 
model_pd.l_d.mean(): -24.513517379760742 
model_pd.lagr.mean(): -24.433273315429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0057], device='cuda:0')), ('power', tensor([-24.5079], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.08024314790964127
epoch£º156	 i:1 	 global-step:3121	 l-p:0.1478944718837738
epoch£º156	 i:2 	 global-step:3122	 l-p:0.15384356677532196
epoch£º156	 i:3 	 global-step:3123	 l-p:0.14433106780052185
epoch£º156	 i:4 	 global-step:3124	 l-p:0.12184745818376541
epoch£º156	 i:5 	 global-step:3125	 l-p:0.16476857662200928
epoch£º156	 i:6 	 global-step:3126	 l-p:0.11904317885637283
epoch£º156	 i:7 	 global-step:3127	 l-p:0.11889221519231796
epoch£º156	 i:8 	 global-step:3128	 l-p:0.0786881074309349
epoch£º156	 i:9 	 global-step:3129	 l-p:0.13320952653884888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[3.1049, 3.0661, 3.0763],
        [3.1049, 3.1183, 3.0312],
        [3.1049, 3.2826, 3.2652],
        [3.1049, 3.0547, 3.0062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.08858931064605713 
model_pd.l_d.mean(): -24.538299560546875 
model_pd.lagr.mean(): -24.449710845947266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1861], device='cuda:0')), ('power', tensor([-24.3522], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.08858931064605713
epoch£º157	 i:1 	 global-step:3141	 l-p:0.14065730571746826
epoch£º157	 i:2 	 global-step:3142	 l-p:0.12416303157806396
epoch£º157	 i:3 	 global-step:3143	 l-p:0.10056141018867493
epoch£º157	 i:4 	 global-step:3144	 l-p:0.12212272733449936
epoch£º157	 i:5 	 global-step:3145	 l-p:0.260974258184433
epoch£º157	 i:6 	 global-step:3146	 l-p:0.09060665965080261
epoch£º157	 i:7 	 global-step:3147	 l-p:0.1289590746164322
epoch£º157	 i:8 	 global-step:3148	 l-p:0.14043128490447998
epoch£º157	 i:9 	 global-step:3149	 l-p:0.15287470817565918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9133, 2.9132, 2.9133],
        [2.9133, 2.9032, 2.9114],
        [2.9133, 2.9132, 2.9133],
        [2.9133, 2.8671, 2.7757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.1279551237821579 
model_pd.l_d.mean(): -24.3311710357666 
model_pd.lagr.mean(): -24.203216552734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1178], device='cuda:0')), ('power', tensor([-24.2134], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.1279551237821579
epoch£º158	 i:1 	 global-step:3161	 l-p:0.16412633657455444
epoch£º158	 i:2 	 global-step:3162	 l-p:0.10087089240550995
epoch£º158	 i:3 	 global-step:3163	 l-p:0.2311728298664093
epoch£º158	 i:4 	 global-step:3164	 l-p:0.13436561822891235
epoch£º158	 i:5 	 global-step:3165	 l-p:0.1345207691192627
epoch£º158	 i:6 	 global-step:3166	 l-p:0.5327295064926147
epoch£º158	 i:7 	 global-step:3167	 l-p:0.12476728111505508
epoch£º158	 i:8 	 global-step:3168	 l-p:0.12066134810447693
epoch£º158	 i:9 	 global-step:3169	 l-p:0.11796407401561737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9972, 2.9329, 2.9170],
        [2.9972, 2.9522, 2.9649],
        [2.9972, 2.9386, 2.9370],
        [2.9972, 2.9664, 2.9824]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.13655544817447662 
model_pd.l_d.mean(): -24.705915451049805 
model_pd.lagr.mean(): -24.569360733032227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1768], device='cuda:0')), ('power', tensor([-24.5291], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.13655544817447662
epoch£º159	 i:1 	 global-step:3181	 l-p:0.14653116464614868
epoch£º159	 i:2 	 global-step:3182	 l-p:0.057025451213121414
epoch£º159	 i:3 	 global-step:3183	 l-p:0.13726364076137543
epoch£º159	 i:4 	 global-step:3184	 l-p:0.11155227571725845
epoch£º159	 i:5 	 global-step:3185	 l-p:0.11955269426107407
epoch£º159	 i:6 	 global-step:3186	 l-p:0.13103997707366943
epoch£º159	 i:7 	 global-step:3187	 l-p:0.13257671892642975
epoch£º159	 i:8 	 global-step:3188	 l-p:0.12878766655921936
epoch£º159	 i:9 	 global-step:3189	 l-p:0.1644703447818756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7807, 2.7296, 2.7474],
        [2.7807, 2.7910, 2.6924],
        [2.7807, 2.7807, 2.7807],
        [2.7807, 2.7797, 2.7807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.13490866124629974 
model_pd.l_d.mean(): -24.97587013244629 
model_pd.lagr.mean(): -24.840961456298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1299], device='cuda:0')), ('power', tensor([-24.8459], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.13490866124629974
epoch£º160	 i:1 	 global-step:3201	 l-p:0.13372763991355896
epoch£º160	 i:2 	 global-step:3202	 l-p:0.1720467507839203
epoch£º160	 i:3 	 global-step:3203	 l-p:0.11612485349178314
epoch£º160	 i:4 	 global-step:3204	 l-p:0.17882785201072693
epoch£º160	 i:5 	 global-step:3205	 l-p:0.11273133754730225
epoch£º160	 i:6 	 global-step:3206	 l-p:0.09070760756731033
epoch£º160	 i:7 	 global-step:3207	 l-p:0.22569839656352997
epoch£º160	 i:8 	 global-step:3208	 l-p:0.110323466360569
epoch£º160	 i:9 	 global-step:3209	 l-p:0.13336162269115448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[3.3733, 3.3298, 3.3100],
        [3.3733, 3.3375, 3.3439],
        [3.3733, 3.4635, 3.3928],
        [3.3733, 3.4989, 3.4428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.11223361641168594 
model_pd.l_d.mean(): -24.781309127807617 
model_pd.lagr.mean(): -24.66907501220703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3972], device='cuda:0')), ('power', tensor([-24.3841], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.11223361641168594
epoch£º161	 i:1 	 global-step:3221	 l-p:0.11374716460704803
epoch£º161	 i:2 	 global-step:3222	 l-p:0.10687494277954102
epoch£º161	 i:3 	 global-step:3223	 l-p:0.1183057650923729
epoch£º161	 i:4 	 global-step:3224	 l-p:0.08809883147478104
epoch£º161	 i:5 	 global-step:3225	 l-p:0.11448756605386734
epoch£º161	 i:6 	 global-step:3226	 l-p:0.16643482446670532
epoch£º161	 i:7 	 global-step:3227	 l-p:0.15295355021953583
epoch£º161	 i:8 	 global-step:3228	 l-p:0.08163318783044815
epoch£º161	 i:9 	 global-step:3229	 l-p:0.13066689670085907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8276, 2.7818, 2.6693],
        [2.8276, 2.8266, 2.8276],
        [2.8276, 2.7600, 2.7684],
        [2.8276, 2.8206, 2.7129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.13951532542705536 
model_pd.l_d.mean(): -24.66037940979004 
model_pd.lagr.mean(): -24.520864486694336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0726], device='cuda:0')), ('power', tensor([-24.5878], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.13951532542705536
epoch£º162	 i:1 	 global-step:3241	 l-p:-0.03777481988072395
epoch£º162	 i:2 	 global-step:3242	 l-p:0.11341865360736847
epoch£º162	 i:3 	 global-step:3243	 l-p:0.1313542127609253
epoch£º162	 i:4 	 global-step:3244	 l-p:0.1534118503332138
epoch£º162	 i:5 	 global-step:3245	 l-p:0.07393019646406174
epoch£º162	 i:6 	 global-step:3246	 l-p:0.1391201764345169
epoch£º162	 i:7 	 global-step:3247	 l-p:-0.6310110092163086
epoch£º162	 i:8 	 global-step:3248	 l-p:0.168278306722641
epoch£º162	 i:9 	 global-step:3249	 l-p:1.8129501342773438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9069, 3.0004, 2.9340],
        [2.9069, 2.9069, 2.9069],
        [2.9069, 2.8273, 2.7452],
        [2.9069, 2.9069, 2.9069]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.11181065440177917 
model_pd.l_d.mean(): -23.829866409301758 
model_pd.lagr.mean(): -23.718055725097656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0070], device='cuda:0')), ('power', tensor([-23.8229], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.11181065440177917
epoch£º163	 i:1 	 global-step:3261	 l-p:0.10062631964683533
epoch£º163	 i:2 	 global-step:3262	 l-p:0.1326311081647873
epoch£º163	 i:3 	 global-step:3263	 l-p:0.13938069343566895
epoch£º163	 i:4 	 global-step:3264	 l-p:0.11461154371500015
epoch£º163	 i:5 	 global-step:3265	 l-p:0.12476956844329834
epoch£º163	 i:6 	 global-step:3266	 l-p:0.12335839867591858
epoch£º163	 i:7 	 global-step:3267	 l-p:0.1183844730257988
epoch£º163	 i:8 	 global-step:3268	 l-p:0.11050190031528473
epoch£º163	 i:9 	 global-step:3269	 l-p:0.1359025537967682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6649, 2.5526, 2.5038],
        [2.6649, 2.6577, 2.6640],
        [2.6649, 2.6632, 2.6648],
        [2.6649, 2.6338, 2.6537]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.14586244523525238 
model_pd.l_d.mean(): -24.63970375061035 
model_pd.lagr.mean(): -24.49384117126465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0035], device='cuda:0')), ('power', tensor([-24.6432], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.14586244523525238
epoch£º164	 i:1 	 global-step:3281	 l-p:0.12802249193191528
epoch£º164	 i:2 	 global-step:3282	 l-p:0.10931943356990814
epoch£º164	 i:3 	 global-step:3283	 l-p:0.04332192987203598
epoch£º164	 i:4 	 global-step:3284	 l-p:0.17536544799804688
epoch£º164	 i:5 	 global-step:3285	 l-p:0.12634985148906708
epoch£º164	 i:6 	 global-step:3286	 l-p:0.1282704770565033
epoch£º164	 i:7 	 global-step:3287	 l-p:0.1301019936800003
epoch£º164	 i:8 	 global-step:3288	 l-p:0.1252160370349884
epoch£º164	 i:9 	 global-step:3289	 l-p:0.35650503635406494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8556, 2.8556, 2.8556],
        [2.8556, 2.8842, 2.7845],
        [2.8556, 2.8077, 2.6893],
        [2.8556, 2.7609, 2.7243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.0584719181060791 
model_pd.l_d.mean(): -24.62177848815918 
model_pd.lagr.mean(): -24.56330680847168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0954], device='cuda:0')), ('power', tensor([-24.5263], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.0584719181060791
epoch£º165	 i:1 	 global-step:3301	 l-p:0.09743743389844894
epoch£º165	 i:2 	 global-step:3302	 l-p:0.14563895761966705
epoch£º165	 i:3 	 global-step:3303	 l-p:0.12618805468082428
epoch£º165	 i:4 	 global-step:3304	 l-p:0.1515481024980545
epoch£º165	 i:5 	 global-step:3305	 l-p:0.12577609717845917
epoch£º165	 i:6 	 global-step:3306	 l-p:0.11160387098789215
epoch£º165	 i:7 	 global-step:3307	 l-p:0.023507820442318916
epoch£º165	 i:8 	 global-step:3308	 l-p:0.14596126973628998
epoch£º165	 i:9 	 global-step:3309	 l-p:0.12157116830348969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9014, 2.8983, 2.9012],
        [2.9014, 2.8382, 2.8527],
        [2.9014, 2.8336, 2.8448],
        [2.9014, 2.9014, 2.9014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.07758939266204834 
model_pd.l_d.mean(): -24.05643081665039 
model_pd.lagr.mean(): -23.97884178161621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0467], device='cuda:0')), ('power', tensor([-24.0097], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.07758939266204834
epoch£º166	 i:1 	 global-step:3321	 l-p:0.1444966048002243
epoch£º166	 i:2 	 global-step:3322	 l-p:0.1262994110584259
epoch£º166	 i:3 	 global-step:3323	 l-p:0.09468328207731247
epoch£º166	 i:4 	 global-step:3324	 l-p:0.10754002630710602
epoch£º166	 i:5 	 global-step:3325	 l-p:0.15239016711711884
epoch£º166	 i:6 	 global-step:3326	 l-p:0.12319232523441315
epoch£º166	 i:7 	 global-step:3327	 l-p:0.1415308117866516
epoch£º166	 i:8 	 global-step:3328	 l-p:0.12230530381202698
epoch£º166	 i:9 	 global-step:3329	 l-p:0.2015155404806137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9756, 2.9756, 2.9756],
        [2.9756, 2.8887, 2.8672],
        [2.9756, 3.2182, 3.2399],
        [2.9756, 2.9581, 2.9713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.12471786886453629 
model_pd.l_d.mean(): -24.269390106201172 
model_pd.lagr.mean(): -24.144672393798828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1153], device='cuda:0')), ('power', tensor([-24.1541], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.12471786886453629
epoch£º167	 i:1 	 global-step:3341	 l-p:0.10509584099054337
epoch£º167	 i:2 	 global-step:3342	 l-p:0.17294079065322876
epoch£º167	 i:3 	 global-step:3343	 l-p:0.11950603872537613
epoch£º167	 i:4 	 global-step:3344	 l-p:0.009274467825889587
epoch£º167	 i:5 	 global-step:3345	 l-p:0.12584321200847626
epoch£º167	 i:6 	 global-step:3346	 l-p:0.13544665277004242
epoch£º167	 i:7 	 global-step:3347	 l-p:0.12475032359361649
epoch£º167	 i:8 	 global-step:3348	 l-p:0.14036570489406586
epoch£º167	 i:9 	 global-step:3349	 l-p:0.10769139975309372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9468, 2.9455, 2.9468],
        [2.9468, 3.1188, 3.0914],
        [2.9468, 2.9468, 2.9468],
        [2.9468, 2.9468, 2.9468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.13437321782112122 
model_pd.l_d.mean(): -23.915746688842773 
model_pd.lagr.mean(): -23.781373977661133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0469], device='cuda:0')), ('power', tensor([-23.8689], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.13437321782112122
epoch£º168	 i:1 	 global-step:3361	 l-p:0.12272821366786957
epoch£º168	 i:2 	 global-step:3362	 l-p:0.12209130823612213
epoch£º168	 i:3 	 global-step:3363	 l-p:0.1308441460132599
epoch£º168	 i:4 	 global-step:3364	 l-p:0.16083736717700958
epoch£º168	 i:5 	 global-step:3365	 l-p:0.15786896646022797
epoch£º168	 i:6 	 global-step:3366	 l-p:0.1016758531332016
epoch£º168	 i:7 	 global-step:3367	 l-p:0.15982036292552948
epoch£º168	 i:8 	 global-step:3368	 l-p:0.11864417791366577
epoch£º168	 i:9 	 global-step:3369	 l-p:0.11425977200269699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0496, 3.0491, 3.0496],
        [3.0496, 3.0221, 3.0400],
        [3.0496, 2.9642, 2.9473],
        [3.0496, 2.9950, 3.0133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.10679597407579422 
model_pd.l_d.mean(): -24.83034896850586 
model_pd.lagr.mean(): -24.723552703857422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2384], device='cuda:0')), ('power', tensor([-24.5919], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.10679597407579422
epoch£º169	 i:1 	 global-step:3381	 l-p:0.1217322126030922
epoch£º169	 i:2 	 global-step:3382	 l-p:0.14492030441761017
epoch£º169	 i:3 	 global-step:3383	 l-p:0.1384022980928421
epoch£º169	 i:4 	 global-step:3384	 l-p:0.10684539377689362
epoch£º169	 i:5 	 global-step:3385	 l-p:0.14660681784152985
epoch£º169	 i:6 	 global-step:3386	 l-p:0.20486807823181152
epoch£º169	 i:7 	 global-step:3387	 l-p:0.25269776582717896
epoch£º169	 i:8 	 global-step:3388	 l-p:0.12829641997814178
epoch£º169	 i:9 	 global-step:3389	 l-p:0.1367146521806717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9022, 2.7927, 2.7460],
        [2.9022, 2.9002, 2.9021],
        [2.9022, 2.8665, 2.8881],
        [2.9022, 2.9007, 2.9021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.13242562115192413 
model_pd.l_d.mean(): -23.99182891845703 
model_pd.lagr.mean(): -23.859403610229492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0219], device='cuda:0')), ('power', tensor([-23.9699], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.13242562115192413
epoch£º170	 i:1 	 global-step:3401	 l-p:0.19019626080989838
epoch£º170	 i:2 	 global-step:3402	 l-p:0.12907157838344574
epoch£º170	 i:3 	 global-step:3403	 l-p:0.1323421597480774
epoch£º170	 i:4 	 global-step:3404	 l-p:0.12214092910289764
epoch£º170	 i:5 	 global-step:3405	 l-p:0.13526475429534912
epoch£º170	 i:6 	 global-step:3406	 l-p:0.07748062163591385
epoch£º170	 i:7 	 global-step:3407	 l-p:-0.6763768792152405
epoch£º170	 i:8 	 global-step:3408	 l-p:0.23717127740383148
epoch£º170	 i:9 	 global-step:3409	 l-p:0.09297527372837067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8641, 2.7777, 2.6427],
        [2.8641, 2.8608, 2.8639],
        [2.8641, 2.8250, 2.8480],
        [2.8641, 2.8640, 2.8641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.11308092623949051 
model_pd.l_d.mean(): -23.933088302612305 
model_pd.lagr.mean(): -23.82000732421875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0079], device='cuda:0')), ('power', tensor([-23.9410], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.11308092623949051
epoch£º171	 i:1 	 global-step:3421	 l-p:0.11118321865797043
epoch£º171	 i:2 	 global-step:3422	 l-p:0.13133446872234344
epoch£º171	 i:3 	 global-step:3423	 l-p:0.13902528584003448
epoch£º171	 i:4 	 global-step:3424	 l-p:-0.028954433277249336
epoch£º171	 i:5 	 global-step:3425	 l-p:-0.050092075020074844
epoch£º171	 i:6 	 global-step:3426	 l-p:0.11445145308971405
epoch£º171	 i:7 	 global-step:3427	 l-p:0.1259724646806717
epoch£º171	 i:8 	 global-step:3428	 l-p:0.12968668341636658
epoch£º171	 i:9 	 global-step:3429	 l-p:0.14012639224529266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0249, 3.0233, 3.0248],
        [3.0249, 3.0125, 3.0227],
        [3.0249, 2.9201, 2.8390],
        [3.0249, 3.0249, 3.0249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.13082723319530487 
model_pd.l_d.mean(): -24.861068725585938 
model_pd.lagr.mean(): -24.730241775512695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2281], device='cuda:0')), ('power', tensor([-24.6330], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.13082723319530487
epoch£º172	 i:1 	 global-step:3441	 l-p:0.15845723450183868
epoch£º172	 i:2 	 global-step:3442	 l-p:0.11027736216783524
epoch£º172	 i:3 	 global-step:3443	 l-p:0.12808988988399506
epoch£º172	 i:4 	 global-step:3444	 l-p:-0.021981477737426758
epoch£º172	 i:5 	 global-step:3445	 l-p:0.12982115149497986
epoch£º172	 i:6 	 global-step:3446	 l-p:0.14444495737552643
epoch£º172	 i:7 	 global-step:3447	 l-p:0.14466892182826996
epoch£º172	 i:8 	 global-step:3448	 l-p:0.20053988695144653
epoch£º172	 i:9 	 global-step:3449	 l-p:0.12950101494789124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[2.7084, 2.7834, 2.6961],
        [2.7084, 2.6070, 2.6179],
        [2.7084, 2.6294, 2.4799],
        [2.7084, 2.6008, 2.6059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.11443810909986496 
model_pd.l_d.mean(): -24.327960968017578 
model_pd.lagr.mean(): -24.21352195739746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0625], device='cuda:0')), ('power', tensor([-24.3904], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.11443810909986496
epoch£º173	 i:1 	 global-step:3461	 l-p:0.1619817316532135
epoch£º173	 i:2 	 global-step:3462	 l-p:0.14234808087348938
epoch£º173	 i:3 	 global-step:3463	 l-p:0.1301151067018509
epoch£º173	 i:4 	 global-step:3464	 l-p:0.1298520416021347
epoch£º173	 i:5 	 global-step:3465	 l-p:0.14378659427165985
epoch£º173	 i:6 	 global-step:3466	 l-p:0.08148874342441559
epoch£º173	 i:7 	 global-step:3467	 l-p:0.12088876217603683
epoch£º173	 i:8 	 global-step:3468	 l-p:0.11581097543239594
epoch£º173	 i:9 	 global-step:3469	 l-p:0.14507637917995453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0446, 3.1164, 3.0184],
        [3.0446, 3.0179, 3.0364],
        [3.0446, 2.9635, 2.9742],
        [3.0446, 2.9418, 2.9196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.10777541995048523 
model_pd.l_d.mean(): -24.607885360717773 
model_pd.lagr.mean(): -24.500110626220703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1762], device='cuda:0')), ('power', tensor([-24.4317], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.10777541995048523
epoch£º174	 i:1 	 global-step:3481	 l-p:0.13195863366127014
epoch£º174	 i:2 	 global-step:3482	 l-p:0.10456398129463196
epoch£º174	 i:3 	 global-step:3483	 l-p:0.1170692965388298
epoch£º174	 i:4 	 global-step:3484	 l-p:0.14466455578804016
epoch£º174	 i:5 	 global-step:3485	 l-p:-0.09167893975973129
epoch£º174	 i:6 	 global-step:3486	 l-p:0.14681503176689148
epoch£º174	 i:7 	 global-step:3487	 l-p:0.1378973126411438
epoch£º174	 i:8 	 global-step:3488	 l-p:0.10395698994398117
epoch£º174	 i:9 	 global-step:3489	 l-p:0.07265579700469971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7348, 2.7283, 2.7342],
        [2.7348, 2.7115, 2.7292],
        [2.7348, 2.6611, 2.6890],
        [2.7348, 2.6385, 2.6571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.12996280193328857 
model_pd.l_d.mean(): -24.897781372070312 
model_pd.lagr.mean(): -24.767818450927734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0987], device='cuda:0')), ('power', tensor([-24.7991], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.12996280193328857
epoch£º175	 i:1 	 global-step:3501	 l-p:0.12988851964473724
epoch£º175	 i:2 	 global-step:3502	 l-p:0.1380438357591629
epoch£º175	 i:3 	 global-step:3503	 l-p:0.10106812417507172
epoch£º175	 i:4 	 global-step:3504	 l-p:0.17839480936527252
epoch£º175	 i:5 	 global-step:3505	 l-p:0.19024425745010376
epoch£º175	 i:6 	 global-step:3506	 l-p:0.06469012796878815
epoch£º175	 i:7 	 global-step:3507	 l-p:0.09505359083414078
epoch£º175	 i:8 	 global-step:3508	 l-p:0.19788715243339539
epoch£º175	 i:9 	 global-step:3509	 l-p:-0.0028032874688506126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4018, 3.4018, 3.4018],
        [3.4018, 3.3719, 3.3905],
        [3.4018, 3.4147, 3.2947],
        [3.4018, 3.3174, 3.2526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.11816314607858658 
model_pd.l_d.mean(): -24.131269454956055 
model_pd.lagr.mean(): -24.013105392456055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2838], device='cuda:0')), ('power', tensor([-23.8474], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.11816314607858658
epoch£º176	 i:1 	 global-step:3521	 l-p:0.10930627584457397
epoch£º176	 i:2 	 global-step:3522	 l-p:0.10435520112514496
epoch£º176	 i:3 	 global-step:3523	 l-p:0.10713985562324524
epoch£º176	 i:4 	 global-step:3524	 l-p:0.08715511858463287
epoch£º176	 i:5 	 global-step:3525	 l-p:0.10983889549970627
epoch£º176	 i:6 	 global-step:3526	 l-p:0.12767893075942993
epoch£º176	 i:7 	 global-step:3527	 l-p:0.10871701687574387
epoch£º176	 i:8 	 global-step:3528	 l-p:0.00894552655518055
epoch£º176	 i:9 	 global-step:3529	 l-p:0.06884346902370453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1572, 3.3941, 3.3881],
        [3.1572, 3.1552, 3.1571],
        [3.1572, 3.0622, 3.0570],
        [3.1572, 3.1562, 3.1571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.08785123378038406 
model_pd.l_d.mean(): -24.4017391204834 
model_pd.lagr.mean(): -24.313888549804688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2048], device='cuda:0')), ('power', tensor([-24.1970], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.08785123378038406
epoch£º177	 i:1 	 global-step:3541	 l-p:0.1314653903245926
epoch£º177	 i:2 	 global-step:3542	 l-p:0.03806738182902336
epoch£º177	 i:3 	 global-step:3543	 l-p:0.12546443939208984
epoch£º177	 i:4 	 global-step:3544	 l-p:0.22775563597679138
epoch£º177	 i:5 	 global-step:3545	 l-p:0.2160254567861557
epoch£º177	 i:6 	 global-step:3546	 l-p:0.12385568022727966
epoch£º177	 i:7 	 global-step:3547	 l-p:-0.045622628182172775
epoch£º177	 i:8 	 global-step:3548	 l-p:0.12674294412136078
epoch£º177	 i:9 	 global-step:3549	 l-p:0.10318687558174133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8968, 2.8961, 2.8967],
        [2.8968, 2.8967, 2.8968],
        [2.8968, 2.8968, 2.8968],
        [2.8968, 2.8046, 2.8231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.14283247292041779 
model_pd.l_d.mean(): -23.802013397216797 
model_pd.lagr.mean(): -23.659181594848633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0218], device='cuda:0')), ('power', tensor([-23.7802], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.14283247292041779
epoch£º178	 i:1 	 global-step:3561	 l-p:0.1329958736896515
epoch£º178	 i:2 	 global-step:3562	 l-p:0.13013388216495514
epoch£º178	 i:3 	 global-step:3563	 l-p:0.042695846408605576
epoch£º178	 i:4 	 global-step:3564	 l-p:0.13718348741531372
epoch£º178	 i:5 	 global-step:3565	 l-p:0.1357482373714447
epoch£º178	 i:6 	 global-step:3566	 l-p:0.07216282933950424
epoch£º178	 i:7 	 global-step:3567	 l-p:0.2242811918258667
epoch£º178	 i:8 	 global-step:3568	 l-p:0.05919821560382843
epoch£º178	 i:9 	 global-step:3569	 l-p:0.1595904380083084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7131, 2.7091, 2.7129],
        [2.7131, 2.6634, 2.6928],
        [2.7131, 2.6654, 2.6943],
        [2.7131, 2.7114, 2.7131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.13487893342971802 
model_pd.l_d.mean(): -24.85274887084961 
model_pd.lagr.mean(): -24.717870712280273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0429], device='cuda:0')), ('power', tensor([-24.8099], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.13487893342971802
epoch£º179	 i:1 	 global-step:3581	 l-p:0.12901858985424042
epoch£º179	 i:2 	 global-step:3582	 l-p:0.19849227368831635
epoch£º179	 i:3 	 global-step:3583	 l-p:0.08870764821767807
epoch£º179	 i:4 	 global-step:3584	 l-p:0.15181337296962738
epoch£º179	 i:5 	 global-step:3585	 l-p:0.1307561993598938
epoch£º179	 i:6 	 global-step:3586	 l-p:0.06453515589237213
epoch£º179	 i:7 	 global-step:3587	 l-p:0.10449586808681488
epoch£º179	 i:8 	 global-step:3588	 l-p:0.12187433242797852
epoch£º179	 i:9 	 global-step:3589	 l-p:0.11338433623313904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2765, 3.2740, 3.2763],
        [3.2765, 3.2747, 3.2764],
        [3.2765, 3.1657, 3.1037],
        [3.2765, 3.2765, 3.2765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.11607209593057632 
model_pd.l_d.mean(): -24.85233497619629 
model_pd.lagr.mean(): -24.736263275146484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3437], device='cuda:0')), ('power', tensor([-24.5086], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.11607209593057632
epoch£º180	 i:1 	 global-step:3601	 l-p:0.12274541705846786
epoch£º180	 i:2 	 global-step:3602	 l-p:0.056499872356653214
epoch£º180	 i:3 	 global-step:3603	 l-p:0.1372556984424591
epoch£º180	 i:4 	 global-step:3604	 l-p:0.1646205335855484
epoch£º180	 i:5 	 global-step:3605	 l-p:0.11689010262489319
epoch£º180	 i:6 	 global-step:3606	 l-p:0.14057499170303345
epoch£º180	 i:7 	 global-step:3607	 l-p:0.1388830691576004
epoch£º180	 i:8 	 global-step:3608	 l-p:0.10626091063022614
epoch£º180	 i:9 	 global-step:3609	 l-p:0.14728820323944092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8291, 2.8095, 2.8252],
        [2.8291, 2.7227, 2.7412],
        [2.8291, 2.6716, 2.5208],
        [2.8291, 2.7409, 2.5687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.1363667994737625 
model_pd.l_d.mean(): -24.784883499145508 
model_pd.lagr.mean(): -24.648517608642578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0940], device='cuda:0')), ('power', tensor([-24.6908], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.1363667994737625
epoch£º181	 i:1 	 global-step:3621	 l-p:0.13039717078208923
epoch£º181	 i:2 	 global-step:3622	 l-p:0.13811032474040985
epoch£º181	 i:3 	 global-step:3623	 l-p:0.14277704060077667
epoch£º181	 i:4 	 global-step:3624	 l-p:0.14169447124004364
epoch£º181	 i:5 	 global-step:3625	 l-p:-0.003328847698867321
epoch£º181	 i:6 	 global-step:3626	 l-p:0.12846532464027405
epoch£º181	 i:7 	 global-step:3627	 l-p:0.11350856721401215
epoch£º181	 i:8 	 global-step:3628	 l-p:0.1301155686378479
epoch£º181	 i:9 	 global-step:3629	 l-p:0.07939635217189789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9406, 2.9174, 2.9353],
        [2.9406, 2.8562, 2.8836],
        [2.9406, 2.8078, 2.7929],
        [2.9406, 2.9406, 2.9406]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.11567289382219315 
model_pd.l_d.mean(): -24.681703567504883 
model_pd.lagr.mean(): -24.566030502319336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1614], device='cuda:0')), ('power', tensor([-24.5203], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.11567289382219315
epoch£º182	 i:1 	 global-step:3641	 l-p:0.17858833074569702
epoch£º182	 i:2 	 global-step:3642	 l-p:0.1308109313249588
epoch£º182	 i:3 	 global-step:3643	 l-p:0.12833191454410553
epoch£º182	 i:4 	 global-step:3644	 l-p:0.12021730095148087
epoch£º182	 i:5 	 global-step:3645	 l-p:0.13461221754550934
epoch£º182	 i:6 	 global-step:3646	 l-p:0.0981571152806282
epoch£º182	 i:7 	 global-step:3647	 l-p:0.12377161532640457
epoch£º182	 i:8 	 global-step:3648	 l-p:0.05740024521946907
epoch£º182	 i:9 	 global-step:3649	 l-p:0.09158994257450104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9984, 2.8437, 2.7217],
        [2.9984, 2.8617, 2.8384],
        [2.9984, 2.9763, 2.9935],
        [2.9984, 2.9534, 2.7875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.15508303046226501 
model_pd.l_d.mean(): -24.712425231933594 
model_pd.lagr.mean(): -24.557342529296875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1701], device='cuda:0')), ('power', tensor([-24.5423], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.15508303046226501
epoch£º183	 i:1 	 global-step:3661	 l-p:0.09747646003961563
epoch£º183	 i:2 	 global-step:3662	 l-p:0.1271008402109146
epoch£º183	 i:3 	 global-step:3663	 l-p:0.14532968401908875
epoch£º183	 i:4 	 global-step:3664	 l-p:0.12356806546449661
epoch£º183	 i:5 	 global-step:3665	 l-p:0.11238846182823181
epoch£º183	 i:6 	 global-step:3666	 l-p:0.12454402446746826
epoch£º183	 i:7 	 global-step:3667	 l-p:0.12953928112983704
epoch£º183	 i:8 	 global-step:3668	 l-p:0.468737930059433
epoch£º183	 i:9 	 global-step:3669	 l-p:0.10948926955461502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6740, 2.5529, 2.5763],
        [2.6740, 2.5738, 2.3854],
        [2.6740, 2.5983, 2.6342],
        [2.6740, 2.6740, 2.6740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.18231551349163055 
model_pd.l_d.mean(): -24.13653564453125 
model_pd.lagr.mean(): -23.954219818115234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0693], device='cuda:0')), ('power', tensor([-24.2058], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.18231551349163055
epoch£º184	 i:1 	 global-step:3681	 l-p:0.12416013330221176
epoch£º184	 i:2 	 global-step:3682	 l-p:0.1289847195148468
epoch£º184	 i:3 	 global-step:3683	 l-p:0.12391558289527893
epoch£º184	 i:4 	 global-step:3684	 l-p:0.129205122590065
epoch£º184	 i:5 	 global-step:3685	 l-p:0.04321727529168129
epoch£º184	 i:6 	 global-step:3686	 l-p:0.13261942565441132
epoch£º184	 i:7 	 global-step:3687	 l-p:0.013982128351926804
epoch£º184	 i:8 	 global-step:3688	 l-p:0.014993619173765182
epoch£º184	 i:9 	 global-step:3689	 l-p:0.1351575255393982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8839, 2.7375, 2.7245],
        [2.8839, 2.8839, 2.8839],
        [2.8839, 2.8425, 2.8700],
        [2.8839, 2.8131, 2.6319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.09876186400651932 
model_pd.l_d.mean(): -24.287324905395508 
model_pd.lagr.mean(): -24.188562393188477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0315], device='cuda:0')), ('power', tensor([-24.2558], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.09876186400651932
epoch£º185	 i:1 	 global-step:3701	 l-p:0.20394107699394226
epoch£º185	 i:2 	 global-step:3702	 l-p:0.08674293011426926
epoch£º185	 i:3 	 global-step:3703	 l-p:0.15190324187278748
epoch£º185	 i:4 	 global-step:3704	 l-p:0.10942939668893814
epoch£º185	 i:5 	 global-step:3705	 l-p:0.10326115041971207
epoch£º185	 i:6 	 global-step:3706	 l-p:0.10959582030773163
epoch£º185	 i:7 	 global-step:3707	 l-p:0.11489490419626236
epoch£º185	 i:8 	 global-step:3708	 l-p:0.12018770724534988
epoch£º185	 i:9 	 global-step:3709	 l-p:0.12307746708393097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2501, 3.2501, 3.2501],
        [3.2501, 3.1289, 2.9861],
        [3.2501, 3.1202, 2.9910],
        [3.2501, 3.1312, 3.1200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.1313561052083969 
model_pd.l_d.mean(): -24.566543579101562 
model_pd.lagr.mean(): -24.43518829345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2451], device='cuda:0')), ('power', tensor([-24.3214], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.1313561052083969
epoch£º186	 i:1 	 global-step:3721	 l-p:0.12936221063137054
epoch£º186	 i:2 	 global-step:3722	 l-p:0.17589209973812103
epoch£º186	 i:3 	 global-step:3723	 l-p:0.14782460033893585
epoch£º186	 i:4 	 global-step:3724	 l-p:0.10480746626853943
epoch£º186	 i:5 	 global-step:3725	 l-p:0.10043125599622726
epoch£º186	 i:6 	 global-step:3726	 l-p:0.14101098477840424
epoch£º186	 i:7 	 global-step:3727	 l-p:0.14391548931598663
epoch£º186	 i:8 	 global-step:3728	 l-p:0.12939704954624176
epoch£º186	 i:9 	 global-step:3729	 l-p:0.18938496708869934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6460, 2.6457, 2.6460],
        [2.6460, 2.4531, 2.4053],
        [2.6460, 2.4264, 2.2937],
        [2.6460, 2.6459, 2.6460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.11082153022289276 
model_pd.l_d.mean(): -24.35075569152832 
model_pd.lagr.mean(): -24.23993492126465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0777], device='cuda:0')), ('power', tensor([-24.4285], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.11082153022289276
epoch£º187	 i:1 	 global-step:3741	 l-p:0.12773199379444122
epoch£º187	 i:2 	 global-step:3742	 l-p:0.11952149122953415
epoch£º187	 i:3 	 global-step:3743	 l-p:0.1334504634141922
epoch£º187	 i:4 	 global-step:3744	 l-p:0.1297205537557602
epoch£º187	 i:5 	 global-step:3745	 l-p:0.10556109249591827
epoch£º187	 i:6 	 global-step:3746	 l-p:0.07547260820865631
epoch£º187	 i:7 	 global-step:3747	 l-p:0.1405668407678604
epoch£º187	 i:8 	 global-step:3748	 l-p:-1.0563019514083862
epoch£º187	 i:9 	 global-step:3749	 l-p:0.11347983777523041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7328, 2.6242, 2.4222],
        [2.7328, 2.5981, 2.6178],
        [2.7328, 2.7327, 2.7328],
        [2.7328, 2.6198, 2.4168]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.09199386090040207 
model_pd.l_d.mean(): -24.903648376464844 
model_pd.lagr.mean(): -24.811655044555664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0577], device='cuda:0')), ('power', tensor([-24.8460], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.09199386090040207
epoch£º188	 i:1 	 global-step:3761	 l-p:0.07534394413232803
epoch£º188	 i:2 	 global-step:3762	 l-p:0.14405356347560883
epoch£º188	 i:3 	 global-step:3763	 l-p:0.11387042701244354
epoch£º188	 i:4 	 global-step:3764	 l-p:0.18648025393486023
epoch£º188	 i:5 	 global-step:3765	 l-p:0.09973257035017014
epoch£º188	 i:6 	 global-step:3766	 l-p:0.30080050230026245
epoch£º188	 i:7 	 global-step:3767	 l-p:0.21074767410755157
epoch£º188	 i:8 	 global-step:3768	 l-p:0.1202426627278328
epoch£º188	 i:9 	 global-step:3769	 l-p:0.10776279121637344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2870, 3.4314, 3.3419],
        [3.2870, 3.4403, 3.3559],
        [3.2870, 3.2869, 3.2870],
        [3.2870, 3.2870, 3.2870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.12723727524280548 
model_pd.l_d.mean(): -24.33335304260254 
model_pd.lagr.mean(): -24.20611572265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2533], device='cuda:0')), ('power', tensor([-24.0801], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.12723727524280548
epoch£º189	 i:1 	 global-step:3781	 l-p:0.1008068323135376
epoch£º189	 i:2 	 global-step:3782	 l-p:0.17196303606033325
epoch£º189	 i:3 	 global-step:3783	 l-p:0.11341065913438797
epoch£º189	 i:4 	 global-step:3784	 l-p:0.12224709987640381
epoch£º189	 i:5 	 global-step:3785	 l-p:0.12128766626119614
epoch£º189	 i:6 	 global-step:3786	 l-p:0.08508387207984924
epoch£º189	 i:7 	 global-step:3787	 l-p:0.11790784448385239
epoch£º189	 i:8 	 global-step:3788	 l-p:0.11397866904735565
epoch£º189	 i:9 	 global-step:3789	 l-p:0.11522657424211502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5734, 2.5734, 2.5734],
        [2.5734, 2.5734, 2.5734],
        [2.5734, 2.5734, 2.5734],
        [2.5734, 2.5245, 2.5574]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.11927393823862076 
model_pd.l_d.mean(): -24.735517501831055 
model_pd.lagr.mean(): -24.616243362426758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0531], device='cuda:0')), ('power', tensor([-24.7886], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.11927393823862076
epoch£º190	 i:1 	 global-step:3801	 l-p:0.1430528461933136
epoch£º190	 i:2 	 global-step:3802	 l-p:0.12602078914642334
epoch£º190	 i:3 	 global-step:3803	 l-p:0.15005406737327576
epoch£º190	 i:4 	 global-step:3804	 l-p:0.03456747159361839
epoch£º190	 i:5 	 global-step:3805	 l-p:0.09345008432865143
epoch£º190	 i:6 	 global-step:3806	 l-p:0.0036952304653823376
epoch£º190	 i:7 	 global-step:3807	 l-p:0.06592608243227005
epoch£º190	 i:8 	 global-step:3808	 l-p:0.12391570955514908
epoch£º190	 i:9 	 global-step:3809	 l-p:0.19837374985218048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3009, 3.4138, 3.3018],
        [3.3009, 3.2085, 3.2338],
        [3.3009, 3.3007, 3.3009],
        [3.3009, 3.3009, 3.3009]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.1233164444565773 
model_pd.l_d.mean(): -23.966337203979492 
model_pd.lagr.mean(): -23.843021392822266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1530], device='cuda:0')), ('power', tensor([-23.8134], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.1233164444565773
epoch£º191	 i:1 	 global-step:3821	 l-p:0.11015636473894119
epoch£º191	 i:2 	 global-step:3822	 l-p:0.1088767871260643
epoch£º191	 i:3 	 global-step:3823	 l-p:0.15020285546779633
epoch£º191	 i:4 	 global-step:3824	 l-p:0.10363784432411194
epoch£º191	 i:5 	 global-step:3825	 l-p:0.17000453174114227
epoch£º191	 i:6 	 global-step:3826	 l-p:0.12271147221326828
epoch£º191	 i:7 	 global-step:3827	 l-p:0.13049033284187317
epoch£º191	 i:8 	 global-step:3828	 l-p:0.10641735792160034
epoch£º191	 i:9 	 global-step:3829	 l-p:0.1253090500831604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1767, 3.1271, 3.1576],
        [3.1767, 3.1421, 3.1667],
        [3.1767, 3.2027, 3.0451],
        [3.1767, 3.1138, 3.1470]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.2094813883304596 
model_pd.l_d.mean(): -24.594337463378906 
model_pd.lagr.mean(): -24.384855270385742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2151], device='cuda:0')), ('power', tensor([-24.3792], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.2094813883304596
epoch£º192	 i:1 	 global-step:3841	 l-p:0.10973092168569565
epoch£º192	 i:2 	 global-step:3842	 l-p:0.1716134399175644
epoch£º192	 i:3 	 global-step:3843	 l-p:0.06427759677171707
epoch£º192	 i:4 	 global-step:3844	 l-p:0.1338021606206894
epoch£º192	 i:5 	 global-step:3845	 l-p:0.1317986100912094
epoch£º192	 i:6 	 global-step:3846	 l-p:0.1382182091474533
epoch£º192	 i:7 	 global-step:3847	 l-p:0.15507273375988007
epoch£º192	 i:8 	 global-step:3848	 l-p:0.462476909160614
epoch£º192	 i:9 	 global-step:3849	 l-p:0.14048312604427338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1460, 3.1268, 2.9482],
        [3.1460, 3.0985, 3.1288],
        [3.1460, 3.1371, 3.1450],
        [3.1460, 3.1454, 3.1460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.14522410929203033 
model_pd.l_d.mean(): -24.75055694580078 
model_pd.lagr.mean(): -24.60533332824707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2406], device='cuda:0')), ('power', tensor([-24.5099], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.14522410929203033
epoch£º193	 i:1 	 global-step:3861	 l-p:0.11635765433311462
epoch£º193	 i:2 	 global-step:3862	 l-p:0.1471571922302246
epoch£º193	 i:3 	 global-step:3863	 l-p:0.11772019416093826
epoch£º193	 i:4 	 global-step:3864	 l-p:0.2535265386104584
epoch£º193	 i:5 	 global-step:3865	 l-p:0.10670646280050278
epoch£º193	 i:6 	 global-step:3866	 l-p:0.10966626554727554
epoch£º193	 i:7 	 global-step:3867	 l-p:0.13479511439800262
epoch£º193	 i:8 	 global-step:3868	 l-p:0.14267921447753906
epoch£º193	 i:9 	 global-step:3869	 l-p:0.13549409806728363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8279, 2.8279, 2.8279],
        [2.8279, 2.6320, 2.4131],
        [2.8279, 2.6165, 2.5427],
        [2.8279, 2.7407, 2.7809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.13441836833953857 
model_pd.l_d.mean(): -25.003585815429688 
model_pd.lagr.mean(): -24.86916732788086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1257], device='cuda:0')), ('power', tensor([-24.8779], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.13441836833953857
epoch£º194	 i:1 	 global-step:3881	 l-p:0.11978711932897568
epoch£º194	 i:2 	 global-step:3882	 l-p:0.1416769176721573
epoch£º194	 i:3 	 global-step:3883	 l-p:0.1336955428123474
epoch£º194	 i:4 	 global-step:3884	 l-p:0.15843074023723602
epoch£º194	 i:5 	 global-step:3885	 l-p:0.12896312773227692
epoch£º194	 i:6 	 global-step:3886	 l-p:0.14682537317276
epoch£º194	 i:7 	 global-step:3887	 l-p:0.1268085092306137
epoch£º194	 i:8 	 global-step:3888	 l-p:0.14469759166240692
epoch£º194	 i:9 	 global-step:3889	 l-p:0.03539824113249779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8071, 2.6679, 2.6957],
        [2.8071, 2.5876, 2.3694],
        [2.8071, 2.7952, 2.8057],
        [2.8071, 2.5721, 2.3789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.09553123265504837 
model_pd.l_d.mean(): -24.769399642944336 
model_pd.lagr.mean(): -24.67386817932129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0620], device='cuda:0')), ('power', tensor([-24.7074], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.09553123265504837
epoch£º195	 i:1 	 global-step:3901	 l-p:0.008778037503361702
epoch£º195	 i:2 	 global-step:3902	 l-p:0.1261269897222519
epoch£º195	 i:3 	 global-step:3903	 l-p:0.16291232407093048
epoch£º195	 i:4 	 global-step:3904	 l-p:0.10831810534000397
epoch£º195	 i:5 	 global-step:3905	 l-p:0.11483003944158554
epoch£º195	 i:6 	 global-step:3906	 l-p:0.11092282831668854
epoch£º195	 i:7 	 global-step:3907	 l-p:0.17776736617088318
epoch£º195	 i:8 	 global-step:3908	 l-p:0.11020180583000183
epoch£º195	 i:9 	 global-step:3909	 l-p:0.12343034148216248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3822, 3.3822, 3.3822],
        [3.3822, 3.3583, 3.3771],
        [3.3822, 3.3810, 3.3822],
        [3.3822, 3.3636, 3.3789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.1787121742963791 
model_pd.l_d.mean(): -24.26284408569336 
model_pd.lagr.mean(): -24.084131240844727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2379], device='cuda:0')), ('power', tensor([-24.0249], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.1787121742963791
epoch£º196	 i:1 	 global-step:3921	 l-p:0.1201663389801979
epoch£º196	 i:2 	 global-step:3922	 l-p:0.13291269540786743
epoch£º196	 i:3 	 global-step:3923	 l-p:0.11568881571292877
epoch£º196	 i:4 	 global-step:3924	 l-p:0.1275598555803299
epoch£º196	 i:5 	 global-step:3925	 l-p:0.12658807635307312
epoch£º196	 i:6 	 global-step:3926	 l-p:0.11593573540449142
epoch£º196	 i:7 	 global-step:3927	 l-p:0.12053295224905014
epoch£º196	 i:8 	 global-step:3928	 l-p:0.15482981503009796
epoch£º196	 i:9 	 global-step:3929	 l-p:-0.2633382976055145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.2570, 2.2542, 2.2568],
        [2.2570, 1.9374, 1.7268],
        [2.2570, 2.2570, 2.2570],
        [2.2570, 2.1941, 2.2354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): -0.0035206221509724855 
model_pd.l_d.mean(): -24.163734436035156 
model_pd.lagr.mean(): -24.167255401611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3438], device='cuda:0')), ('power', tensor([-24.5076], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:-0.0035206221509724855
epoch£º197	 i:1 	 global-step:3941	 l-p:0.2562824487686157
epoch£º197	 i:2 	 global-step:3942	 l-p:0.1271563470363617
epoch£º197	 i:3 	 global-step:3943	 l-p:0.1524648219347
epoch£º197	 i:4 	 global-step:3944	 l-p:0.9623522162437439
epoch£º197	 i:5 	 global-step:3945	 l-p:0.15830737352371216
epoch£º197	 i:6 	 global-step:3946	 l-p:0.14182104170322418
epoch£º197	 i:7 	 global-step:3947	 l-p:0.13568450510501862
epoch£º197	 i:8 	 global-step:3948	 l-p:0.26820293068885803
epoch£º197	 i:9 	 global-step:3949	 l-p:0.0454941987991333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9606, 2.8897, 2.9293],
        [2.9606, 2.9599, 2.9606],
        [2.9606, 2.9606, 2.9606],
        [2.9606, 2.9549, 2.9602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.10888971388339996 
model_pd.l_d.mean(): -24.81453514099121 
model_pd.lagr.mean(): -24.705644607543945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1631], device='cuda:0')), ('power', tensor([-24.6514], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.10888971388339996
epoch£º198	 i:1 	 global-step:3961	 l-p:0.14127983152866364
epoch£º198	 i:2 	 global-step:3962	 l-p:0.14732596278190613
epoch£º198	 i:3 	 global-step:3963	 l-p:0.10862068086862564
epoch£º198	 i:4 	 global-step:3964	 l-p:0.2326088696718216
epoch£º198	 i:5 	 global-step:3965	 l-p:0.12937119603157043
epoch£º198	 i:6 	 global-step:3966	 l-p:0.1073109582066536
epoch£º198	 i:7 	 global-step:3967	 l-p:0.19935181736946106
epoch£º198	 i:8 	 global-step:3968	 l-p:0.13185879588127136
epoch£º198	 i:9 	 global-step:3969	 l-p:0.13944852352142334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1862, 3.1808, 3.1858],
        [3.1862, 3.1430, 3.1728],
        [3.1862, 3.1862, 3.1862],
        [3.1862, 3.0139, 2.9914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.11290492862462997 
model_pd.l_d.mean(): -24.1379451751709 
model_pd.lagr.mean(): -24.025039672851562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1785], device='cuda:0')), ('power', tensor([-23.9595], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.11290492862462997
epoch£º199	 i:1 	 global-step:3981	 l-p:0.09875161200761795
epoch£º199	 i:2 	 global-step:3982	 l-p:-4.257020950317383
epoch£º199	 i:3 	 global-step:3983	 l-p:0.12487334758043289
epoch£º199	 i:4 	 global-step:3984	 l-p:0.11741127073764801
epoch£º199	 i:5 	 global-step:3985	 l-p:0.06313179433345795
epoch£º199	 i:6 	 global-step:3986	 l-p:0.18067246675491333
epoch£º199	 i:7 	 global-step:3987	 l-p:0.09678033739328384
epoch£º199	 i:8 	 global-step:3988	 l-p:0.3666398525238037
epoch£º199	 i:9 	 global-step:3989	 l-p:0.12545213103294373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8822, 2.8822, 2.8822],
        [2.8822, 2.8790, 2.8821],
        [2.8822, 2.8296, 2.8649],
        [2.8822, 2.6293, 2.4452]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.12717857956886292 
model_pd.l_d.mean(): -24.856056213378906 
model_pd.lagr.mean(): -24.728878021240234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1132], device='cuda:0')), ('power', tensor([-24.7429], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.12717857956886292
epoch£º200	 i:1 	 global-step:4001	 l-p:0.13226188719272614
epoch£º200	 i:2 	 global-step:4002	 l-p:0.12277344614267349
epoch£º200	 i:3 	 global-step:4003	 l-p:0.13490737974643707
epoch£º200	 i:4 	 global-step:4004	 l-p:0.09818228334188461
epoch£º200	 i:5 	 global-step:4005	 l-p:0.1537918597459793
epoch£º200	 i:6 	 global-step:4006	 l-p:0.11082328855991364
epoch£º200	 i:7 	 global-step:4007	 l-p:0.11821001768112183
epoch£º200	 i:8 	 global-step:4008	 l-p:0.12850871682167053
epoch£º200	 i:9 	 global-step:4009	 l-p:0.13271769881248474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7265, 2.7257, 2.7265],
        [2.7265, 2.4642, 2.2230],
        [2.7265, 2.7265, 2.7265],
        [2.7265, 2.5310, 2.5329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.13502635061740875 
model_pd.l_d.mean(): -24.014516830444336 
model_pd.lagr.mean(): -23.87948989868164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1182], device='cuda:0')), ('power', tensor([-24.1327], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.13502635061740875
epoch£º201	 i:1 	 global-step:4021	 l-p:-0.007336997892707586
epoch£º201	 i:2 	 global-step:4022	 l-p:0.07557538151741028
epoch£º201	 i:3 	 global-step:4023	 l-p:0.13679860532283783
epoch£º201	 i:4 	 global-step:4024	 l-p:0.1246156170964241
epoch£º201	 i:5 	 global-step:4025	 l-p:0.1119614988565445
epoch£º201	 i:6 	 global-step:4026	 l-p:0.1250206083059311
epoch£º201	 i:7 	 global-step:4027	 l-p:-3.465580940246582
epoch£º201	 i:8 	 global-step:4028	 l-p:0.10916689038276672
epoch£º201	 i:9 	 global-step:4029	 l-p:0.12697002291679382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8962, 2.8942, 2.8961],
        [2.8962, 2.8954, 2.8962],
        [2.8962, 2.7157, 2.7209],
        [2.8962, 2.8393, 2.8764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.10580965131521225 
model_pd.l_d.mean(): -24.62211799621582 
model_pd.lagr.mean(): -24.516307830810547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1002], device='cuda:0')), ('power', tensor([-24.5219], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.10580965131521225
epoch£º202	 i:1 	 global-step:4041	 l-p:-0.23326916992664337
epoch£º202	 i:2 	 global-step:4042	 l-p:0.11145620048046112
epoch£º202	 i:3 	 global-step:4043	 l-p:0.12685145437717438
epoch£º202	 i:4 	 global-step:4044	 l-p:0.1090538501739502
epoch£º202	 i:5 	 global-step:4045	 l-p:0.13114401698112488
epoch£º202	 i:6 	 global-step:4046	 l-p:0.3678355813026428
epoch£º202	 i:7 	 global-step:4047	 l-p:0.15887761116027832
epoch£º202	 i:8 	 global-step:4048	 l-p:0.023926805704832077
epoch£º202	 i:9 	 global-step:4049	 l-p:0.12410379201173782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8454, 2.6194, 2.3657],
        [2.8454, 2.6526, 2.6512],
        [2.8454, 2.6923, 2.7230],
        [2.8454, 2.8434, 2.8453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.12125980108976364 
model_pd.l_d.mean(): -24.14999008178711 
model_pd.lagr.mean(): -24.028730392456055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0199], device='cuda:0')), ('power', tensor([-24.1301], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.12125980108976364
epoch£º203	 i:1 	 global-step:4061	 l-p:-0.11266158521175385
epoch£º203	 i:2 	 global-step:4062	 l-p:0.11523435264825821
epoch£º203	 i:3 	 global-step:4063	 l-p:0.10810760408639908
epoch£º203	 i:4 	 global-step:4064	 l-p:0.13680431246757507
epoch£º203	 i:5 	 global-step:4065	 l-p:0.16613295674324036
epoch£º203	 i:6 	 global-step:4066	 l-p:0.17023837566375732
epoch£º203	 i:7 	 global-step:4067	 l-p:0.0823279395699501
epoch£º203	 i:8 	 global-step:4068	 l-p:0.11631932109594345
epoch£º203	 i:9 	 global-step:4069	 l-p:0.12532207369804382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7981, 2.5472, 2.2947],
        [2.7981, 2.7981, 2.7981],
        [2.7981, 2.5449, 2.2939],
        [2.7981, 2.5865, 2.5703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.13369546830654144 
model_pd.l_d.mean(): -24.803367614746094 
model_pd.lagr.mean(): -24.6696720123291 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0292], device='cuda:0')), ('power', tensor([-24.7741], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.13369546830654144
epoch£º204	 i:1 	 global-step:4081	 l-p:0.13422471284866333
epoch£º204	 i:2 	 global-step:4082	 l-p:0.1201913133263588
epoch£º204	 i:3 	 global-step:4083	 l-p:0.15444672107696533
epoch£º204	 i:4 	 global-step:4084	 l-p:0.13137033581733704
epoch£º204	 i:5 	 global-step:4085	 l-p:0.12051185965538025
epoch£º204	 i:6 	 global-step:4086	 l-p:0.11767791211605072
epoch£º204	 i:7 	 global-step:4087	 l-p:0.09498833119869232
epoch£º204	 i:8 	 global-step:4088	 l-p:0.14274950325489044
epoch£º204	 i:9 	 global-step:4089	 l-p:0.1291881501674652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6552, 2.4744, 2.5004],
        [2.6552, 2.6552, 2.6552],
        [2.6552, 2.6536, 2.6552],
        [2.6552, 2.6552, 2.6552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.1219363734126091 
model_pd.l_d.mean(): -24.485733032226562 
model_pd.lagr.mean(): -24.36379623413086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0695], device='cuda:0')), ('power', tensor([-24.5553], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.1219363734126091
epoch£º205	 i:1 	 global-step:4101	 l-p:0.13826535642147064
epoch£º205	 i:2 	 global-step:4102	 l-p:0.11263588070869446
epoch£º205	 i:3 	 global-step:4103	 l-p:0.1311793327331543
epoch£º205	 i:4 	 global-step:4104	 l-p:0.13055115938186646
epoch£º205	 i:5 	 global-step:4105	 l-p:-0.005197463091462851
epoch£º205	 i:6 	 global-step:4106	 l-p:0.1318591982126236
epoch£º205	 i:7 	 global-step:4107	 l-p:0.1396133452653885
epoch£º205	 i:8 	 global-step:4108	 l-p:0.15850667655467987
epoch£º205	 i:9 	 global-step:4109	 l-p:0.05840441957116127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6945, 2.5633, 2.6107],
        [2.6945, 2.5557, 2.2902],
        [2.6945, 2.4242, 2.1556],
        [2.6945, 2.6944, 2.6945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.05493314564228058 
model_pd.l_d.mean(): -24.76676368713379 
model_pd.lagr.mean(): -24.711830139160156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0536], device='cuda:0')), ('power', tensor([-24.8203], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.05493314564228058
epoch£º206	 i:1 	 global-step:4121	 l-p:0.12641476094722748
epoch£º206	 i:2 	 global-step:4122	 l-p:0.09420937299728394
epoch£º206	 i:3 	 global-step:4123	 l-p:0.10971710085868835
epoch£º206	 i:4 	 global-step:4124	 l-p:0.10676360875368118
epoch£º206	 i:5 	 global-step:4125	 l-p:0.05361050367355347
epoch£º206	 i:6 	 global-step:4126	 l-p:0.12838099896907806
epoch£º206	 i:7 	 global-step:4127	 l-p:0.1339147686958313
epoch£º206	 i:8 	 global-step:4128	 l-p:0.1268657147884369
epoch£º206	 i:9 	 global-step:4129	 l-p:0.11221294105052948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8977, 2.8069, 2.8534],
        [2.8977, 2.8977, 2.8977],
        [2.8977, 2.6229, 2.4489],
        [2.8977, 2.6473, 2.5644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.12193885445594788 
model_pd.l_d.mean(): -24.54876708984375 
model_pd.lagr.mean(): -24.426828384399414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0967], device='cuda:0')), ('power', tensor([-24.4520], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.12193885445594788
epoch£º207	 i:1 	 global-step:4141	 l-p:0.16798928380012512
epoch£º207	 i:2 	 global-step:4142	 l-p:0.12401659041643143
epoch£º207	 i:3 	 global-step:4143	 l-p:0.07240565866231918
epoch£º207	 i:4 	 global-step:4144	 l-p:0.10889215022325516
epoch£º207	 i:5 	 global-step:4145	 l-p:0.1068330928683281
epoch£º207	 i:6 	 global-step:4146	 l-p:0.141495019197464
epoch£º207	 i:7 	 global-step:4147	 l-p:0.0976911410689354
epoch£º207	 i:8 	 global-step:4148	 l-p:0.16054120659828186
epoch£º207	 i:9 	 global-step:4149	 l-p:0.11907466500997543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8599, 2.8587, 2.8598],
        [2.8599, 2.6012, 2.3400],
        [2.8599, 2.6201, 2.5706],
        [2.8599, 2.8599, 2.8599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.11145120114088058 
model_pd.l_d.mean(): -24.994596481323242 
model_pd.lagr.mean(): -24.88314437866211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1490], device='cuda:0')), ('power', tensor([-24.8456], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.11145120114088058
epoch£º208	 i:1 	 global-step:4161	 l-p:0.01796570047736168
epoch£º208	 i:2 	 global-step:4162	 l-p:0.15396861732006073
epoch£º208	 i:3 	 global-step:4163	 l-p:0.10613956302404404
epoch£º208	 i:4 	 global-step:4164	 l-p:0.09553224593400955
epoch£º208	 i:5 	 global-step:4165	 l-p:0.07580890506505966
epoch£º208	 i:6 	 global-step:4166	 l-p:0.12748326361179352
epoch£º208	 i:7 	 global-step:4167	 l-p:0.2071506232023239
epoch£º208	 i:8 	 global-step:4168	 l-p:0.12806223332881927
epoch£º208	 i:9 	 global-step:4169	 l-p:0.13215994834899902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[2.8221, 2.5296, 2.3113],
        [2.8221, 2.7250, 2.4689],
        [2.8221, 2.6582, 2.6925],
        [2.8221, 2.5322, 2.3669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.11748791486024857 
model_pd.l_d.mean(): -24.76446533203125 
model_pd.lagr.mean(): -24.646976470947266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0103], device='cuda:0')), ('power', tensor([-24.7748], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:0.11748791486024857
epoch£º209	 i:1 	 global-step:4181	 l-p:0.13177885115146637
epoch£º209	 i:2 	 global-step:4182	 l-p:0.128431037068367
epoch£º209	 i:3 	 global-step:4183	 l-p:0.11796281486749649
epoch£º209	 i:4 	 global-step:4184	 l-p:0.1475888192653656
epoch£º209	 i:5 	 global-step:4185	 l-p:0.11617328226566315
epoch£º209	 i:6 	 global-step:4186	 l-p:0.13596603274345398
epoch£º209	 i:7 	 global-step:4187	 l-p:0.16251695156097412
epoch£º209	 i:8 	 global-step:4188	 l-p:0.13669642806053162
epoch£º209	 i:9 	 global-step:4189	 l-p:0.13607573509216309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5411, 2.5411, 2.5411],
        [2.5411, 2.5411, 2.5411],
        [2.5411, 2.2583, 2.1932],
        [2.5411, 2.5411, 2.5411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.15614980459213257 
model_pd.l_d.mean(): -24.977331161499023 
model_pd.lagr.mean(): -24.821182250976562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0668], device='cuda:0')), ('power', tensor([-25.0442], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.15614980459213257
epoch£º210	 i:1 	 global-step:4201	 l-p:0.1388753205537796
epoch£º210	 i:2 	 global-step:4202	 l-p:0.13669687509536743
epoch£º210	 i:3 	 global-step:4203	 l-p:0.1211305782198906
epoch£º210	 i:4 	 global-step:4204	 l-p:0.14139269292354584
epoch£º210	 i:5 	 global-step:4205	 l-p:0.15299540758132935
epoch£º210	 i:6 	 global-step:4206	 l-p:0.13349948823451996
epoch£º210	 i:7 	 global-step:4207	 l-p:0.11203764379024506
epoch£º210	 i:8 	 global-step:4208	 l-p:0.09204085916280746
epoch£º210	 i:9 	 global-step:4209	 l-p:0.14233233034610748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1014, 2.8518, 2.6224],
        [3.1014, 3.1012, 3.1014],
        [3.1014, 2.8748, 2.8219],
        [3.1014, 3.1013, 3.1014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.14859698712825775 
model_pd.l_d.mean(): -24.39997100830078 
model_pd.lagr.mean(): -24.251373291015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1688], device='cuda:0')), ('power', tensor([-24.2312], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.14859698712825775
epoch£º211	 i:1 	 global-step:4221	 l-p:0.11946021765470505
epoch£º211	 i:2 	 global-step:4222	 l-p:0.10631401836872101
epoch£º211	 i:3 	 global-step:4223	 l-p:0.005951819475740194
epoch£º211	 i:4 	 global-step:4224	 l-p:0.10137995332479477
epoch£º211	 i:5 	 global-step:4225	 l-p:0.1095680370926857
epoch£º211	 i:6 	 global-step:4226	 l-p:0.1184612512588501
epoch£º211	 i:7 	 global-step:4227	 l-p:0.16827845573425293
epoch£º211	 i:8 	 global-step:4228	 l-p:0.139455646276474
epoch£º211	 i:9 	 global-step:4229	 l-p:0.1595730036497116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1425, 2.9183, 2.6665],
        [3.1425, 3.1404, 3.1424],
        [3.1425, 3.1425, 3.1425],
        [3.1425, 2.9098, 2.6645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.11957704275846481 
model_pd.l_d.mean(): -24.9628849029541 
model_pd.lagr.mean(): -24.843307495117188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2830], device='cuda:0')), ('power', tensor([-24.6799], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.11957704275846481
epoch£º212	 i:1 	 global-step:4241	 l-p:0.13682709634304047
epoch£º212	 i:2 	 global-step:4242	 l-p:0.07178474217653275
epoch£º212	 i:3 	 global-step:4243	 l-p:0.17058467864990234
epoch£º212	 i:4 	 global-step:4244	 l-p:0.11847983300685883
epoch£º212	 i:5 	 global-step:4245	 l-p:0.11726516485214233
epoch£º212	 i:6 	 global-step:4246	 l-p:0.09249232709407806
epoch£º212	 i:7 	 global-step:4247	 l-p:0.13152970373630524
epoch£º212	 i:8 	 global-step:4248	 l-p:0.1253657341003418
epoch£º212	 i:9 	 global-step:4249	 l-p:0.12923116981983185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1526, 3.1433, 3.1517],
        [3.1526, 3.1526, 3.1526],
        [3.1526, 2.9127, 2.8256],
        [3.1526, 3.0969, 3.1341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.08891810476779938 
model_pd.l_d.mean(): -24.3355655670166 
model_pd.lagr.mean(): -24.246646881103516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1272], device='cuda:0')), ('power', tensor([-24.2084], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.08891810476779938
epoch£º213	 i:1 	 global-step:4261	 l-p:0.14096152782440186
epoch£º213	 i:2 	 global-step:4262	 l-p:0.25367045402526855
epoch£º213	 i:3 	 global-step:4263	 l-p:0.12187716364860535
epoch£º213	 i:4 	 global-step:4264	 l-p:0.16487063467502594
epoch£º213	 i:5 	 global-step:4265	 l-p:0.10455593466758728
epoch£º213	 i:6 	 global-step:4266	 l-p:0.1008789911866188
epoch£º213	 i:7 	 global-step:4267	 l-p:0.14297735691070557
epoch£º213	 i:8 	 global-step:4268	 l-p:0.1251326948404312
epoch£º213	 i:9 	 global-step:4269	 l-p:0.11633091419935226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1306, 3.1284, 3.1305],
        [3.1306, 3.0145, 3.0596],
        [3.1306, 2.8904, 2.8117],
        [3.1306, 3.1306, 3.1306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.13159802556037903 
model_pd.l_d.mean(): -24.808517456054688 
model_pd.lagr.mean(): -24.67691993713379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2292], device='cuda:0')), ('power', tensor([-24.5793], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.13159802556037903
epoch£º214	 i:1 	 global-step:4281	 l-p:0.08875418454408646
epoch£º214	 i:2 	 global-step:4282	 l-p:0.16837798058986664
epoch£º214	 i:3 	 global-step:4283	 l-p:0.11519370973110199
epoch£º214	 i:4 	 global-step:4284	 l-p:0.14762438833713531
epoch£º214	 i:5 	 global-step:4285	 l-p:0.11453418433666229
epoch£º214	 i:6 	 global-step:4286	 l-p:0.15553320944309235
epoch£º214	 i:7 	 global-step:4287	 l-p:0.09214068949222565
epoch£º214	 i:8 	 global-step:4288	 l-p:0.06718365848064423
epoch£º214	 i:9 	 global-step:4289	 l-p:0.14385564625263214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0413, 2.7905, 2.7121],
        [3.0413, 3.0376, 3.0411],
        [3.0413, 2.9737, 3.0159],
        [3.0413, 3.0410, 3.0413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.14264702796936035 
model_pd.l_d.mean(): -24.8112850189209 
model_pd.lagr.mean(): -24.668638229370117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1698], device='cuda:0')), ('power', tensor([-24.6415], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.14264702796936035
epoch£º215	 i:1 	 global-step:4301	 l-p:0.4242222011089325
epoch£º215	 i:2 	 global-step:4302	 l-p:0.08975019305944443
epoch£º215	 i:3 	 global-step:4303	 l-p:0.12565717101097107
epoch£º215	 i:4 	 global-step:4304	 l-p:0.11422492563724518
epoch£º215	 i:5 	 global-step:4305	 l-p:0.1224287748336792
epoch£º215	 i:6 	 global-step:4306	 l-p:0.3642953336238861
epoch£º215	 i:7 	 global-step:4307	 l-p:0.08878248929977417
epoch£º215	 i:8 	 global-step:4308	 l-p:0.058227308094501495
epoch£º215	 i:9 	 global-step:4309	 l-p:0.08549786359071732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8570, 2.8485, 2.8563],
        [2.8570, 2.5495, 2.3292],
        [2.8570, 2.7746, 2.8225],
        [2.8570, 2.8570, 2.8570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.11130033433437347 
model_pd.l_d.mean(): -24.259567260742188 
model_pd.lagr.mean(): -24.14826774597168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0349], device='cuda:0')), ('power', tensor([-24.2246], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.11130033433437347
epoch£º216	 i:1 	 global-step:4321	 l-p:0.10400709509849548
epoch£º216	 i:2 	 global-step:4322	 l-p:0.12525755167007446
epoch£º216	 i:3 	 global-step:4323	 l-p:0.13914059102535248
epoch£º216	 i:4 	 global-step:4324	 l-p:0.05435299128293991
epoch£º216	 i:5 	 global-step:4325	 l-p:0.12937703728675842
epoch£º216	 i:6 	 global-step:4326	 l-p:0.23340797424316406
epoch£º216	 i:7 	 global-step:4327	 l-p:0.08596973121166229
epoch£º216	 i:8 	 global-step:4328	 l-p:0.12701165676116943
epoch£º216	 i:9 	 global-step:4329	 l-p:0.14710116386413574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6209, 2.5567, 2.6001],
        [2.6209, 2.2817, 2.0103],
        [2.6209, 2.3067, 2.1937],
        [2.6209, 2.6209, 2.6209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.1417814940214157 
model_pd.l_d.mean(): -24.972036361694336 
model_pd.lagr.mean(): -24.83025550842285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0103], device='cuda:0')), ('power', tensor([-24.9823], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.1417814940214157
epoch£º217	 i:1 	 global-step:4341	 l-p:0.1427290290594101
epoch£º217	 i:2 	 global-step:4342	 l-p:0.14152808487415314
epoch£º217	 i:3 	 global-step:4343	 l-p:0.11791827529668808
epoch£º217	 i:4 	 global-step:4344	 l-p:0.16015243530273438
epoch£º217	 i:5 	 global-step:4345	 l-p:0.12382189184427261
epoch£º217	 i:6 	 global-step:4346	 l-p:0.11900636553764343
epoch£º217	 i:7 	 global-step:4347	 l-p:0.14055301249027252
epoch£º217	 i:8 	 global-step:4348	 l-p:0.11174315959215164
epoch£º217	 i:9 	 global-step:4349	 l-p:0.11305748671293259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7570, 2.7303, 2.7524],
        [2.7570, 2.7570, 2.7570],
        [2.7570, 2.7557, 2.7570],
        [2.7570, 2.7570, 2.7570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): -0.047393836081027985 
model_pd.l_d.mean(): -24.939390182495117 
model_pd.lagr.mean(): -24.986783981323242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0693], device='cuda:0')), ('power', tensor([-24.8701], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:-0.047393836081027985
epoch£º218	 i:1 	 global-step:4361	 l-p:0.10856284201145172
epoch£º218	 i:2 	 global-step:4362	 l-p:0.1221773698925972
epoch£º218	 i:3 	 global-step:4363	 l-p:0.13419243693351746
epoch£º218	 i:4 	 global-step:4364	 l-p:0.04903076961636543
epoch£º218	 i:5 	 global-step:4365	 l-p:0.10219261795282364
epoch£º218	 i:6 	 global-step:4366	 l-p:0.13473238050937653
epoch£º218	 i:7 	 global-step:4367	 l-p:0.06088746711611748
epoch£º218	 i:8 	 global-step:4368	 l-p:0.09484422206878662
epoch£º218	 i:9 	 global-step:4369	 l-p:0.13618935644626617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0430, 3.0066, 3.0348],
        [3.0430, 2.9785, 3.0204],
        [3.0430, 2.7805, 2.6992],
        [3.0430, 3.0112, 2.7671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.11855797469615936 
model_pd.l_d.mean(): -24.12399673461914 
model_pd.lagr.mean(): -24.00543785095215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0932], device='cuda:0')), ('power', tensor([-24.0308], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.11855797469615936
epoch£º219	 i:1 	 global-step:4381	 l-p:0.10386545956134796
epoch£º219	 i:2 	 global-step:4382	 l-p:0.1320820450782776
epoch£º219	 i:3 	 global-step:4383	 l-p:0.2055194228887558
epoch£º219	 i:4 	 global-step:4384	 l-p:0.08866328001022339
epoch£º219	 i:5 	 global-step:4385	 l-p:0.05835528299212456
epoch£º219	 i:6 	 global-step:4386	 l-p:0.08222732692956924
epoch£º219	 i:7 	 global-step:4387	 l-p:0.12348340451717377
epoch£º219	 i:8 	 global-step:4388	 l-p:0.10906294733285904
epoch£º219	 i:9 	 global-step:4389	 l-p:0.13254201412200928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0697, 3.0664, 3.0695],
        [3.0697, 2.8886, 2.9137],
        [3.0697, 3.0697, 3.0697],
        [3.0697, 2.9994, 3.0434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.10244748741388321 
model_pd.l_d.mean(): -24.739543914794922 
model_pd.lagr.mean(): -24.637096405029297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1873], device='cuda:0')), ('power', tensor([-24.5522], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.10244748741388321
epoch£º220	 i:1 	 global-step:4401	 l-p:0.11273153126239777
epoch£º220	 i:2 	 global-step:4402	 l-p:-1.073870062828064
epoch£º220	 i:3 	 global-step:4403	 l-p:0.12009334564208984
epoch£º220	 i:4 	 global-step:4404	 l-p:0.11997877806425095
epoch£º220	 i:5 	 global-step:4405	 l-p:0.13455131649971008
epoch£º220	 i:6 	 global-step:4406	 l-p:0.17878299951553345
epoch£º220	 i:7 	 global-step:4407	 l-p:0.08828384429216385
epoch£º220	 i:8 	 global-step:4408	 l-p:0.09051387757062912
epoch£º220	 i:9 	 global-step:4409	 l-p:0.12552906572818756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9846, 2.7005, 2.4239],
        [2.9846, 2.6794, 2.4777],
        [2.9846, 2.7397, 2.7056],
        [2.9846, 2.9825, 2.9846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.13445042073726654 
model_pd.l_d.mean(): -24.706130981445312 
model_pd.lagr.mean(): -24.571680068969727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0971], device='cuda:0')), ('power', tensor([-24.6090], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.13445042073726654
epoch£º221	 i:1 	 global-step:4421	 l-p:0.10824813693761826
epoch£º221	 i:2 	 global-step:4422	 l-p:0.12666307389736176
epoch£º221	 i:3 	 global-step:4423	 l-p:0.04348550736904144
epoch£º221	 i:4 	 global-step:4424	 l-p:0.11349938064813614
epoch£º221	 i:5 	 global-step:4425	 l-p:0.1420467048883438
epoch£º221	 i:6 	 global-step:4426	 l-p:0.13939014077186584
epoch£º221	 i:7 	 global-step:4427	 l-p:0.11816646158695221
epoch£º221	 i:8 	 global-step:4428	 l-p:0.1241147592663765
epoch£º221	 i:9 	 global-step:4429	 l-p:0.12679366767406464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[2.6162, 2.4458, 2.1361],
        [2.6162, 2.2772, 1.9719],
        [2.6162, 2.4269, 2.1132],
        [2.6162, 2.2681, 1.9718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.10676225274801254 
model_pd.l_d.mean(): -24.494234085083008 
model_pd.lagr.mean(): -24.38747215270996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1671], device='cuda:0')), ('power', tensor([-24.6613], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.10676225274801254
epoch£º222	 i:1 	 global-step:4441	 l-p:0.15766485035419464
epoch£º222	 i:2 	 global-step:4442	 l-p:0.1321970671415329
epoch£º222	 i:3 	 global-step:4443	 l-p:0.14076557755470276
epoch£º222	 i:4 	 global-step:4444	 l-p:0.11803366988897324
epoch£º222	 i:5 	 global-step:4445	 l-p:0.14277607202529907
epoch£º222	 i:6 	 global-step:4446	 l-p:0.1555381864309311
epoch£º222	 i:7 	 global-step:4447	 l-p:0.13686293363571167
epoch£º222	 i:8 	 global-step:4448	 l-p:0.15219730138778687
epoch£º222	 i:9 	 global-step:4449	 l-p:0.1317003071308136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6158, 2.6101, 2.6155],
        [2.6158, 2.3299, 2.2918],
        [2.6158, 2.2736, 2.1350],
        [2.6158, 2.5354, 2.5859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.1608847975730896 
model_pd.l_d.mean(): -25.0384464263916 
model_pd.lagr.mean(): -24.877561569213867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0443], device='cuda:0')), ('power', tensor([-25.0828], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.1608847975730896
epoch£º223	 i:1 	 global-step:4461	 l-p:0.10149247944355011
epoch£º223	 i:2 	 global-step:4462	 l-p:0.1265701800584793
epoch£º223	 i:3 	 global-step:4463	 l-p:0.12903542816638947
epoch£º223	 i:4 	 global-step:4464	 l-p:0.10926143079996109
epoch£º223	 i:5 	 global-step:4465	 l-p:0.14742536842823029
epoch£º223	 i:6 	 global-step:4466	 l-p:0.06444115191698074
epoch£º223	 i:7 	 global-step:4467	 l-p:0.12098340690135956
epoch£º223	 i:8 	 global-step:4468	 l-p:0.13628119230270386
epoch£º223	 i:9 	 global-step:4469	 l-p:0.13342837989330292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8072, 2.8072, 2.8072],
        [2.8072, 2.8072, 2.8072],
        [2.8072, 2.6662, 2.7215],
        [2.8072, 2.7966, 2.8062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12020660936832428 
model_pd.l_d.mean(): -24.883581161499023 
model_pd.lagr.mean(): -24.76337432861328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1166], device='cuda:0')), ('power', tensor([-24.7670], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.12020660936832428
epoch£º224	 i:1 	 global-step:4481	 l-p:0.12051378935575485
epoch£º224	 i:2 	 global-step:4482	 l-p:0.13548459112644196
epoch£º224	 i:3 	 global-step:4483	 l-p:0.11160736531019211
epoch£º224	 i:4 	 global-step:4484	 l-p:0.12330178171396255
epoch£º224	 i:5 	 global-step:4485	 l-p:0.11820300668478012
epoch£º224	 i:6 	 global-step:4486	 l-p:0.1473718136548996
epoch£º224	 i:7 	 global-step:4487	 l-p:0.13577893376350403
epoch£º224	 i:8 	 global-step:4488	 l-p:-0.267939954996109
epoch£º224	 i:9 	 global-step:4489	 l-p:0.12518897652626038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7456, 2.7456, 2.7456],
        [2.7456, 2.7456, 2.7456],
        [2.7456, 2.7456, 2.7456],
        [2.7456, 2.4521, 2.3972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.12135013937950134 
model_pd.l_d.mean(): -24.566638946533203 
model_pd.lagr.mean(): -24.445289611816406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0012], device='cuda:0')), ('power', tensor([-24.5679], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.12135013937950134
epoch£º225	 i:1 	 global-step:4501	 l-p:0.10709594190120697
epoch£º225	 i:2 	 global-step:4502	 l-p:0.11806441098451614
epoch£º225	 i:3 	 global-step:4503	 l-p:0.14144866168498993
epoch£º225	 i:4 	 global-step:4504	 l-p:0.05508310720324516
epoch£º225	 i:5 	 global-step:4505	 l-p:0.1423529088497162
epoch£º225	 i:6 	 global-step:4506	 l-p:0.09719453006982803
epoch£º225	 i:7 	 global-step:4507	 l-p:0.11175783723592758
epoch£º225	 i:8 	 global-step:4508	 l-p:0.11798538267612457
epoch£º225	 i:9 	 global-step:4509	 l-p:0.15922115743160248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8165, 2.5029, 2.4068],
        [2.8165, 2.4917, 2.1758],
        [2.8165, 2.5957, 2.6183],
        [2.8165, 2.8151, 2.8164]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.11322089284658432 
model_pd.l_d.mean(): -24.942325592041016 
model_pd.lagr.mean(): -24.829105377197266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0622], device='cuda:0')), ('power', tensor([-24.8801], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.11322089284658432
epoch£º226	 i:1 	 global-step:4521	 l-p:0.1405111402273178
epoch£º226	 i:2 	 global-step:4522	 l-p:0.16060678660869598
epoch£º226	 i:3 	 global-step:4523	 l-p:-0.02008923515677452
epoch£º226	 i:4 	 global-step:4524	 l-p:0.1320747584104538
epoch£º226	 i:5 	 global-step:4525	 l-p:0.10688074678182602
epoch£º226	 i:6 	 global-step:4526	 l-p:0.07399642467498779
epoch£º226	 i:7 	 global-step:4527	 l-p:0.10839778929948807
epoch£º226	 i:8 	 global-step:4528	 l-p:0.12749800086021423
epoch£º226	 i:9 	 global-step:4529	 l-p:-0.39531809091567993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9561, 2.6846, 2.6414],
        [2.9561, 2.9390, 2.9540],
        [2.9561, 2.8332, 2.8892],
        [2.9561, 2.9540, 2.9561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.1284063756465912 
model_pd.l_d.mean(): -24.655105590820312 
model_pd.lagr.mean(): -24.52669906616211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0944], device='cuda:0')), ('power', tensor([-24.5607], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.1284063756465912
epoch£º227	 i:1 	 global-step:4541	 l-p:0.23549231886863708
epoch£º227	 i:2 	 global-step:4542	 l-p:0.1139698177576065
epoch£º227	 i:3 	 global-step:4543	 l-p:0.12163611501455307
epoch£º227	 i:4 	 global-step:4544	 l-p:0.19469724595546722
epoch£º227	 i:5 	 global-step:4545	 l-p:0.12634696066379547
epoch£º227	 i:6 	 global-step:4546	 l-p:-0.15523451566696167
epoch£º227	 i:7 	 global-step:4547	 l-p:0.1467394083738327
epoch£º227	 i:8 	 global-step:4548	 l-p:0.09359166771173477
epoch£º227	 i:9 	 global-step:4549	 l-p:0.1363760232925415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0137, 3.0136, 3.0137],
        [3.0137, 3.0133, 3.0137],
        [3.0137, 3.0098, 3.0135],
        [3.0137, 2.8706, 2.9238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.11966508626937866 
model_pd.l_d.mean(): -24.855003356933594 
model_pd.lagr.mean(): -24.73533821105957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1462], device='cuda:0')), ('power', tensor([-24.7088], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.11966508626937866
epoch£º228	 i:1 	 global-step:4561	 l-p:0.11025195568799973
epoch£º228	 i:2 	 global-step:4562	 l-p:0.12095972895622253
epoch£º228	 i:3 	 global-step:4563	 l-p:0.1428878754377365
epoch£º228	 i:4 	 global-step:4564	 l-p:0.04031197354197502
epoch£º228	 i:5 	 global-step:4565	 l-p:0.011810030788183212
epoch£º228	 i:6 	 global-step:4566	 l-p:0.12887677550315857
epoch£º228	 i:7 	 global-step:4567	 l-p:0.14710381627082825
epoch£º228	 i:8 	 global-step:4568	 l-p:0.11701441556215286
epoch£º228	 i:9 	 global-step:4569	 l-p:0.10541988909244537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8202, 2.8163, 2.8200],
        [2.8202, 2.4964, 2.1708],
        [2.8202, 2.7230, 2.7783],
        [2.8202, 2.7463, 2.7945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.11990071833133698 
model_pd.l_d.mean(): -24.07367706298828 
model_pd.lagr.mean(): -23.95377540588379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0582], device='cuda:0')), ('power', tensor([-24.1319], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.11990071833133698
epoch£º229	 i:1 	 global-step:4581	 l-p:0.12359859049320221
epoch£º229	 i:2 	 global-step:4582	 l-p:0.13521674275398254
epoch£º229	 i:3 	 global-step:4583	 l-p:0.1603512018918991
epoch£º229	 i:4 	 global-step:4584	 l-p:0.12370754033327103
epoch£º229	 i:5 	 global-step:4585	 l-p:0.14237090945243835
epoch£º229	 i:6 	 global-step:4586	 l-p:0.17897307872772217
epoch£º229	 i:7 	 global-step:4587	 l-p:0.14246681332588196
epoch£º229	 i:8 	 global-step:4588	 l-p:0.13056181371212006
epoch£º229	 i:9 	 global-step:4589	 l-p:0.1386702060699463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[2.5268, 2.3970, 2.4607],
        [2.5268, 2.2660, 1.9217],
        [2.5268, 2.3694, 2.4326],
        [2.5268, 2.1534, 2.0139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.12423750013113022 
model_pd.l_d.mean(): -24.274364471435547 
model_pd.lagr.mean(): -24.150127410888672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1906], device='cuda:0')), ('power', tensor([-24.4650], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.12423750013113022
epoch£º230	 i:1 	 global-step:4601	 l-p:0.1480494737625122
epoch£º230	 i:2 	 global-step:4602	 l-p:0.122304767370224
epoch£º230	 i:3 	 global-step:4603	 l-p:0.13135004043579102
epoch£º230	 i:4 	 global-step:4604	 l-p:0.11315722018480301
epoch£º230	 i:5 	 global-step:4605	 l-p:0.10052141547203064
epoch£º230	 i:6 	 global-step:4606	 l-p:0.13003703951835632
epoch£º230	 i:7 	 global-step:4607	 l-p:0.11721798032522202
epoch£º230	 i:8 	 global-step:4608	 l-p:0.12647609412670135
epoch£º230	 i:9 	 global-step:4609	 l-p:0.11648575216531754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9489, 2.9489, 2.9489],
        [2.9489, 2.9443, 2.9487],
        [2.9489, 2.7913, 2.4696],
        [2.9489, 2.7426, 2.7764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): -2.1309783458709717 
model_pd.l_d.mean(): -24.69874382019043 
model_pd.lagr.mean(): -26.829721450805664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0595], device='cuda:0')), ('power', tensor([-24.6392], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:-2.1309783458709717
epoch£º231	 i:1 	 global-step:4621	 l-p:0.1262669712305069
epoch£º231	 i:2 	 global-step:4622	 l-p:0.09286815673112869
epoch£º231	 i:3 	 global-step:4623	 l-p:0.16227416694164276
epoch£º231	 i:4 	 global-step:4624	 l-p:0.12359340488910675
epoch£º231	 i:5 	 global-step:4625	 l-p:0.1276492029428482
epoch£º231	 i:6 	 global-step:4626	 l-p:0.10679689049720764
epoch£º231	 i:7 	 global-step:4627	 l-p:0.12163401395082474
epoch£º231	 i:8 	 global-step:4628	 l-p:0.10990685969591141
epoch£º231	 i:9 	 global-step:4629	 l-p:0.11568176746368408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8734, 2.8719, 2.8733],
        [2.8734, 2.7147, 2.7720],
        [2.8734, 2.5080, 2.2110],
        [2.8734, 2.8453, 2.8687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.18143825232982635 
model_pd.l_d.mean(): -24.944974899291992 
model_pd.lagr.mean(): -24.76353645324707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0818], device='cuda:0')), ('power', tensor([-24.8632], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.18143825232982635
epoch£º232	 i:1 	 global-step:4641	 l-p:0.13587525486946106
epoch£º232	 i:2 	 global-step:4642	 l-p:0.13340319693088531
epoch£º232	 i:3 	 global-step:4643	 l-p:0.13144631683826447
epoch£º232	 i:4 	 global-step:4644	 l-p:0.1406848132610321
epoch£º232	 i:5 	 global-step:4645	 l-p:0.12683352828025818
epoch£º232	 i:6 	 global-step:4646	 l-p:0.14359544217586517
epoch£º232	 i:7 	 global-step:4647	 l-p:0.1275966763496399
epoch£º232	 i:8 	 global-step:4648	 l-p:0.12408441305160522
epoch£º232	 i:9 	 global-step:4649	 l-p:0.1344788670539856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6322, 2.4336, 2.4879],
        [2.6322, 2.5071, 2.5709],
        [2.6322, 2.2236, 1.9841],
        [2.6322, 2.3293, 1.9699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.12536576390266418 
model_pd.l_d.mean(): -24.7547607421875 
model_pd.lagr.mean(): -24.62939453125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1439], device='cuda:0')), ('power', tensor([-24.8986], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.12536576390266418
epoch£º233	 i:1 	 global-step:4661	 l-p:0.14347313344478607
epoch£º233	 i:2 	 global-step:4662	 l-p:0.09602809697389603
epoch£º233	 i:3 	 global-step:4663	 l-p:0.13610385358333588
epoch£º233	 i:4 	 global-step:4664	 l-p:0.11600993573665619
epoch£º233	 i:5 	 global-step:4665	 l-p:0.136886328458786
epoch£º233	 i:6 	 global-step:4666	 l-p:0.1104976236820221
epoch£º233	 i:7 	 global-step:4667	 l-p:0.14000995457172394
epoch£º233	 i:8 	 global-step:4668	 l-p:0.12306497246026993
epoch£º233	 i:9 	 global-step:4669	 l-p:0.12892423570156097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6157, 2.5788, 2.6086],
        [2.6157, 2.5295, 2.5848],
        [2.6157, 2.5011, 2.5642],
        [2.6157, 2.6150, 2.6157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.108042411506176 
model_pd.l_d.mean(): -24.72333526611328 
model_pd.lagr.mean(): -24.615293502807617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1212], device='cuda:0')), ('power', tensor([-24.8446], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.108042411506176
epoch£º234	 i:1 	 global-step:4681	 l-p:0.17932364344596863
epoch£º234	 i:2 	 global-step:4682	 l-p:0.1257234513759613
epoch£º234	 i:3 	 global-step:4683	 l-p:0.11830529570579529
epoch£º234	 i:4 	 global-step:4684	 l-p:0.14558099210262299
epoch£º234	 i:5 	 global-step:4685	 l-p:0.15530681610107422
epoch£º234	 i:6 	 global-step:4686	 l-p:0.12846559286117554
epoch£º234	 i:7 	 global-step:4687	 l-p:0.1273641735315323
epoch£º234	 i:8 	 global-step:4688	 l-p:0.12840983271598816
epoch£º234	 i:9 	 global-step:4689	 l-p:0.1483360379934311
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6092, 2.2429, 1.8757],
        [2.6092, 2.6091, 2.6092],
        [2.6092, 2.5634, 2.5991],
        [2.6092, 2.6091, 2.6092]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.13664498925209045 
model_pd.l_d.mean(): -24.936138153076172 
model_pd.lagr.mean(): -24.79949378967285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0351], device='cuda:0')), ('power', tensor([-24.9712], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.13664498925209045
epoch£º235	 i:1 	 global-step:4701	 l-p:0.1594032198190689
epoch£º235	 i:2 	 global-step:4702	 l-p:0.13268548250198364
epoch£º235	 i:3 	 global-step:4703	 l-p:0.16352581977844238
epoch£º235	 i:4 	 global-step:4704	 l-p:0.1522575467824936
epoch£º235	 i:5 	 global-step:4705	 l-p:0.15226714313030243
epoch£º235	 i:6 	 global-step:4706	 l-p:0.09353471547365189
epoch£º235	 i:7 	 global-step:4707	 l-p:0.12129154056310654
epoch£º235	 i:8 	 global-step:4708	 l-p:0.13613508641719818
epoch£º235	 i:9 	 global-step:4709	 l-p:0.0017891167663037777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8429, 2.4571, 2.2729],
        [2.8429, 2.8372, 2.8426],
        [2.8429, 2.4977, 2.4055],
        [2.8429, 2.6834, 2.7466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.2672342360019684 
model_pd.l_d.mean(): -24.907367706298828 
model_pd.lagr.mean(): -24.640132904052734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0473], device='cuda:0')), ('power', tensor([-24.8600], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.2672342360019684
epoch£º236	 i:1 	 global-step:4721	 l-p:0.10275444388389587
epoch£º236	 i:2 	 global-step:4722	 l-p:0.03581725060939789
epoch£º236	 i:3 	 global-step:4723	 l-p:0.12654484808444977
epoch£º236	 i:4 	 global-step:4724	 l-p:0.12532304227352142
epoch£º236	 i:5 	 global-step:4725	 l-p:-1.4806736707687378
epoch£º236	 i:6 	 global-step:4726	 l-p:-0.006290349643677473
epoch£º236	 i:7 	 global-step:4727	 l-p:0.11800553649663925
epoch£º236	 i:8 	 global-step:4728	 l-p:0.10482220351696014
epoch£º236	 i:9 	 global-step:4729	 l-p:0.12223228067159653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2508, 3.0766, 2.7424],
        [3.2508, 2.9865, 2.9638],
        [3.2508, 3.2508, 3.2508],
        [3.2508, 2.9433, 2.8480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.13904815912246704 
model_pd.l_d.mean(): -24.39552879333496 
model_pd.lagr.mean(): -24.256481170654297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1783], device='cuda:0')), ('power', tensor([-24.2172], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.13904815912246704
epoch£º237	 i:1 	 global-step:4741	 l-p:0.12192584574222565
epoch£º237	 i:2 	 global-step:4742	 l-p:0.12017133086919785
epoch£º237	 i:3 	 global-step:4743	 l-p:0.1180691123008728
epoch£º237	 i:4 	 global-step:4744	 l-p:0.10501031577587128
epoch£º237	 i:5 	 global-step:4745	 l-p:0.09579490125179291
epoch£º237	 i:6 	 global-step:4746	 l-p:0.09104280918836594
epoch£º237	 i:7 	 global-step:4747	 l-p:0.11305481940507889
epoch£º237	 i:8 	 global-step:4748	 l-p:0.10937721282243729
epoch£º237	 i:9 	 global-step:4749	 l-p:0.18558703362941742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7574, 2.4932, 2.1204],
        [2.7574, 2.5229, 2.5645],
        [2.7574, 2.3547, 1.9981],
        [2.7574, 2.7574, 2.7574]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.09503074735403061 
model_pd.l_d.mean(): -23.540254592895508 
model_pd.lagr.mean(): -23.44522476196289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2146], device='cuda:0')), ('power', tensor([-23.7549], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.09503074735403061
epoch£º238	 i:1 	 global-step:4761	 l-p:0.13736318051815033
epoch£º238	 i:2 	 global-step:4762	 l-p:0.12589715421199799
epoch£º238	 i:3 	 global-step:4763	 l-p:0.13626335561275482
epoch£º238	 i:4 	 global-step:4764	 l-p:0.1504524201154709
epoch£º238	 i:5 	 global-step:4765	 l-p:0.16518810391426086
epoch£º238	 i:6 	 global-step:4766	 l-p:0.18476349115371704
epoch£º238	 i:7 	 global-step:4767	 l-p:0.167207732796669
epoch£º238	 i:8 	 global-step:4768	 l-p:0.14121754467487335
epoch£º238	 i:9 	 global-step:4769	 l-p:0.14061205089092255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[2.5631, 2.1178, 1.7654],
        [2.5631, 2.2070, 1.8312],
        [2.5631, 2.2450, 2.2333],
        [2.5631, 2.1058, 1.8040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.20697814226150513 
model_pd.l_d.mean(): -24.92578125 
model_pd.lagr.mean(): -24.71880340576172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1493], device='cuda:0')), ('power', tensor([-25.0751], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.20697814226150513
epoch£º239	 i:1 	 global-step:4781	 l-p:0.10957998782396317
epoch£º239	 i:2 	 global-step:4782	 l-p:0.1362237185239792
epoch£º239	 i:3 	 global-step:4783	 l-p:0.12148775905370712
epoch£º239	 i:4 	 global-step:4784	 l-p:0.13617824018001556
epoch£º239	 i:5 	 global-step:4785	 l-p:0.11888402700424194
epoch£º239	 i:6 	 global-step:4786	 l-p:0.3093051016330719
epoch£º239	 i:7 	 global-step:4787	 l-p:0.1417076140642166
epoch£º239	 i:8 	 global-step:4788	 l-p:0.13599584996700287
epoch£º239	 i:9 	 global-step:4789	 l-p:0.12822912633419037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7549, 2.3208, 2.0162],
        [2.7549, 2.5322, 2.5832],
        [2.7549, 2.4646, 2.4684],
        [2.7549, 2.7499, 2.7546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.13558240234851837 
model_pd.l_d.mean(): -24.982078552246094 
model_pd.lagr.mean(): -24.84649658203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0593], device='cuda:0')), ('power', tensor([-24.9227], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.13558240234851837
epoch£º240	 i:1 	 global-step:4801	 l-p:0.13615475594997406
epoch£º240	 i:2 	 global-step:4802	 l-p:0.13286444544792175
epoch£º240	 i:3 	 global-step:4803	 l-p:0.15790534019470215
epoch£º240	 i:4 	 global-step:4804	 l-p:0.11815006285905838
epoch£º240	 i:5 	 global-step:4805	 l-p:0.1646612584590912
epoch£º240	 i:6 	 global-step:4806	 l-p:0.13946424424648285
epoch£º240	 i:7 	 global-step:4807	 l-p:0.13356783986091614
epoch£º240	 i:8 	 global-step:4808	 l-p:0.14197863638401031
epoch£º240	 i:9 	 global-step:4809	 l-p:0.14256736636161804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6557, 2.3966, 2.4345],
        [2.6557, 2.6557, 2.6557],
        [2.6557, 2.2183, 1.8511],
        [2.6557, 2.2991, 1.9133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.1549498587846756 
model_pd.l_d.mean(): -24.921201705932617 
model_pd.lagr.mean(): -24.766252517700195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0489], device='cuda:0')), ('power', tensor([-24.9701], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.1549498587846756
epoch£º241	 i:1 	 global-step:4821	 l-p:0.12999868392944336
epoch£º241	 i:2 	 global-step:4822	 l-p:0.08324109017848969
epoch£º241	 i:3 	 global-step:4823	 l-p:0.147857666015625
epoch£º241	 i:4 	 global-step:4824	 l-p:0.1387925148010254
epoch£º241	 i:5 	 global-step:4825	 l-p:0.1300000697374344
epoch£º241	 i:6 	 global-step:4826	 l-p:0.11988763511180878
epoch£º241	 i:7 	 global-step:4827	 l-p:0.12344803661108017
epoch£º241	 i:8 	 global-step:4828	 l-p:0.13852953910827637
epoch£º241	 i:9 	 global-step:4829	 l-p:0.15277139842510223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6020, 2.1390, 1.8756],
        [2.6020, 2.4562, 2.5279],
        [2.6020, 2.6006, 2.6020],
        [2.6020, 2.1946, 1.8100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.15058250725269318 
model_pd.l_d.mean(): -24.88962173461914 
model_pd.lagr.mean(): -24.739038467407227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1063], device='cuda:0')), ('power', tensor([-24.9959], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.15058250725269318
epoch£º242	 i:1 	 global-step:4841	 l-p:0.13832256197929382
epoch£º242	 i:2 	 global-step:4842	 l-p:0.1560177057981491
epoch£º242	 i:3 	 global-step:4843	 l-p:0.14100785553455353
epoch£º242	 i:4 	 global-step:4844	 l-p:0.14976975321769714
epoch£º242	 i:5 	 global-step:4845	 l-p:0.12056266516447067
epoch£º242	 i:6 	 global-step:4846	 l-p:0.14032909274101257
epoch£º242	 i:7 	 global-step:4847	 l-p:0.1368514746427536
epoch£º242	 i:8 	 global-step:4848	 l-p:0.12451040744781494
epoch£º242	 i:9 	 global-step:4849	 l-p:0.11346040666103363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7898, 2.6041, 2.6711],
        [2.7898, 2.7887, 2.7898],
        [2.7898, 2.7898, 2.7899],
        [2.7898, 2.3962, 2.0034]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.13986444473266602 
model_pd.l_d.mean(): -24.748577117919922 
model_pd.lagr.mean(): -24.608713150024414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0153], device='cuda:0')), ('power', tensor([-24.7333], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.13986444473266602
epoch£º243	 i:1 	 global-step:4861	 l-p:0.12811584770679474
epoch£º243	 i:2 	 global-step:4862	 l-p:0.33038130402565
epoch£º243	 i:3 	 global-step:4863	 l-p:0.12111740559339523
epoch£º243	 i:4 	 global-step:4864	 l-p:0.10951152443885803
epoch£º243	 i:5 	 global-step:4865	 l-p:0.14496861398220062
epoch£º243	 i:6 	 global-step:4866	 l-p:0.10565713793039322
epoch£º243	 i:7 	 global-step:4867	 l-p:0.14737044274806976
epoch£º243	 i:8 	 global-step:4868	 l-p:0.13281944394111633
epoch£º243	 i:9 	 global-step:4869	 l-p:0.13703535497188568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[2.5549, 2.4044, 2.4786],
        [2.5549, 2.0704, 1.7915],
        [2.5549, 2.3224, 2.3845],
        [2.5549, 2.0911, 1.7137]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.14575041830539703 
model_pd.l_d.mean(): -24.97572898864746 
model_pd.lagr.mean(): -24.829978942871094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1229], device='cuda:0')), ('power', tensor([-25.0986], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.14575041830539703
epoch£º244	 i:1 	 global-step:4881	 l-p:0.15113458037376404
epoch£º244	 i:2 	 global-step:4882	 l-p:0.200599804520607
epoch£º244	 i:3 	 global-step:4883	 l-p:0.1393350064754486
epoch£º244	 i:4 	 global-step:4884	 l-p:0.2999548316001892
epoch£º244	 i:5 	 global-step:4885	 l-p:0.1520095020532608
epoch£º244	 i:6 	 global-step:4886	 l-p:0.13252772390842438
epoch£º244	 i:7 	 global-step:4887	 l-p:0.14016593992710114
epoch£º244	 i:8 	 global-step:4888	 l-p:0.06451267004013062
epoch£º244	 i:9 	 global-step:4889	 l-p:0.13920091092586517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8310, 2.8309, 2.8310],
        [2.8310, 2.4309, 2.3018],
        [2.8310, 2.3772, 2.0879],
        [2.8310, 2.8310, 2.8310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.11313051730394363 
model_pd.l_d.mean(): -24.69755744934082 
model_pd.lagr.mean(): -24.584426879882812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0299], device='cuda:0')), ('power', tensor([-24.7275], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.11313051730394363
epoch£º245	 i:1 	 global-step:4901	 l-p:0.13029441237449646
epoch£º245	 i:2 	 global-step:4902	 l-p:0.12422101944684982
epoch£º245	 i:3 	 global-step:4903	 l-p:0.1169719323515892
epoch£º245	 i:4 	 global-step:4904	 l-p:0.1518571972846985
epoch£º245	 i:5 	 global-step:4905	 l-p:0.13271205127239227
epoch£º245	 i:6 	 global-step:4906	 l-p:0.06414498388767242
epoch£º245	 i:7 	 global-step:4907	 l-p:0.1404721736907959
epoch£º245	 i:8 	 global-step:4908	 l-p:0.11799941956996918
epoch£º245	 i:9 	 global-step:4909	 l-p:0.11918695271015167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7666, 2.3054, 1.9326],
        [2.7666, 2.7623, 2.7664],
        [2.7666, 2.5116, 2.5586],
        [2.7666, 2.7665, 2.7666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.12199664115905762 
model_pd.l_d.mean(): -24.5052433013916 
model_pd.lagr.mean(): -24.38324737548828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0740], device='cuda:0')), ('power', tensor([-24.5793], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.12199664115905762
epoch£º246	 i:1 	 global-step:4921	 l-p:0.1388184279203415
epoch£º246	 i:2 	 global-step:4922	 l-p:0.1424461156129837
epoch£º246	 i:3 	 global-step:4923	 l-p:0.14934147894382477
epoch£º246	 i:4 	 global-step:4924	 l-p:0.15471376478672028
epoch£º246	 i:5 	 global-step:4925	 l-p:0.15076622366905212
epoch£º246	 i:6 	 global-step:4926	 l-p:0.25925442576408386
epoch£º246	 i:7 	 global-step:4927	 l-p:0.15662881731987
epoch£º246	 i:8 	 global-step:4928	 l-p:0.1561240404844284
epoch£º246	 i:9 	 global-step:4929	 l-p:0.1288585662841797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7150, 2.7150, 2.7150],
        [2.7150, 2.6102, 2.6752],
        [2.7150, 2.3719, 2.3535],
        [2.7150, 2.7149, 2.7150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.14160829782485962 
model_pd.l_d.mean(): -24.72061538696289 
model_pd.lagr.mean(): -24.57900619506836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0306], device='cuda:0')), ('power', tensor([-24.7512], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.14160829782485962
epoch£º247	 i:1 	 global-step:4941	 l-p:0.1306212842464447
epoch£º247	 i:2 	 global-step:4942	 l-p:0.1422482430934906
epoch£º247	 i:3 	 global-step:4943	 l-p:0.12221609055995941
epoch£º247	 i:4 	 global-step:4944	 l-p:0.13082796335220337
epoch£º247	 i:5 	 global-step:4945	 l-p:0.14821001887321472
epoch£º247	 i:6 	 global-step:4946	 l-p:0.1263701468706131
epoch£º247	 i:7 	 global-step:4947	 l-p:0.14248143136501312
epoch£º247	 i:8 	 global-step:4948	 l-p:0.13364845514297485
epoch£º247	 i:9 	 global-step:4949	 l-p:0.13085971772670746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5493, 2.5493, 2.5493],
        [2.5493, 2.5492, 2.5493],
        [2.5493, 2.4445, 2.5108],
        [2.5493, 2.5492, 2.5493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.6088440418243408 
model_pd.l_d.mean(): -25.014341354370117 
model_pd.lagr.mean(): -24.40549659729004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0815], device='cuda:0')), ('power', tensor([-25.0958], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.6088440418243408
epoch£º248	 i:1 	 global-step:4961	 l-p:0.1613590270280838
epoch£º248	 i:2 	 global-step:4962	 l-p:0.14087937772274017
epoch£º248	 i:3 	 global-step:4963	 l-p:0.16501662135124207
epoch£º248	 i:4 	 global-step:4964	 l-p:0.1399378627538681
epoch£º248	 i:5 	 global-step:4965	 l-p:0.14448164403438568
epoch£º248	 i:6 	 global-step:4966	 l-p:0.1412758082151413
epoch£º248	 i:7 	 global-step:4967	 l-p:0.13197633624076843
epoch£º248	 i:8 	 global-step:4968	 l-p:0.11786352097988129
epoch£º248	 i:9 	 global-step:4969	 l-p:0.14499050378799438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[2.7305, 2.2374, 1.8700],
        [2.7305, 2.2443, 1.8598],
        [2.7305, 2.4001, 2.4013],
        [2.7305, 2.3393, 1.9257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.1393721103668213 
model_pd.l_d.mean(): -25.054935455322266 
model_pd.lagr.mean(): -24.915563583374023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0174], device='cuda:0')), ('power', tensor([-25.0724], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.1393721103668213
epoch£º249	 i:1 	 global-step:4981	 l-p:0.1424250602722168
epoch£º249	 i:2 	 global-step:4982	 l-p:0.11745280772447586
epoch£º249	 i:3 	 global-step:4983	 l-p:0.14468340575695038
epoch£º249	 i:4 	 global-step:4984	 l-p:0.1302105039358139
epoch£º249	 i:5 	 global-step:4985	 l-p:0.11766498535871506
epoch£º249	 i:6 	 global-step:4986	 l-p:0.15096257627010345
epoch£º249	 i:7 	 global-step:4987	 l-p:0.12317191064357758
epoch£º249	 i:8 	 global-step:4988	 l-p:0.1431756317615509
epoch£º249	 i:9 	 global-step:4989	 l-p:0.16179296374320984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7136, 2.2533, 2.0734],
        [2.7136, 2.7136, 2.7136],
        [2.7136, 2.5675, 2.6433],
        [2.7136, 2.7129, 2.7136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.13832470774650574 
model_pd.l_d.mean(): -24.555776596069336 
model_pd.lagr.mean(): -24.417451858520508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1067], device='cuda:0')), ('power', tensor([-24.6625], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.13832470774650574
epoch£º250	 i:1 	 global-step:5001	 l-p:0.14913485944271088
epoch£º250	 i:2 	 global-step:5002	 l-p:0.12791648507118225
epoch£º250	 i:3 	 global-step:5003	 l-p:0.09268060326576233
epoch£º250	 i:4 	 global-step:5004	 l-p:0.13447104394435883
epoch£º250	 i:5 	 global-step:5005	 l-p:0.13660898804664612
epoch£º250	 i:6 	 global-step:5006	 l-p:0.14194011688232422
epoch£º250	 i:7 	 global-step:5007	 l-p:0.13522909581661224
epoch£º250	 i:8 	 global-step:5008	 l-p:0.1757681965827942
epoch£º250	 i:9 	 global-step:5009	 l-p:0.09258183091878891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5165, 2.0537, 1.9188],
        [2.5165, 2.5145, 2.5165],
        [2.5165, 2.1579, 2.1581],
        [2.5165, 2.5165, 2.5165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.26354968547821045 
model_pd.l_d.mean(): -24.863109588623047 
model_pd.lagr.mean(): -24.599559783935547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1896], device='cuda:0')), ('power', tensor([-25.0527], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.26354968547821045
epoch£º251	 i:1 	 global-step:5021	 l-p:0.16602489352226257
epoch£º251	 i:2 	 global-step:5022	 l-p:0.14440366625785828
epoch£º251	 i:3 	 global-step:5023	 l-p:0.13776805996894836
epoch£º251	 i:4 	 global-step:5024	 l-p:0.14705239236354828
epoch£º251	 i:5 	 global-step:5025	 l-p:0.11603139340877533
epoch£º251	 i:6 	 global-step:5026	 l-p:0.14803574979305267
epoch£º251	 i:7 	 global-step:5027	 l-p:0.13115660846233368
epoch£º251	 i:8 	 global-step:5028	 l-p:0.13690222799777985
epoch£º251	 i:9 	 global-step:5029	 l-p:0.1347164511680603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6399, 2.6363, 2.6398],
        [2.6399, 2.2641, 1.8497],
        [2.6399, 2.2879, 2.2882],
        [2.6399, 2.1761, 2.0246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.13242872059345245 
model_pd.l_d.mean(): -24.81939697265625 
model_pd.lagr.mean(): -24.686967849731445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1513], device='cuda:0')), ('power', tensor([-24.9707], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.13242872059345245
epoch£º252	 i:1 	 global-step:5041	 l-p:0.18066953122615814
epoch£º252	 i:2 	 global-step:5042	 l-p:0.13681459426879883
epoch£º252	 i:3 	 global-step:5043	 l-p:0.19942571222782135
epoch£º252	 i:4 	 global-step:5044	 l-p:0.16556300222873688
epoch£º252	 i:5 	 global-step:5045	 l-p:0.12608574330806732
epoch£º252	 i:6 	 global-step:5046	 l-p:0.1564471572637558
epoch£º252	 i:7 	 global-step:5047	 l-p:0.12173673510551453
epoch£º252	 i:8 	 global-step:5048	 l-p:0.11871349066495895
epoch£º252	 i:9 	 global-step:5049	 l-p:0.15566310286521912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[2.7535, 2.3938, 1.9699],
        [2.7535, 2.3877, 2.3690],
        [2.7535, 2.4039, 2.4019],
        [2.7535, 2.2363, 1.8479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.12656521797180176 
model_pd.l_d.mean(): -25.006868362426758 
model_pd.lagr.mean(): -24.88030242919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0052], device='cuda:0')), ('power', tensor([-25.0120], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.12656521797180176
epoch£º253	 i:1 	 global-step:5061	 l-p:0.1380690038204193
epoch£º253	 i:2 	 global-step:5062	 l-p:0.11256334185600281
epoch£º253	 i:3 	 global-step:5063	 l-p:0.1090555265545845
epoch£º253	 i:4 	 global-step:5064	 l-p:0.12952537834644318
epoch£º253	 i:5 	 global-step:5065	 l-p:0.12685604393482208
epoch£º253	 i:6 	 global-step:5066	 l-p:0.11889532953500748
epoch£º253	 i:7 	 global-step:5067	 l-p:0.11569592356681824
epoch£º253	 i:8 	 global-step:5068	 l-p:0.12933439016342163
epoch£º253	 i:9 	 global-step:5069	 l-p:0.1332027018070221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6565, 2.5147, 2.5931],
        [2.6565, 2.3746, 2.4316],
        [2.6565, 2.6564, 2.6565],
        [2.6565, 2.6564, 2.6565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.14111267030239105 
model_pd.l_d.mean(): -24.85536766052246 
model_pd.lagr.mean(): -24.71425437927246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0843], device='cuda:0')), ('power', tensor([-24.9397], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.14111267030239105
epoch£º254	 i:1 	 global-step:5081	 l-p:0.25517842173576355
epoch£º254	 i:2 	 global-step:5082	 l-p:0.16855181753635406
epoch£º254	 i:3 	 global-step:5083	 l-p:0.16075542569160461
epoch£º254	 i:4 	 global-step:5084	 l-p:0.11870352178812027
epoch£º254	 i:5 	 global-step:5085	 l-p:0.13193736970424652
epoch£º254	 i:6 	 global-step:5086	 l-p:0.11840125918388367
epoch£º254	 i:7 	 global-step:5087	 l-p:0.14763949811458588
epoch£º254	 i:8 	 global-step:5088	 l-p:0.15388436615467072
epoch£º254	 i:9 	 global-step:5089	 l-p:0.13232305645942688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[2.6968, 2.3066, 1.8794],
        [2.6968, 2.2495, 2.1429],
        [2.6968, 2.1555, 1.7623],
        [2.6968, 2.3792, 1.9590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.13881435990333557 
model_pd.l_d.mean(): -24.27716636657715 
model_pd.lagr.mean(): -24.138351440429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1492], device='cuda:0')), ('power', tensor([-24.4263], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.13881435990333557
epoch£º255	 i:1 	 global-step:5101	 l-p:0.11714242398738861
epoch£º255	 i:2 	 global-step:5102	 l-p:0.1574602723121643
epoch£º255	 i:3 	 global-step:5103	 l-p:0.13420209288597107
epoch£º255	 i:4 	 global-step:5104	 l-p:0.1449269950389862
epoch£º255	 i:5 	 global-step:5105	 l-p:0.13774193823337555
epoch£º255	 i:6 	 global-step:5106	 l-p:0.12057256698608398
epoch£º255	 i:7 	 global-step:5107	 l-p:0.18478870391845703
epoch£º255	 i:8 	 global-step:5108	 l-p:0.17144277691841125
epoch£º255	 i:9 	 global-step:5109	 l-p:0.15530771017074585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6182, 2.3603, 2.4327],
        [2.6182, 2.5573, 2.6042],
        [2.6182, 2.6182, 2.6182],
        [2.6182, 2.0549, 1.6905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.1557229608297348 
model_pd.l_d.mean(): -25.045713424682617 
model_pd.lagr.mean(): -24.889989852905273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0644], device='cuda:0')), ('power', tensor([-25.1101], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.1557229608297348
epoch£º256	 i:1 	 global-step:5121	 l-p:0.17670413851737976
epoch£º256	 i:2 	 global-step:5122	 l-p:0.1489555686712265
epoch£º256	 i:3 	 global-step:5123	 l-p:0.1447126567363739
epoch£º256	 i:4 	 global-step:5124	 l-p:0.20095665752887726
epoch£º256	 i:5 	 global-step:5125	 l-p:0.14522147178649902
epoch£º256	 i:6 	 global-step:5126	 l-p:0.13599587976932526
epoch£º256	 i:7 	 global-step:5127	 l-p:0.16019535064697266
epoch£º256	 i:8 	 global-step:5128	 l-p:0.11563894152641296
epoch£º256	 i:9 	 global-step:5129	 l-p:0.14380405843257904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7865, 2.7861, 2.7865],
        [2.7865, 2.7865, 2.7865],
        [2.7865, 2.2882, 2.0846],
        [2.7865, 2.6779, 2.7474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.11830859631299973 
model_pd.l_d.mean(): -24.592409133911133 
model_pd.lagr.mean(): -24.47410011291504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0128], device='cuda:0')), ('power', tensor([-24.6052], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.11830859631299973
epoch£º257	 i:1 	 global-step:5141	 l-p:0.1298830509185791
epoch£º257	 i:2 	 global-step:5142	 l-p:0.08255477994680405
epoch£º257	 i:3 	 global-step:5143	 l-p:0.1268525868654251
epoch£º257	 i:4 	 global-step:5144	 l-p:0.13257716596126556
epoch£º257	 i:5 	 global-step:5145	 l-p:0.14653798937797546
epoch£º257	 i:6 	 global-step:5146	 l-p:0.1312921941280365
epoch£º257	 i:7 	 global-step:5147	 l-p:0.14860402047634125
epoch£º257	 i:8 	 global-step:5148	 l-p:0.16722236573696136
epoch£º257	 i:9 	 global-step:5149	 l-p:0.14361582696437836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6099, 2.6042, 2.6096],
        [2.6099, 2.3466, 2.4208],
        [2.6099, 2.6099, 2.6099],
        [2.6099, 2.0331, 1.6628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.14140741527080536 
model_pd.l_d.mean(): -24.84418487548828 
model_pd.lagr.mean(): -24.702777862548828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1349], device='cuda:0')), ('power', tensor([-24.9791], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.14140741527080536
epoch£º258	 i:1 	 global-step:5161	 l-p:0.14833508431911469
epoch£º258	 i:2 	 global-step:5162	 l-p:0.2541787922382355
epoch£º258	 i:3 	 global-step:5163	 l-p:0.1284053772687912
epoch£º258	 i:4 	 global-step:5164	 l-p:0.13798671960830688
epoch£º258	 i:5 	 global-step:5165	 l-p:0.18655185401439667
epoch£º258	 i:6 	 global-step:5166	 l-p:0.1738021969795227
epoch£º258	 i:7 	 global-step:5167	 l-p:0.13106323778629303
epoch£º258	 i:8 	 global-step:5168	 l-p:0.1220288947224617
epoch£º258	 i:9 	 global-step:5169	 l-p:0.13915835320949554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7408, 2.1768, 1.7931],
        [2.7408, 2.7408, 2.7408],
        [2.7408, 2.7404, 2.7408],
        [2.7408, 2.3708, 2.3739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11729530245065689 
model_pd.l_d.mean(): -24.343231201171875 
model_pd.lagr.mean(): -24.225934982299805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0824], device='cuda:0')), ('power', tensor([-24.4256], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.11729530245065689
epoch£º259	 i:1 	 global-step:5181	 l-p:0.144649937748909
epoch£º259	 i:2 	 global-step:5182	 l-p:0.14269986748695374
epoch£º259	 i:3 	 global-step:5183	 l-p:0.14300338923931122
epoch£º259	 i:4 	 global-step:5184	 l-p:0.16698822379112244
epoch£º259	 i:5 	 global-step:5185	 l-p:0.15124258399009705
epoch£º259	 i:6 	 global-step:5186	 l-p:0.12191413342952728
epoch£º259	 i:7 	 global-step:5187	 l-p:0.1306549459695816
epoch£º259	 i:8 	 global-step:5188	 l-p:0.14306241273880005
epoch£º259	 i:9 	 global-step:5189	 l-p:0.15110668540000916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7013, 2.1510, 1.7215],
        [2.7013, 2.1341, 1.8157],
        [2.7013, 2.7013, 2.7013],
        [2.7013, 2.2983, 2.2767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.13120393455028534 
model_pd.l_d.mean(): -24.87889289855957 
model_pd.lagr.mean(): -24.74768829345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1361], device='cuda:0')), ('power', tensor([-25.0150], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.13120393455028534
epoch£º260	 i:1 	 global-step:5201	 l-p:0.1281653195619583
epoch£º260	 i:2 	 global-step:5202	 l-p:0.15847548842430115
epoch£º260	 i:3 	 global-step:5203	 l-p:0.10442301630973816
epoch£º260	 i:4 	 global-step:5204	 l-p:0.15017279982566833
epoch£º260	 i:5 	 global-step:5205	 l-p:0.13570250570774078
epoch£º260	 i:6 	 global-step:5206	 l-p:0.1457824856042862
epoch£º260	 i:7 	 global-step:5207	 l-p:0.10286581516265869
epoch£º260	 i:8 	 global-step:5208	 l-p:0.2596498727798462
epoch£º260	 i:9 	 global-step:5209	 l-p:0.1512461006641388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5383, 2.5383, 2.5383],
        [2.5383, 1.9332, 1.5850],
        [2.5383, 1.9304, 1.5628],
        [2.5383, 2.4055, 2.4860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.30664244294166565 
model_pd.l_d.mean(): -24.85590934753418 
model_pd.lagr.mean(): -24.549266815185547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1565], device='cuda:0')), ('power', tensor([-25.0124], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.30664244294166565
epoch£º261	 i:1 	 global-step:5221	 l-p:0.11272339522838593
epoch£º261	 i:2 	 global-step:5222	 l-p:0.16528159379959106
epoch£º261	 i:3 	 global-step:5223	 l-p:0.12204577773809433
epoch£º261	 i:4 	 global-step:5224	 l-p:0.139454185962677
epoch£º261	 i:5 	 global-step:5225	 l-p:0.10075993090867996
epoch£º261	 i:6 	 global-step:5226	 l-p:0.11590823531150818
epoch£º261	 i:7 	 global-step:5227	 l-p:0.17893753945827484
epoch£º261	 i:8 	 global-step:5228	 l-p:0.1195022463798523
epoch£º261	 i:9 	 global-step:5229	 l-p:0.1233409196138382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1735, 3.1274, 3.1647],
        [3.1735, 3.0152, 3.0954],
        [3.1735, 3.1583, 3.1721],
        [3.1735, 3.1608, 3.1725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.16101421415805817 
model_pd.l_d.mean(): -24.482444763183594 
model_pd.lagr.mean(): -24.321430206298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1329], device='cuda:0')), ('power', tensor([-24.3496], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.16101421415805817
epoch£º262	 i:1 	 global-step:5241	 l-p:0.11303933709859848
epoch£º262	 i:2 	 global-step:5242	 l-p:0.11857282370328903
epoch£º262	 i:3 	 global-step:5243	 l-p:-0.09092314541339874
epoch£º262	 i:4 	 global-step:5244	 l-p:0.10970256477594376
epoch£º262	 i:5 	 global-step:5245	 l-p:0.0162833109498024
epoch£º262	 i:6 	 global-step:5246	 l-p:0.08239975571632385
epoch£º262	 i:7 	 global-step:5247	 l-p:0.1542830765247345
epoch£º262	 i:8 	 global-step:5248	 l-p:0.12466313689947128
epoch£º262	 i:9 	 global-step:5249	 l-p:0.07384201139211655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4471, 3.0140, 2.5781],
        [3.4471, 2.9771, 2.6444],
        [3.4471, 3.3577, 3.4182],
        [3.4471, 2.9960, 2.5806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.1904505491256714 
model_pd.l_d.mean(): -24.31012725830078 
model_pd.lagr.mean(): -24.11967658996582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2585], device='cuda:0')), ('power', tensor([-24.0516], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.1904505491256714
epoch£º263	 i:1 	 global-step:5261	 l-p:0.10850818455219269
epoch£º263	 i:2 	 global-step:5262	 l-p:0.04715908318758011
epoch£º263	 i:3 	 global-step:5263	 l-p:0.10820792615413666
epoch£º263	 i:4 	 global-step:5264	 l-p:0.12116708606481552
epoch£º263	 i:5 	 global-step:5265	 l-p:0.11765658855438232
epoch£º263	 i:6 	 global-step:5266	 l-p:1.4472014904022217
epoch£º263	 i:7 	 global-step:5267	 l-p:0.11925391852855682
epoch£º263	 i:8 	 global-step:5268	 l-p:0.1163216382265091
epoch£º263	 i:9 	 global-step:5269	 l-p:0.09870277345180511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5049, 3.4744, 3.5004],
        [3.5049, 3.0904, 2.6529],
        [3.5049, 3.2236, 2.7740],
        [3.5049, 3.0473, 2.6785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.1270734816789627 
model_pd.l_d.mean(): -24.712717056274414 
model_pd.lagr.mean(): -24.585643768310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3350], device='cuda:0')), ('power', tensor([-24.3777], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.1270734816789627
epoch£º264	 i:1 	 global-step:5281	 l-p:0.5212408900260925
epoch£º264	 i:2 	 global-step:5282	 l-p:0.11423606425523758
epoch£º264	 i:3 	 global-step:5283	 l-p:0.10955442488193512
epoch£º264	 i:4 	 global-step:5284	 l-p:0.15663917362689972
epoch£º264	 i:5 	 global-step:5285	 l-p:0.111834317445755
epoch£º264	 i:6 	 global-step:5286	 l-p:-0.07475437968969345
epoch£º264	 i:7 	 global-step:5287	 l-p:0.11964452266693115
epoch£º264	 i:8 	 global-step:5288	 l-p:0.10425075888633728
epoch£º264	 i:9 	 global-step:5289	 l-p:0.10673941671848297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8369, 2.4126, 1.9552],
        [2.8369, 2.3353, 2.1691],
        [2.8369, 2.8369, 2.8369],
        [2.8369, 2.8361, 2.8369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.04434804245829582 
model_pd.l_d.mean(): -24.677932739257812 
model_pd.lagr.mean(): -24.63358497619629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0635], device='cuda:0')), ('power', tensor([-24.7414], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.04434804245829582
epoch£º265	 i:1 	 global-step:5301	 l-p:0.14513148367404938
epoch£º265	 i:2 	 global-step:5302	 l-p:0.14466863870620728
epoch£º265	 i:3 	 global-step:5303	 l-p:0.2076059877872467
epoch£º265	 i:4 	 global-step:5304	 l-p:0.1165483370423317
epoch£º265	 i:5 	 global-step:5305	 l-p:0.014396791346371174
epoch£º265	 i:6 	 global-step:5306	 l-p:0.18601255118846893
epoch£º265	 i:7 	 global-step:5307	 l-p:0.18159790337085724
epoch£º265	 i:8 	 global-step:5308	 l-p:0.1830814629793167
epoch£º265	 i:9 	 global-step:5309	 l-p:0.13669544458389282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6939, 2.6754, 2.6921],
        [2.6939, 2.6903, 2.6938],
        [2.6939, 2.6074, 2.6692],
        [2.6939, 2.6925, 2.6939]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.13125403225421906 
model_pd.l_d.mean(): -24.923418045043945 
model_pd.lagr.mean(): -24.792163848876953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0586], device='cuda:0')), ('power', tensor([-24.9820], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.13125403225421906
epoch£º266	 i:1 	 global-step:5321	 l-p:0.1305437982082367
epoch£º266	 i:2 	 global-step:5322	 l-p:0.12571892142295837
epoch£º266	 i:3 	 global-step:5323	 l-p:0.1480526626110077
epoch£º266	 i:4 	 global-step:5324	 l-p:0.13278304040431976
epoch£º266	 i:5 	 global-step:5325	 l-p:0.13851958513259888
epoch£º266	 i:6 	 global-step:5326	 l-p:0.13505949079990387
epoch£º266	 i:7 	 global-step:5327	 l-p:0.14088840782642365
epoch£º266	 i:8 	 global-step:5328	 l-p:0.13434414565563202
epoch£º266	 i:9 	 global-step:5329	 l-p:0.14019636809825897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6625, 2.6625, 2.6625],
        [2.6625, 2.2172, 2.1683],
        [2.6625, 2.6625, 2.6625],
        [2.6625, 2.6014, 2.6491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.13775184750556946 
model_pd.l_d.mean(): -24.804489135742188 
model_pd.lagr.mean(): -24.666736602783203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1577], device='cuda:0')), ('power', tensor([-24.9622], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.13775184750556946
epoch£º267	 i:1 	 global-step:5341	 l-p:0.170402392745018
epoch£º267	 i:2 	 global-step:5342	 l-p:0.1353527158498764
epoch£º267	 i:3 	 global-step:5343	 l-p:0.16132579743862152
epoch£º267	 i:4 	 global-step:5344	 l-p:0.1400623619556427
epoch£º267	 i:5 	 global-step:5345	 l-p:0.15854282677173615
epoch£º267	 i:6 	 global-step:5346	 l-p:0.1529068499803543
epoch£º267	 i:7 	 global-step:5347	 l-p:0.1273382604122162
epoch£º267	 i:8 	 global-step:5348	 l-p:0.13716262578964233
epoch£º267	 i:9 	 global-step:5349	 l-p:0.15130800008773804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7973, 2.7955, 2.7972],
        [2.7973, 2.2350, 1.9644],
        [2.7973, 2.7326, 2.7823],
        [2.7973, 2.7973, 2.7973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.12963436543941498 
model_pd.l_d.mean(): -24.870573043823242 
model_pd.lagr.mean(): -24.740938186645508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0399], device='cuda:0')), ('power', tensor([-24.9105], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.12963436543941498
epoch£º268	 i:1 	 global-step:5361	 l-p:0.1402750015258789
epoch£º268	 i:2 	 global-step:5362	 l-p:0.14407385885715485
epoch£º268	 i:3 	 global-step:5363	 l-p:0.12782727181911469
epoch£º268	 i:4 	 global-step:5364	 l-p:0.13308902084827423
epoch£º268	 i:5 	 global-step:5365	 l-p:0.21072348952293396
epoch£º268	 i:6 	 global-step:5366	 l-p:0.13788658380508423
epoch£º268	 i:7 	 global-step:5367	 l-p:0.14069217443466187
epoch£º268	 i:8 	 global-step:5368	 l-p:0.13754206895828247
epoch£º268	 i:9 	 global-step:5369	 l-p:0.13758210837841034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6997, 2.6958, 2.6995],
        [2.6997, 2.6708, 2.6960],
        [2.6997, 2.4987, 2.5905],
        [2.6997, 2.5642, 2.6462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.16944022476673126 
model_pd.l_d.mean(): -25.01865577697754 
model_pd.lagr.mean(): -24.84921646118164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1135], device='cuda:0')), ('power', tensor([-25.1321], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.16944022476673126
epoch£º269	 i:1 	 global-step:5381	 l-p:0.1831163913011551
epoch£º269	 i:2 	 global-step:5382	 l-p:0.15579648315906525
epoch£º269	 i:3 	 global-step:5383	 l-p:0.12481451034545898
epoch£º269	 i:4 	 global-step:5384	 l-p:0.13653962314128876
epoch£º269	 i:5 	 global-step:5385	 l-p:0.13637401163578033
epoch£º269	 i:6 	 global-step:5386	 l-p:0.1152452826499939
epoch£º269	 i:7 	 global-step:5387	 l-p:0.17410366237163544
epoch£º269	 i:8 	 global-step:5388	 l-p:0.15266810357570648
epoch£º269	 i:9 	 global-step:5389	 l-p:0.16191448271274567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7559, 2.7315, 2.7531],
        [2.7559, 2.1537, 1.8167],
        [2.7559, 2.7520, 2.7558],
        [2.7559, 2.2576, 2.1430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.12622705101966858 
model_pd.l_d.mean(): -25.015174865722656 
model_pd.lagr.mean(): -24.888948440551758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0128], device='cuda:0')), ('power', tensor([-25.0024], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.12622705101966858
epoch£º270	 i:1 	 global-step:5401	 l-p:0.13805589079856873
epoch£º270	 i:2 	 global-step:5402	 l-p:0.12078827619552612
epoch£º270	 i:3 	 global-step:5403	 l-p:0.14544104039669037
epoch£º270	 i:4 	 global-step:5404	 l-p:0.1303890496492386
epoch£º270	 i:5 	 global-step:5405	 l-p:0.12487819045782089
epoch£º270	 i:6 	 global-step:5406	 l-p:0.13102489709854126
epoch£º270	 i:7 	 global-step:5407	 l-p:0.12103589624166489
epoch£º270	 i:8 	 global-step:5408	 l-p:0.13296440243721008
epoch£º270	 i:9 	 global-step:5409	 l-p:0.14695127308368683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6018, 2.6018, 2.6018],
        [2.6018, 2.1092, 2.0302],
        [2.6018, 1.9600, 1.5570],
        [2.6018, 2.0527, 1.6101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.216343492269516 
model_pd.l_d.mean(): -25.01479721069336 
model_pd.lagr.mean(): -24.79845428466797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1331], device='cuda:0')), ('power', tensor([-25.1479], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.216343492269516
epoch£º271	 i:1 	 global-step:5421	 l-p:0.13163228332996368
epoch£º271	 i:2 	 global-step:5422	 l-p:0.18370860815048218
epoch£º271	 i:3 	 global-step:5423	 l-p:0.16516004502773285
epoch£º271	 i:4 	 global-step:5424	 l-p:0.15152886509895325
epoch£º271	 i:5 	 global-step:5425	 l-p:0.12890252470970154
epoch£º271	 i:6 	 global-step:5426	 l-p:0.17823772132396698
epoch£º271	 i:7 	 global-step:5427	 l-p:0.13223415613174438
epoch£º271	 i:8 	 global-step:5428	 l-p:0.14062857627868652
epoch£º271	 i:9 	 global-step:5429	 l-p:0.12890052795410156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7410, 2.6808, 2.7283],
        [2.7410, 2.1269, 1.6824],
        [2.7410, 2.6246, 2.7009],
        [2.7410, 2.7025, 2.7351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.16031929850578308 
model_pd.l_d.mean(): -24.46641731262207 
model_pd.lagr.mean(): -24.30609893798828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0970], device='cuda:0')), ('power', tensor([-24.5634], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.16031929850578308
epoch£º272	 i:1 	 global-step:5441	 l-p:0.12960384786128998
epoch£º272	 i:2 	 global-step:5442	 l-p:0.11184215545654297
epoch£º272	 i:3 	 global-step:5443	 l-p:0.15544173121452332
epoch£º272	 i:4 	 global-step:5444	 l-p:0.14709867537021637
epoch£º272	 i:5 	 global-step:5445	 l-p:0.1327882558107376
epoch£º272	 i:6 	 global-step:5446	 l-p:0.1787734180688858
epoch£º272	 i:7 	 global-step:5447	 l-p:0.14368855953216553
epoch£º272	 i:8 	 global-step:5448	 l-p:0.18200786411762238
epoch£º272	 i:9 	 global-step:5449	 l-p:0.15372569859027863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6695, 2.2327, 2.2210],
        [2.6695, 2.6586, 2.6688],
        [2.6695, 2.5274, 2.6133],
        [2.6695, 2.6106, 2.6575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.14877088367938995 
model_pd.l_d.mean(): -24.22105598449707 
model_pd.lagr.mean(): -24.072284698486328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2075], device='cuda:0')), ('power', tensor([-24.4286], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.14877088367938995
epoch£º273	 i:1 	 global-step:5461	 l-p:0.14009784162044525
epoch£º273	 i:2 	 global-step:5462	 l-p:0.12800340354442596
epoch£º273	 i:3 	 global-step:5463	 l-p:0.14149732887744904
epoch£º273	 i:4 	 global-step:5464	 l-p:0.11869010329246521
epoch£º273	 i:5 	 global-step:5465	 l-p:0.13228489458560944
epoch£º273	 i:6 	 global-step:5466	 l-p:0.13362036645412445
epoch£º273	 i:7 	 global-step:5467	 l-p:0.1379668265581131
epoch£º273	 i:8 	 global-step:5468	 l-p:-1.8014583587646484
epoch£º273	 i:9 	 global-step:5469	 l-p:0.1936502754688263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6278, 1.9990, 1.6997],
        [2.6278, 2.0648, 1.6140],
        [2.6278, 2.6131, 2.6266],
        [2.6278, 2.0640, 1.6133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.14934863150119781 
model_pd.l_d.mean(): -25.077014923095703 
model_pd.lagr.mean(): -24.92766571044922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0643], device='cuda:0')), ('power', tensor([-25.1413], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.14934863150119781
epoch£º274	 i:1 	 global-step:5481	 l-p:0.13987025618553162
epoch£º274	 i:2 	 global-step:5482	 l-p:0.15768224000930786
epoch£º274	 i:3 	 global-step:5483	 l-p:0.15920040011405945
epoch£º274	 i:4 	 global-step:5484	 l-p:0.14224092662334442
epoch£º274	 i:5 	 global-step:5485	 l-p:0.14330749213695526
epoch£º274	 i:6 	 global-step:5486	 l-p:0.1272081583738327
epoch£º274	 i:7 	 global-step:5487	 l-p:0.12720544636249542
epoch£º274	 i:8 	 global-step:5488	 l-p:0.16585570573806763
epoch£º274	 i:9 	 global-step:5489	 l-p:0.176918163895607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7279, 2.0828, 1.7108],
        [2.7279, 2.0790, 1.6908],
        [2.7279, 2.7279, 2.7279],
        [2.7279, 2.6684, 2.7157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.14594075083732605 
model_pd.l_d.mean(): -25.097265243530273 
model_pd.lagr.mean(): -24.951324462890625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0131], device='cuda:0')), ('power', tensor([-25.0842], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.14594075083732605
epoch£º275	 i:1 	 global-step:5501	 l-p:0.13133446872234344
epoch£º275	 i:2 	 global-step:5502	 l-p:0.12104562669992447
epoch£º275	 i:3 	 global-step:5503	 l-p:0.14306795597076416
epoch£º275	 i:4 	 global-step:5504	 l-p:0.12950187921524048
epoch£º275	 i:5 	 global-step:5505	 l-p:0.14413900673389435
epoch£º275	 i:6 	 global-step:5506	 l-p:0.11447720229625702
epoch£º275	 i:7 	 global-step:5507	 l-p:0.10035107284784317
epoch£º275	 i:8 	 global-step:5508	 l-p:0.15601596236228943
epoch£º275	 i:9 	 global-step:5509	 l-p:0.1444079726934433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6774, 2.0815, 1.6248],
        [2.6774, 2.6089, 2.6620],
        [2.6774, 2.6774, 2.6774],
        [2.6774, 2.6700, 2.6770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.14222446084022522 
model_pd.l_d.mean(): -25.1433162689209 
model_pd.lagr.mean(): -25.00109100341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0130], device='cuda:0')), ('power', tensor([-25.1563], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.14222446084022522
epoch£º276	 i:1 	 global-step:5521	 l-p:0.1941782683134079
epoch£º276	 i:2 	 global-step:5522	 l-p:0.08184005320072174
epoch£º276	 i:3 	 global-step:5523	 l-p:0.16952979564666748
epoch£º276	 i:4 	 global-step:5524	 l-p:0.1522466093301773
epoch£º276	 i:5 	 global-step:5525	 l-p:0.11094249784946442
epoch£º276	 i:6 	 global-step:5526	 l-p:0.21025367081165314
epoch£º276	 i:7 	 global-step:5527	 l-p:0.13190899789333344
epoch£º276	 i:8 	 global-step:5528	 l-p:0.12297920137643814
epoch£º276	 i:9 	 global-step:5529	 l-p:0.1753132939338684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6747, 2.0210, 1.5806],
        [2.6747, 2.0445, 1.5934],
        [2.6747, 2.6737, 2.6747],
        [2.6747, 2.0062, 1.5926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.16100278496742249 
model_pd.l_d.mean(): -24.93505859375 
model_pd.lagr.mean(): -24.77405548095703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1486], device='cuda:0')), ('power', tensor([-25.0837], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.16100278496742249
epoch£º277	 i:1 	 global-step:5541	 l-p:0.1290668100118637
epoch£º277	 i:2 	 global-step:5542	 l-p:0.13623711466789246
epoch£º277	 i:3 	 global-step:5543	 l-p:0.15337087213993073
epoch£º277	 i:4 	 global-step:5544	 l-p:0.21356891095638275
epoch£º277	 i:5 	 global-step:5545	 l-p:0.15365691483020782
epoch£º277	 i:6 	 global-step:5546	 l-p:0.14070816338062286
epoch£º277	 i:7 	 global-step:5547	 l-p:0.1951005458831787
epoch£º277	 i:8 	 global-step:5548	 l-p:0.12335969507694244
epoch£º277	 i:9 	 global-step:5549	 l-p:0.13862943649291992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8074, 2.7749, 2.8030],
        [2.8074, 2.6967, 2.7718],
        [2.8074, 2.5281, 2.6180],
        [2.8074, 2.1818, 1.7123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.12325386703014374 
model_pd.l_d.mean(): -24.490995407104492 
model_pd.lagr.mean(): -24.367740631103516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0716], device='cuda:0')), ('power', tensor([-24.5626], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.12325386703014374
epoch£º278	 i:1 	 global-step:5561	 l-p:0.1421593278646469
epoch£º278	 i:2 	 global-step:5562	 l-p:0.15472926199436188
epoch£º278	 i:3 	 global-step:5563	 l-p:0.10667995363473892
epoch£º278	 i:4 	 global-step:5564	 l-p:0.13735869526863098
epoch£º278	 i:5 	 global-step:5565	 l-p:0.12209326773881912
epoch£º278	 i:6 	 global-step:5566	 l-p:0.1302870661020279
epoch£º278	 i:7 	 global-step:5567	 l-p:0.13174356520175934
epoch£º278	 i:8 	 global-step:5568	 l-p:0.1873149275779724
epoch£º278	 i:9 	 global-step:5569	 l-p:0.14886455237865448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5664, 2.1623, 2.2043],
        [2.5664, 2.4270, 2.5148],
        [2.5664, 1.8865, 1.4624],
        [2.5664, 2.1967, 2.2605]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.17451211810112 
model_pd.l_d.mean(): -25.02670669555664 
model_pd.lagr.mean(): -24.85219383239746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1228], device='cuda:0')), ('power', tensor([-25.1495], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.17451211810112
epoch£º279	 i:1 	 global-step:5581	 l-p:0.046802274882793427
epoch£º279	 i:2 	 global-step:5582	 l-p:0.13804247975349426
epoch£º279	 i:3 	 global-step:5583	 l-p:0.14265795052051544
epoch£º279	 i:4 	 global-step:5584	 l-p:0.1742882877588272
epoch£º279	 i:5 	 global-step:5585	 l-p:0.16355057060718536
epoch£º279	 i:6 	 global-step:5586	 l-p:0.14308606088161469
epoch£º279	 i:7 	 global-step:5587	 l-p:0.12104255706071854
epoch£º279	 i:8 	 global-step:5588	 l-p:0.13615067303180695
epoch£º279	 i:9 	 global-step:5589	 l-p:0.1411030888557434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8198, 2.8155, 2.8196],
        [2.8198, 2.6863, 2.7708],
        [2.8198, 2.5790, 2.6776],
        [2.8198, 2.4521, 2.5064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.12829896807670593 
model_pd.l_d.mean(): -24.82295799255371 
model_pd.lagr.mean(): -24.694658279418945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0026], device='cuda:0')), ('power', tensor([-24.8255], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.12829896807670593
epoch£º280	 i:1 	 global-step:5601	 l-p:0.13359969854354858
epoch£º280	 i:2 	 global-step:5602	 l-p:0.15264034271240234
epoch£º280	 i:3 	 global-step:5603	 l-p:0.1444232165813446
epoch£º280	 i:4 	 global-step:5604	 l-p:0.15279950201511383
epoch£º280	 i:5 	 global-step:5605	 l-p:0.17667579650878906
epoch£º280	 i:6 	 global-step:5606	 l-p:0.12232308834791183
epoch£º280	 i:7 	 global-step:5607	 l-p:0.13680866360664368
epoch£º280	 i:8 	 global-step:5608	 l-p:0.14528757333755493
epoch£º280	 i:9 	 global-step:5609	 l-p:0.1638486385345459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6842, 2.1290, 2.0061],
        [2.6842, 2.6819, 2.6842],
        [2.6842, 2.6294, 2.6740],
        [2.6842, 2.4495, 2.5518]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): -0.22070632874965668 
model_pd.l_d.mean(): -25.078065872192383 
model_pd.lagr.mean(): -25.29877281188965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0731], device='cuda:0')), ('power', tensor([-25.1512], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:-0.22070632874965668
epoch£º281	 i:1 	 global-step:5621	 l-p:0.1506725549697876
epoch£º281	 i:2 	 global-step:5622	 l-p:0.14838238060474396
epoch£º281	 i:3 	 global-step:5623	 l-p:0.11377700418233871
epoch£º281	 i:4 	 global-step:5624	 l-p:0.1556348204612732
epoch£º281	 i:5 	 global-step:5625	 l-p:0.11307448893785477
epoch£º281	 i:6 	 global-step:5626	 l-p:0.14398038387298584
epoch£º281	 i:7 	 global-step:5627	 l-p:0.13715215027332306
epoch£º281	 i:8 	 global-step:5628	 l-p:0.13998501002788544
epoch£º281	 i:9 	 global-step:5629	 l-p:0.1312226504087448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[2.6195, 2.3685, 2.4721],
        [2.6195, 2.0143, 1.8292],
        [2.6195, 2.3369, 2.4363],
        [2.6195, 1.9945, 1.5373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.11135848611593246 
model_pd.l_d.mean(): -24.32482147216797 
model_pd.lagr.mean(): -24.213462829589844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3408], device='cuda:0')), ('power', tensor([-24.6657], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.11135848611593246
epoch£º282	 i:1 	 global-step:5641	 l-p:0.1599590927362442
epoch£º282	 i:2 	 global-step:5642	 l-p:0.2411072850227356
epoch£º282	 i:3 	 global-step:5643	 l-p:0.19952483475208282
epoch£º282	 i:4 	 global-step:5644	 l-p:0.16148875653743744
epoch£º282	 i:5 	 global-step:5645	 l-p:0.1492728441953659
epoch£º282	 i:6 	 global-step:5646	 l-p:0.09795878827571869
epoch£º282	 i:7 	 global-step:5647	 l-p:0.10562843084335327
epoch£º282	 i:8 	 global-step:5648	 l-p:0.1223229393362999
epoch£º282	 i:9 	 global-step:5649	 l-p:0.12687644362449646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8645, 2.2034, 1.8389],
        [2.8645, 2.2790, 1.7845],
        [2.8645, 2.2034, 1.8389],
        [2.8645, 2.8461, 2.8629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.1341281533241272 
model_pd.l_d.mean(): -24.83325958251953 
model_pd.lagr.mean(): -24.69913101196289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0451], device='cuda:0')), ('power', tensor([-24.8784], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.1341281533241272
epoch£º283	 i:1 	 global-step:5661	 l-p:0.12802229821681976
epoch£º283	 i:2 	 global-step:5662	 l-p:0.1382422000169754
epoch£º283	 i:3 	 global-step:5663	 l-p:0.17881660163402557
epoch£º283	 i:4 	 global-step:5664	 l-p:0.08307003974914551
epoch£º283	 i:5 	 global-step:5665	 l-p:0.16537830233573914
epoch£º283	 i:6 	 global-step:5666	 l-p:0.1581735908985138
epoch£º283	 i:7 	 global-step:5667	 l-p:0.09312158077955246
epoch£º283	 i:8 	 global-step:5668	 l-p:0.19200335443019867
epoch£º283	 i:9 	 global-step:5669	 l-p:0.17250943183898926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8060, 2.7496, 2.7953],
        [2.8060, 2.7743, 2.8020],
        [2.8060, 2.8045, 2.8060],
        [2.8060, 2.2001, 1.9890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.15720687806606293 
model_pd.l_d.mean(): -25.121095657348633 
model_pd.lagr.mean(): -24.96388816833496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0146], device='cuda:0')), ('power', tensor([-25.1065], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.15720687806606293
epoch£º284	 i:1 	 global-step:5681	 l-p:0.13680192828178406
epoch£º284	 i:2 	 global-step:5682	 l-p:0.1222691684961319
epoch£º284	 i:3 	 global-step:5683	 l-p:0.1307297796010971
epoch£º284	 i:4 	 global-step:5684	 l-p:0.12299560010433197
epoch£º284	 i:5 	 global-step:5685	 l-p:0.14693570137023926
epoch£º284	 i:6 	 global-step:5686	 l-p:0.13999347388744354
epoch£º284	 i:7 	 global-step:5687	 l-p:0.14842107892036438
epoch£º284	 i:8 	 global-step:5688	 l-p:0.13061195611953735
epoch£º284	 i:9 	 global-step:5689	 l-p:0.10793985426425934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7448, 2.7448, 2.7448],
        [2.7448, 2.6984, 2.7372],
        [2.7448, 2.4922, 2.5952],
        [2.7448, 2.0687, 1.7272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.11974227428436279 
model_pd.l_d.mean(): -25.03350067138672 
model_pd.lagr.mean(): -24.913759231567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0461], device='cuda:0')), ('power', tensor([-25.0796], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.11974227428436279
epoch£º285	 i:1 	 global-step:5701	 l-p:0.13882094621658325
epoch£º285	 i:2 	 global-step:5702	 l-p:0.15882274508476257
epoch£º285	 i:3 	 global-step:5703	 l-p:0.13844813406467438
epoch£º285	 i:4 	 global-step:5704	 l-p:0.14658628404140472
epoch£º285	 i:5 	 global-step:5705	 l-p:0.11373663693666458
epoch£º285	 i:6 	 global-step:5706	 l-p:0.10017381608486176
epoch£º285	 i:7 	 global-step:5707	 l-p:0.1459692120552063
epoch£º285	 i:8 	 global-step:5708	 l-p:-0.1573372483253479
epoch£º285	 i:9 	 global-step:5709	 l-p:0.1419784128665924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6300, 2.5474, 2.6099],
        [2.6300, 2.6298, 2.6300],
        [2.6300, 2.6300, 2.6300],
        [2.6300, 2.6187, 2.6292]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.14640413224697113 
model_pd.l_d.mean(): -25.062557220458984 
model_pd.lagr.mean(): -24.916152954101562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0686], device='cuda:0')), ('power', tensor([-25.1311], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.14640413224697113
epoch£º286	 i:1 	 global-step:5721	 l-p:0.1498597264289856
epoch£º286	 i:2 	 global-step:5722	 l-p:0.23011265695095062
epoch£º286	 i:3 	 global-step:5723	 l-p:0.2563283145427704
epoch£º286	 i:4 	 global-step:5724	 l-p:0.04460679739713669
epoch£º286	 i:5 	 global-step:5725	 l-p:0.11809303611516953
epoch£º286	 i:6 	 global-step:5726	 l-p:0.12301462143659592
epoch£º286	 i:7 	 global-step:5727	 l-p:0.11697126179933548
epoch£º286	 i:8 	 global-step:5728	 l-p:0.13354989886283875
epoch£º286	 i:9 	 global-step:5729	 l-p:0.12377563863992691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8948, 2.2417, 1.9284],
        [2.8948, 2.2113, 1.8058],
        [2.8948, 2.5343, 2.6021],
        [2.8948, 2.7672, 2.8508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.11450239270925522 
model_pd.l_d.mean(): -24.95946502685547 
model_pd.lagr.mean(): -24.84496307373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0248], device='cuda:0')), ('power', tensor([-24.9347], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.11450239270925522
epoch£º287	 i:1 	 global-step:5741	 l-p:0.11898267269134521
epoch£º287	 i:2 	 global-step:5742	 l-p:0.15112893283367157
epoch£º287	 i:3 	 global-step:5743	 l-p:0.19031226634979248
epoch£º287	 i:4 	 global-step:5744	 l-p:0.1528228372335434
epoch£º287	 i:5 	 global-step:5745	 l-p:0.17996223270893097
epoch£º287	 i:6 	 global-step:5746	 l-p:0.1881028115749359
epoch£º287	 i:7 	 global-step:5747	 l-p:0.16570830345153809
epoch£º287	 i:8 	 global-step:5748	 l-p:0.15442202985286713
epoch£º287	 i:9 	 global-step:5749	 l-p:0.16738618910312653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8436, 2.7684, 2.8263],
        [2.8436, 2.8436, 2.8436],
        [2.8436, 2.8373, 2.8433],
        [2.8436, 2.2180, 1.9865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.12691037356853485 
model_pd.l_d.mean(): -25.019224166870117 
model_pd.lagr.mean(): -24.89231300354004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0049], device='cuda:0')), ('power', tensor([-25.0143], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.12691037356853485
epoch£º288	 i:1 	 global-step:5761	 l-p:0.1380726844072342
epoch£º288	 i:2 	 global-step:5762	 l-p:0.10742103308439255
epoch£º288	 i:3 	 global-step:5763	 l-p:0.12893103063106537
epoch£º288	 i:4 	 global-step:5764	 l-p:0.1341067999601364
epoch£º288	 i:5 	 global-step:5765	 l-p:0.12851330637931824
epoch£º288	 i:6 	 global-step:5766	 l-p:0.13786713778972626
epoch£º288	 i:7 	 global-step:5767	 l-p:0.1356499046087265
epoch£º288	 i:8 	 global-step:5768	 l-p:0.11232093721628189
epoch£º288	 i:9 	 global-step:5769	 l-p:-0.0032194708473980427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5837, 1.8470, 1.4438],
        [2.5837, 2.5774, 2.5834],
        [2.5837, 2.5792, 2.5835],
        [2.5837, 2.5083, 2.5668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.14054171741008759 
model_pd.l_d.mean(): -24.421829223632812 
model_pd.lagr.mean(): -24.281288146972656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2331], device='cuda:0')), ('power', tensor([-24.6549], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.14054171741008759
epoch£º289	 i:1 	 global-step:5781	 l-p:0.11127017438411713
epoch£º289	 i:2 	 global-step:5782	 l-p:0.5011588335037231
epoch£º289	 i:3 	 global-step:5783	 l-p:0.14923641085624695
epoch£º289	 i:4 	 global-step:5784	 l-p:0.1417708396911621
epoch£º289	 i:5 	 global-step:5785	 l-p:0.1699792891740799
epoch£º289	 i:6 	 global-step:5786	 l-p:0.1488749086856842
epoch£º289	 i:7 	 global-step:5787	 l-p:0.13655228912830353
epoch£º289	 i:8 	 global-step:5788	 l-p:0.12523823976516724
epoch£º289	 i:9 	 global-step:5789	 l-p:0.147454634308815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7243, 2.7242, 2.7243],
        [2.7243, 2.7050, 2.7225],
        [2.7243, 2.7243, 2.7243],
        [2.7243, 2.3562, 2.4326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.1438780426979065 
model_pd.l_d.mean(): -25.039825439453125 
model_pd.lagr.mean(): -24.895946502685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0032], device='cuda:0')), ('power', tensor([-25.0366], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.1438780426979065
epoch£º290	 i:1 	 global-step:5801	 l-p:0.14472199976444244
epoch£º290	 i:2 	 global-step:5802	 l-p:0.1543319821357727
epoch£º290	 i:3 	 global-step:5803	 l-p:0.18994948267936707
epoch£º290	 i:4 	 global-step:5804	 l-p:0.19889357686042786
epoch£º290	 i:5 	 global-step:5805	 l-p:0.10924148559570312
epoch£º290	 i:6 	 global-step:5806	 l-p:0.12359663099050522
epoch£º290	 i:7 	 global-step:5807	 l-p:0.16652388870716095
epoch£º290	 i:8 	 global-step:5808	 l-p:0.49393343925476074
epoch£º290	 i:9 	 global-step:5809	 l-p:0.19137533009052277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6387, 2.1966, 2.2340],
        [2.6387, 1.9089, 1.5381],
        [2.6387, 2.5590, 2.6202],
        [2.6387, 2.6385, 2.6387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 2.1643269062042236 
model_pd.l_d.mean(): -25.175594329833984 
model_pd.lagr.mean(): -23.011266708374023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0216], device='cuda:0')), ('power', tensor([-25.1972], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:2.1643269062042236
epoch£º291	 i:1 	 global-step:5821	 l-p:0.1139678955078125
epoch£º291	 i:2 	 global-step:5822	 l-p:0.1529756486415863
epoch£º291	 i:3 	 global-step:5823	 l-p:0.13356776535511017
epoch£º291	 i:4 	 global-step:5824	 l-p:0.11907844245433807
epoch£º291	 i:5 	 global-step:5825	 l-p:0.11884599179029465
epoch£º291	 i:6 	 global-step:5826	 l-p:0.142535001039505
epoch£º291	 i:7 	 global-step:5827	 l-p:0.15994790196418762
epoch£º291	 i:8 	 global-step:5828	 l-p:-0.004798131063580513
epoch£º291	 i:9 	 global-step:5829	 l-p:0.15545609593391418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6643, 2.6642, 2.6643],
        [2.6643, 2.6322, 2.6603],
        [2.6643, 2.6618, 2.6642],
        [2.6643, 2.6642, 2.6643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.15618844330310822 
model_pd.l_d.mean(): -24.86722183227539 
model_pd.lagr.mean(): -24.71103286743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1139], device='cuda:0')), ('power', tensor([-24.9811], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.15618844330310822
epoch£º292	 i:1 	 global-step:5841	 l-p:0.15648886561393738
epoch£º292	 i:2 	 global-step:5842	 l-p:0.14089363813400269
epoch£º292	 i:3 	 global-step:5843	 l-p:0.14207689464092255
epoch£º292	 i:4 	 global-step:5844	 l-p:-0.06526228040456772
epoch£º292	 i:5 	 global-step:5845	 l-p:0.18404650688171387
epoch£º292	 i:6 	 global-step:5846	 l-p:0.14735649526119232
epoch£º292	 i:7 	 global-step:5847	 l-p:0.07927015423774719
epoch£º292	 i:8 	 global-step:5848	 l-p:0.20867572724819183
epoch£º292	 i:9 	 global-step:5849	 l-p:0.15581634640693665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[2.6036, 2.0885, 2.0710],
        [2.6036, 2.0965, 2.0867],
        [2.6036, 2.3326, 2.4437],
        [2.6036, 2.3802, 2.4912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.15615208446979523 
model_pd.l_d.mean(): -25.13658332824707 
model_pd.lagr.mean(): -24.980430603027344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1003], device='cuda:0')), ('power', tensor([-25.2369], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.15615208446979523
epoch£º293	 i:1 	 global-step:5861	 l-p:0.14935711026191711
epoch£º293	 i:2 	 global-step:5862	 l-p:0.11145725846290588
epoch£º293	 i:3 	 global-step:5863	 l-p:0.027360739186406136
epoch£º293	 i:4 	 global-step:5864	 l-p:0.14701935648918152
epoch£º293	 i:5 	 global-step:5865	 l-p:0.12790612876415253
epoch£º293	 i:6 	 global-step:5866	 l-p:0.1703374981880188
epoch£º293	 i:7 	 global-step:5867	 l-p:0.31209075450897217
epoch£º293	 i:8 	 global-step:5868	 l-p:0.14954087138175964
epoch£º293	 i:9 	 global-step:5869	 l-p:0.13752414286136627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[2.8431, 2.5337, 2.6337],
        [2.8431, 2.1131, 1.6854],
        [2.8431, 2.1111, 1.6484],
        [2.8431, 2.2932, 2.2163]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.14137090742588043 
model_pd.l_d.mean(): -24.602449417114258 
model_pd.lagr.mean(): -24.461078643798828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0345], device='cuda:0')), ('power', tensor([-24.6370], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.14137090742588043
epoch£º294	 i:1 	 global-step:5881	 l-p:0.13986293971538544
epoch£º294	 i:2 	 global-step:5882	 l-p:0.14378701150417328
epoch£º294	 i:3 	 global-step:5883	 l-p:0.13479194045066833
epoch£º294	 i:4 	 global-step:5884	 l-p:0.11555114388465881
epoch£º294	 i:5 	 global-step:5885	 l-p:0.1514207124710083
epoch£º294	 i:6 	 global-step:5886	 l-p:0.23672665655612946
epoch£º294	 i:7 	 global-step:5887	 l-p:0.1274501532316208
epoch£º294	 i:8 	 global-step:5888	 l-p:0.15717469155788422
epoch£º294	 i:9 	 global-step:5889	 l-p:-0.302006334066391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6958, 2.2713, 2.3271],
        [2.6958, 2.6944, 2.6957],
        [2.6958, 2.6864, 2.6952],
        [2.6958, 2.6957, 2.6958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.15783153474330902 
model_pd.l_d.mean(): -25.223876953125 
model_pd.lagr.mean(): -25.0660457611084 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0498], device='cuda:0')), ('power', tensor([-25.2737], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.15783153474330902
epoch£º295	 i:1 	 global-step:5901	 l-p:0.13603465259075165
epoch£º295	 i:2 	 global-step:5902	 l-p:0.09528917074203491
epoch£º295	 i:3 	 global-step:5903	 l-p:0.15667445957660675
epoch£º295	 i:4 	 global-step:5904	 l-p:0.1613655388355255
epoch£º295	 i:5 	 global-step:5905	 l-p:0.13794663548469543
epoch£º295	 i:6 	 global-step:5906	 l-p:0.11798257380723953
epoch£º295	 i:7 	 global-step:5907	 l-p:0.12503542006015778
epoch£º295	 i:8 	 global-step:5908	 l-p:0.13382725417613983
epoch£º295	 i:9 	 global-step:5909	 l-p:0.12981224060058594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7784, 2.3037, 2.3189],
        [2.7784, 2.7409, 2.7733],
        [2.7784, 2.7738, 2.7782],
        [2.7784, 2.7784, 2.7784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.1485472321510315 
model_pd.l_d.mean(): -25.002296447753906 
model_pd.lagr.mean(): -24.853748321533203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0219], device='cuda:0')), ('power', tensor([-25.0242], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.1485472321510315
epoch£º296	 i:1 	 global-step:5921	 l-p:0.15837550163269043
epoch£º296	 i:2 	 global-step:5922	 l-p:2.2793562412261963
epoch£º296	 i:3 	 global-step:5923	 l-p:0.07502501457929611
epoch£º296	 i:4 	 global-step:5924	 l-p:0.12482776492834091
epoch£º296	 i:5 	 global-step:5925	 l-p:0.9902129173278809
epoch£º296	 i:6 	 global-step:5926	 l-p:0.2836620509624481
epoch£º296	 i:7 	 global-step:5927	 l-p:0.19764521718025208
epoch£º296	 i:8 	 global-step:5928	 l-p:0.16750693321228027
epoch£º296	 i:9 	 global-step:5929	 l-p:0.03987501561641693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8363, 2.2023, 1.6956],
        [2.8363, 2.4512, 2.5261],
        [2.8363, 2.7575, 2.8183],
        [2.8363, 2.8316, 2.8361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.127780944108963 
model_pd.l_d.mean(): -24.6307315826416 
model_pd.lagr.mean(): -24.50295066833496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0950], device='cuda:0')), ('power', tensor([-24.7258], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.127780944108963
epoch£º297	 i:1 	 global-step:5941	 l-p:0.11954797804355621
epoch£º297	 i:2 	 global-step:5942	 l-p:0.11795016378164291
epoch£º297	 i:3 	 global-step:5943	 l-p:0.13285230100154877
epoch£º297	 i:4 	 global-step:5944	 l-p:0.11740277707576752
epoch£º297	 i:5 	 global-step:5945	 l-p:0.11222892254590988
epoch£º297	 i:6 	 global-step:5946	 l-p:0.17098943889141083
epoch£º297	 i:7 	 global-step:5947	 l-p:0.1568933129310608
epoch£º297	 i:8 	 global-step:5948	 l-p:0.19899703562259674
epoch£º297	 i:9 	 global-step:5949	 l-p:0.1341506540775299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6963, 2.6962, 2.6963],
        [2.6963, 2.4809, 2.5922],
        [2.6963, 2.6955, 2.6963],
        [2.6963, 2.1256, 1.6308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.14008699357509613 
model_pd.l_d.mean(): -24.738433837890625 
model_pd.lagr.mean(): -24.598346710205078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0932], device='cuda:0')), ('power', tensor([-24.8316], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.14008699357509613
epoch£º298	 i:1 	 global-step:5961	 l-p:0.16422481834888458
epoch£º298	 i:2 	 global-step:5962	 l-p:0.1604980081319809
epoch£º298	 i:3 	 global-step:5963	 l-p:0.1345949023962021
epoch£º298	 i:4 	 global-step:5964	 l-p:0.10866749286651611
epoch£º298	 i:5 	 global-step:5965	 l-p:0.12977799773216248
epoch£º298	 i:6 	 global-step:5966	 l-p:0.18011455237865448
epoch£º298	 i:7 	 global-step:5967	 l-p:0.14889682829380035
epoch£º298	 i:8 	 global-step:5968	 l-p:0.13225224614143372
epoch£º298	 i:9 	 global-step:5969	 l-p:0.1468067318201065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7894, 2.7290, 2.7782],
        [2.7894, 2.4681, 2.5718],
        [2.7894, 2.2704, 2.2507],
        [2.7894, 2.1154, 1.8775]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.12131602317094803 
model_pd.l_d.mean(): -25.010900497436523 
model_pd.lagr.mean(): -24.889583587646484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0344], device='cuda:0')), ('power', tensor([-25.0453], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.12131602317094803
epoch£º299	 i:1 	 global-step:5981	 l-p:0.1792432963848114
epoch£º299	 i:2 	 global-step:5982	 l-p:0.19244466722011566
epoch£º299	 i:3 	 global-step:5983	 l-p:0.14186790585517883
epoch£º299	 i:4 	 global-step:5984	 l-p:0.1501998007297516
epoch£º299	 i:5 	 global-step:5985	 l-p:0.18100745975971222
epoch£º299	 i:6 	 global-step:5986	 l-p:0.15413562953472137
epoch£º299	 i:7 	 global-step:5987	 l-p:0.09118381887674332
epoch£º299	 i:8 	 global-step:5988	 l-p:0.11872994899749756
epoch£º299	 i:9 	 global-step:5989	 l-p:0.42283716797828674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7221, 2.0529, 1.5607],
        [2.7221, 2.5607, 2.6605],
        [2.7221, 2.7221, 2.7221],
        [2.7221, 2.5666, 2.6645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.17664115130901337 
model_pd.l_d.mean(): -25.10462760925293 
model_pd.lagr.mean(): -24.92798614501953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0377], device='cuda:0')), ('power', tensor([-25.1423], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.17664115130901337
epoch£º300	 i:1 	 global-step:6001	 l-p:0.20581397414207458
epoch£º300	 i:2 	 global-step:6002	 l-p:0.10457513481378555
epoch£º300	 i:3 	 global-step:6003	 l-p:0.1206948384642601
epoch£º300	 i:4 	 global-step:6004	 l-p:0.12659433484077454
epoch£º300	 i:5 	 global-step:6005	 l-p:0.11224964261054993
epoch£º300	 i:6 	 global-step:6006	 l-p:0.12127932906150818
epoch£º300	 i:7 	 global-step:6007	 l-p:0.09117443114519119
epoch£º300	 i:8 	 global-step:6008	 l-p:0.07683958113193512
epoch£º300	 i:9 	 global-step:6009	 l-p:0.12243040651082993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2126, 2.9189, 3.0197],
        [3.2126, 3.2126, 3.2126],
        [3.2126, 2.5552, 2.2610],
        [3.2126, 3.2126, 3.2126]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.15151453018188477 
model_pd.l_d.mean(): -24.46717643737793 
model_pd.lagr.mean(): -24.315662384033203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0794], device='cuda:0')), ('power', tensor([-24.3877], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.15151453018188477
epoch£º301	 i:1 	 global-step:6021	 l-p:0.02826380915939808
epoch£º301	 i:2 	 global-step:6022	 l-p:0.16175803542137146
epoch£º301	 i:3 	 global-step:6023	 l-p:0.11155269294977188
epoch£º301	 i:4 	 global-step:6024	 l-p:0.0970623791217804
epoch£º301	 i:5 	 global-step:6025	 l-p:0.10105058550834656
epoch£º301	 i:6 	 global-step:6026	 l-p:0.10601615905761719
epoch£º301	 i:7 	 global-step:6027	 l-p:0.12201639264822006
epoch£º301	 i:8 	 global-step:6028	 l-p:0.09664063155651093
epoch£º301	 i:9 	 global-step:6029	 l-p:0.11173049360513687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3253, 2.9789, 3.0588],
        [3.3253, 3.3188, 3.3250],
        [3.3253, 2.7432, 2.1682],
        [3.3253, 3.3195, 3.3250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.11033102869987488 
model_pd.l_d.mean(): -24.925142288208008 
model_pd.lagr.mean(): -24.81481170654297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2570], device='cuda:0')), ('power', tensor([-24.6682], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.11033102869987488
epoch£º302	 i:1 	 global-step:6041	 l-p:0.11205132305622101
epoch£º302	 i:2 	 global-step:6042	 l-p:-0.0005255222204141319
epoch£º302	 i:3 	 global-step:6043	 l-p:0.10074535012245178
epoch£º302	 i:4 	 global-step:6044	 l-p:0.13408970832824707
epoch£º302	 i:5 	 global-step:6045	 l-p:0.12953417003154755
epoch£º302	 i:6 	 global-step:6046	 l-p:0.23937071859836578
epoch£º302	 i:7 	 global-step:6047	 l-p:0.16357506811618805
epoch£º302	 i:8 	 global-step:6048	 l-p:0.17930908501148224
epoch£º302	 i:9 	 global-step:6049	 l-p:-0.18514640629291534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6091, 2.6090, 2.6091],
        [2.6091, 2.2594, 2.3640],
        [2.6091, 1.9951, 1.5075],
        [2.6091, 2.0919, 2.0965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.16716822981834412 
model_pd.l_d.mean(): -24.491073608398438 
model_pd.lagr.mean(): -24.32390594482422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1679], device='cuda:0')), ('power', tensor([-24.6590], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.16716822981834412
epoch£º303	 i:1 	 global-step:6061	 l-p:0.1967727243900299
epoch£º303	 i:2 	 global-step:6062	 l-p:0.1485617309808731
epoch£º303	 i:3 	 global-step:6063	 l-p:0.19287873804569244
epoch£º303	 i:4 	 global-step:6064	 l-p:0.1506781131029129
epoch£º303	 i:5 	 global-step:6065	 l-p:0.15111251175403595
epoch£º303	 i:6 	 global-step:6066	 l-p:0.11051177978515625
epoch£º303	 i:7 	 global-step:6067	 l-p:0.1213453859090805
epoch£º303	 i:8 	 global-step:6068	 l-p:0.13058720529079437
epoch£º303	 i:9 	 global-step:6069	 l-p:0.07671111822128296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9601, 2.9601, 2.9601],
        [2.9601, 2.9601, 2.9601],
        [2.9601, 2.9537, 2.9598],
        [2.9601, 2.9579, 2.9601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.1294105499982834 
model_pd.l_d.mean(): -24.452978134155273 
model_pd.lagr.mean(): -24.32356834411621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0021], device='cuda:0')), ('power', tensor([-24.4509], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.1294105499982834
epoch£º304	 i:1 	 global-step:6081	 l-p:0.08172778785228729
epoch£º304	 i:2 	 global-step:6082	 l-p:0.13258054852485657
epoch£º304	 i:3 	 global-step:6083	 l-p:0.14017605781555176
epoch£º304	 i:4 	 global-step:6084	 l-p:0.14195169508457184
epoch£º304	 i:5 	 global-step:6085	 l-p:0.17636092007160187
epoch£º304	 i:6 	 global-step:6086	 l-p:0.08628464490175247
epoch£º304	 i:7 	 global-step:6087	 l-p:0.22084857523441315
epoch£º304	 i:8 	 global-step:6088	 l-p:0.14537429809570312
epoch£º304	 i:9 	 global-step:6089	 l-p:0.15697437524795532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6770, 2.1427, 2.1307],
        [2.6770, 2.6763, 2.6770],
        [2.6770, 1.9847, 1.4971],
        [2.6770, 2.3937, 2.5106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.18221037089824677 
model_pd.l_d.mean(): -24.928823471069336 
model_pd.lagr.mean(): -24.746612548828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1424], device='cuda:0')), ('power', tensor([-25.0712], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.18221037089824677
epoch£º305	 i:1 	 global-step:6101	 l-p:0.14978210628032684
epoch£º305	 i:2 	 global-step:6102	 l-p:0.13812173902988434
epoch£º305	 i:3 	 global-step:6103	 l-p:0.10963765531778336
epoch£º305	 i:4 	 global-step:6104	 l-p:-0.13433149456977844
epoch£º305	 i:5 	 global-step:6105	 l-p:0.1477513611316681
epoch£º305	 i:6 	 global-step:6106	 l-p:0.13963575661182404
epoch£º305	 i:7 	 global-step:6107	 l-p:0.15835049748420715
epoch£º305	 i:8 	 global-step:6108	 l-p:0.11572635173797607
epoch£º305	 i:9 	 global-step:6109	 l-p:0.10516323149204254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9411, 2.9411, 2.9411],
        [2.9411, 2.2732, 1.7446],
        [2.9411, 2.9411, 2.9411],
        [2.9411, 2.3023, 2.1296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.12674136459827423 
model_pd.l_d.mean(): -24.411062240600586 
model_pd.lagr.mean(): -24.284320831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0463], device='cuda:0')), ('power', tensor([-24.4574], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.12674136459827423
epoch£º306	 i:1 	 global-step:6121	 l-p:0.11573883146047592
epoch£º306	 i:2 	 global-step:6122	 l-p:0.13022111356258392
epoch£º306	 i:3 	 global-step:6123	 l-p:0.3194236755371094
epoch£º306	 i:4 	 global-step:6124	 l-p:0.1576661616563797
epoch£º306	 i:5 	 global-step:6125	 l-p:0.15949012339115143
epoch£º306	 i:6 	 global-step:6126	 l-p:0.17884624004364014
epoch£º306	 i:7 	 global-step:6127	 l-p:0.13776622712612152
epoch£º306	 i:8 	 global-step:6128	 l-p:-0.12192761898040771
epoch£º306	 i:9 	 global-step:6129	 l-p:0.19806867837905884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7898, 2.7898, 2.7898],
        [2.7898, 2.5896, 2.7007],
        [2.7898, 2.0142, 1.5306],
        [2.7898, 2.6104, 2.7166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.1500447690486908 
model_pd.l_d.mean(): -25.124969482421875 
model_pd.lagr.mean(): -24.974924087524414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0642], device='cuda:0')), ('power', tensor([-25.1892], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.1500447690486908
epoch£º307	 i:1 	 global-step:6141	 l-p:0.130892813205719
epoch£º307	 i:2 	 global-step:6142	 l-p:0.11568190902471542
epoch£º307	 i:3 	 global-step:6143	 l-p:0.13703827559947968
epoch£º307	 i:4 	 global-step:6144	 l-p:0.13459904491901398
epoch£º307	 i:5 	 global-step:6145	 l-p:0.02962423302233219
epoch£º307	 i:6 	 global-step:6146	 l-p:9.408181190490723
epoch£º307	 i:7 	 global-step:6147	 l-p:0.27922412753105164
epoch£º307	 i:8 	 global-step:6148	 l-p:0.17115779221057892
epoch£º307	 i:9 	 global-step:6149	 l-p:0.1315317004919052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7136, 2.6579, 2.7041],
        [2.7136, 2.6343, 2.6963],
        [2.7136, 2.4763, 2.5941],
        [2.7136, 2.0561, 1.8992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.136079803109169 
model_pd.l_d.mean(): -25.24432373046875 
model_pd.lagr.mean(): -25.108243942260742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0022], device='cuda:0')), ('power', tensor([-25.2422], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.136079803109169
epoch£º308	 i:1 	 global-step:6161	 l-p:0.3025330901145935
epoch£º308	 i:2 	 global-step:6162	 l-p:0.13630688190460205
epoch£º308	 i:3 	 global-step:6163	 l-p:0.1499181091785431
epoch£º308	 i:4 	 global-step:6164	 l-p:0.15562164783477783
epoch£º308	 i:5 	 global-step:6165	 l-p:0.22076000273227692
epoch£º308	 i:6 	 global-step:6166	 l-p:0.1325533390045166
epoch£º308	 i:7 	 global-step:6167	 l-p:0.17269611358642578
epoch£º308	 i:8 	 global-step:6168	 l-p:0.16320379078388214
epoch£º308	 i:9 	 global-step:6169	 l-p:0.12047789990901947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9335, 2.5271, 2.6045],
        [2.9335, 2.1578, 1.7202],
        [2.9335, 2.9320, 2.9334],
        [2.9335, 2.1567, 1.6535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.12086287885904312 
model_pd.l_d.mean(): -24.40635108947754 
model_pd.lagr.mean(): -24.28548812866211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0195], device='cuda:0')), ('power', tensor([-24.4259], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.12086287885904312
epoch£º309	 i:1 	 global-step:6181	 l-p:0.13036629557609558
epoch£º309	 i:2 	 global-step:6182	 l-p:0.1425551325082779
epoch£º309	 i:3 	 global-step:6183	 l-p:0.12213104218244553
epoch£º309	 i:4 	 global-step:6184	 l-p:0.13476409018039703
epoch£º309	 i:5 	 global-step:6185	 l-p:0.16325576603412628
epoch£º309	 i:6 	 global-step:6186	 l-p:0.1810736358165741
epoch£º309	 i:7 	 global-step:6187	 l-p:0.05936351791024208
epoch£º309	 i:8 	 global-step:6188	 l-p:0.28642967343330383
epoch£º309	 i:9 	 global-step:6189	 l-p:0.1437690258026123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8144, 2.7672, 2.8073],
        [2.8144, 2.8133, 2.8144],
        [2.8144, 2.0229, 1.6100],
        [2.8144, 2.0504, 1.5486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.16769108176231384 
model_pd.l_d.mean(): -25.174673080444336 
model_pd.lagr.mean(): -25.006982803344727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0198], device='cuda:0')), ('power', tensor([-25.1945], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.16769108176231384
epoch£º310	 i:1 	 global-step:6201	 l-p:0.20664137601852417
epoch£º310	 i:2 	 global-step:6202	 l-p:0.12853869795799255
epoch£º310	 i:3 	 global-step:6203	 l-p:0.12183782458305359
epoch£º310	 i:4 	 global-step:6204	 l-p:0.16239269077777863
epoch£º310	 i:5 	 global-step:6205	 l-p:0.152199849486351
epoch£º310	 i:6 	 global-step:6206	 l-p:0.1596541553735733
epoch£º310	 i:7 	 global-step:6207	 l-p:0.14586277306079865
epoch£º310	 i:8 	 global-step:6208	 l-p:0.11465658247470856
epoch£º310	 i:9 	 global-step:6209	 l-p:0.14957796037197113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[2.7518, 2.1921, 2.1678],
        [2.7518, 2.2045, 2.1926],
        [2.7518, 2.3609, 2.4563],
        [2.7518, 2.1255, 2.0235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.12800918519496918 
model_pd.l_d.mean(): -24.519893646240234 
model_pd.lagr.mean(): -24.391883850097656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1404], device='cuda:0')), ('power', tensor([-24.6603], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.12800918519496918
epoch£º311	 i:1 	 global-step:6221	 l-p:0.5814390182495117
epoch£º311	 i:2 	 global-step:6222	 l-p:-1.5689116716384888
epoch£º311	 i:3 	 global-step:6223	 l-p:0.1520654410123825
epoch£º311	 i:4 	 global-step:6224	 l-p:0.1389153003692627
epoch£º311	 i:5 	 global-step:6225	 l-p:0.1363835632801056
epoch£º311	 i:6 	 global-step:6226	 l-p:0.08944863080978394
epoch£º311	 i:7 	 global-step:6227	 l-p:0.08348660171031952
epoch£º311	 i:8 	 global-step:6228	 l-p:0.1663353443145752
epoch£º311	 i:9 	 global-step:6229	 l-p:0.15148794651031494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7255, 2.1499, 2.1149],
        [2.7255, 2.6857, 2.7202],
        [2.7255, 2.7255, 2.7255],
        [2.7255, 1.9368, 1.4520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.26685279607772827 
model_pd.l_d.mean(): -25.16921615600586 
model_pd.lagr.mean(): -24.902362823486328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0475], device='cuda:0')), ('power', tensor([-25.2167], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.26685279607772827
epoch£º312	 i:1 	 global-step:6241	 l-p:0.0901716947555542
epoch£º312	 i:2 	 global-step:6242	 l-p:0.10770399123430252
epoch£º312	 i:3 	 global-step:6243	 l-p:0.15280544757843018
epoch£º312	 i:4 	 global-step:6244	 l-p:0.15093015134334564
epoch£º312	 i:5 	 global-step:6245	 l-p:0.18147607147693634
epoch£º312	 i:6 	 global-step:6246	 l-p:0.15246279537677765
epoch£º312	 i:7 	 global-step:6247	 l-p:0.14354462921619415
epoch£º312	 i:8 	 global-step:6248	 l-p:0.15498189628124237
epoch£º312	 i:9 	 global-step:6249	 l-p:-1.0197752714157104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6150, 2.4169, 2.5316],
        [2.6150, 1.8624, 1.3817],
        [2.6150, 2.0280, 1.9943],
        [2.6150, 2.5463, 2.6018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.21602223813533783 
model_pd.l_d.mean(): -25.132211685180664 
model_pd.lagr.mean(): -24.916189193725586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1081], device='cuda:0')), ('power', tensor([-25.2403], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.21602223813533783
epoch£º313	 i:1 	 global-step:6261	 l-p:0.1703866869211197
epoch£º313	 i:2 	 global-step:6262	 l-p:0.05596094951033592
epoch£º313	 i:3 	 global-step:6263	 l-p:0.19050253927707672
epoch£º313	 i:4 	 global-step:6264	 l-p:0.14781230688095093
epoch£º313	 i:5 	 global-step:6265	 l-p:0.20452874898910522
epoch£º313	 i:6 	 global-step:6266	 l-p:0.13561783730983734
epoch£º313	 i:7 	 global-step:6267	 l-p:0.12363152205944061
epoch£º313	 i:8 	 global-step:6268	 l-p:0.11289601773023605
epoch£º313	 i:9 	 global-step:6269	 l-p:0.13967306911945343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8582, 2.8582, 2.8582],
        [2.8582, 2.7724, 2.8386],
        [2.8582, 2.5704, 2.6909],
        [2.8582, 2.4825, 2.5845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.12650790810585022 
model_pd.l_d.mean(): -24.800243377685547 
model_pd.lagr.mean(): -24.673734664916992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0797], device='cuda:0')), ('power', tensor([-24.8800], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.12650790810585022
epoch£º314	 i:1 	 global-step:6281	 l-p:0.14850136637687683
epoch£º314	 i:2 	 global-step:6282	 l-p:0.18955819308757782
epoch£º314	 i:3 	 global-step:6283	 l-p:0.1607414186000824
epoch£º314	 i:4 	 global-step:6284	 l-p:0.12868189811706543
epoch£º314	 i:5 	 global-step:6285	 l-p:0.17804598808288574
epoch£º314	 i:6 	 global-step:6286	 l-p:0.18246160447597504
epoch£º314	 i:7 	 global-step:6287	 l-p:0.14975765347480774
epoch£º314	 i:8 	 global-step:6288	 l-p:0.10131703317165375
epoch£º314	 i:9 	 global-step:6289	 l-p:0.14835621416568756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8373, 2.0323, 1.6298],
        [2.8373, 2.3008, 2.3048],
        [2.8373, 2.8373, 2.8373],
        [2.8373, 2.0252, 1.5278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.13894477486610413 
model_pd.l_d.mean(): -24.948925018310547 
model_pd.lagr.mean(): -24.809980392456055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0410], device='cuda:0')), ('power', tensor([-24.9899], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.13894477486610413
epoch£º315	 i:1 	 global-step:6301	 l-p:0.24056926369667053
epoch£º315	 i:2 	 global-step:6302	 l-p:0.11348262429237366
epoch£º315	 i:3 	 global-step:6303	 l-p:0.1371985524892807
epoch£º315	 i:4 	 global-step:6304	 l-p:0.14095138013362885
epoch£º315	 i:5 	 global-step:6305	 l-p:0.2811647057533264
epoch£º315	 i:6 	 global-step:6306	 l-p:-0.17488010227680206
epoch£º315	 i:7 	 global-step:6307	 l-p:0.21990466117858887
epoch£º315	 i:8 	 global-step:6308	 l-p:0.1346137374639511
epoch£º315	 i:9 	 global-step:6309	 l-p:0.15032142400741577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7544, 2.6960, 2.7444],
        [2.7544, 2.7452, 2.7539],
        [2.7544, 2.1214, 2.0306],
        [2.7544, 2.5428, 2.6604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.1513523906469345 
model_pd.l_d.mean(): -25.22906494140625 
model_pd.lagr.mean(): -25.077713012695312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0246], device='cuda:0')), ('power', tensor([-25.2045], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.1513523906469345
epoch£º316	 i:1 	 global-step:6321	 l-p:-0.7806283235549927
epoch£º316	 i:2 	 global-step:6322	 l-p:0.21223363280296326
epoch£º316	 i:3 	 global-step:6323	 l-p:0.15132032334804535
epoch£º316	 i:4 	 global-step:6324	 l-p:0.12056221067905426
epoch£º316	 i:5 	 global-step:6325	 l-p:0.149331733584404
epoch£º316	 i:6 	 global-step:6326	 l-p:0.12372245639562607
epoch£º316	 i:7 	 global-step:6327	 l-p:0.20287704467773438
epoch£º316	 i:8 	 global-step:6328	 l-p:0.12308020144701004
epoch£º316	 i:9 	 global-step:6329	 l-p:0.13642024993896484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8525, 2.6188, 2.7393],
        [2.8525, 2.8463, 2.8522],
        [2.8525, 2.4258, 2.5097],
        [2.8525, 2.4755, 2.5805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.13774189352989197 
model_pd.l_d.mean(): -24.985729217529297 
model_pd.lagr.mean(): -24.84798812866211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0501], device='cuda:0')), ('power', tensor([-25.0358], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.13774189352989197
epoch£º317	 i:1 	 global-step:6341	 l-p:0.12086416035890579
epoch£º317	 i:2 	 global-step:6342	 l-p:0.1595204770565033
epoch£º317	 i:3 	 global-step:6343	 l-p:0.20000913739204407
epoch£º317	 i:4 	 global-step:6344	 l-p:0.18621617555618286
epoch£º317	 i:5 	 global-step:6345	 l-p:0.1664956957101822
epoch£º317	 i:6 	 global-step:6346	 l-p:0.12597104907035828
epoch£º317	 i:7 	 global-step:6347	 l-p:0.12683500349521637
epoch£º317	 i:8 	 global-step:6348	 l-p:0.14928317070007324
epoch£º317	 i:9 	 global-step:6349	 l-p:0.14231523871421814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6882, 2.4380, 2.5628],
        [2.6882, 2.6870, 2.6882],
        [2.6882, 2.6261, 2.6773],
        [2.6882, 2.6843, 2.6881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.5742888450622559 
model_pd.l_d.mean(): -24.988332748413086 
model_pd.lagr.mean(): -24.414043426513672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1487], device='cuda:0')), ('power', tensor([-25.1371], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.5742888450622559
epoch£º318	 i:1 	 global-step:6361	 l-p:0.15749521553516388
epoch£º318	 i:2 	 global-step:6362	 l-p:0.14871607720851898
epoch£º318	 i:3 	 global-step:6363	 l-p:0.2215830534696579
epoch£º318	 i:4 	 global-step:6364	 l-p:0.14359740912914276
epoch£º318	 i:5 	 global-step:6365	 l-p:0.219747856259346
epoch£º318	 i:6 	 global-step:6366	 l-p:0.20031367242336273
epoch£º318	 i:7 	 global-step:6367	 l-p:0.20125919580459595
epoch£º318	 i:8 	 global-step:6368	 l-p:0.11725732684135437
epoch£º318	 i:9 	 global-step:6369	 l-p:0.12207198888063431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0088, 2.6304, 2.7323],
        [3.0088, 2.4892, 2.5046],
        [3.0088, 3.0088, 3.0088],
        [3.0088, 3.0088, 3.0088]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.13811033964157104 
model_pd.l_d.mean(): -25.041213989257812 
model_pd.lagr.mean(): -24.90310287475586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0400], device='cuda:0')), ('power', tensor([-25.0012], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.13811033964157104
epoch£º319	 i:1 	 global-step:6381	 l-p:0.11135315895080566
epoch£º319	 i:2 	 global-step:6382	 l-p:0.12665987014770508
epoch£º319	 i:3 	 global-step:6383	 l-p:0.13705119490623474
epoch£º319	 i:4 	 global-step:6384	 l-p:0.15304698050022125
epoch£º319	 i:5 	 global-step:6385	 l-p:0.15014302730560303
epoch£º319	 i:6 	 global-step:6386	 l-p:0.1536174714565277
epoch£º319	 i:7 	 global-step:6387	 l-p:0.11523694545030594
epoch£º319	 i:8 	 global-step:6388	 l-p:-0.2599467933177948
epoch£º319	 i:9 	 global-step:6389	 l-p:0.34346309304237366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6491, 2.1151, 2.1455],
        [2.6491, 1.9550, 1.8105],
        [2.6491, 1.9882, 1.8881],
        [2.6491, 2.6231, 2.6466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.0530678816139698 
model_pd.l_d.mean(): -24.965286254882812 
model_pd.lagr.mean(): -24.91221809387207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1153], device='cuda:0')), ('power', tensor([-25.0806], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.0530678816139698
epoch£º320	 i:1 	 global-step:6401	 l-p:0.17115499079227448
epoch£º320	 i:2 	 global-step:6402	 l-p:0.1001594215631485
epoch£º320	 i:3 	 global-step:6403	 l-p:0.11962040513753891
epoch£º320	 i:4 	 global-step:6404	 l-p:0.15993589162826538
epoch£º320	 i:5 	 global-step:6405	 l-p:0.1313529908657074
epoch£º320	 i:6 	 global-step:6406	 l-p:0.12735404074192047
epoch£º320	 i:7 	 global-step:6407	 l-p:0.15889981389045715
epoch£º320	 i:8 	 global-step:6408	 l-p:0.19234269857406616
epoch£º320	 i:9 	 global-step:6409	 l-p:0.16873666644096375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7638, 2.6306, 2.7231],
        [2.7638, 1.9103, 1.4429],
        [2.7638, 2.7275, 2.7594],
        [2.7638, 2.7619, 2.7638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.12956248223781586 
model_pd.l_d.mean(): -24.69023323059082 
model_pd.lagr.mean(): -24.560670852661133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1097], device='cuda:0')), ('power', tensor([-24.7999], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.12956248223781586
epoch£º321	 i:1 	 global-step:6421	 l-p:0.43612799048423767
epoch£º321	 i:2 	 global-step:6422	 l-p:0.12332447618246078
epoch£º321	 i:3 	 global-step:6423	 l-p:0.14880771934986115
epoch£º321	 i:4 	 global-step:6424	 l-p:0.1386127769947052
epoch£º321	 i:5 	 global-step:6425	 l-p:0.49113813042640686
epoch£º321	 i:6 	 global-step:6426	 l-p:0.13758495450019836
epoch£º321	 i:7 	 global-step:6427	 l-p:0.2465996891260147
epoch£º321	 i:8 	 global-step:6428	 l-p:0.156589537858963
epoch£º321	 i:9 	 global-step:6429	 l-p:0.15857736766338348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8358, 2.2037, 1.6676],
        [2.8358, 2.8350, 2.8358],
        [2.8358, 2.8326, 2.8357],
        [2.8358, 2.8358, 2.8358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.15687474608421326 
model_pd.l_d.mean(): -25.102140426635742 
model_pd.lagr.mean(): -24.94526481628418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0008], device='cuda:0')), ('power', tensor([-25.1029], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.15687474608421326
epoch£º322	 i:1 	 global-step:6441	 l-p:0.15061889588832855
epoch£º322	 i:2 	 global-step:6442	 l-p:0.11928998678922653
epoch£º322	 i:3 	 global-step:6443	 l-p:0.18963399529457092
epoch£º322	 i:4 	 global-step:6444	 l-p:0.10305868834257126
epoch£º322	 i:5 	 global-step:6445	 l-p:0.1496991664171219
epoch£º322	 i:6 	 global-step:6446	 l-p:0.13883979618549347
epoch£º322	 i:7 	 global-step:6447	 l-p:-0.2755472958087921
epoch£º322	 i:8 	 global-step:6448	 l-p:0.3838053047657013
epoch£º322	 i:9 	 global-step:6449	 l-p:0.20183123648166656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8314, 2.8313, 2.8314],
        [2.8314, 2.7950, 2.8270],
        [2.8314, 2.0193, 1.6604],
        [2.8314, 2.6757, 2.7780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.14905337989330292 
model_pd.l_d.mean(): -25.017288208007812 
model_pd.lagr.mean(): -24.868234634399414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0603], device='cuda:0')), ('power', tensor([-25.0776], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.14905337989330292
epoch£º323	 i:1 	 global-step:6461	 l-p:0.136688232421875
epoch£º323	 i:2 	 global-step:6462	 l-p:0.08750265836715698
epoch£º323	 i:3 	 global-step:6463	 l-p:0.13319391012191772
epoch£º323	 i:4 	 global-step:6464	 l-p:0.1324305534362793
epoch£º323	 i:5 	 global-step:6465	 l-p:0.1310780942440033
epoch£º323	 i:6 	 global-step:6466	 l-p:0.39012137055397034
epoch£º323	 i:7 	 global-step:6467	 l-p:0.13817480206489563
epoch£º323	 i:8 	 global-step:6468	 l-p:0.14875487983226776
epoch£º323	 i:9 	 global-step:6469	 l-p:0.20991946756839752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6033, 2.6032, 2.6033],
        [2.6033, 2.6032, 2.6033],
        [2.6033, 2.5245, 2.5873],
        [2.6033, 2.1867, 2.2947]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.3646999001502991 
model_pd.l_d.mean(): -24.819578170776367 
model_pd.lagr.mean(): -24.454877853393555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3119], device='cuda:0')), ('power', tensor([-25.1315], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.3646999001502991
epoch£º324	 i:1 	 global-step:6481	 l-p:0.1931864321231842
epoch£º324	 i:2 	 global-step:6482	 l-p:0.14877815544605255
epoch£º324	 i:3 	 global-step:6483	 l-p:0.18460948765277863
epoch£º324	 i:4 	 global-step:6484	 l-p:0.11919893324375153
epoch£º324	 i:5 	 global-step:6485	 l-p:0.146405890583992
epoch£º324	 i:6 	 global-step:6486	 l-p:0.1301136016845703
epoch£º324	 i:7 	 global-step:6487	 l-p:0.12043464183807373
epoch£º324	 i:8 	 global-step:6488	 l-p:0.13860978186130524
epoch£º324	 i:9 	 global-step:6489	 l-p:0.14404241740703583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7993, 2.2158, 2.2038],
        [2.7993, 2.7211, 2.7833],
        [2.7993, 2.7981, 2.7993],
        [2.7993, 2.0312, 1.7718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.13367338478565216 
model_pd.l_d.mean(): -24.596019744873047 
model_pd.lagr.mean(): -24.46234703063965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0946], device='cuda:0')), ('power', tensor([-24.6906], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.13367338478565216
epoch£º325	 i:1 	 global-step:6501	 l-p:0.08342539519071579
epoch£º325	 i:2 	 global-step:6502	 l-p:0.08256940543651581
epoch£º325	 i:3 	 global-step:6503	 l-p:0.18765538930892944
epoch£º325	 i:4 	 global-step:6504	 l-p:0.15225641429424286
epoch£º325	 i:5 	 global-step:6505	 l-p:0.15355874598026276
epoch£º325	 i:6 	 global-step:6506	 l-p:0.24414214491844177
epoch£º325	 i:7 	 global-step:6507	 l-p:0.19878536462783813
epoch£º325	 i:8 	 global-step:6508	 l-p:0.15301771461963654
epoch£º325	 i:9 	 global-step:6509	 l-p:0.14835132658481598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9785, 2.2440, 2.0133],
        [2.9785, 2.9711, 2.9782],
        [2.9785, 2.3359, 1.7787],
        [2.9785, 2.2686, 2.0794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.1177453026175499 
model_pd.l_d.mean(): -24.93099594116211 
model_pd.lagr.mean(): -24.813251495361328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0216], device='cuda:0')), ('power', tensor([-24.9094], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.1177453026175499
epoch£º326	 i:1 	 global-step:6521	 l-p:0.11406157165765762
epoch£º326	 i:2 	 global-step:6522	 l-p:0.1479940116405487
epoch£º326	 i:3 	 global-step:6523	 l-p:0.15233555436134338
epoch£º326	 i:4 	 global-step:6524	 l-p:0.13905154168605804
epoch£º326	 i:5 	 global-step:6525	 l-p:0.21847224235534668
epoch£º326	 i:6 	 global-step:6526	 l-p:0.12548664212226868
epoch£º326	 i:7 	 global-step:6527	 l-p:0.15033124387264252
epoch£º326	 i:8 	 global-step:6528	 l-p:0.12707558274269104
epoch£º326	 i:9 	 global-step:6529	 l-p:0.14992257952690125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6406, 1.9592, 1.4435],
        [2.6406, 2.6393, 2.6406],
        [2.6406, 2.3236, 2.4552],
        [2.6406, 2.6323, 2.6402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.15933768451213837 
model_pd.l_d.mean(): -24.939655303955078 
model_pd.lagr.mean(): -24.780317306518555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1577], device='cuda:0')), ('power', tensor([-25.0974], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.15933768451213837
epoch£º327	 i:1 	 global-step:6541	 l-p:0.09073792397975922
epoch£º327	 i:2 	 global-step:6542	 l-p:0.1264805793762207
epoch£º327	 i:3 	 global-step:6543	 l-p:-0.438210129737854
epoch£º327	 i:4 	 global-step:6544	 l-p:0.1484048068523407
epoch£º327	 i:5 	 global-step:6545	 l-p:0.14657773077487946
epoch£º327	 i:6 	 global-step:6546	 l-p:0.11227795481681824
epoch£º327	 i:7 	 global-step:6547	 l-p:0.12755846977233887
epoch£º327	 i:8 	 global-step:6548	 l-p:0.1097554937005043
epoch£º327	 i:9 	 global-step:6549	 l-p:0.12307071685791016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9428, 2.8908, 2.9348],
        [2.9428, 2.9229, 2.9412],
        [2.9428, 2.2145, 2.0092],
        [2.9428, 2.1293, 1.5896]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.13444824516773224 
model_pd.l_d.mean(): -24.873485565185547 
model_pd.lagr.mean(): -24.739036560058594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0349], device='cuda:0')), ('power', tensor([-24.9083], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.13444824516773224
epoch£º328	 i:1 	 global-step:6561	 l-p:0.12726803123950958
epoch£º328	 i:2 	 global-step:6562	 l-p:0.11700231581926346
epoch£º328	 i:3 	 global-step:6563	 l-p:-0.06701470911502838
epoch£º328	 i:4 	 global-step:6564	 l-p:0.1332704722881317
epoch£º328	 i:5 	 global-step:6565	 l-p:0.1308930218219757
epoch£º328	 i:6 	 global-step:6566	 l-p:0.11676262319087982
epoch£º328	 i:7 	 global-step:6567	 l-p:0.21769095957279205
epoch£º328	 i:8 	 global-step:6568	 l-p:0.13931845128536224
epoch£º328	 i:9 	 global-step:6569	 l-p:0.10856849700212479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9081, 2.9080, 2.9081],
        [2.9081, 2.0410, 1.5733],
        [2.9081, 2.9013, 2.9078],
        [2.9081, 2.6513, 2.7789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.13250915706157684 
model_pd.l_d.mean(): -24.512664794921875 
model_pd.lagr.mean(): -24.380155563354492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1349], device='cuda:0')), ('power', tensor([-24.6476], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.13250915706157684
epoch£º329	 i:1 	 global-step:6581	 l-p:0.10684215277433395
epoch£º329	 i:2 	 global-step:6582	 l-p:0.12139207869768143
epoch£º329	 i:3 	 global-step:6583	 l-p:0.1560855507850647
epoch£º329	 i:4 	 global-step:6584	 l-p:0.135128915309906
epoch£º329	 i:5 	 global-step:6585	 l-p:-0.22456695139408112
epoch£º329	 i:6 	 global-step:6586	 l-p:0.15158964693546295
epoch£º329	 i:7 	 global-step:6587	 l-p:0.1744198203086853
epoch£º329	 i:8 	 global-step:6588	 l-p:0.1532210409641266
epoch£º329	 i:9 	 global-step:6589	 l-p:0.0720258504152298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.4937, 2.4877, 2.4934],
        [2.4937, 2.4848, 2.4932],
        [2.4937, 2.4886, 2.4935],
        [2.4937, 2.4937, 2.4937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.18866220116615295 
model_pd.l_d.mean(): -25.132156372070312 
model_pd.lagr.mean(): -24.94349479675293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2100], device='cuda:0')), ('power', tensor([-25.3421], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.18866220116615295
epoch£º330	 i:1 	 global-step:6601	 l-p:0.08156181871891022
epoch£º330	 i:2 	 global-step:6602	 l-p:0.15781570971012115
epoch£º330	 i:3 	 global-step:6603	 l-p:0.07721291482448578
epoch£º330	 i:4 	 global-step:6604	 l-p:0.16599036753177643
epoch£º330	 i:5 	 global-step:6605	 l-p:0.16157272458076477
epoch£º330	 i:6 	 global-step:6606	 l-p:0.12445293366909027
epoch£º330	 i:7 	 global-step:6607	 l-p:0.29683151841163635
epoch£º330	 i:8 	 global-step:6608	 l-p:0.15387888252735138
epoch£º330	 i:9 	 global-step:6609	 l-p:0.12160459160804749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9049, 2.1990, 2.0461],
        [2.9049, 2.2095, 1.6592],
        [2.9049, 2.8792, 2.9025],
        [2.9049, 2.1490, 1.6036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.11956361681222916 
model_pd.l_d.mean(): -24.92159652709961 
model_pd.lagr.mean(): -24.802032470703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0303], device='cuda:0')), ('power', tensor([-24.9519], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.11956361681222916
epoch£º331	 i:1 	 global-step:6621	 l-p:0.161396786570549
epoch£º331	 i:2 	 global-step:6622	 l-p:0.1301278918981552
epoch£º331	 i:3 	 global-step:6623	 l-p:0.37403208017349243
epoch£º331	 i:4 	 global-step:6624	 l-p:0.1574380248785019
epoch£º331	 i:5 	 global-step:6625	 l-p:0.15073122084140778
epoch£º331	 i:6 	 global-step:6626	 l-p:0.14086994528770447
epoch£º331	 i:7 	 global-step:6627	 l-p:0.2559017837047577
epoch£º331	 i:8 	 global-step:6628	 l-p:0.14121784269809723
epoch£º331	 i:9 	 global-step:6629	 l-p:0.13298116624355316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7117, 2.1462, 2.1733],
        [2.7117, 1.8423, 1.4682],
        [2.7117, 1.9824, 1.8282],
        [2.7117, 2.6891, 2.7098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.16643700003623962 
model_pd.l_d.mean(): -25.044950485229492 
model_pd.lagr.mean(): -24.87851333618164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1670], device='cuda:0')), ('power', tensor([-25.2120], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.16643700003623962
epoch£º332	 i:1 	 global-step:6641	 l-p:0.18491746485233307
epoch£º332	 i:2 	 global-step:6642	 l-p:-0.04972226917743683
epoch£º332	 i:3 	 global-step:6643	 l-p:0.12387488782405853
epoch£º332	 i:4 	 global-step:6644	 l-p:3.3762950897216797
epoch£º332	 i:5 	 global-step:6645	 l-p:0.1544446498155594
epoch£º332	 i:6 	 global-step:6646	 l-p:0.14508134126663208
epoch£º332	 i:7 	 global-step:6647	 l-p:0.1696833074092865
epoch£º332	 i:8 	 global-step:6648	 l-p:0.17739613354206085
epoch£º332	 i:9 	 global-step:6649	 l-p:0.12812532484531403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8409, 2.8409, 2.8409],
        [2.8409, 1.9490, 1.4932],
        [2.8409, 2.8409, 2.8409],
        [2.8409, 2.8408, 2.8409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.15588273108005524 
model_pd.l_d.mean(): -25.051681518554688 
model_pd.lagr.mean(): -24.89579963684082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0557], device='cuda:0')), ('power', tensor([-25.1074], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.15588273108005524
epoch£º333	 i:1 	 global-step:6661	 l-p:0.12977369129657745
epoch£º333	 i:2 	 global-step:6662	 l-p:0.13501913845539093
epoch£º333	 i:3 	 global-step:6663	 l-p:0.13790644705295563
epoch£º333	 i:4 	 global-step:6664	 l-p:0.1363685131072998
epoch£º333	 i:5 	 global-step:6665	 l-p:0.21984916925430298
epoch£º333	 i:6 	 global-step:6666	 l-p:0.14046508073806763
epoch£º333	 i:7 	 global-step:6667	 l-p:0.1518973559141159
epoch£º333	 i:8 	 global-step:6668	 l-p:0.17364402115345
epoch£º333	 i:9 	 global-step:6669	 l-p:0.5545173287391663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8600, 2.8570, 2.8600],
        [2.8600, 2.8394, 2.8584],
        [2.8600, 2.2023, 2.1325],
        [2.8600, 2.3955, 2.4859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.15253274142742157 
model_pd.l_d.mean(): -25.212535858154297 
model_pd.lagr.mean(): -25.06000328063965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0358], device='cuda:0')), ('power', tensor([-25.1767], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.15253274142742157
epoch£º334	 i:1 	 global-step:6681	 l-p:0.18976710736751556
epoch£º334	 i:2 	 global-step:6682	 l-p:0.1343439221382141
epoch£º334	 i:3 	 global-step:6683	 l-p:0.12945909798145294
epoch£º334	 i:4 	 global-step:6684	 l-p:0.10889743268489838
epoch£º334	 i:5 	 global-step:6685	 l-p:0.12822771072387695
epoch£º334	 i:6 	 global-step:6686	 l-p:0.13510818779468536
epoch£º334	 i:7 	 global-step:6687	 l-p:0.16716209053993225
epoch£º334	 i:8 	 global-step:6688	 l-p:0.1439225673675537
epoch£º334	 i:9 	 global-step:6689	 l-p:0.13552477955818176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6406, 2.3165, 2.4534],
        [2.6406, 1.8970, 1.3809],
        [2.6406, 2.6406, 2.6407],
        [2.6406, 1.9376, 1.4144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.1249316856265068 
model_pd.l_d.mean(): -24.568248748779297 
model_pd.lagr.mean(): -24.443317413330078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2154], device='cuda:0')), ('power', tensor([-24.7836], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.1249316856265068
epoch£º335	 i:1 	 global-step:6701	 l-p:-0.7456560134887695
epoch£º335	 i:2 	 global-step:6702	 l-p:0.13970941305160522
epoch£º335	 i:3 	 global-step:6703	 l-p:0.07606533169746399
epoch£º335	 i:4 	 global-step:6704	 l-p:0.38218119740486145
epoch£º335	 i:5 	 global-step:6705	 l-p:-0.02134408988058567
epoch£º335	 i:6 	 global-step:6706	 l-p:0.2713676393032074
epoch£º335	 i:7 	 global-step:6707	 l-p:0.11679711192846298
epoch£º335	 i:8 	 global-step:6708	 l-p:0.1357157826423645
epoch£º335	 i:9 	 global-step:6709	 l-p:0.11935584992170334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[3.0357, 2.7361, 2.8677],
        [3.0357, 2.5531, 2.6283],
        [3.0357, 2.1496, 1.6520],
        [3.0357, 2.6955, 2.8235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.11324715614318848 
model_pd.l_d.mean(): -24.584299087524414 
model_pd.lagr.mean(): -24.471052169799805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0668], device='cuda:0')), ('power', tensor([-24.5175], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.11324715614318848
epoch£º336	 i:1 	 global-step:6721	 l-p:0.13523449003696442
epoch£º336	 i:2 	 global-step:6722	 l-p:0.11327492445707321
epoch£º336	 i:3 	 global-step:6723	 l-p:0.18276527523994446
epoch£º336	 i:4 	 global-step:6724	 l-p:0.15035024285316467
epoch£º336	 i:5 	 global-step:6725	 l-p:0.1489897519350052
epoch£º336	 i:6 	 global-step:6726	 l-p:0.10323669016361237
epoch£º336	 i:7 	 global-step:6727	 l-p:0.16473694145679474
epoch£º336	 i:8 	 global-step:6728	 l-p:0.2572994828224182
epoch£º336	 i:9 	 global-step:6729	 l-p:0.05980127304792404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6414, 1.9958, 1.4603],
        [2.6414, 2.3911, 2.5250],
        [2.6414, 1.8115, 1.5412],
        [2.6414, 1.7728, 1.2838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.18346011638641357 
model_pd.l_d.mean(): -24.341472625732422 
model_pd.lagr.mean(): -24.15801239013672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2471], device='cuda:0')), ('power', tensor([-24.5885], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.18346011638641357
epoch£º337	 i:1 	 global-step:6741	 l-p:0.2704942524433136
epoch£º337	 i:2 	 global-step:6742	 l-p:0.20910777151584625
epoch£º337	 i:3 	 global-step:6743	 l-p:0.1235453262925148
epoch£º337	 i:4 	 global-step:6744	 l-p:0.13746632635593414
epoch£º337	 i:5 	 global-step:6745	 l-p:0.08093217760324478
epoch£º337	 i:6 	 global-step:6746	 l-p:0.09336278587579727
epoch£º337	 i:7 	 global-step:6747	 l-p:0.12080847471952438
epoch£º337	 i:8 	 global-step:6748	 l-p:0.12582898139953613
epoch£º337	 i:9 	 global-step:6749	 l-p:0.12129315733909607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0371, 2.6607, 2.7826],
        [3.0371, 2.8593, 2.9717],
        [3.0371, 3.0015, 3.0329],
        [3.0371, 3.0371, 3.0371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.1194213256239891 
model_pd.l_d.mean(): -24.89967155456543 
model_pd.lagr.mean(): -24.780250549316406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0394], device='cuda:0')), ('power', tensor([-24.8602], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.1194213256239891
epoch£º338	 i:1 	 global-step:6761	 l-p:0.11600110679864883
epoch£º338	 i:2 	 global-step:6762	 l-p:0.18196141719818115
epoch£º338	 i:3 	 global-step:6763	 l-p:0.1409628838300705
epoch£º338	 i:4 	 global-step:6764	 l-p:0.12642723321914673
epoch£º338	 i:5 	 global-step:6765	 l-p:0.2733250856399536
epoch£º338	 i:6 	 global-step:6766	 l-p:-0.6980776190757751
epoch£º338	 i:7 	 global-step:6767	 l-p:0.01080179214477539
epoch£º338	 i:8 	 global-step:6768	 l-p:0.15111654996871948
epoch£º338	 i:9 	 global-step:6769	 l-p:0.19043710827827454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8323, 2.0093, 1.4751],
        [2.8323, 2.5536, 2.6888],
        [2.8323, 2.6449, 2.7619],
        [2.8323, 2.8323, 2.8323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.13399028778076172 
model_pd.l_d.mean(): -25.05101776123047 
model_pd.lagr.mean(): -24.91702651977539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0656], device='cuda:0')), ('power', tensor([-25.1167], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.13399028778076172
epoch£º339	 i:1 	 global-step:6781	 l-p:0.14293193817138672
epoch£º339	 i:2 	 global-step:6782	 l-p:0.12462323904037476
epoch£º339	 i:3 	 global-step:6783	 l-p:0.14728105068206787
epoch£º339	 i:4 	 global-step:6784	 l-p:0.14011186361312866
epoch£º339	 i:5 	 global-step:6785	 l-p:0.15513937175273895
epoch£º339	 i:6 	 global-step:6786	 l-p:0.15496991574764252
epoch£º339	 i:7 	 global-step:6787	 l-p:0.137330561876297
epoch£º339	 i:8 	 global-step:6788	 l-p:0.19000175595283508
epoch£º339	 i:9 	 global-step:6789	 l-p:0.22949962317943573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7812, 1.8790, 1.3776],
        [2.7812, 2.7644, 2.7800],
        [2.7812, 2.7801, 2.7812],
        [2.7812, 2.4102, 2.5411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.13942338526248932 
model_pd.l_d.mean(): -24.858224868774414 
model_pd.lagr.mean(): -24.718801498413086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1282], device='cuda:0')), ('power', tensor([-24.9864], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.13942338526248932
epoch£º340	 i:1 	 global-step:6801	 l-p:0.15736930072307587
epoch£º340	 i:2 	 global-step:6802	 l-p:-0.17538093030452728
epoch£º340	 i:3 	 global-step:6803	 l-p:0.18851861357688904
epoch£º340	 i:4 	 global-step:6804	 l-p:0.1480272263288498
epoch£º340	 i:5 	 global-step:6805	 l-p:0.16898123919963837
epoch£º340	 i:6 	 global-step:6806	 l-p:0.4617967903614044
epoch£º340	 i:7 	 global-step:6807	 l-p:0.1342327892780304
epoch£º340	 i:8 	 global-step:6808	 l-p:0.16827525198459625
epoch£º340	 i:9 	 global-step:6809	 l-p:0.1185634508728981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9315, 2.9258, 2.9313],
        [2.9315, 2.4442, 2.5277],
        [2.9315, 2.0634, 1.5198],
        [2.9315, 2.0706, 1.5253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.13423503935337067 
model_pd.l_d.mean(): -24.934345245361328 
model_pd.lagr.mean(): -24.80010986328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0755], device='cuda:0')), ('power', tensor([-25.0098], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.13423503935337067
epoch£º341	 i:1 	 global-step:6821	 l-p:0.1412597894668579
epoch£º341	 i:2 	 global-step:6822	 l-p:-0.04682986065745354
epoch£º341	 i:3 	 global-step:6823	 l-p:0.1255149394273758
epoch£º341	 i:4 	 global-step:6824	 l-p:0.14457961916923523
epoch£º341	 i:5 	 global-step:6825	 l-p:0.13719221949577332
epoch£º341	 i:6 	 global-step:6826	 l-p:0.1570267677307129
epoch£º341	 i:7 	 global-step:6827	 l-p:0.14227771759033203
epoch£º341	 i:8 	 global-step:6828	 l-p:0.16533035039901733
epoch£º341	 i:9 	 global-step:6829	 l-p:0.16137738525867462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6313, 2.6313, 2.6313],
        [2.6313, 1.7946, 1.2924],
        [2.6313, 1.8923, 1.3693],
        [2.6313, 1.7950, 1.2927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.2192668467760086 
model_pd.l_d.mean(): -25.209121704101562 
model_pd.lagr.mean(): -24.98985481262207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1394], device='cuda:0')), ('power', tensor([-25.3486], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.2192668467760086
epoch£º342	 i:1 	 global-step:6841	 l-p:0.03137587010860443
epoch£º342	 i:2 	 global-step:6842	 l-p:0.14877229928970337
epoch£º342	 i:3 	 global-step:6843	 l-p:0.12268376350402832
epoch£º342	 i:4 	 global-step:6844	 l-p:0.12032166868448257
epoch£º342	 i:5 	 global-step:6845	 l-p:0.16594815254211426
epoch£º342	 i:6 	 global-step:6846	 l-p:0.14087995886802673
epoch£º342	 i:7 	 global-step:6847	 l-p:0.036946941167116165
epoch£º342	 i:8 	 global-step:6848	 l-p:0.13479222357273102
epoch£º342	 i:9 	 global-step:6849	 l-p:0.13811691105365753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9629, 2.1394, 1.5803],
        [2.9629, 2.9629, 2.9629],
        [2.9629, 2.2461, 1.6775],
        [2.9629, 2.8127, 2.9153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.1324276477098465 
model_pd.l_d.mean(): -25.11524200439453 
model_pd.lagr.mean(): -24.98281478881836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0739], device='cuda:0')), ('power', tensor([-25.0414], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.1324276477098465
epoch£º343	 i:1 	 global-step:6861	 l-p:0.13342563807964325
epoch£º343	 i:2 	 global-step:6862	 l-p:0.13920342922210693
epoch£º343	 i:3 	 global-step:6863	 l-p:0.10080226510763168
epoch£º343	 i:4 	 global-step:6864	 l-p:0.14611782133579254
epoch£º343	 i:5 	 global-step:6865	 l-p:0.2286955565214157
epoch£º343	 i:6 	 global-step:6866	 l-p:0.16700002551078796
epoch£º343	 i:7 	 global-step:6867	 l-p:0.08370578289031982
epoch£º343	 i:8 	 global-step:6868	 l-p:0.16443602740764618
epoch£º343	 i:9 	 global-step:6869	 l-p:0.15495967864990234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[2.8235, 2.0869, 1.5370],
        [2.8235, 1.9955, 1.4582],
        [2.8235, 2.3506, 2.4501],
        [2.8235, 2.3722, 2.4814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.15247882902622223 
model_pd.l_d.mean(): -25.1627197265625 
model_pd.lagr.mean(): -25.01024055480957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0068], device='cuda:0')), ('power', tensor([-25.1560], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.15247882902622223
epoch£º344	 i:1 	 global-step:6881	 l-p:0.17521673440933228
epoch£º344	 i:2 	 global-step:6882	 l-p:0.12595275044441223
epoch£º344	 i:3 	 global-step:6883	 l-p:0.11538674682378769
epoch£º344	 i:4 	 global-step:6884	 l-p:0.13623502850532532
epoch£º344	 i:5 	 global-step:6885	 l-p:0.14382125437259674
epoch£º344	 i:6 	 global-step:6886	 l-p:0.12610970437526703
epoch£º344	 i:7 	 global-step:6887	 l-p:0.2809591591358185
epoch£º344	 i:8 	 global-step:6888	 l-p:0.16373853385448456
epoch£º344	 i:9 	 global-step:6889	 l-p:0.05686085671186447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6752, 2.5697, 2.6502],
        [2.6752, 1.9308, 1.3994],
        [2.6752, 2.5406, 2.6370],
        [2.6752, 1.8325, 1.5654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.14743107557296753 
model_pd.l_d.mean(): -24.714916229248047 
model_pd.lagr.mean(): -24.567485809326172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1336], device='cuda:0')), ('power', tensor([-24.8485], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.14743107557296753
epoch£º345	 i:1 	 global-step:6901	 l-p:0.16494567692279816
epoch£º345	 i:2 	 global-step:6902	 l-p:0.13283024728298187
epoch£º345	 i:3 	 global-step:6903	 l-p:0.15370652079582214
epoch£º345	 i:4 	 global-step:6904	 l-p:0.19754867255687714
epoch£º345	 i:5 	 global-step:6905	 l-p:0.16085314750671387
epoch£º345	 i:6 	 global-step:6906	 l-p:0.11633708328008652
epoch£º345	 i:7 	 global-step:6907	 l-p:0.14054900407791138
epoch£º345	 i:8 	 global-step:6908	 l-p:0.12395743280649185
epoch£º345	 i:9 	 global-step:6909	 l-p:0.10510602593421936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9586, 2.8973, 2.9485],
        [2.9586, 2.9579, 2.9586],
        [2.9586, 2.7797, 2.8944],
        [2.9586, 2.0829, 1.7020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.1173146441578865 
model_pd.l_d.mean(): -24.801130294799805 
model_pd.lagr.mean(): -24.683815002441406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0749], device='cuda:0')), ('power', tensor([-24.8760], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.1173146441578865
epoch£º346	 i:1 	 global-step:6921	 l-p:0.14044855535030365
epoch£º346	 i:2 	 global-step:6922	 l-p:0.17172515392303467
epoch£º346	 i:3 	 global-step:6923	 l-p:0.12826000154018402
epoch£º346	 i:4 	 global-step:6924	 l-p:0.07769887149333954
epoch£º346	 i:5 	 global-step:6925	 l-p:0.17135605216026306
epoch£º346	 i:6 	 global-step:6926	 l-p:0.14330658316612244
epoch£º346	 i:7 	 global-step:6927	 l-p:0.15279150009155273
epoch£º346	 i:8 	 global-step:6928	 l-p:0.19256097078323364
epoch£º346	 i:9 	 global-step:6929	 l-p:0.04876948893070221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7582, 2.7582, 2.7582],
        [2.7582, 1.9087, 1.6232],
        [2.7582, 2.0441, 1.4960],
        [2.7582, 2.7318, 2.7558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.13959455490112305 
model_pd.l_d.mean(): -24.88978385925293 
model_pd.lagr.mean(): -24.75018882751465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1626], device='cuda:0')), ('power', tensor([-25.0524], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.13959455490112305
epoch£º347	 i:1 	 global-step:6941	 l-p:0.2149689793586731
epoch£º347	 i:2 	 global-step:6942	 l-p:0.21341201663017273
epoch£º347	 i:3 	 global-step:6943	 l-p:0.13513346016407013
epoch£º347	 i:4 	 global-step:6944	 l-p:0.10634750872850418
epoch£º347	 i:5 	 global-step:6945	 l-p:0.1114940270781517
epoch£º347	 i:6 	 global-step:6946	 l-p:0.1295412927865982
epoch£º347	 i:7 	 global-step:6947	 l-p:0.10769788175821304
epoch£º347	 i:8 	 global-step:6948	 l-p:0.11395949870347977
epoch£º347	 i:9 	 global-step:6949	 l-p:0.13168899714946747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8443, 2.5395, 2.6798],
        [2.8443, 2.6728, 2.7855],
        [2.8443, 2.5653, 2.7041],
        [2.8443, 2.8443, 2.8443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.14466339349746704 
model_pd.l_d.mean(): -25.029891967773438 
model_pd.lagr.mean(): -24.885229110717773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0142], device='cuda:0')), ('power', tensor([-25.0441], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.14466339349746704
epoch£º348	 i:1 	 global-step:6961	 l-p:0.16336297988891602
epoch£º348	 i:2 	 global-step:6962	 l-p:0.14985768496990204
epoch£º348	 i:3 	 global-step:6963	 l-p:-0.41191741824150085
epoch£º348	 i:4 	 global-step:6964	 l-p:0.16934074461460114
epoch£º348	 i:5 	 global-step:6965	 l-p:0.15319912135601044
epoch£º348	 i:6 	 global-step:6966	 l-p:0.27395179867744446
epoch£º348	 i:7 	 global-step:6967	 l-p:0.10351049900054932
epoch£º348	 i:8 	 global-step:6968	 l-p:0.14767609536647797
epoch£º348	 i:9 	 global-step:6969	 l-p:0.3493576645851135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9911, 2.9911, 2.9911],
        [2.9911, 2.9831, 2.9908],
        [2.9911, 2.9811, 2.9906],
        [2.9911, 2.8240, 2.9344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.12255094200372696 
model_pd.l_d.mean(): -24.439836502075195 
model_pd.lagr.mean(): -24.317285537719727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0732], device='cuda:0')), ('power', tensor([-24.5131], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.12255094200372696
epoch£º349	 i:1 	 global-step:6981	 l-p:0.11832468956708908
epoch£º349	 i:2 	 global-step:6982	 l-p:0.14604642987251282
epoch£º349	 i:3 	 global-step:6983	 l-p:0.12045249342918396
epoch£º349	 i:4 	 global-step:6984	 l-p:0.11053003370761871
epoch£º349	 i:5 	 global-step:6985	 l-p:0.14246582984924316
epoch£º349	 i:6 	 global-step:6986	 l-p:0.1698766052722931
epoch£º349	 i:7 	 global-step:6987	 l-p:0.1216164156794548
epoch£º349	 i:8 	 global-step:6988	 l-p:0.7870165109634399
epoch£º349	 i:9 	 global-step:6989	 l-p:0.024172639474272728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5371, 2.5267, 2.5366],
        [2.5371, 1.6685, 1.1835],
        [2.5371, 1.6158, 1.1562],
        [2.5371, 1.6724, 1.1861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.17169512808322906 
model_pd.l_d.mean(): -25.174333572387695 
model_pd.lagr.mean(): -25.00263786315918 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2649], device='cuda:0')), ('power', tensor([-25.4393], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.17169512808322906
epoch£º350	 i:1 	 global-step:7001	 l-p:-0.10931915044784546
epoch£º350	 i:2 	 global-step:7002	 l-p:0.09432493895292282
epoch£º350	 i:3 	 global-step:7003	 l-p:0.12434481084346771
epoch£º350	 i:4 	 global-step:7004	 l-p:0.27991944551467896
epoch£º350	 i:5 	 global-step:7005	 l-p:0.1320018768310547
epoch£º350	 i:6 	 global-step:7006	 l-p:0.10524621605873108
epoch£º350	 i:7 	 global-step:7007	 l-p:0.11934289336204529
epoch£º350	 i:8 	 global-step:7008	 l-p:0.11094693839550018
epoch£º350	 i:9 	 global-step:7009	 l-p:0.11037248373031616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0223, 2.7590, 2.8947],
        [3.0223, 2.6665, 2.8014],
        [3.0223, 2.1097, 1.6553],
        [3.0223, 2.1010, 1.6269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.10736841708421707 
model_pd.l_d.mean(): -24.91505241394043 
model_pd.lagr.mean(): -24.80768394470215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0215], device='cuda:0')), ('power', tensor([-24.9365], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.10736841708421707
epoch£º351	 i:1 	 global-step:7021	 l-p:0.16283389925956726
epoch£º351	 i:2 	 global-step:7022	 l-p:0.0899805873632431
epoch£º351	 i:3 	 global-step:7023	 l-p:0.17072662711143494
epoch£º351	 i:4 	 global-step:7024	 l-p:0.13316121697425842
epoch£º351	 i:5 	 global-step:7025	 l-p:0.16600152850151062
epoch£º351	 i:6 	 global-step:7026	 l-p:0.24297431111335754
epoch£º351	 i:7 	 global-step:7027	 l-p:0.13352210819721222
epoch£º351	 i:8 	 global-step:7028	 l-p:0.04227278754115105
epoch£º351	 i:9 	 global-step:7029	 l-p:0.14300958812236786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8694, 2.6284, 2.7624],
        [2.8694, 2.0004, 1.6751],
        [2.8694, 2.4407, 2.5634],
        [2.8694, 1.9207, 1.4444]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.17338286340236664 
model_pd.l_d.mean(): -25.269943237304688 
model_pd.lagr.mean(): -25.096559524536133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0392], device='cuda:0')), ('power', tensor([-25.2308], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.17338286340236664
epoch£º352	 i:1 	 global-step:7041	 l-p:0.1395162045955658
epoch£º352	 i:2 	 global-step:7042	 l-p:0.12543204426765442
epoch£º352	 i:3 	 global-step:7043	 l-p:0.1492549628019333
epoch£º352	 i:4 	 global-step:7044	 l-p:0.13000378012657166
epoch£º352	 i:5 	 global-step:7045	 l-p:0.13476668298244476
epoch£º352	 i:6 	 global-step:7046	 l-p:0.17123207449913025
epoch£º352	 i:7 	 global-step:7047	 l-p:0.1210176944732666
epoch£º352	 i:8 	 global-step:7048	 l-p:0.17385300993919373
epoch£º352	 i:9 	 global-step:7049	 l-p:0.14093255996704102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7065, 2.6584, 2.7000],
        [2.7065, 2.6896, 2.7054],
        [2.7065, 2.6914, 2.7056],
        [2.7065, 2.7065, 2.7066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.16795779764652252 
model_pd.l_d.mean(): -25.090944290161133 
model_pd.lagr.mean(): -24.92298698425293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1054], device='cuda:0')), ('power', tensor([-25.1963], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.16795779764652252
epoch£º353	 i:1 	 global-step:7061	 l-p:0.2225835919380188
epoch£º353	 i:2 	 global-step:7062	 l-p:0.10450410842895508
epoch£º353	 i:3 	 global-step:7063	 l-p:0.17403815686702728
epoch£º353	 i:4 	 global-step:7064	 l-p:0.10726115852594376
epoch£º353	 i:5 	 global-step:7065	 l-p:0.13334034383296967
epoch£º353	 i:6 	 global-step:7066	 l-p:0.11908525228500366
epoch£º353	 i:7 	 global-step:7067	 l-p:0.1328551024198532
epoch£º353	 i:8 	 global-step:7068	 l-p:0.15546253323554993
epoch£º353	 i:9 	 global-step:7069	 l-p:0.10367707163095474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9036, 2.6977, 2.8230],
        [2.9036, 1.9500, 1.4406],
        [2.9036, 2.2145, 1.6420],
        [2.9036, 2.8573, 2.8974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.13146281242370605 
model_pd.l_d.mean(): -25.153034210205078 
model_pd.lagr.mean(): -25.02157211303711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0544], device='cuda:0')), ('power', tensor([-25.0987], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.13146281242370605
epoch£º354	 i:1 	 global-step:7081	 l-p:0.20924249291419983
epoch£º354	 i:2 	 global-step:7082	 l-p:0.12403134256601334
epoch£º354	 i:3 	 global-step:7083	 l-p:0.04742751643061638
epoch£º354	 i:4 	 global-step:7084	 l-p:0.17972393333911896
epoch£º354	 i:5 	 global-step:7085	 l-p:0.15474826097488403
epoch£º354	 i:6 	 global-step:7086	 l-p:0.1421273797750473
epoch£º354	 i:7 	 global-step:7087	 l-p:0.13628655672073364
epoch£º354	 i:8 	 global-step:7088	 l-p:0.127700075507164
epoch£º354	 i:9 	 global-step:7089	 l-p:0.07470963895320892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8857, 2.1978, 1.6264],
        [2.8857, 2.7573, 2.8507],
        [2.8857, 2.8605, 2.8835],
        [2.8857, 2.8769, 2.8853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.131001278758049 
model_pd.l_d.mean(): -24.871313095092773 
model_pd.lagr.mean(): -24.740312576293945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0071], device='cuda:0')), ('power', tensor([-24.8784], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.131001278758049
epoch£º355	 i:1 	 global-step:7101	 l-p:0.1297481507062912
epoch£º355	 i:2 	 global-step:7102	 l-p:-0.21112306416034698
epoch£º355	 i:3 	 global-step:7103	 l-p:0.13183604180812836
epoch£º355	 i:4 	 global-step:7104	 l-p:0.18033649027347565
epoch£º355	 i:5 	 global-step:7105	 l-p:0.22372670471668243
epoch£º355	 i:6 	 global-step:7106	 l-p:0.17433612048625946
epoch£º355	 i:7 	 global-step:7107	 l-p:0.2340412437915802
epoch£º355	 i:8 	 global-step:7108	 l-p:0.13561785221099854
epoch£º355	 i:9 	 global-step:7109	 l-p:0.15151630342006683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9116, 2.5437, 2.6823],
        [2.9116, 2.0167, 1.4672],
        [2.9116, 2.9115, 2.9116],
        [2.9116, 2.5261, 2.6618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.13758178055286407 
model_pd.l_d.mean(): -24.829181671142578 
model_pd.lagr.mean(): -24.691600799560547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0245], device='cuda:0')), ('power', tensor([-24.8537], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.13758178055286407
epoch£º356	 i:1 	 global-step:7121	 l-p:0.1369091421365738
epoch£º356	 i:2 	 global-step:7122	 l-p:0.13738802075386047
epoch£º356	 i:3 	 global-step:7123	 l-p:0.19816020131111145
epoch£º356	 i:4 	 global-step:7124	 l-p:0.10363076627254486
epoch£º356	 i:5 	 global-step:7125	 l-p:0.12261313945055008
epoch£º356	 i:6 	 global-step:7126	 l-p:0.21963633596897125
epoch£º356	 i:7 	 global-step:7127	 l-p:0.4352915585041046
epoch£º356	 i:8 	 global-step:7128	 l-p:0.1520715057849884
epoch£º356	 i:9 	 global-step:7129	 l-p:0.13431578874588013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0240, 2.3242, 1.7367],
        [3.0240, 3.0107, 3.0232],
        [3.0240, 2.9577, 3.0127],
        [3.0240, 2.5058, 2.5857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.10744229704141617 
model_pd.l_d.mean(): -24.81317710876465 
model_pd.lagr.mean(): -24.705734252929688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0356], device='cuda:0')), ('power', tensor([-24.8488], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.10744229704141617
epoch£º357	 i:1 	 global-step:7141	 l-p:0.14020948112010956
epoch£º357	 i:2 	 global-step:7142	 l-p:0.1194804385304451
epoch£º357	 i:3 	 global-step:7143	 l-p:0.1451282501220703
epoch£º357	 i:4 	 global-step:7144	 l-p:0.13522189855575562
epoch£º357	 i:5 	 global-step:7145	 l-p:0.1324564516544342
epoch£º357	 i:6 	 global-step:7146	 l-p:0.13531175255775452
epoch£º357	 i:7 	 global-step:7147	 l-p:0.14127890765666962
epoch£º357	 i:8 	 global-step:7148	 l-p:0.22104351222515106
epoch£º357	 i:9 	 global-step:7149	 l-p:0.17242103815078735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5744, 2.5683, 2.5742],
        [2.5744, 2.5743, 2.5744],
        [2.5744, 1.8450, 1.7746],
        [2.5744, 1.7450, 1.2353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): -0.057355593889951706 
model_pd.l_d.mean(): -25.165935516357422 
model_pd.lagr.mean(): -25.223291397094727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1702], device='cuda:0')), ('power', tensor([-25.3361], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:-0.057355593889951706
epoch£º358	 i:1 	 global-step:7161	 l-p:0.15511038899421692
epoch£º358	 i:2 	 global-step:7162	 l-p:0.1277112066745758
epoch£º358	 i:3 	 global-step:7163	 l-p:0.1804436892271042
epoch£º358	 i:4 	 global-step:7164	 l-p:0.053371790796518326
epoch£º358	 i:5 	 global-step:7165	 l-p:0.12069761008024216
epoch£º358	 i:6 	 global-step:7166	 l-p:0.13513538241386414
epoch£º358	 i:7 	 global-step:7167	 l-p:0.1183549165725708
epoch£º358	 i:8 	 global-step:7168	 l-p:0.12053272873163223
epoch£º358	 i:9 	 global-step:7169	 l-p:0.1365845650434494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9040, 2.7304, 2.8450],
        [2.9040, 2.9040, 2.9040],
        [2.9040, 2.2817, 2.2946],
        [2.9040, 2.9038, 2.9040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.1303245574235916 
model_pd.l_d.mean(): -24.972620010375977 
model_pd.lagr.mean(): -24.842294692993164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0322], device='cuda:0')), ('power', tensor([-25.0048], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.1303245574235916
epoch£º359	 i:1 	 global-step:7181	 l-p:0.12703463435173035
epoch£º359	 i:2 	 global-step:7182	 l-p:0.15105874836444855
epoch£º359	 i:3 	 global-step:7183	 l-p:0.13298043608665466
epoch£º359	 i:4 	 global-step:7184	 l-p:0.07189616560935974
epoch£º359	 i:5 	 global-step:7185	 l-p:0.17678523063659668
epoch£º359	 i:6 	 global-step:7186	 l-p:0.05585922300815582
epoch£º359	 i:7 	 global-step:7187	 l-p:0.09879623353481293
epoch£º359	 i:8 	 global-step:7188	 l-p:0.17158406972885132
epoch£º359	 i:9 	 global-step:7189	 l-p:0.13192857801914215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9581, 2.4437, 2.5322],
        [2.9581, 2.4308, 2.5118],
        [2.9581, 2.5658, 2.7009],
        [2.9581, 2.9579, 2.9581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.11653318256139755 
model_pd.l_d.mean(): -24.924697875976562 
model_pd.lagr.mean(): -24.808164596557617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0676], device='cuda:0')), ('power', tensor([-24.9923], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.11653318256139755
epoch£º360	 i:1 	 global-step:7201	 l-p:0.14188207685947418
epoch£º360	 i:2 	 global-step:7202	 l-p:0.15039193630218506
epoch£º360	 i:3 	 global-step:7203	 l-p:0.1611448973417282
epoch£º360	 i:4 	 global-step:7204	 l-p:0.12172283977270126
epoch£º360	 i:5 	 global-step:7205	 l-p:0.1531486213207245
epoch£º360	 i:6 	 global-step:7206	 l-p:0.1567961424589157
epoch£º360	 i:7 	 global-step:7207	 l-p:0.05714304745197296
epoch£º360	 i:8 	 global-step:7208	 l-p:0.14057151973247528
epoch£º360	 i:9 	 global-step:7209	 l-p:0.17018979787826538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7206, 2.7207, 2.7207],
        [2.7206, 2.7205, 2.7206],
        [2.7206, 2.7196, 2.7206],
        [2.7206, 2.7098, 2.7201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.22005809843540192 
model_pd.l_d.mean(): -25.144378662109375 
model_pd.lagr.mean(): -24.924320220947266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1196], device='cuda:0')), ('power', tensor([-25.2640], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.22005809843540192
epoch£º361	 i:1 	 global-step:7221	 l-p:0.20522087812423706
epoch£º361	 i:2 	 global-step:7222	 l-p:0.1720074862241745
epoch£º361	 i:3 	 global-step:7223	 l-p:0.1572401523590088
epoch£º361	 i:4 	 global-step:7224	 l-p:0.1042976900935173
epoch£º361	 i:5 	 global-step:7225	 l-p:0.10351075232028961
epoch£º361	 i:6 	 global-step:7226	 l-p:0.12834137678146362
epoch£º361	 i:7 	 global-step:7227	 l-p:0.13795827329158783
epoch£º361	 i:8 	 global-step:7228	 l-p:0.2281499058008194
epoch£º361	 i:9 	 global-step:7229	 l-p:0.03954104334115982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8012, 2.2273, 2.2903],
        [2.8012, 2.8012, 2.8012],
        [2.8012, 2.8011, 2.8012],
        [2.8012, 1.8793, 1.3507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.1553293913602829 
model_pd.l_d.mean(): -25.08816146850586 
model_pd.lagr.mean(): -24.932832717895508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1213], device='cuda:0')), ('power', tensor([-25.2094], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.1553293913602829
epoch£º362	 i:1 	 global-step:7241	 l-p:0.11477134376764297
epoch£º362	 i:2 	 global-step:7242	 l-p:0.854998767375946
epoch£º362	 i:3 	 global-step:7243	 l-p:0.18084746599197388
epoch£º362	 i:4 	 global-step:7244	 l-p:0.18169482052326202
epoch£º362	 i:5 	 global-step:7245	 l-p:0.11863328516483307
epoch£º362	 i:6 	 global-step:7246	 l-p:0.13789215683937073
epoch£º362	 i:7 	 global-step:7247	 l-p:0.1358056217432022
epoch£º362	 i:8 	 global-step:7248	 l-p:0.12486883252859116
epoch£º362	 i:9 	 global-step:7249	 l-p:0.18800993263721466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9374, 2.5894, 2.7331],
        [2.9374, 2.9331, 2.9373],
        [2.9374, 2.9269, 2.9369],
        [2.9374, 2.2803, 2.2664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.15507003664970398 
model_pd.l_d.mean(): -25.125537872314453 
model_pd.lagr.mean(): -24.970468521118164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0497], device='cuda:0')), ('power', tensor([-25.0759], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.15507003664970398
epoch£º363	 i:1 	 global-step:7261	 l-p:0.1647164523601532
epoch£º363	 i:2 	 global-step:7262	 l-p:0.06623411178588867
epoch£º363	 i:3 	 global-step:7263	 l-p:0.12576396763324738
epoch£º363	 i:4 	 global-step:7264	 l-p:0.1447053849697113
epoch£º363	 i:5 	 global-step:7265	 l-p:0.21522946655750275
epoch£º363	 i:6 	 global-step:7266	 l-p:0.13464760780334473
epoch£º363	 i:7 	 global-step:7267	 l-p:-0.21720390021800995
epoch£º363	 i:8 	 global-step:7268	 l-p:0.13849598169326782
epoch£º363	 i:9 	 global-step:7269	 l-p:0.15823101997375488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7531, 2.0697, 1.5045],
        [2.7531, 2.3976, 2.5448],
        [2.7531, 2.7523, 2.7531],
        [2.7531, 2.0260, 1.9559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.7045252323150635 
model_pd.l_d.mean(): -24.676040649414062 
model_pd.lagr.mean(): -23.971515655517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1608], device='cuda:0')), ('power', tensor([-24.8368], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.7045252323150635
epoch£º364	 i:1 	 global-step:7281	 l-p:0.13972768187522888
epoch£º364	 i:2 	 global-step:7282	 l-p:0.12139018625020981
epoch£º364	 i:3 	 global-step:7283	 l-p:0.14982262253761292
epoch£º364	 i:4 	 global-step:7284	 l-p:0.16024591028690338
epoch£º364	 i:5 	 global-step:7285	 l-p:0.033266883343458176
epoch£º364	 i:6 	 global-step:7286	 l-p:0.17944703996181488
epoch£º364	 i:7 	 global-step:7287	 l-p:0.15374615788459778
epoch£º364	 i:8 	 global-step:7288	 l-p:0.12276113033294678
epoch£º364	 i:9 	 global-step:7289	 l-p:0.15105466544628143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9610, 2.9610, 2.9610],
        [2.9610, 2.3022, 2.2885],
        [2.9610, 2.4270, 2.5103],
        [2.9610, 2.9158, 2.9552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.1431991159915924 
model_pd.l_d.mean(): -25.112285614013672 
model_pd.lagr.mean(): -24.969085693359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0572], device='cuda:0')), ('power', tensor([-25.0550], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.1431991159915924
epoch£º365	 i:1 	 global-step:7301	 l-p:0.13556188344955444
epoch£º365	 i:2 	 global-step:7302	 l-p:0.13817109167575836
epoch£º365	 i:3 	 global-step:7303	 l-p:0.9054396152496338
epoch£º365	 i:4 	 global-step:7304	 l-p:0.151120126247406
epoch£º365	 i:5 	 global-step:7305	 l-p:0.15041379630565643
epoch£º365	 i:6 	 global-step:7306	 l-p:0.141713947057724
epoch£º365	 i:7 	 global-step:7307	 l-p:0.07976873219013214
epoch£º365	 i:8 	 global-step:7308	 l-p:0.12095391750335693
epoch£º365	 i:9 	 global-step:7309	 l-p:0.163302481174469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6226, 2.5945, 2.6200],
        [2.6226, 1.8398, 1.3049],
        [2.6226, 1.8879, 1.3431],
        [2.6226, 2.6003, 2.6209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.16662436723709106 
model_pd.l_d.mean(): -24.464717864990234 
model_pd.lagr.mean(): -24.298093795776367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3753], device='cuda:0')), ('power', tensor([-24.8400], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.16662436723709106
epoch£º366	 i:1 	 global-step:7321	 l-p:1.4352902173995972
epoch£º366	 i:2 	 global-step:7322	 l-p:0.19022352993488312
epoch£º366	 i:3 	 global-step:7323	 l-p:0.15494409203529358
epoch£º366	 i:4 	 global-step:7324	 l-p:0.16848938167095184
epoch£º366	 i:5 	 global-step:7325	 l-p:0.12888386845588684
epoch£º366	 i:6 	 global-step:7326	 l-p:0.13665100932121277
epoch£º366	 i:7 	 global-step:7327	 l-p:0.08963458985090256
epoch£º366	 i:8 	 global-step:7328	 l-p:0.13560691475868225
epoch£º366	 i:9 	 global-step:7329	 l-p:0.1061282530426979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[3.0919, 2.3574, 1.7560],
        [3.0919, 2.4776, 2.4974],
        [3.0919, 2.6249, 2.7379],
        [3.0919, 2.1869, 1.8006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.11426085233688354 
model_pd.l_d.mean(): -24.653303146362305 
model_pd.lagr.mean(): -24.53904151916504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0416], device='cuda:0')), ('power', tensor([-24.6949], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.11426085233688354
epoch£º367	 i:1 	 global-step:7341	 l-p:0.13623565435409546
epoch£º367	 i:2 	 global-step:7342	 l-p:0.09886983036994934
epoch£º367	 i:3 	 global-step:7343	 l-p:-0.12426459789276123
epoch£º367	 i:4 	 global-step:7344	 l-p:0.9987396001815796
epoch£º367	 i:5 	 global-step:7345	 l-p:0.17974631488323212
epoch£º367	 i:6 	 global-step:7346	 l-p:0.19162076711654663
epoch£º367	 i:7 	 global-step:7347	 l-p:0.1328413486480713
epoch£º367	 i:8 	 global-step:7348	 l-p:0.10371574014425278
epoch£º367	 i:9 	 global-step:7349	 l-p:0.14296045899391174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9139, 2.9139, 2.9139],
        [2.9139, 2.9139, 2.9139],
        [2.9139, 2.8959, 2.9126],
        [2.9139, 2.0725, 1.8384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.17533119022846222 
model_pd.l_d.mean(): -24.98088264465332 
model_pd.lagr.mean(): -24.805551528930664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0363], device='cuda:0')), ('power', tensor([-25.0171], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.17533119022846222
epoch£º368	 i:1 	 global-step:7361	 l-p:-0.0893695130944252
epoch£º368	 i:2 	 global-step:7362	 l-p:0.13137927651405334
epoch£º368	 i:3 	 global-step:7363	 l-p:0.15898397564888
epoch£º368	 i:4 	 global-step:7364	 l-p:0.12148477882146835
epoch£º368	 i:5 	 global-step:7365	 l-p:0.1424504667520523
epoch£º368	 i:6 	 global-step:7366	 l-p:0.16122926771640778
epoch£º368	 i:7 	 global-step:7367	 l-p:0.13004934787750244
epoch£º368	 i:8 	 global-step:7368	 l-p:0.12597937881946564
epoch£º368	 i:9 	 global-step:7369	 l-p:0.16301032900810242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5698, 1.6071, 1.2352],
        [2.5698, 2.5568, 2.5691],
        [2.5698, 2.5698, 2.5698],
        [2.5698, 1.7952, 1.2636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.07554266601800919 
model_pd.l_d.mean(): -24.85249900817871 
model_pd.lagr.mean(): -24.77695655822754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3345], device='cuda:0')), ('power', tensor([-25.1870], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.07554266601800919
epoch£º369	 i:1 	 global-step:7381	 l-p:0.18817773461341858
epoch£º369	 i:2 	 global-step:7382	 l-p:0.7395755648612976
epoch£º369	 i:3 	 global-step:7383	 l-p:0.11820537596940994
epoch£º369	 i:4 	 global-step:7384	 l-p:-0.003560352139174938
epoch£º369	 i:5 	 global-step:7385	 l-p:0.13810259103775024
epoch£º369	 i:6 	 global-step:7386	 l-p:0.1969083994626999
epoch£º369	 i:7 	 global-step:7387	 l-p:0.18167510628700256
epoch£º369	 i:8 	 global-step:7388	 l-p:0.15891097486019135
epoch£º369	 i:9 	 global-step:7389	 l-p:0.12104672938585281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0748, 2.2137, 1.9220],
        [3.0748, 3.0575, 3.0736],
        [3.0748, 2.2076, 1.6162],
        [3.0748, 2.7022, 2.8434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.11922939121723175 
model_pd.l_d.mean(): -24.146682739257812 
model_pd.lagr.mean(): -24.02745246887207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0789], device='cuda:0')), ('power', tensor([-24.2256], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.11922939121723175
epoch£º370	 i:1 	 global-step:7401	 l-p:0.12313100695610046
epoch£º370	 i:2 	 global-step:7402	 l-p:0.14323841035366058
epoch£º370	 i:3 	 global-step:7403	 l-p:0.12637785077095032
epoch£º370	 i:4 	 global-step:7404	 l-p:0.14900946617126465
epoch£º370	 i:5 	 global-step:7405	 l-p:0.19240905344486237
epoch£º370	 i:6 	 global-step:7406	 l-p:0.10293769836425781
epoch£º370	 i:7 	 global-step:7407	 l-p:0.15690584480762482
epoch£º370	 i:8 	 global-step:7408	 l-p:0.15126295387744904
epoch£º370	 i:9 	 global-step:7409	 l-p:0.18803609907627106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7049, 2.7048, 2.7049],
        [2.7049, 2.6961, 2.7045],
        [2.7049, 1.9256, 1.3756],
        [2.7049, 2.6141, 2.6865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.10881292819976807 
model_pd.l_d.mean(): -24.993223190307617 
model_pd.lagr.mean(): -24.884410858154297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1355], device='cuda:0')), ('power', tensor([-25.1287], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.10881292819976807
epoch£º371	 i:1 	 global-step:7421	 l-p:0.14498277008533478
epoch£º371	 i:2 	 global-step:7422	 l-p:0.1271778792142868
epoch£º371	 i:3 	 global-step:7423	 l-p:0.15011650323867798
epoch£º371	 i:4 	 global-step:7424	 l-p:0.11023454368114471
epoch£º371	 i:5 	 global-step:7425	 l-p:0.2742445766925812
epoch£º371	 i:6 	 global-step:7426	 l-p:0.08529354631900787
epoch£º371	 i:7 	 global-step:7427	 l-p:0.19163878262043
epoch£º371	 i:8 	 global-step:7428	 l-p:0.11096454411745071
epoch£º371	 i:9 	 global-step:7429	 l-p:0.15754549205303192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0374, 2.3750, 1.7737],
        [3.0374, 3.0373, 3.0374],
        [3.0374, 3.0283, 3.0370],
        [3.0374, 2.1988, 1.9603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.1408367156982422 
model_pd.l_d.mean(): -24.626205444335938 
model_pd.lagr.mean(): -24.485368728637695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0195], device='cuda:0')), ('power', tensor([-24.6458], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.1408367156982422
epoch£º372	 i:1 	 global-step:7441	 l-p:0.12373778969049454
epoch£º372	 i:2 	 global-step:7442	 l-p:0.103090800344944
epoch£º372	 i:3 	 global-step:7443	 l-p:0.11042966693639755
epoch£º372	 i:4 	 global-step:7444	 l-p:0.11415578424930573
epoch£º372	 i:5 	 global-step:7445	 l-p:0.11620327085256577
epoch£º372	 i:6 	 global-step:7446	 l-p:0.2027726173400879
epoch£º372	 i:7 	 global-step:7447	 l-p:-0.09470470249652863
epoch£º372	 i:8 	 global-step:7448	 l-p:0.1425459235906601
epoch£º372	 i:9 	 global-step:7449	 l-p:3.119298219680786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7554, 1.9741, 1.4158],
        [2.7554, 2.6113, 2.7145],
        [2.7554, 2.7554, 2.7554],
        [2.7554, 1.8179, 1.4802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.15520381927490234 
model_pd.l_d.mean(): -24.529451370239258 
model_pd.lagr.mean(): -24.374248504638672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1769], device='cuda:0')), ('power', tensor([-24.7063], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.15520381927490234
epoch£º373	 i:1 	 global-step:7461	 l-p:0.19948185980319977
epoch£º373	 i:2 	 global-step:7462	 l-p:1.694854497909546
epoch£º373	 i:3 	 global-step:7463	 l-p:0.14564739167690277
epoch£º373	 i:4 	 global-step:7464	 l-p:0.11526956409215927
epoch£º373	 i:5 	 global-step:7465	 l-p:0.12775230407714844
epoch£º373	 i:6 	 global-step:7466	 l-p:0.1458124965429306
epoch£º373	 i:7 	 global-step:7467	 l-p:0.22633136808872223
epoch£º373	 i:8 	 global-step:7468	 l-p:0.15657436847686768
epoch£º373	 i:9 	 global-step:7469	 l-p:0.10617586970329285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7880, 1.9818, 1.8326],
        [2.7880, 2.5692, 2.7027],
        [2.7880, 2.6296, 2.7398],
        [2.7880, 2.0477, 1.4785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.10409966856241226 
model_pd.l_d.mean(): -25.189359664916992 
model_pd.lagr.mean(): -25.08526039123535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0968], device='cuda:0')), ('power', tensor([-25.2861], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.10409966856241226
epoch£º374	 i:1 	 global-step:7481	 l-p:0.22147616744041443
epoch£º374	 i:2 	 global-step:7482	 l-p:0.4640235900878906
epoch£º374	 i:3 	 global-step:7483	 l-p:0.12436921894550323
epoch£º374	 i:4 	 global-step:7484	 l-p:0.1276559829711914
epoch£º374	 i:5 	 global-step:7485	 l-p:0.13038094341754913
epoch£º374	 i:6 	 global-step:7486	 l-p:0.16641734540462494
epoch£º374	 i:7 	 global-step:7487	 l-p:0.07475613802671432
epoch£º374	 i:8 	 global-step:7488	 l-p:0.14251627027988434
epoch£º374	 i:9 	 global-step:7489	 l-p:0.3117165267467499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7865, 2.7597, 2.7841],
        [2.7865, 2.7865, 2.7865],
        [2.7865, 2.7548, 2.7833],
        [2.7865, 2.7858, 2.7865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): -0.00172617903444916 
model_pd.l_d.mean(): -25.0580997467041 
model_pd.lagr.mean(): -25.059825897216797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0368], device='cuda:0')), ('power', tensor([-25.0949], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:-0.00172617903444916
epoch£º375	 i:1 	 global-step:7501	 l-p:0.12789836525917053
epoch£º375	 i:2 	 global-step:7502	 l-p:0.15055261552333832
epoch£º375	 i:3 	 global-step:7503	 l-p:0.24446918070316315
epoch£º375	 i:4 	 global-step:7504	 l-p:0.125954732298851
epoch£º375	 i:5 	 global-step:7505	 l-p:-0.12011035531759262
epoch£º375	 i:6 	 global-step:7506	 l-p:0.1290590763092041
epoch£º375	 i:7 	 global-step:7507	 l-p:0.12357392907142639
epoch£º375	 i:8 	 global-step:7508	 l-p:0.14801928400993347
epoch£º375	 i:9 	 global-step:7509	 l-p:0.1242954209446907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7866, 2.4857, 2.6359],
        [2.7866, 1.8748, 1.5752],
        [2.7866, 2.7858, 2.7866],
        [2.7866, 2.1216, 2.1321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.12221319228410721 
model_pd.l_d.mean(): -24.761722564697266 
model_pd.lagr.mean(): -24.639509201049805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1317], device='cuda:0')), ('power', tensor([-24.8934], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.12221319228410721
epoch£º376	 i:1 	 global-step:7521	 l-p:0.32621046900749207
epoch£º376	 i:2 	 global-step:7522	 l-p:0.01495078019797802
epoch£º376	 i:3 	 global-step:7523	 l-p:0.15839006006717682
epoch£º376	 i:4 	 global-step:7524	 l-p:0.02438633143901825
epoch£º376	 i:5 	 global-step:7525	 l-p:0.44264212250709534
epoch£º376	 i:6 	 global-step:7526	 l-p:0.221901997923851
epoch£º376	 i:7 	 global-step:7527	 l-p:0.12598170340061188
epoch£º376	 i:8 	 global-step:7528	 l-p:0.14990173280239105
epoch£º376	 i:9 	 global-step:7529	 l-p:0.1815081387758255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9306, 2.2285, 2.1941],
        [2.9306, 2.6406, 2.7883],
        [2.9306, 2.6909, 2.8295],
        [2.9306, 2.1384, 1.5562]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.15469007194042206 
model_pd.l_d.mean(): -24.976694107055664 
model_pd.lagr.mean(): -24.822004318237305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0114], device='cuda:0')), ('power', tensor([-24.9881], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.15469007194042206
epoch£º377	 i:1 	 global-step:7541	 l-p:0.13781118392944336
epoch£º377	 i:2 	 global-step:7542	 l-p:0.1329188495874405
epoch£º377	 i:3 	 global-step:7543	 l-p:0.13004359602928162
epoch£º377	 i:4 	 global-step:7544	 l-p:0.11086941510438919
epoch£º377	 i:5 	 global-step:7545	 l-p:0.12693457305431366
epoch£º377	 i:6 	 global-step:7546	 l-p:0.16045762598514557
epoch£º377	 i:7 	 global-step:7547	 l-p:0.15995123982429504
epoch£º377	 i:8 	 global-step:7548	 l-p:0.11932670325040817
epoch£º377	 i:9 	 global-step:7549	 l-p:0.1474900245666504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8542, 2.0002, 1.7848],
        [2.8542, 2.8542, 2.8542],
        [2.8542, 2.4236, 2.5640],
        [2.8542, 2.6228, 2.7601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.18078388273715973 
model_pd.l_d.mean(): -24.86643409729004 
model_pd.lagr.mean(): -24.685649871826172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1356], device='cuda:0')), ('power', tensor([-25.0021], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.18078388273715973
epoch£º378	 i:1 	 global-step:7561	 l-p:0.23748238384723663
epoch£º378	 i:2 	 global-step:7562	 l-p:0.1474253386259079
epoch£º378	 i:3 	 global-step:7563	 l-p:0.1388787180185318
epoch£º378	 i:4 	 global-step:7564	 l-p:0.10744108259677887
epoch£º378	 i:5 	 global-step:7565	 l-p:0.14643794298171997
epoch£º378	 i:6 	 global-step:7566	 l-p:0.1467401683330536
epoch£º378	 i:7 	 global-step:7567	 l-p:0.13469526171684265
epoch£º378	 i:8 	 global-step:7568	 l-p:0.14758259057998657
epoch£º378	 i:9 	 global-step:7569	 l-p:0.17117255926132202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6844, 2.0612, 2.1173],
        [2.6844, 2.6844, 2.6844],
        [2.6844, 2.6161, 2.6732],
        [2.6844, 1.9601, 1.9285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.5199902653694153 
model_pd.l_d.mean(): -24.66386604309082 
model_pd.lagr.mean(): -24.143875122070312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2826], device='cuda:0')), ('power', tensor([-24.9465], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.5199902653694153
epoch£º379	 i:1 	 global-step:7581	 l-p:0.13016925752162933
epoch£º379	 i:2 	 global-step:7582	 l-p:-0.18604736030101776
epoch£º379	 i:3 	 global-step:7583	 l-p:0.026559611782431602
epoch£º379	 i:4 	 global-step:7584	 l-p:0.1832466423511505
epoch£º379	 i:5 	 global-step:7585	 l-p:0.13990743458271027
epoch£º379	 i:6 	 global-step:7586	 l-p:0.22781863808631897
epoch£º379	 i:7 	 global-step:7587	 l-p:0.12748564779758453
epoch£º379	 i:8 	 global-step:7588	 l-p:0.125399649143219
epoch£º379	 i:9 	 global-step:7589	 l-p:0.13918232917785645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9520, 1.9345, 1.4167],
        [2.9520, 2.8599, 2.9332],
        [2.9520, 2.9502, 2.9520],
        [2.9520, 2.9341, 2.9508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.13393060863018036 
model_pd.l_d.mean(): -25.00705909729004 
model_pd.lagr.mean(): -24.87312889099121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0021], device='cuda:0')), ('power', tensor([-25.0050], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.13393060863018036
epoch£º380	 i:1 	 global-step:7601	 l-p:0.1928948163986206
epoch£º380	 i:2 	 global-step:7602	 l-p:0.171187162399292
epoch£º380	 i:3 	 global-step:7603	 l-p:0.1439467817544937
epoch£º380	 i:4 	 global-step:7604	 l-p:0.13619129359722137
epoch£º380	 i:5 	 global-step:7605	 l-p:0.1242879256606102
epoch£º380	 i:6 	 global-step:7606	 l-p:0.128304585814476
epoch£º380	 i:7 	 global-step:7607	 l-p:0.2025640457868576
epoch£º380	 i:8 	 global-step:7608	 l-p:0.10988034307956696
epoch£º380	 i:9 	 global-step:7609	 l-p:0.1538025289773941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9221, 2.9216, 2.9221],
        [2.9221, 2.0062, 1.4377],
        [2.9221, 2.6451, 2.7929],
        [2.9221, 2.3609, 2.4493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.22770293056964874 
model_pd.l_d.mean(): -24.9288272857666 
model_pd.lagr.mean(): -24.70112419128418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1280], device='cuda:0')), ('power', tensor([-25.0568], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.22770293056964874
epoch£º381	 i:1 	 global-step:7621	 l-p:0.14031074941158295
epoch£º381	 i:2 	 global-step:7622	 l-p:0.1326397806406021
epoch£º381	 i:3 	 global-step:7623	 l-p:0.12537498772144318
epoch£º381	 i:4 	 global-step:7624	 l-p:0.13333739340305328
epoch£º381	 i:5 	 global-step:7625	 l-p:0.1303248256444931
epoch£º381	 i:6 	 global-step:7626	 l-p:0.14177608489990234
epoch£º381	 i:7 	 global-step:7627	 l-p:0.15792247653007507
epoch£º381	 i:8 	 global-step:7628	 l-p:0.16923941671848297
epoch£º381	 i:9 	 global-step:7629	 l-p:0.22036240994930267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6894, 2.2611, 2.4104],
        [2.6894, 2.6879, 2.6893],
        [2.6894, 2.6847, 2.6892],
        [2.6894, 2.1278, 2.2281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.049690406769514084 
model_pd.l_d.mean(): -25.204578399658203 
model_pd.lagr.mean(): -25.154888153076172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0359], device='cuda:0')), ('power', tensor([-25.2405], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.049690406769514084
epoch£º382	 i:1 	 global-step:7641	 l-p:0.1337621808052063
epoch£º382	 i:2 	 global-step:7642	 l-p:0.21426434814929962
epoch£º382	 i:3 	 global-step:7643	 l-p:0.14040309190750122
epoch£º382	 i:4 	 global-step:7644	 l-p:0.13062509894371033
epoch£º382	 i:5 	 global-step:7645	 l-p:0.11069417744874954
epoch£º382	 i:6 	 global-step:7646	 l-p:0.1531148999929428
epoch£º382	 i:7 	 global-step:7647	 l-p:0.148427352309227
epoch£º382	 i:8 	 global-step:7648	 l-p:0.20405307412147522
epoch£º382	 i:9 	 global-step:7649	 l-p:0.12165563553571701
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8559, 2.8370, 2.8545],
        [2.8559, 2.0323, 1.8786],
        [2.8559, 2.8559, 2.8559],
        [2.8559, 2.5760, 2.7254]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.14515568315982819 
model_pd.l_d.mean(): -24.96150779724121 
model_pd.lagr.mean(): -24.81635284423828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0671], device='cuda:0')), ('power', tensor([-25.0286], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.14515568315982819
epoch£º383	 i:1 	 global-step:7661	 l-p:0.024156995117664337
epoch£º383	 i:2 	 global-step:7662	 l-p:0.15854865312576294
epoch£º383	 i:3 	 global-step:7663	 l-p:0.14624802768230438
epoch£º383	 i:4 	 global-step:7664	 l-p:0.12819957733154297
epoch£º383	 i:5 	 global-step:7665	 l-p:0.3712688386440277
epoch£º383	 i:6 	 global-step:7666	 l-p:0.13218824565410614
epoch£º383	 i:7 	 global-step:7667	 l-p:0.11062119901180267
epoch£º383	 i:8 	 global-step:7668	 l-p:0.13488218188285828
epoch£º383	 i:9 	 global-step:7669	 l-p:0.7710334658622742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7910, 1.9005, 1.3470],
        [2.7910, 2.1260, 2.1523],
        [2.7910, 1.7762, 1.2654],
        [2.7910, 2.6030, 2.7272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.06202499568462372 
model_pd.l_d.mean(): -25.251174926757812 
model_pd.lagr.mean(): -25.189149856567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0946], device='cuda:0')), ('power', tensor([-25.3458], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.06202499568462372
epoch£º384	 i:1 	 global-step:7681	 l-p:0.15155893564224243
epoch£º384	 i:2 	 global-step:7682	 l-p:0.12501485645771027
epoch£º384	 i:3 	 global-step:7683	 l-p:0.13540983200073242
epoch£º384	 i:4 	 global-step:7684	 l-p:0.13030819594860077
epoch£º384	 i:5 	 global-step:7685	 l-p:0.14930793642997742
epoch£º384	 i:6 	 global-step:7686	 l-p:0.16082321107387543
epoch£º384	 i:7 	 global-step:7687	 l-p:0.22691573202610016
epoch£º384	 i:8 	 global-step:7688	 l-p:0.09028179198503494
epoch£º384	 i:9 	 global-step:7689	 l-p:0.12880046665668488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0368, 2.2039, 2.0196],
        [3.0368, 3.0243, 3.0361],
        [3.0368, 3.0366, 3.0368],
        [3.0368, 2.1314, 1.5394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.12308145314455032 
model_pd.l_d.mean(): -24.951311111450195 
model_pd.lagr.mean(): -24.828229904174805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0105], device='cuda:0')), ('power', tensor([-24.9618], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.12308145314455032
epoch£º385	 i:1 	 global-step:7701	 l-p:0.11687688529491425
epoch£º385	 i:2 	 global-step:7702	 l-p:0.14321745932102203
epoch£º385	 i:3 	 global-step:7703	 l-p:0.12432501465082169
epoch£º385	 i:4 	 global-step:7704	 l-p:0.13731153309345245
epoch£º385	 i:5 	 global-step:7705	 l-p:-0.04695262759923935
epoch£º385	 i:6 	 global-step:7706	 l-p:0.11061502248048782
epoch£º385	 i:7 	 global-step:7707	 l-p:0.12444787472486496
epoch£º385	 i:8 	 global-step:7708	 l-p:0.12392181158065796
epoch£º385	 i:9 	 global-step:7709	 l-p:0.03771912306547165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7598, 2.4484, 2.6040],
        [2.7598, 1.7360, 1.2379],
        [2.7598, 2.7598, 2.7598],
        [2.7598, 2.5308, 2.6700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.18063518404960632 
model_pd.l_d.mean(): -24.99059295654297 
model_pd.lagr.mean(): -24.80995750427246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1872], device='cuda:0')), ('power', tensor([-25.1778], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.18063518404960632
epoch£º386	 i:1 	 global-step:7721	 l-p:0.1878587305545807
epoch£º386	 i:2 	 global-step:7722	 l-p:0.13280662894248962
epoch£º386	 i:3 	 global-step:7723	 l-p:0.14511895179748535
epoch£º386	 i:4 	 global-step:7724	 l-p:0.1250714361667633
epoch£º386	 i:5 	 global-step:7725	 l-p:0.11621734499931335
epoch£º386	 i:6 	 global-step:7726	 l-p:0.09767560660839081
epoch£º386	 i:7 	 global-step:7727	 l-p:0.11557785421609879
epoch£º386	 i:8 	 global-step:7728	 l-p:0.16126343607902527
epoch£º386	 i:9 	 global-step:7729	 l-p:-0.9515711069107056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7972, 2.6225, 2.7415],
        [2.7972, 1.8520, 1.3074],
        [2.7972, 2.7941, 2.7971],
        [2.7972, 2.4763, 2.6324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.16570459306240082 
model_pd.l_d.mean(): -25.22336196899414 
model_pd.lagr.mean(): -25.05765724182129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0896], device='cuda:0')), ('power', tensor([-25.3129], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.16570459306240082
epoch£º387	 i:1 	 global-step:7741	 l-p:-0.09290553629398346
epoch£º387	 i:2 	 global-step:7742	 l-p:0.11143912374973297
epoch£º387	 i:3 	 global-step:7743	 l-p:-0.055373094975948334
epoch£º387	 i:4 	 global-step:7744	 l-p:0.09518760442733765
epoch£º387	 i:5 	 global-step:7745	 l-p:0.2199888825416565
epoch£º387	 i:6 	 global-step:7746	 l-p:0.13537876307964325
epoch£º387	 i:7 	 global-step:7747	 l-p:-0.026698073372244835
epoch£º387	 i:8 	 global-step:7748	 l-p:0.1724502593278885
epoch£º387	 i:9 	 global-step:7749	 l-p:0.11339972913265228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0923, 3.0735, 3.0910],
        [3.0923, 2.9167, 3.0349],
        [3.0923, 3.0693, 3.0905],
        [3.0923, 2.3028, 2.1780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.09064994752407074 
model_pd.l_d.mean(): -24.638444900512695 
model_pd.lagr.mean(): -24.547794342041016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1074], device='cuda:0')), ('power', tensor([-24.7458], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.09064994752407074
epoch£º388	 i:1 	 global-step:7761	 l-p:0.12703584134578705
epoch£º388	 i:2 	 global-step:7762	 l-p:0.13087807595729828
epoch£º388	 i:3 	 global-step:7763	 l-p:0.1375959813594818
epoch£º388	 i:4 	 global-step:7764	 l-p:0.1372290402650833
epoch£º388	 i:5 	 global-step:7765	 l-p:0.1806747019290924
epoch£º388	 i:6 	 global-step:7766	 l-p:0.07444555312395096
epoch£º388	 i:7 	 global-step:7767	 l-p:0.2644593119621277
epoch£º388	 i:8 	 global-step:7768	 l-p:0.1727716475725174
epoch£º388	 i:9 	 global-step:7769	 l-p:0.13995002210140228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8343, 1.8262, 1.4170],
        [2.8343, 2.0611, 1.9859],
        [2.8343, 2.8331, 2.8343],
        [2.8343, 2.5307, 2.6854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.08756355196237564 
model_pd.l_d.mean(): -24.689516067504883 
model_pd.lagr.mean(): -24.601951599121094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1627], device='cuda:0')), ('power', tensor([-24.8522], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.08756355196237564
epoch£º389	 i:1 	 global-step:7781	 l-p:0.11307623982429504
epoch£º389	 i:2 	 global-step:7782	 l-p:0.15720631182193756
epoch£º389	 i:3 	 global-step:7783	 l-p:0.14425955712795258
epoch£º389	 i:4 	 global-step:7784	 l-p:-0.31867095828056335
epoch£º389	 i:5 	 global-step:7785	 l-p:0.1707264631986618
epoch£º389	 i:6 	 global-step:7786	 l-p:0.15648384392261505
epoch£º389	 i:7 	 global-step:7787	 l-p:0.5956753492355347
epoch£º389	 i:8 	 global-step:7788	 l-p:0.1613018810749054
epoch£º389	 i:9 	 global-step:7789	 l-p:0.14516597986221313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0837, 3.0823, 3.0837],
        [3.0837, 2.5302, 2.6251],
        [3.0837, 2.2045, 1.5985],
        [3.0837, 3.0823, 3.0837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.14408822357654572 
model_pd.l_d.mean(): -25.07467269897461 
model_pd.lagr.mean(): -24.930583953857422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0841], device='cuda:0')), ('power', tensor([-24.9906], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.14408822357654572
epoch£º390	 i:1 	 global-step:7801	 l-p:0.14273329079151154
epoch£º390	 i:2 	 global-step:7802	 l-p:0.11659088730812073
epoch£º390	 i:3 	 global-step:7803	 l-p:0.12944479286670685
epoch£º390	 i:4 	 global-step:7804	 l-p:0.1332111358642578
epoch£º390	 i:5 	 global-step:7805	 l-p:0.09395899623632431
epoch£º390	 i:6 	 global-step:7806	 l-p:0.15866824984550476
epoch£º390	 i:7 	 global-step:7807	 l-p:0.2208074927330017
epoch£º390	 i:8 	 global-step:7808	 l-p:0.15880152583122253
epoch£º390	 i:9 	 global-step:7809	 l-p:0.038019727915525436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8809, 2.8283, 2.8738],
        [2.8809, 2.0203, 1.8354],
        [2.8809, 2.1059, 2.0302],
        [2.8809, 2.8574, 2.8791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.18612872064113617 
model_pd.l_d.mean(): -25.16025733947754 
model_pd.lagr.mean(): -24.97412872314453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0023], device='cuda:0')), ('power', tensor([-25.1626], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.18612872064113617
epoch£º391	 i:1 	 global-step:7821	 l-p:0.16924048960208893
epoch£º391	 i:2 	 global-step:7822	 l-p:0.12138684839010239
epoch£º391	 i:3 	 global-step:7823	 l-p:0.10737196356058121
epoch£º391	 i:4 	 global-step:7824	 l-p:0.10315053910017014
epoch£º391	 i:5 	 global-step:7825	 l-p:0.10000979155302048
epoch£º391	 i:6 	 global-step:7826	 l-p:0.12826432287693024
epoch£º391	 i:7 	 global-step:7827	 l-p:0.07235150784254074
epoch£º391	 i:8 	 global-step:7828	 l-p:0.10216020047664642
epoch£º391	 i:9 	 global-step:7829	 l-p:0.14852093160152435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9783, 2.7038, 2.8536],
        [2.9783, 2.9768, 2.9782],
        [2.9783, 2.2600, 1.6520],
        [2.9783, 2.9783, 2.9783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.14808782935142517 
model_pd.l_d.mean(): -24.80790138244629 
model_pd.lagr.mean(): -24.659812927246094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0233], device='cuda:0')), ('power', tensor([-24.7846], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.14808782935142517
epoch£º392	 i:1 	 global-step:7841	 l-p:0.20550191402435303
epoch£º392	 i:2 	 global-step:7842	 l-p:0.16199012100696564
epoch£º392	 i:3 	 global-step:7843	 l-p:2.061647415161133
epoch£º392	 i:4 	 global-step:7844	 l-p:-0.3981895446777344
epoch£º392	 i:5 	 global-step:7845	 l-p:0.11149284243583679
epoch£º392	 i:6 	 global-step:7846	 l-p:0.2549477219581604
epoch£º392	 i:7 	 global-step:7847	 l-p:0.04077973589301109
epoch£º392	 i:8 	 global-step:7848	 l-p:0.11964698880910873
epoch£º392	 i:9 	 global-step:7849	 l-p:0.1109727993607521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0841, 2.2613, 1.6483],
        [3.0841, 3.0841, 3.0841],
        [3.0841, 2.0447, 1.4788],
        [3.0841, 3.0840, 3.0841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.1000504121184349 
model_pd.l_d.mean(): -24.983749389648438 
model_pd.lagr.mean(): -24.883699417114258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0078], device='cuda:0')), ('power', tensor([-24.9916], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.1000504121184349
epoch£º393	 i:1 	 global-step:7861	 l-p:0.13457174599170685
epoch£º393	 i:2 	 global-step:7862	 l-p:0.13993686437606812
epoch£º393	 i:3 	 global-step:7863	 l-p:0.11258918792009354
epoch£º393	 i:4 	 global-step:7864	 l-p:0.11748090386390686
epoch£º393	 i:5 	 global-step:7865	 l-p:0.13771173357963562
epoch£º393	 i:6 	 global-step:7866	 l-p:0.10370885580778122
epoch£º393	 i:7 	 global-step:7867	 l-p:0.138851135969162
epoch£º393	 i:8 	 global-step:7868	 l-p:0.2824721932411194
epoch£º393	 i:9 	 global-step:7869	 l-p:-0.16337737441062927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6891, 2.3841, 2.5424],
        [2.6891, 2.6868, 2.6891],
        [2.6891, 1.9253, 1.8844],
        [2.6891, 1.8889, 1.3246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.13425478339195251 
model_pd.l_d.mean(): -25.17560386657715 
model_pd.lagr.mean(): -25.041349411010742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1377], device='cuda:0')), ('power', tensor([-25.3133], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.13425478339195251
epoch£º394	 i:1 	 global-step:7881	 l-p:-0.12601710855960846
epoch£º394	 i:2 	 global-step:7882	 l-p:0.14314623177051544
epoch£º394	 i:3 	 global-step:7883	 l-p:0.30115604400634766
epoch£º394	 i:4 	 global-step:7884	 l-p:0.1554863005876541
epoch£º394	 i:5 	 global-step:7885	 l-p:0.22346210479736328
epoch£º394	 i:6 	 global-step:7886	 l-p:0.15487942099571228
epoch£º394	 i:7 	 global-step:7887	 l-p:0.13234780728816986
epoch£º394	 i:8 	 global-step:7888	 l-p:0.11868884414434433
epoch£º394	 i:9 	 global-step:7889	 l-p:0.14524513483047485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8933, 2.8842, 2.8929],
        [2.8933, 2.4835, 2.6385],
        [2.8933, 2.8934, 2.8933],
        [2.8933, 2.5980, 2.7529]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.12071303278207779 
model_pd.l_d.mean(): -24.929893493652344 
model_pd.lagr.mean(): -24.809181213378906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0945], device='cuda:0')), ('power', tensor([-25.0244], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.12071303278207779
epoch£º395	 i:1 	 global-step:7901	 l-p:0.13466845452785492
epoch£º395	 i:2 	 global-step:7902	 l-p:0.17977496981620789
epoch£º395	 i:3 	 global-step:7903	 l-p:0.18513619899749756
epoch£º395	 i:4 	 global-step:7904	 l-p:0.15409542620182037
epoch£º395	 i:5 	 global-step:7905	 l-p:0.12037087976932526
epoch£º395	 i:6 	 global-step:7906	 l-p:0.15506048500537872
epoch£º395	 i:7 	 global-step:7907	 l-p:0.13015536963939667
epoch£º395	 i:8 	 global-step:7908	 l-p:0.23240166902542114
epoch£º395	 i:9 	 global-step:7909	 l-p:0.15787352621555328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9312, 1.8817, 1.3470],
        [2.9312, 1.9000, 1.3496],
        [2.9312, 2.8126, 2.9031],
        [2.9312, 2.8609, 2.9197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.13940487802028656 
model_pd.l_d.mean(): -25.277225494384766 
model_pd.lagr.mean(): -25.137821197509766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0635], device='cuda:0')), ('power', tensor([-25.2137], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.13940487802028656
epoch£º396	 i:1 	 global-step:7921	 l-p:0.1480151116847992
epoch£º396	 i:2 	 global-step:7922	 l-p:0.11425068229436874
epoch£º396	 i:3 	 global-step:7923	 l-p:-0.10599936544895172
epoch£º396	 i:4 	 global-step:7924	 l-p:0.13324035704135895
epoch£º396	 i:5 	 global-step:7925	 l-p:0.13947203755378723
epoch£º396	 i:6 	 global-step:7926	 l-p:0.15345008671283722
epoch£º396	 i:7 	 global-step:7927	 l-p:0.1510375589132309
epoch£º396	 i:8 	 global-step:7928	 l-p:0.13434278964996338
epoch£º396	 i:9 	 global-step:7929	 l-p:0.17124681174755096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9147, 2.1206, 1.5247],
        [2.9147, 2.8822, 2.9115],
        [2.9147, 2.5311, 2.6896],
        [2.9147, 2.5687, 2.7281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.11219853162765503 
model_pd.l_d.mean(): -24.62759780883789 
model_pd.lagr.mean(): -24.515399932861328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1914], device='cuda:0')), ('power', tensor([-24.8190], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.11219853162765503
epoch£º397	 i:1 	 global-step:7941	 l-p:0.20131026208400726
epoch£º397	 i:2 	 global-step:7942	 l-p:0.1797403246164322
epoch£º397	 i:3 	 global-step:7943	 l-p:-1.7152974605560303
epoch£º397	 i:4 	 global-step:7944	 l-p:0.0792769193649292
epoch£º397	 i:5 	 global-step:7945	 l-p:0.14487488567829132
epoch£º397	 i:6 	 global-step:7946	 l-p:0.09336473047733307
epoch£º397	 i:7 	 global-step:7947	 l-p:0.13167232275009155
epoch£º397	 i:8 	 global-step:7948	 l-p:0.12698499858379364
epoch£º397	 i:9 	 global-step:7949	 l-p:0.1461590826511383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9099, 1.8638, 1.3242],
        [2.9099, 2.9099, 2.9099],
        [2.9099, 2.9007, 2.9095],
        [2.9099, 1.8499, 1.3329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.24650423228740692 
model_pd.l_d.mean(): -24.683101654052734 
model_pd.lagr.mean(): -24.43659782409668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1536], device='cuda:0')), ('power', tensor([-24.8367], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.24650423228740692
epoch£º398	 i:1 	 global-step:7961	 l-p:0.12227340787649155
epoch£º398	 i:2 	 global-step:7962	 l-p:0.08211541175842285
epoch£º398	 i:3 	 global-step:7963	 l-p:0.15020751953125
epoch£º398	 i:4 	 global-step:7964	 l-p:0.17631518840789795
epoch£º398	 i:5 	 global-step:7965	 l-p:0.18752698600292206
epoch£º398	 i:6 	 global-step:7966	 l-p:-0.036864958703517914
epoch£º398	 i:7 	 global-step:7967	 l-p:0.13777296245098114
epoch£º398	 i:8 	 global-step:7968	 l-p:0.12538783252239227
epoch£º398	 i:9 	 global-step:7969	 l-p:0.16403472423553467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0922, 3.0896, 3.0921],
        [3.0922, 2.7399, 2.8972],
        [3.0922, 2.0520, 1.5544],
        [3.0922, 2.2739, 2.1402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.11131089925765991 
model_pd.l_d.mean(): -24.888347625732422 
model_pd.lagr.mean(): -24.777036666870117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0066], device='cuda:0')), ('power', tensor([-24.8817], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.11131089925765991
epoch£º399	 i:1 	 global-step:7981	 l-p:0.13674594461917877
epoch£º399	 i:2 	 global-step:7982	 l-p:0.14800140261650085
epoch£º399	 i:3 	 global-step:7983	 l-p:0.14190156757831573
epoch£º399	 i:4 	 global-step:7984	 l-p:0.12012799829244614
epoch£º399	 i:5 	 global-step:7985	 l-p:0.1360442191362381
epoch£º399	 i:6 	 global-step:7986	 l-p:0.13574786484241486
epoch£º399	 i:7 	 global-step:7987	 l-p:0.22951319813728333
epoch£º399	 i:8 	 global-step:7988	 l-p:0.30848386883735657
epoch£º399	 i:9 	 global-step:7989	 l-p:0.1436322182416916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6651, 1.7029, 1.4237],
        [2.6651, 1.6126, 1.1584],
        [2.6651, 2.5621, 2.6435],
        [2.6651, 1.7274, 1.4826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): -0.7717218399047852 
model_pd.l_d.mean(): -24.841157913208008 
model_pd.lagr.mean(): -25.61288070678711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2112], device='cuda:0')), ('power', tensor([-25.0523], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:-0.7717218399047852
epoch£º400	 i:1 	 global-step:8001	 l-p:0.13871979713439941
epoch£º400	 i:2 	 global-step:8002	 l-p:0.09943942725658417
epoch£º400	 i:3 	 global-step:8003	 l-p:0.23734354972839355
epoch£º400	 i:4 	 global-step:8004	 l-p:0.4094679057598114
epoch£º400	 i:5 	 global-step:8005	 l-p:0.06664468348026276
epoch£º400	 i:6 	 global-step:8006	 l-p:0.16105107963085175
epoch£º400	 i:7 	 global-step:8007	 l-p:0.19843359291553497
epoch£º400	 i:8 	 global-step:8008	 l-p:0.17620286345481873
epoch£º400	 i:9 	 global-step:8009	 l-p:0.12643155455589294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0850, 2.0257, 1.4672],
        [3.0850, 3.0837, 3.0849],
        [3.0850, 2.0271, 1.4965],
        [3.0850, 2.0719, 1.4801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.12323388457298279 
model_pd.l_d.mean(): -24.70684814453125 
model_pd.lagr.mean(): -24.583614349365234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1362], device='cuda:0')), ('power', tensor([-24.8431], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.12323388457298279
epoch£º401	 i:1 	 global-step:8021	 l-p:0.1312445104122162
epoch£º401	 i:2 	 global-step:8022	 l-p:0.11648939549922943
epoch£º401	 i:3 	 global-step:8023	 l-p:0.09072349965572357
epoch£º401	 i:4 	 global-step:8024	 l-p:0.13729159533977509
epoch£º401	 i:5 	 global-step:8025	 l-p:0.13840779662132263
epoch£º401	 i:6 	 global-step:8026	 l-p:0.1371825784444809
epoch£º401	 i:7 	 global-step:8027	 l-p:0.10723073780536652
epoch£º401	 i:8 	 global-step:8028	 l-p:0.21046985685825348
epoch£º401	 i:9 	 global-step:8029	 l-p:0.15360064804553986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8606, 2.8606, 2.8606],
        [2.8606, 2.8579, 2.8605],
        [2.8606, 2.4956, 2.6573],
        [2.8606, 2.8606, 2.8606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.13986454904079437 
model_pd.l_d.mean(): -25.14830780029297 
model_pd.lagr.mean(): -25.00844383239746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0262], device='cuda:0')), ('power', tensor([-25.1221], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.13986454904079437
epoch£º402	 i:1 	 global-step:8041	 l-p:0.1645272672176361
epoch£º402	 i:2 	 global-step:8042	 l-p:0.18545882403850555
epoch£º402	 i:3 	 global-step:8043	 l-p:0.12040404975414276
epoch£º402	 i:4 	 global-step:8044	 l-p:0.15237632393836975
epoch£º402	 i:5 	 global-step:8045	 l-p:-16.576610565185547
epoch£º402	 i:6 	 global-step:8046	 l-p:0.16382639110088348
epoch£º402	 i:7 	 global-step:8047	 l-p:0.12916791439056396
epoch£º402	 i:8 	 global-step:8048	 l-p:0.1052275151014328
epoch£º402	 i:9 	 global-step:8049	 l-p:-0.14450359344482422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6523, 2.3470, 2.5077],
        [2.6523, 2.6521, 2.6523],
        [2.6523, 2.3756, 2.5310],
        [2.6523, 2.5530, 2.6320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.15985152125358582 
model_pd.l_d.mean(): -25.112628936767578 
model_pd.lagr.mean(): -24.952777862548828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1192], device='cuda:0')), ('power', tensor([-25.2319], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.15985152125358582
epoch£º403	 i:1 	 global-step:8061	 l-p:0.09721940755844116
epoch£º403	 i:2 	 global-step:8062	 l-p:-0.3554139733314514
epoch£º403	 i:3 	 global-step:8063	 l-p:0.16427451372146606
epoch£º403	 i:4 	 global-step:8064	 l-p:0.09318684041500092
epoch£º403	 i:5 	 global-step:8065	 l-p:0.146549791097641
epoch£º403	 i:6 	 global-step:8066	 l-p:0.1209326982498169
epoch£º403	 i:7 	 global-step:8067	 l-p:0.1859956681728363
epoch£º403	 i:8 	 global-step:8068	 l-p:0.10951822251081467
epoch£º403	 i:9 	 global-step:8069	 l-p:0.14445550739765167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0683, 2.0065, 1.4790],
        [3.0683, 2.1329, 1.5290],
        [3.0683, 2.8759, 3.0028],
        [3.0683, 3.0401, 3.0658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.13901053369045258 
model_pd.l_d.mean(): -24.580120086669922 
model_pd.lagr.mean(): -24.44110870361328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0936], device='cuda:0')), ('power', tensor([-24.6737], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.13901053369045258
epoch£º404	 i:1 	 global-step:8081	 l-p:0.10794386267662048
epoch£º404	 i:2 	 global-step:8082	 l-p:0.14057040214538574
epoch£º404	 i:3 	 global-step:8083	 l-p:0.12676405906677246
epoch£º404	 i:4 	 global-step:8084	 l-p:0.12357927113771439
epoch£º404	 i:5 	 global-step:8085	 l-p:0.5166913270950317
epoch£º404	 i:6 	 global-step:8086	 l-p:0.2172113060951233
epoch£º404	 i:7 	 global-step:8087	 l-p:0.13516853749752045
epoch£º404	 i:8 	 global-step:8088	 l-p:0.11786872893571854
epoch£º404	 i:9 	 global-step:8089	 l-p:-0.026518063619732857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8080, 1.8809, 1.6369],
        [2.8080, 2.8080, 2.8080],
        [2.8080, 2.8080, 2.8080],
        [2.8080, 2.1228, 2.1585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.12457023561000824 
model_pd.l_d.mean(): -25.060993194580078 
model_pd.lagr.mean(): -24.93642234802246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1142], device='cuda:0')), ('power', tensor([-25.1752], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.12457023561000824
epoch£º405	 i:1 	 global-step:8101	 l-p:0.12635144591331482
epoch£º405	 i:2 	 global-step:8102	 l-p:0.11861003935337067
epoch£º405	 i:3 	 global-step:8103	 l-p:0.11695815622806549
epoch£º405	 i:4 	 global-step:8104	 l-p:0.06904856860637665
epoch£º405	 i:5 	 global-step:8105	 l-p:-0.32508453726768494
epoch£º405	 i:6 	 global-step:8106	 l-p:0.20629748702049255
epoch£º405	 i:7 	 global-step:8107	 l-p:0.17672449350357056
epoch£º405	 i:8 	 global-step:8108	 l-p:0.1375429332256317
epoch£º405	 i:9 	 global-step:8109	 l-p:0.14546307921409607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9087, 2.5403, 2.7018],
        [2.9087, 2.7889, 2.8804],
        [2.9087, 2.2349, 2.2737],
        [2.9087, 1.8407, 1.3229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.35747599601745605 
model_pd.l_d.mean(): -24.804264068603516 
model_pd.lagr.mean(): -24.446788787841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1120], device='cuda:0')), ('power', tensor([-24.9163], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.35747599601745605
epoch£º406	 i:1 	 global-step:8121	 l-p:0.08849437534809113
epoch£º406	 i:2 	 global-step:8122	 l-p:0.13725873827934265
epoch£º406	 i:3 	 global-step:8123	 l-p:0.14494957029819489
epoch£º406	 i:4 	 global-step:8124	 l-p:0.15123741328716278
epoch£º406	 i:5 	 global-step:8125	 l-p:0.20374886691570282
epoch£º406	 i:6 	 global-step:8126	 l-p:0.19907024502754211
epoch£º406	 i:7 	 global-step:8127	 l-p:0.16148240864276886
epoch£º406	 i:8 	 global-step:8128	 l-p:0.12969812750816345
epoch£º406	 i:9 	 global-step:8129	 l-p:0.12842896580696106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9968, 2.9614, 2.9932],
        [2.9968, 2.9653, 2.9938],
        [2.9968, 1.9377, 1.4484],
        [2.9968, 2.0626, 1.7827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.3561249375343323 
model_pd.l_d.mean(): -25.098461151123047 
model_pd.lagr.mean(): -24.74233627319336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0430], device='cuda:0')), ('power', tensor([-25.0555], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.3561249375343323
epoch£º407	 i:1 	 global-step:8141	 l-p:0.11762555688619614
epoch£º407	 i:2 	 global-step:8142	 l-p:0.13561514019966125
epoch£º407	 i:3 	 global-step:8143	 l-p:0.15805509686470032
epoch£º407	 i:4 	 global-step:8144	 l-p:0.1563994437456131
epoch£º407	 i:5 	 global-step:8145	 l-p:0.1529371589422226
epoch£º407	 i:6 	 global-step:8146	 l-p:0.1536974012851715
epoch£º407	 i:7 	 global-step:8147	 l-p:0.48924151062965393
epoch£º407	 i:8 	 global-step:8148	 l-p:0.1287466138601303
epoch£º407	 i:9 	 global-step:8149	 l-p:0.1448981761932373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8019, 2.5911, 2.7266],
        [2.8019, 2.6203, 2.7440],
        [2.8019, 2.1144, 2.1503],
        [2.8019, 1.7490, 1.3149]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.17441818118095398 
model_pd.l_d.mean(): -25.222410202026367 
model_pd.lagr.mean(): -25.047992706298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1332], device='cuda:0')), ('power', tensor([-25.3556], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.17441818118095398
epoch£º408	 i:1 	 global-step:8161	 l-p:0.25298750400543213
epoch£º408	 i:2 	 global-step:8162	 l-p:0.1179165244102478
epoch£º408	 i:3 	 global-step:8163	 l-p:0.11186493933200836
epoch£º408	 i:4 	 global-step:8164	 l-p:0.021423883736133575
epoch£º408	 i:5 	 global-step:8165	 l-p:0.12485109269618988
epoch£º408	 i:6 	 global-step:8166	 l-p:0.13768190145492554
epoch£º408	 i:7 	 global-step:8167	 l-p:0.13817179203033447
epoch£º408	 i:8 	 global-step:8168	 l-p:0.14829251170158386
epoch£º408	 i:9 	 global-step:8169	 l-p:0.10162283480167389
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9098, 2.7793, 2.8771],
        [2.9098, 2.7642, 2.8702],
        [2.9098, 2.9003, 2.9094],
        [2.9098, 2.9031, 2.9096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.1432299166917801 
model_pd.l_d.mean(): -25.126800537109375 
model_pd.lagr.mean(): -24.983570098876953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0150], device='cuda:0')), ('power', tensor([-25.1118], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.1432299166917801
epoch£º409	 i:1 	 global-step:8181	 l-p:0.19589516520500183
epoch£º409	 i:2 	 global-step:8182	 l-p:0.2596370577812195
epoch£º409	 i:3 	 global-step:8183	 l-p:0.11771373450756073
epoch£º409	 i:4 	 global-step:8184	 l-p:0.14731912314891815
epoch£º409	 i:5 	 global-step:8185	 l-p:0.12571726739406586
epoch£º409	 i:6 	 global-step:8186	 l-p:0.14639811217784882
epoch£º409	 i:7 	 global-step:8187	 l-p:0.12807407975196838
epoch£º409	 i:8 	 global-step:8188	 l-p:0.14259447157382965
epoch£º409	 i:9 	 global-step:8189	 l-p:0.10890846699476242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0374, 2.8271, 2.9613],
        [3.0374, 2.6197, 2.7750],
        [3.0374, 3.0037, 3.0340],
        [3.0374, 3.0374, 3.0374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.13514190912246704 
model_pd.l_d.mean(): -25.038278579711914 
model_pd.lagr.mean(): -24.90313720703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0169], device='cuda:0')), ('power', tensor([-25.0214], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.13514190912246704
epoch£º410	 i:1 	 global-step:8201	 l-p:0.13287967443466187
epoch£º410	 i:2 	 global-step:8202	 l-p:-1.3098961114883423
epoch£º410	 i:3 	 global-step:8203	 l-p:0.13259468972682953
epoch£º410	 i:4 	 global-step:8204	 l-p:0.16675180196762085
epoch£º410	 i:5 	 global-step:8205	 l-p:0.13295643031597137
epoch£º410	 i:6 	 global-step:8206	 l-p:0.286750465631485
epoch£º410	 i:7 	 global-step:8207	 l-p:0.13690003752708435
epoch£º410	 i:8 	 global-step:8208	 l-p:0.1565401256084442
epoch£º410	 i:9 	 global-step:8209	 l-p:0.06439024955034256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8392, 2.8392, 2.8392],
        [2.8392, 2.8356, 2.8391],
        [2.8392, 2.8383, 2.8392],
        [2.8392, 2.8326, 2.8389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): -1.7014132738113403 
model_pd.l_d.mean(): -25.01062774658203 
model_pd.lagr.mean(): -26.7120418548584 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0757], device='cuda:0')), ('power', tensor([-25.0863], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:-1.7014132738113403
epoch£º411	 i:1 	 global-step:8221	 l-p:0.14880114793777466
epoch£º411	 i:2 	 global-step:8222	 l-p:0.1785217672586441
epoch£º411	 i:3 	 global-step:8223	 l-p:0.20937858521938324
epoch£º411	 i:4 	 global-step:8224	 l-p:0.07105110585689545
epoch£º411	 i:5 	 global-step:8225	 l-p:0.16201232373714447
epoch£º411	 i:6 	 global-step:8226	 l-p:0.16259706020355225
epoch£º411	 i:7 	 global-step:8227	 l-p:0.17157787084579468
epoch£º411	 i:8 	 global-step:8228	 l-p:0.12197332084178925
epoch£º411	 i:9 	 global-step:8229	 l-p:0.13190127909183502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0963, 3.0636, 3.0931],
        [3.0963, 2.3816, 1.7530],
        [3.0963, 3.0963, 3.0963],
        [3.0963, 2.6871, 2.8427]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.1306859403848648 
model_pd.l_d.mean(): -24.564916610717773 
model_pd.lagr.mean(): -24.43423080444336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0329], device='cuda:0')), ('power', tensor([-24.5978], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.1306859403848648
epoch£º412	 i:1 	 global-step:8241	 l-p:0.12558940052986145
epoch£º412	 i:2 	 global-step:8242	 l-p:0.1185104176402092
epoch£º412	 i:3 	 global-step:8243	 l-p:0.12752266228199005
epoch£º412	 i:4 	 global-step:8244	 l-p:0.13799026608467102
epoch£º412	 i:5 	 global-step:8245	 l-p:0.13805249333381653
epoch£º412	 i:6 	 global-step:8246	 l-p:0.16295626759529114
epoch£º412	 i:7 	 global-step:8247	 l-p:0.15972255170345306
epoch£º412	 i:8 	 global-step:8248	 l-p:0.15915976464748383
epoch£º412	 i:9 	 global-step:8249	 l-p:0.026843175292015076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8873, 2.5734, 2.7336],
        [2.8873, 2.8873, 2.8873],
        [2.8873, 2.1082, 1.5085],
        [2.8873, 2.8873, 2.8874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.1480575054883957 
model_pd.l_d.mean(): -24.87221908569336 
model_pd.lagr.mean(): -24.72416114807129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0965], device='cuda:0')), ('power', tensor([-24.9687], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.1480575054883957
epoch£º413	 i:1 	 global-step:8261	 l-p:0.14645004272460938
epoch£º413	 i:2 	 global-step:8262	 l-p:0.1403973251581192
epoch£º413	 i:3 	 global-step:8263	 l-p:-1.2259453535079956
epoch£º413	 i:4 	 global-step:8264	 l-p:0.12372034043073654
epoch£º413	 i:5 	 global-step:8265	 l-p:0.2121736705303192
epoch£º413	 i:6 	 global-step:8266	 l-p:0.19773142039775848
epoch£º413	 i:7 	 global-step:8267	 l-p:0.172307550907135
epoch£º413	 i:8 	 global-step:8268	 l-p:0.9252879023551941
epoch£º413	 i:9 	 global-step:8269	 l-p:0.13279233872890472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8749, 2.8550, 2.8735],
        [2.8749, 1.8007, 1.2931],
        [2.8749, 2.8749, 2.8749],
        [2.8749, 2.7289, 2.8353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.22361990809440613 
model_pd.l_d.mean(): -24.742944717407227 
model_pd.lagr.mean(): -24.519325256347656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1272], device='cuda:0')), ('power', tensor([-24.8702], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.22361990809440613
epoch£º414	 i:1 	 global-step:8281	 l-p:0.14332109689712524
epoch£º414	 i:2 	 global-step:8282	 l-p:0.19384489953517914
epoch£º414	 i:3 	 global-step:8283	 l-p:0.14506636559963226
epoch£º414	 i:4 	 global-step:8284	 l-p:0.12622509896755219
epoch£º414	 i:5 	 global-step:8285	 l-p:0.12403170019388199
epoch£º414	 i:6 	 global-step:8286	 l-p:0.14464227855205536
epoch£º414	 i:7 	 global-step:8287	 l-p:0.14042936265468597
epoch£º414	 i:8 	 global-step:8288	 l-p:0.12869657576084137
epoch£º414	 i:9 	 global-step:8289	 l-p:0.1472259908914566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8756, 1.8016, 1.3131],
        [2.8756, 1.9510, 1.3748],
        [2.8756, 1.8207, 1.3813],
        [2.8756, 2.7954, 2.8615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.30931082367897034 
model_pd.l_d.mean(): -24.877132415771484 
model_pd.lagr.mean(): -24.567821502685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1889], device='cuda:0')), ('power', tensor([-25.0661], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.30931082367897034
epoch£º415	 i:1 	 global-step:8301	 l-p:0.12833039462566376
epoch£º415	 i:2 	 global-step:8302	 l-p:0.19365327060222626
epoch£º415	 i:3 	 global-step:8303	 l-p:0.15797890722751617
epoch£º415	 i:4 	 global-step:8304	 l-p:0.17017078399658203
epoch£º415	 i:5 	 global-step:8305	 l-p:0.12238241732120514
epoch£º415	 i:6 	 global-step:8306	 l-p:0.1545937955379486
epoch£º415	 i:7 	 global-step:8307	 l-p:0.13622142374515533
epoch£º415	 i:8 	 global-step:8308	 l-p:0.17147018015384674
epoch£º415	 i:9 	 global-step:8309	 l-p:0.12507589161396027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9923, 2.7976, 2.9264],
        [2.9923, 2.9921, 2.9923],
        [2.9923, 1.9216, 1.4229],
        [2.9923, 2.9923, 2.9923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.15003293752670288 
model_pd.l_d.mean(): -25.148174285888672 
model_pd.lagr.mean(): -24.99814224243164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0206], device='cuda:0')), ('power', tensor([-25.1688], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.15003293752670288
epoch£º416	 i:1 	 global-step:8321	 l-p:0.14725282788276672
epoch£º416	 i:2 	 global-step:8322	 l-p:0.25251293182373047
epoch£º416	 i:3 	 global-step:8323	 l-p:0.14264313876628876
epoch£º416	 i:4 	 global-step:8324	 l-p:0.11148612946271896
epoch£º416	 i:5 	 global-step:8325	 l-p:0.13264472782611847
epoch£º416	 i:6 	 global-step:8326	 l-p:0.1462678164243698
epoch£º416	 i:7 	 global-step:8327	 l-p:0.1707993894815445
epoch£º416	 i:8 	 global-step:8328	 l-p:0.11710897088050842
epoch£º416	 i:9 	 global-step:8329	 l-p:0.12563472986221313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9525, 2.9054, 2.9467],
        [2.9525, 2.8500, 2.9309],
        [2.9525, 2.6383, 2.7982],
        [2.9525, 2.9525, 2.9525]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.17139510810375214 
model_pd.l_d.mean(): -25.015167236328125 
model_pd.lagr.mean(): -24.843772888183594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0064], device='cuda:0')), ('power', tensor([-25.0215], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.17139510810375214
epoch£º417	 i:1 	 global-step:8341	 l-p:0.13988977670669556
epoch£º417	 i:2 	 global-step:8342	 l-p:0.13335567712783813
epoch£º417	 i:3 	 global-step:8343	 l-p:0.12160192430019379
epoch£º417	 i:4 	 global-step:8344	 l-p:0.13265086710453033
epoch£º417	 i:5 	 global-step:8345	 l-p:0.16278396546840668
epoch£º417	 i:6 	 global-step:8346	 l-p:0.07901676744222641
epoch£º417	 i:7 	 global-step:8347	 l-p:0.1250714510679245
epoch£º417	 i:8 	 global-step:8348	 l-p:0.21928103268146515
epoch£º417	 i:9 	 global-step:8349	 l-p:0.18124179542064667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8230, 2.8221, 2.8229],
        [2.8230, 2.8224, 2.8230],
        [2.8230, 1.7623, 1.2388],
        [2.8230, 2.2879, 2.4187]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): -1.349340796470642 
model_pd.l_d.mean(): -24.8291015625 
model_pd.lagr.mean(): -26.178442001342773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1601], device='cuda:0')), ('power', tensor([-24.9892], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:-1.349340796470642
epoch£º418	 i:1 	 global-step:8361	 l-p:0.16027995944023132
epoch£º418	 i:2 	 global-step:8362	 l-p:0.042547449469566345
epoch£º418	 i:3 	 global-step:8363	 l-p:0.12982872128486633
epoch£º418	 i:4 	 global-step:8364	 l-p:0.13983051478862762
epoch£º418	 i:5 	 global-step:8365	 l-p:0.22372806072235107
epoch£º418	 i:6 	 global-step:8366	 l-p:0.11880326271057129
epoch£º418	 i:7 	 global-step:8367	 l-p:0.1594911515712738
epoch£º418	 i:8 	 global-step:8368	 l-p:0.3298287093639374
epoch£º418	 i:9 	 global-step:8369	 l-p:0.20197848975658417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9261, 2.3981, 2.5282],
        [2.9261, 1.8719, 1.4286],
        [2.9261, 1.9769, 1.6977],
        [2.9261, 2.8755, 2.9196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.12098926305770874 
model_pd.l_d.mean(): -25.072223663330078 
model_pd.lagr.mean(): -24.951234817504883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1136], device='cuda:0')), ('power', tensor([-25.1858], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.12098926305770874
epoch£º419	 i:1 	 global-step:8381	 l-p:0.2347465604543686
epoch£º419	 i:2 	 global-step:8382	 l-p:0.16407695412635803
epoch£º419	 i:3 	 global-step:8383	 l-p:0.12759362161159515
epoch£º419	 i:4 	 global-step:8384	 l-p:0.1338949054479599
epoch£º419	 i:5 	 global-step:8385	 l-p:0.14350460469722748
epoch£º419	 i:6 	 global-step:8386	 l-p:0.12470676004886627
epoch£º419	 i:7 	 global-step:8387	 l-p:0.12162932008504868
epoch£º419	 i:8 	 global-step:8388	 l-p:0.13345685601234436
epoch£º419	 i:9 	 global-step:8389	 l-p:0.16690059006214142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8484, 1.7739, 1.2614],
        [2.8484, 2.3821, 2.5347],
        [2.8484, 2.1573, 2.1936],
        [2.8484, 2.8460, 2.8484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.0011408042628318071 
model_pd.l_d.mean(): -24.77230453491211 
model_pd.lagr.mean(): -24.771163940429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0998], device='cuda:0')), ('power', tensor([-24.8721], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.0011408042628318071
epoch£º420	 i:1 	 global-step:8401	 l-p:0.14164504408836365
epoch£º420	 i:2 	 global-step:8402	 l-p:0.1603456288576126
epoch£º420	 i:3 	 global-step:8403	 l-p:0.21486243605613708
epoch£º420	 i:4 	 global-step:8404	 l-p:0.14159907400608063
epoch£º420	 i:5 	 global-step:8405	 l-p:0.16448156535625458
epoch£º420	 i:6 	 global-step:8406	 l-p:0.1261109560728073
epoch£º420	 i:7 	 global-step:8407	 l-p:0.15037992596626282
epoch£º420	 i:8 	 global-step:8408	 l-p:0.1178046390414238
epoch£º420	 i:9 	 global-step:8409	 l-p:0.29850420355796814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8897, 2.1353, 2.1122],
        [2.8897, 2.0365, 1.4454],
        [2.8897, 2.6599, 2.8022],
        [2.8897, 2.8554, 2.8863]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.09466157108545303 
model_pd.l_d.mean(): -24.976884841918945 
model_pd.lagr.mean(): -24.88222312927246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0351], device='cuda:0')), ('power', tensor([-25.0120], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.09466157108545303
epoch£º421	 i:1 	 global-step:8421	 l-p:0.13341201841831207
epoch£º421	 i:2 	 global-step:8422	 l-p:0.11626335978507996
epoch£º421	 i:3 	 global-step:8423	 l-p:0.1412685066461563
epoch£º421	 i:4 	 global-step:8424	 l-p:0.1580926924943924
epoch£º421	 i:5 	 global-step:8425	 l-p:0.30841049551963806
epoch£º421	 i:6 	 global-step:8426	 l-p:0.16218553483486176
epoch£º421	 i:7 	 global-step:8427	 l-p:0.1427433341741562
epoch£º421	 i:8 	 global-step:8428	 l-p:0.13928605616092682
epoch£º421	 i:9 	 global-step:8429	 l-p:0.171770840883255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0012, 2.9148, 2.9851],
        [3.0012, 3.0012, 3.0012],
        [3.0012, 2.9975, 3.0011],
        [3.0012, 1.9721, 1.5543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.13080550730228424 
model_pd.l_d.mean(): -24.957551956176758 
model_pd.lagr.mean(): -24.826745986938477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0162], device='cuda:0')), ('power', tensor([-24.9737], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.13080550730228424
epoch£º422	 i:1 	 global-step:8441	 l-p:0.13166344165802002
epoch£º422	 i:2 	 global-step:8442	 l-p:0.11646656692028046
epoch£º422	 i:3 	 global-step:8443	 l-p:0.0952887013554573
epoch£º422	 i:4 	 global-step:8444	 l-p:0.15766414999961853
epoch£º422	 i:5 	 global-step:8445	 l-p:0.22734813392162323
epoch£º422	 i:6 	 global-step:8446	 l-p:0.19697268307209015
epoch£º422	 i:7 	 global-step:8447	 l-p:0.1491503119468689
epoch£º422	 i:8 	 global-step:8448	 l-p:0.1382175236940384
epoch£º422	 i:9 	 global-step:8449	 l-p:0.12123283743858337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0511, 3.0509, 3.0511],
        [3.0511, 1.9848, 1.4902],
        [3.0511, 2.9482, 3.0293],
        [3.0511, 2.1254, 1.8681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.1712791621685028 
model_pd.l_d.mean(): -25.003292083740234 
model_pd.lagr.mean(): -24.832012176513672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0919], device='cuda:0')), ('power', tensor([-25.0952], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.1712791621685028
epoch£º423	 i:1 	 global-step:8461	 l-p:0.13939782977104187
epoch£º423	 i:2 	 global-step:8462	 l-p:0.13041944801807404
epoch£º423	 i:3 	 global-step:8463	 l-p:0.1401709020137787
epoch£º423	 i:4 	 global-step:8464	 l-p:0.1487293541431427
epoch£º423	 i:5 	 global-step:8465	 l-p:0.12193295359611511
epoch£º423	 i:6 	 global-step:8466	 l-p:0.14826811850070953
epoch£º423	 i:7 	 global-step:8467	 l-p:0.15945477783679962
epoch£º423	 i:8 	 global-step:8468	 l-p:0.14819194376468658
epoch£º423	 i:9 	 global-step:8469	 l-p:0.13693009316921234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8449, 2.3346, 2.4756],
        [2.8449, 2.8326, 2.8443],
        [2.8449, 1.7954, 1.2565],
        [2.8449, 2.8449, 2.8449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.17065198719501495 
model_pd.l_d.mean(): -25.04908561706543 
model_pd.lagr.mean(): -24.878433227539062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1819], device='cuda:0')), ('power', tensor([-25.2310], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.17065198719501495
epoch£º424	 i:1 	 global-step:8481	 l-p:0.1777830272912979
epoch£º424	 i:2 	 global-step:8482	 l-p:0.14953269064426422
epoch£º424	 i:3 	 global-step:8483	 l-p:0.12263211607933044
epoch£º424	 i:4 	 global-step:8484	 l-p:0.11433214694261551
epoch£º424	 i:5 	 global-step:8485	 l-p:0.17527833580970764
epoch£º424	 i:6 	 global-step:8486	 l-p:0.147825688123703
epoch£º424	 i:7 	 global-step:8487	 l-p:0.011111821979284286
epoch£º424	 i:8 	 global-step:8488	 l-p:0.20874011516571045
epoch£º424	 i:9 	 global-step:8489	 l-p:0.13454477488994598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9891, 2.5479, 2.7035],
        [2.9891, 2.2167, 2.1706],
        [2.9891, 1.9073, 1.3662],
        [2.9891, 2.9891, 2.9891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.1595393419265747 
model_pd.l_d.mean(): -24.75155258178711 
model_pd.lagr.mean(): -24.592012405395508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0569], device='cuda:0')), ('power', tensor([-24.8084], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.1595393419265747
epoch£º425	 i:1 	 global-step:8501	 l-p:0.1327321082353592
epoch£º425	 i:2 	 global-step:8502	 l-p:0.1445741206407547
epoch£º425	 i:3 	 global-step:8503	 l-p:0.13239344954490662
epoch£º425	 i:4 	 global-step:8504	 l-p:0.155751571059227
epoch£º425	 i:5 	 global-step:8505	 l-p:-9.957441329956055
epoch£º425	 i:6 	 global-step:8506	 l-p:0.11637326329946518
epoch£º425	 i:7 	 global-step:8507	 l-p:0.1329374760389328
epoch£º425	 i:8 	 global-step:8508	 l-p:0.16532708704471588
epoch£º425	 i:9 	 global-step:8509	 l-p:0.12907585501670837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9706, 2.9238, 2.9649],
        [2.9706, 2.9670, 2.9705],
        [2.9706, 2.9569, 2.9698],
        [2.9706, 1.9585, 1.3802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.23976969718933105 
model_pd.l_d.mean(): -24.610660552978516 
model_pd.lagr.mean(): -24.370891571044922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1659], device='cuda:0')), ('power', tensor([-24.7766], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.23976969718933105
epoch£º426	 i:1 	 global-step:8521	 l-p:0.16859190165996552
epoch£º426	 i:2 	 global-step:8522	 l-p:0.1416017860174179
epoch£º426	 i:3 	 global-step:8523	 l-p:0.14455081522464752
epoch£º426	 i:4 	 global-step:8524	 l-p:0.1282307505607605
epoch£º426	 i:5 	 global-step:8525	 l-p:0.138226717710495
epoch£º426	 i:6 	 global-step:8526	 l-p:0.14247453212738037
epoch£º426	 i:7 	 global-step:8527	 l-p:0.12033369392156601
epoch£º426	 i:8 	 global-step:8528	 l-p:-0.017415884882211685
epoch£º426	 i:9 	 global-step:8529	 l-p:0.1310814619064331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7780, 2.7455, 2.7749],
        [2.7780, 2.7637, 2.7772],
        [2.7780, 2.4562, 2.6204],
        [2.7780, 1.7558, 1.4025]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.1110992282629013 
model_pd.l_d.mean(): -24.85252571105957 
model_pd.lagr.mean(): -24.741426467895508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0793], device='cuda:0')), ('power', tensor([-24.9319], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.1110992282629013
epoch£º427	 i:1 	 global-step:8541	 l-p:0.2109377235174179
epoch£º427	 i:2 	 global-step:8542	 l-p:0.12376836687326431
epoch£º427	 i:3 	 global-step:8543	 l-p:0.13208982348442078
epoch£º427	 i:4 	 global-step:8544	 l-p:0.13696467876434326
epoch£º427	 i:5 	 global-step:8545	 l-p:0.0964801162481308
epoch£º427	 i:6 	 global-step:8546	 l-p:0.2549702227115631
epoch£º427	 i:7 	 global-step:8547	 l-p:0.12316356599330902
epoch£º427	 i:8 	 global-step:8548	 l-p:0.1795356720685959
epoch£º427	 i:9 	 global-step:8549	 l-p:0.14916163682937622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[2.9151, 1.9741, 1.3907],
        [2.9151, 2.1349, 2.0902],
        [2.9151, 1.8990, 1.3313],
        [2.9151, 2.0573, 1.9219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.16354118287563324 
model_pd.l_d.mean(): -25.129253387451172 
model_pd.lagr.mean(): -24.96571159362793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0267], device='cuda:0')), ('power', tensor([-25.1560], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.16354118287563324
epoch£º428	 i:1 	 global-step:8561	 l-p:0.18620917201042175
epoch£º428	 i:2 	 global-step:8562	 l-p:0.1525077372789383
epoch£º428	 i:3 	 global-step:8563	 l-p:0.12312813848257065
epoch£º428	 i:4 	 global-step:8564	 l-p:0.1563308984041214
epoch£º428	 i:5 	 global-step:8565	 l-p:0.11753570288419724
epoch£º428	 i:6 	 global-step:8566	 l-p:0.12034627795219421
epoch£º428	 i:7 	 global-step:8567	 l-p:0.11374196410179138
epoch£º428	 i:8 	 global-step:8568	 l-p:0.1396542489528656
epoch£º428	 i:9 	 global-step:8569	 l-p:0.12174835801124573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0499, 2.1206, 1.8675],
        [3.0499, 3.0494, 3.0499],
        [3.0499, 1.9744, 1.4054],
        [3.0499, 2.2692, 1.6460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.09164207428693771 
model_pd.l_d.mean(): -24.65450096130371 
model_pd.lagr.mean(): -24.56285858154297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1289], device='cuda:0')), ('power', tensor([-24.7834], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.09164207428693771
epoch£º429	 i:1 	 global-step:8581	 l-p:0.14141014218330383
epoch£º429	 i:2 	 global-step:8582	 l-p:0.13614019751548767
epoch£º429	 i:3 	 global-step:8583	 l-p:0.1463579535484314
epoch£º429	 i:4 	 global-step:8584	 l-p:0.13267265260219574
epoch£º429	 i:5 	 global-step:8585	 l-p:0.6075794696807861
epoch£º429	 i:6 	 global-step:8586	 l-p:0.11794034391641617
epoch£º429	 i:7 	 global-step:8587	 l-p:0.12137927860021591
epoch£º429	 i:8 	 global-step:8588	 l-p:0.005669956095516682
epoch£º429	 i:9 	 global-step:8589	 l-p:0.11182533949613571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8803, 2.6957, 2.8214],
        [2.8803, 2.0804, 1.4792],
        [2.8803, 2.4528, 2.6147],
        [2.8803, 2.8735, 2.8801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.41571474075317383 
model_pd.l_d.mean(): -24.6788387298584 
model_pd.lagr.mean(): -24.263124465942383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2339], device='cuda:0')), ('power', tensor([-24.9127], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.41571474075317383
epoch£º430	 i:1 	 global-step:8601	 l-p:0.1528434455394745
epoch£º430	 i:2 	 global-step:8602	 l-p:0.14370183646678925
epoch£º430	 i:3 	 global-step:8603	 l-p:0.1500299870967865
epoch£º430	 i:4 	 global-step:8604	 l-p:0.14069239795207977
epoch£º430	 i:5 	 global-step:8605	 l-p:0.12943877279758453
epoch£º430	 i:6 	 global-step:8606	 l-p:0.13856695592403412
epoch£º430	 i:7 	 global-step:8607	 l-p:0.22913506627082825
epoch£º430	 i:8 	 global-step:8608	 l-p:0.13608628511428833
epoch£º430	 i:9 	 global-step:8609	 l-p:0.2864026725292206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7977, 1.7129, 1.2160],
        [2.7977, 2.0002, 1.4082],
        [2.7977, 2.7321, 2.7878],
        [2.7977, 1.8869, 1.6963]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.06957627832889557 
model_pd.l_d.mean(): -24.947078704833984 
model_pd.lagr.mean(): -24.87750244140625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1427], device='cuda:0')), ('power', tensor([-25.0898], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.06957627832889557
epoch£º431	 i:1 	 global-step:8621	 l-p:0.08631805330514908
epoch£º431	 i:2 	 global-step:8622	 l-p:0.14185215532779694
epoch£º431	 i:3 	 global-step:8623	 l-p:-0.028184765949845314
epoch£º431	 i:4 	 global-step:8624	 l-p:0.1517694890499115
epoch£º431	 i:5 	 global-step:8625	 l-p:0.1446327418088913
epoch£º431	 i:6 	 global-step:8626	 l-p:0.12457739561796188
epoch£º431	 i:7 	 global-step:8627	 l-p:0.09566625207662582
epoch£º431	 i:8 	 global-step:8628	 l-p:0.1205703541636467
epoch£º431	 i:9 	 global-step:8629	 l-p:0.15271349251270294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9214, 2.0829, 1.4814],
        [2.9214, 2.9211, 2.9214],
        [2.9214, 2.8211, 2.9009],
        [2.9214, 2.7278, 2.8573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.11260034143924713 
model_pd.l_d.mean(): -24.973073959350586 
model_pd.lagr.mean(): -24.8604736328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1007], device='cuda:0')), ('power', tensor([-25.0738], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.11260034143924713
epoch£º432	 i:1 	 global-step:8641	 l-p:0.15068452060222626
epoch£º432	 i:2 	 global-step:8642	 l-p:0.2556256353855133
epoch£º432	 i:3 	 global-step:8643	 l-p:0.029465198516845703
epoch£º432	 i:4 	 global-step:8644	 l-p:0.011220435611903667
epoch£º432	 i:5 	 global-step:8645	 l-p:0.1273508071899414
epoch£º432	 i:6 	 global-step:8646	 l-p:0.13828927278518677
epoch£º432	 i:7 	 global-step:8647	 l-p:0.19354991614818573
epoch£º432	 i:8 	 global-step:8648	 l-p:0.2815975844860077
epoch£º432	 i:9 	 global-step:8649	 l-p:0.13520260155200958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9628, 2.2022, 2.1776],
        [2.9628, 1.8843, 1.3315],
        [2.9628, 2.9628, 2.9628],
        [2.9628, 2.8148, 2.9226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.14079317450523376 
model_pd.l_d.mean(): -24.93181610107422 
model_pd.lagr.mean(): -24.79102325439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0775], device='cuda:0')), ('power', tensor([-25.0093], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.14079317450523376
epoch£º433	 i:1 	 global-step:8661	 l-p:0.140738382935524
epoch£º433	 i:2 	 global-step:8662	 l-p:0.13465754687786102
epoch£º433	 i:3 	 global-step:8663	 l-p:0.18379707634449005
epoch£º433	 i:4 	 global-step:8664	 l-p:0.11608661711215973
epoch£º433	 i:5 	 global-step:8665	 l-p:0.01436332706362009
epoch£º433	 i:6 	 global-step:8666	 l-p:-0.15418753027915955
epoch£º433	 i:7 	 global-step:8667	 l-p:0.2017858624458313
epoch£º433	 i:8 	 global-step:8668	 l-p:0.1581619530916214
epoch£º433	 i:9 	 global-step:8669	 l-p:0.14894698560237885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[2.9044, 1.9267, 1.6277],
        [2.9044, 1.8449, 1.2916],
        [2.9044, 2.3284, 2.4442],
        [2.9044, 2.1504, 2.1381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.16056180000305176 
model_pd.l_d.mean(): -25.308067321777344 
model_pd.lagr.mean(): -25.147504806518555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0273], device='cuda:0')), ('power', tensor([-25.3354], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.16056180000305176
epoch£º434	 i:1 	 global-step:8681	 l-p:0.1624196618795395
epoch£º434	 i:2 	 global-step:8682	 l-p:0.1300412267446518
epoch£º434	 i:3 	 global-step:8683	 l-p:0.10676446557044983
epoch£º434	 i:4 	 global-step:8684	 l-p:0.1221589744091034
epoch£º434	 i:5 	 global-step:8685	 l-p:0.8006731867790222
epoch£º434	 i:6 	 global-step:8686	 l-p:0.1599702537059784
epoch£º434	 i:7 	 global-step:8687	 l-p:0.21074382960796356
epoch£º434	 i:8 	 global-step:8688	 l-p:0.16585397720336914
epoch£º434	 i:9 	 global-step:8689	 l-p:0.0656355544924736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9344, 2.9337, 2.9344],
        [2.9344, 2.9344, 2.9344],
        [2.9344, 2.9342, 2.9344],
        [2.9344, 2.0786, 1.4766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.14919260144233704 
model_pd.l_d.mean(): -25.164657592773438 
model_pd.lagr.mean(): -25.015464782714844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0468], device='cuda:0')), ('power', tensor([-25.1178], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.14919260144233704
epoch£º435	 i:1 	 global-step:8701	 l-p:0.15785612165927887
epoch£º435	 i:2 	 global-step:8702	 l-p:0.30054664611816406
epoch£º435	 i:3 	 global-step:8703	 l-p:0.11534101516008377
epoch£º435	 i:4 	 global-step:8704	 l-p:0.11154352873563766
epoch£º435	 i:5 	 global-step:8705	 l-p:0.12824079394340515
epoch£º435	 i:6 	 global-step:8706	 l-p:0.12304321676492691
epoch£º435	 i:7 	 global-step:8707	 l-p:0.12965290248394012
epoch£º435	 i:8 	 global-step:8708	 l-p:0.12352468073368073
epoch£º435	 i:9 	 global-step:8709	 l-p:0.16983607411384583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0334, 2.1686, 1.5537],
        [3.0334, 2.0117, 1.4200],
        [3.0334, 2.9918, 3.0287],
        [3.0334, 1.9381, 1.4095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.25382938981056213 
model_pd.l_d.mean(): -25.056974411010742 
model_pd.lagr.mean(): -24.803144454956055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0489], device='cuda:0')), ('power', tensor([-25.1059], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.25382938981056213
epoch£º436	 i:1 	 global-step:8721	 l-p:0.14324918389320374
epoch£º436	 i:2 	 global-step:8722	 l-p:0.11889123916625977
epoch£º436	 i:3 	 global-step:8723	 l-p:0.13702324032783508
epoch£º436	 i:4 	 global-step:8724	 l-p:0.12286283075809479
epoch£º436	 i:5 	 global-step:8725	 l-p:0.1480269432067871
epoch£º436	 i:6 	 global-step:8726	 l-p:0.18104171752929688
epoch£º436	 i:7 	 global-step:8727	 l-p:0.3036539554595947
epoch£º436	 i:8 	 global-step:8728	 l-p:-0.09467872232198715
epoch£º436	 i:9 	 global-step:8729	 l-p:0.1725188046693802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7636, 2.7431, 2.7622],
        [2.7636, 1.9972, 1.4014],
        [2.7636, 1.6919, 1.1762],
        [2.7636, 1.7030, 1.1796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.16135720908641815 
model_pd.l_d.mean(): -24.737398147583008 
model_pd.lagr.mean(): -24.576040267944336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1379], device='cuda:0')), ('power', tensor([-24.8753], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.16135720908641815
epoch£º437	 i:1 	 global-step:8741	 l-p:0.17347200214862823
epoch£º437	 i:2 	 global-step:8742	 l-p:-0.006768026389181614
epoch£º437	 i:3 	 global-step:8743	 l-p:0.09513197094202042
epoch£º437	 i:4 	 global-step:8744	 l-p:0.3030169904232025
epoch£º437	 i:5 	 global-step:8745	 l-p:-0.12941156327724457
epoch£º437	 i:6 	 global-step:8746	 l-p:0.14038316905498505
epoch£º437	 i:7 	 global-step:8747	 l-p:0.12564824521541595
epoch£º437	 i:8 	 global-step:8748	 l-p:0.1297808736562729
epoch£º437	 i:9 	 global-step:8749	 l-p:0.10570371150970459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1735, 2.4353, 2.4191],
        [3.1735, 2.1066, 1.6024],
        [3.1735, 3.0270, 3.1337],
        [3.1735, 3.1735, 3.1735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.12242473661899567 
model_pd.l_d.mean(): -24.59712028503418 
model_pd.lagr.mean(): -24.474695205688477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0886], device='cuda:0')), ('power', tensor([-24.5085], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.12242473661899567
epoch£º438	 i:1 	 global-step:8761	 l-p:0.12649255990982056
epoch£º438	 i:2 	 global-step:8762	 l-p:0.14197193086147308
epoch£º438	 i:3 	 global-step:8763	 l-p:0.10222010314464569
epoch£º438	 i:4 	 global-step:8764	 l-p:0.2131771743297577
epoch£º438	 i:5 	 global-step:8765	 l-p:0.1458854228258133
epoch£º438	 i:6 	 global-step:8766	 l-p:0.23087355494499207
epoch£º438	 i:7 	 global-step:8767	 l-p:0.14001014828681946
epoch£º438	 i:8 	 global-step:8768	 l-p:0.21923740208148956
epoch£º438	 i:9 	 global-step:8769	 l-p:0.1386367678642273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8445, 1.9033, 1.6720],
        [2.8445, 1.8466, 1.5332],
        [2.8445, 1.8387, 1.5140],
        [2.8445, 2.8433, 2.8445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.13739299774169922 
model_pd.l_d.mean(): -25.214170455932617 
model_pd.lagr.mean(): -25.076778411865234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0045], device='cuda:0')), ('power', tensor([-25.2186], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.13739299774169922
epoch£º439	 i:1 	 global-step:8781	 l-p:0.1466023474931717
epoch£º439	 i:2 	 global-step:8782	 l-p:0.14469797909259796
epoch£º439	 i:3 	 global-step:8783	 l-p:0.14914032816886902
epoch£º439	 i:4 	 global-step:8784	 l-p:0.16746997833251953
epoch£º439	 i:5 	 global-step:8785	 l-p:0.23890900611877441
epoch£º439	 i:6 	 global-step:8786	 l-p:0.15377207100391388
epoch£º439	 i:7 	 global-step:8787	 l-p:0.18415376543998718
epoch£º439	 i:8 	 global-step:8788	 l-p:0.13690894842147827
epoch£º439	 i:9 	 global-step:8789	 l-p:0.12525418400764465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0519, 2.9038, 3.0117],
        [3.0519, 2.9355, 3.0254],
        [3.0519, 2.4598, 2.5631],
        [3.0519, 1.9596, 1.3948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.1523849219083786 
model_pd.l_d.mean(): -24.626890182495117 
model_pd.lagr.mean(): -24.474504470825195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-7.2508e-05], device='cuda:0')), ('power', tensor([-24.6268], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.1523849219083786
epoch£º440	 i:1 	 global-step:8801	 l-p:0.12296926230192184
epoch£º440	 i:2 	 global-step:8802	 l-p:0.1699436604976654
epoch£º440	 i:3 	 global-step:8803	 l-p:0.1142997071146965
epoch£º440	 i:4 	 global-step:8804	 l-p:0.13992410898208618
epoch£º440	 i:5 	 global-step:8805	 l-p:0.12833745777606964
epoch£º440	 i:6 	 global-step:8806	 l-p:0.13218005001544952
epoch£º440	 i:7 	 global-step:8807	 l-p:0.1811802238225937
epoch£º440	 i:8 	 global-step:8808	 l-p:0.15325447916984558
epoch£º440	 i:9 	 global-step:8809	 l-p:0.05098636820912361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8828, 2.8406, 2.8780],
        [2.8828, 1.7916, 1.2608],
        [2.8828, 2.8786, 2.8826],
        [2.8828, 1.9982, 1.8462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.012980377301573753 
model_pd.l_d.mean(): -24.60935401916504 
model_pd.lagr.mean(): -24.59637451171875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1734], device='cuda:0')), ('power', tensor([-24.7828], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.012980377301573753
epoch£º441	 i:1 	 global-step:8821	 l-p:0.1586756706237793
epoch£º441	 i:2 	 global-step:8822	 l-p:0.3202201724052429
epoch£º441	 i:3 	 global-step:8823	 l-p:0.1499520242214203
epoch£º441	 i:4 	 global-step:8824	 l-p:0.19041071832180023
epoch£º441	 i:5 	 global-step:8825	 l-p:0.13444620370864868
epoch£º441	 i:6 	 global-step:8826	 l-p:0.12908761203289032
epoch£º441	 i:7 	 global-step:8827	 l-p:0.1440185159444809
epoch£º441	 i:8 	 global-step:8828	 l-p:0.1139337345957756
epoch£º441	 i:9 	 global-step:8829	 l-p:0.12907935678958893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0003, 2.9923, 3.0000],
        [3.0003, 2.0100, 1.4161],
        [3.0003, 1.9214, 1.4564],
        [3.0003, 2.8885, 2.9757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.13101878762245178 
model_pd.l_d.mean(): -25.00198745727539 
model_pd.lagr.mean(): -24.870967864990234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0110], device='cuda:0')), ('power', tensor([-24.9910], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.13101878762245178
epoch£º442	 i:1 	 global-step:8841	 l-p:0.14052675664424896
epoch£º442	 i:2 	 global-step:8842	 l-p:0.04794634133577347
epoch£º442	 i:3 	 global-step:8843	 l-p:0.4764302372932434
epoch£º442	 i:4 	 global-step:8844	 l-p:0.19964663684368134
epoch£º442	 i:5 	 global-step:8845	 l-p:0.09277573972940445
epoch£º442	 i:6 	 global-step:8846	 l-p:0.21132898330688477
epoch£º442	 i:7 	 global-step:8847	 l-p:0.16233664751052856
epoch£º442	 i:8 	 global-step:8848	 l-p:0.12020894885063171
epoch£º442	 i:9 	 global-step:8849	 l-p:0.14759671688079834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0592, 2.8365, 2.9771],
        [3.0592, 3.0556, 3.0591],
        [3.0592, 3.0541, 3.0591],
        [3.0592, 2.7512, 2.9125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.1128898561000824 
model_pd.l_d.mean(): -24.8966007232666 
model_pd.lagr.mean(): -24.783710479736328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0407], device='cuda:0')), ('power', tensor([-24.8559], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.1128898561000824
epoch£º443	 i:1 	 global-step:8861	 l-p:0.12569770216941833
epoch£º443	 i:2 	 global-step:8862	 l-p:0.11438361555337906
epoch£º443	 i:3 	 global-step:8863	 l-p:0.12799184024333954
epoch£º443	 i:4 	 global-step:8864	 l-p:0.13583537936210632
epoch£º443	 i:5 	 global-step:8865	 l-p:0.13368718326091766
epoch£º443	 i:6 	 global-step:8866	 l-p:0.11470108479261398
epoch£º443	 i:7 	 global-step:8867	 l-p:0.15243111550807953
epoch£º443	 i:8 	 global-step:8868	 l-p:0.1366104632616043
epoch£º443	 i:9 	 global-step:8869	 l-p:0.3126347064971924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9181, 1.8198, 1.2863],
        [2.9181, 2.6142, 2.7765],
        [2.9181, 2.1897, 2.2070],
        [2.9181, 1.8149, 1.3080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): -0.3613654673099518 
model_pd.l_d.mean(): -24.56888771057129 
model_pd.lagr.mean(): -24.930253982543945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1674], device='cuda:0')), ('power', tensor([-24.7363], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:-0.3613654673099518
epoch£º444	 i:1 	 global-step:8881	 l-p:0.15274135768413544
epoch£º444	 i:2 	 global-step:8882	 l-p:0.14639447629451752
epoch£º444	 i:3 	 global-step:8883	 l-p:0.1398259848356247
epoch£º444	 i:4 	 global-step:8884	 l-p:0.08955171704292297
epoch£º444	 i:5 	 global-step:8885	 l-p:0.15607976913452148
epoch£º444	 i:6 	 global-step:8886	 l-p:0.25611618161201477
epoch£º444	 i:7 	 global-step:8887	 l-p:0.46740686893463135
epoch£º444	 i:8 	 global-step:8888	 l-p:0.0701555460691452
epoch£º444	 i:9 	 global-step:8889	 l-p:-0.25487127900123596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7824, 2.7803, 2.7824],
        [2.7824, 1.9206, 1.8102],
        [2.7824, 2.7273, 2.7751],
        [2.7824, 2.2676, 2.4166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.316148966550827 
model_pd.l_d.mean(): -25.193246841430664 
model_pd.lagr.mean(): -24.877098083496094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1174], device='cuda:0')), ('power', tensor([-25.3106], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.316148966550827
epoch£º445	 i:1 	 global-step:8901	 l-p:0.16415835916996002
epoch£º445	 i:2 	 global-step:8902	 l-p:0.0860586166381836
epoch£º445	 i:3 	 global-step:8903	 l-p:0.129887655377388
epoch£º445	 i:4 	 global-step:8904	 l-p:0.12727712094783783
epoch£º445	 i:5 	 global-step:8905	 l-p:0.14077867567539215
epoch£º445	 i:6 	 global-step:8906	 l-p:0.14201700687408447
epoch£º445	 i:7 	 global-step:8907	 l-p:0.13896064460277557
epoch£º445	 i:8 	 global-step:8908	 l-p:0.10880618542432785
epoch£º445	 i:9 	 global-step:8909	 l-p:0.11569831520318985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[3.0343, 2.1924, 2.0826],
        [3.0343, 2.0944, 1.4859],
        [3.0343, 2.0400, 1.7087],
        [3.0343, 2.1111, 1.5004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.26232969760894775 
model_pd.l_d.mean(): -24.951202392578125 
model_pd.lagr.mean(): -24.688873291015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0351], device='cuda:0')), ('power', tensor([-24.9863], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.26232969760894775
epoch£º446	 i:1 	 global-step:8921	 l-p:0.1213037371635437
epoch£º446	 i:2 	 global-step:8922	 l-p:0.12002837657928467
epoch£º446	 i:3 	 global-step:8923	 l-p:0.16564267873764038
epoch£º446	 i:4 	 global-step:8924	 l-p:0.13974818587303162
epoch£º446	 i:5 	 global-step:8925	 l-p:0.060837142169475555
epoch£º446	 i:6 	 global-step:8926	 l-p:0.149864062666893
epoch£º446	 i:7 	 global-step:8927	 l-p:0.16214951872825623
epoch£º446	 i:8 	 global-step:8928	 l-p:-0.18086101114749908
epoch£º446	 i:9 	 global-step:8929	 l-p:0.026888364925980568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8641, 2.8390, 2.8621],
        [2.8641, 2.6396, 2.7821],
        [2.8641, 1.9430, 1.7484],
        [2.8641, 1.9304, 1.7178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.09036420285701752 
model_pd.l_d.mean(): -25.09038734436035 
model_pd.lagr.mean(): -25.000022888183594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1089], device='cuda:0')), ('power', tensor([-25.1992], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.09036420285701752
epoch£º447	 i:1 	 global-step:8941	 l-p:0.14436152577400208
epoch£º447	 i:2 	 global-step:8942	 l-p:0.14532244205474854
epoch£º447	 i:3 	 global-step:8943	 l-p:0.13976535201072693
epoch£º447	 i:4 	 global-step:8944	 l-p:0.08995462208986282
epoch£º447	 i:5 	 global-step:8945	 l-p:0.18693669140338898
epoch£º447	 i:6 	 global-step:8946	 l-p:0.14223581552505493
epoch£º447	 i:7 	 global-step:8947	 l-p:0.1940159946680069
epoch£º447	 i:8 	 global-step:8948	 l-p:0.1942852884531021
epoch£º447	 i:9 	 global-step:8949	 l-p:0.22567462921142578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9903, 2.1282, 1.5157],
        [2.9903, 2.0109, 1.4153],
        [2.9903, 2.8308, 2.9450],
        [2.9903, 2.9899, 2.9903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.12796799838542938 
model_pd.l_d.mean(): -24.825136184692383 
model_pd.lagr.mean(): -24.697168350219727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0936], device='cuda:0')), ('power', tensor([-24.9188], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.12796799838542938
epoch£º448	 i:1 	 global-step:8961	 l-p:0.13887272775173187
epoch£º448	 i:2 	 global-step:8962	 l-p:0.134986013174057
epoch£º448	 i:3 	 global-step:8963	 l-p:0.12436803430318832
epoch£º448	 i:4 	 global-step:8964	 l-p:0.17938460409641266
epoch£º448	 i:5 	 global-step:8965	 l-p:0.1701146513223648
epoch£º448	 i:6 	 global-step:8966	 l-p:0.12323480099439621
epoch£º448	 i:7 	 global-step:8967	 l-p:0.11281566321849823
epoch£º448	 i:8 	 global-step:8968	 l-p:0.1586940735578537
epoch£º448	 i:9 	 global-step:8969	 l-p:0.13337202370166779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9305, 2.9200, 2.9300],
        [2.9305, 2.3106, 2.4080],
        [2.9305, 2.7032, 2.8464],
        [2.9305, 2.6441, 2.8035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.24090133607387543 
model_pd.l_d.mean(): -25.191543579101562 
model_pd.lagr.mean(): -24.950641632080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0237], device='cuda:0')), ('power', tensor([-25.2153], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.24090133607387543
epoch£º449	 i:1 	 global-step:8981	 l-p:0.010016731917858124
epoch£º449	 i:2 	 global-step:8982	 l-p:0.17629459500312805
epoch£º449	 i:3 	 global-step:8983	 l-p:0.17274947464466095
epoch£º449	 i:4 	 global-step:8984	 l-p:0.16133323311805725
epoch£º449	 i:5 	 global-step:8985	 l-p:0.561159610748291
epoch£º449	 i:6 	 global-step:8986	 l-p:0.13939817249774933
epoch£º449	 i:7 	 global-step:8987	 l-p:0.12602590024471283
epoch£º449	 i:8 	 global-step:8988	 l-p:0.18036426603794098
epoch£º449	 i:9 	 global-step:8989	 l-p:0.09719089418649673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8892, 2.8871, 2.8891],
        [2.8892, 2.8884, 2.8892],
        [2.8892, 2.8734, 2.8882],
        [2.8892, 1.8354, 1.2758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.254712849855423 
model_pd.l_d.mean(): -25.214311599731445 
model_pd.lagr.mean(): -24.959598541259766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0826], device='cuda:0')), ('power', tensor([-25.2969], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.254712849855423
epoch£º450	 i:1 	 global-step:9001	 l-p:0.15259556472301483
epoch£º450	 i:2 	 global-step:9002	 l-p:0.12317938357591629
epoch£º450	 i:3 	 global-step:9003	 l-p:0.13036948442459106
epoch£º450	 i:4 	 global-step:9004	 l-p:-0.18533843755722046
epoch£º450	 i:5 	 global-step:9005	 l-p:0.1558523327112198
epoch£º450	 i:6 	 global-step:9006	 l-p:0.16293983161449432
epoch£º450	 i:7 	 global-step:9007	 l-p:0.14538590610027313
epoch£º450	 i:8 	 global-step:9008	 l-p:0.121503084897995
epoch£º450	 i:9 	 global-step:9009	 l-p:0.11759641021490097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0034, 2.8808, 2.9747],
        [3.0034, 2.6792, 2.8441],
        [3.0034, 3.0030, 3.0034],
        [3.0034, 3.0021, 3.0033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.1259215921163559 
model_pd.l_d.mean(): -24.449600219726562 
model_pd.lagr.mean(): -24.323678970336914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1609], device='cuda:0')), ('power', tensor([-24.6105], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.1259215921163559
epoch£º451	 i:1 	 global-step:9021	 l-p:0.1631430834531784
epoch£º451	 i:2 	 global-step:9022	 l-p:0.13567283749580383
epoch£º451	 i:3 	 global-step:9023	 l-p:0.06146840751171112
epoch£º451	 i:4 	 global-step:9024	 l-p:0.18754710257053375
epoch£º451	 i:5 	 global-step:9025	 l-p:0.11474139243364334
epoch£º451	 i:6 	 global-step:9026	 l-p:0.15309013426303864
epoch£º451	 i:7 	 global-step:9027	 l-p:0.13610687851905823
epoch£º451	 i:8 	 global-step:9028	 l-p:0.13465818762779236
epoch£º451	 i:9 	 global-step:9029	 l-p:0.14482393860816956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9393, 2.5249, 2.6919],
        [2.9393, 2.9325, 2.9391],
        [2.9393, 2.9256, 2.9386],
        [2.9393, 2.9393, 2.9393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.163354754447937 
model_pd.l_d.mean(): -24.935083389282227 
model_pd.lagr.mean(): -24.771728515625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0907], device='cuda:0')), ('power', tensor([-25.0258], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.163354754447937
epoch£º452	 i:1 	 global-step:9041	 l-p:0.31380367279052734
epoch£º452	 i:2 	 global-step:9042	 l-p:0.10822168737649918
epoch£º452	 i:3 	 global-step:9043	 l-p:0.13013620674610138
epoch£º452	 i:4 	 global-step:9044	 l-p:0.13002078235149384
epoch£º452	 i:5 	 global-step:9045	 l-p:-0.02352779358625412
epoch£º452	 i:6 	 global-step:9046	 l-p:0.12578530609607697
epoch£º452	 i:7 	 global-step:9047	 l-p:0.6398426294326782
epoch£º452	 i:8 	 global-step:9048	 l-p:0.1467534601688385
epoch£º452	 i:9 	 global-step:9049	 l-p:0.14741024374961853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9020, 2.8950, 2.9017],
        [2.9020, 2.9020, 2.9020],
        [2.9020, 2.8914, 2.9015],
        [2.9020, 2.6841, 2.8244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.227274551987648 
model_pd.l_d.mean(): -25.197715759277344 
model_pd.lagr.mean(): -24.970441818237305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0485], device='cuda:0')), ('power', tensor([-25.2462], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.227274551987648
epoch£º453	 i:1 	 global-step:9061	 l-p:0.15719345211982727
epoch£º453	 i:2 	 global-step:9062	 l-p:-0.05684828758239746
epoch£º453	 i:3 	 global-step:9063	 l-p:31.42786979675293
epoch£º453	 i:4 	 global-step:9064	 l-p:0.13302579522132874
epoch£º453	 i:5 	 global-step:9065	 l-p:0.1358955353498459
epoch£º453	 i:6 	 global-step:9066	 l-p:0.1612280011177063
epoch£º453	 i:7 	 global-step:9067	 l-p:0.18152806162834167
epoch£º453	 i:8 	 global-step:9068	 l-p:-0.16886365413665771
epoch£º453	 i:9 	 global-step:9069	 l-p:0.14198534190654755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0722, 2.5606, 2.7040],
        [3.0722, 1.9614, 1.3996],
        [3.0722, 3.0582, 3.0714],
        [3.0722, 3.0096, 3.0630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.12761715054512024 
model_pd.l_d.mean(): -24.962305068969727 
model_pd.lagr.mean(): -24.834688186645508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0953], device='cuda:0')), ('power', tensor([-24.8670], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.12761715054512024
epoch£º454	 i:1 	 global-step:9081	 l-p:0.15936189889907837
epoch£º454	 i:2 	 global-step:9082	 l-p:0.13584598898887634
epoch£º454	 i:3 	 global-step:9083	 l-p:0.1371573656797409
epoch£º454	 i:4 	 global-step:9084	 l-p:0.15181346237659454
epoch£º454	 i:5 	 global-step:9085	 l-p:0.11757507920265198
epoch£º454	 i:6 	 global-step:9086	 l-p:0.11502790451049805
epoch£º454	 i:7 	 global-step:9087	 l-p:0.07398060709238052
epoch£º454	 i:8 	 global-step:9088	 l-p:0.20609061419963837
epoch£º454	 i:9 	 global-step:9089	 l-p:0.1445000022649765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8904, 2.0212, 1.9012],
        [2.8904, 2.8879, 2.8904],
        [2.8904, 2.8896, 2.8904],
        [2.8904, 2.8904, 2.8904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.13824428617954254 
model_pd.l_d.mean(): -25.010482788085938 
model_pd.lagr.mean(): -24.872238159179688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1117], device='cuda:0')), ('power', tensor([-25.1222], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.13824428617954254
epoch£º455	 i:1 	 global-step:9101	 l-p:0.12982620298862457
epoch£º455	 i:2 	 global-step:9102	 l-p:0.2067900449037552
epoch£º455	 i:3 	 global-step:9103	 l-p:0.140092134475708
epoch£º455	 i:4 	 global-step:9104	 l-p:0.13857534527778625
epoch£º455	 i:5 	 global-step:9105	 l-p:0.11155442148447037
epoch£º455	 i:6 	 global-step:9106	 l-p:-1.3244901895523071
epoch£º455	 i:7 	 global-step:9107	 l-p:0.6207380890846252
epoch£º455	 i:8 	 global-step:9108	 l-p:-0.3169976472854614
epoch£º455	 i:9 	 global-step:9109	 l-p:0.14049303531646729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7944, 2.7944, 2.7944],
        [2.7944, 2.3794, 2.5501],
        [2.7944, 2.7918, 2.7944],
        [2.7944, 2.7805, 2.7936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.11363905668258667 
model_pd.l_d.mean(): -25.061159133911133 
model_pd.lagr.mean(): -24.947519302368164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0852], device='cuda:0')), ('power', tensor([-25.1464], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.11363905668258667
epoch£º456	 i:1 	 global-step:9121	 l-p:0.16584512591362
epoch£º456	 i:2 	 global-step:9122	 l-p:0.14423134922981262
epoch£º456	 i:3 	 global-step:9123	 l-p:0.30096435546875
epoch£º456	 i:4 	 global-step:9124	 l-p:0.12920904159545898
epoch£º456	 i:5 	 global-step:9125	 l-p:0.15135067701339722
epoch£º456	 i:6 	 global-step:9126	 l-p:0.3021935522556305
epoch£º456	 i:7 	 global-step:9127	 l-p:0.15500575304031372
epoch£º456	 i:8 	 global-step:9128	 l-p:0.15960797667503357
epoch£º456	 i:9 	 global-step:9129	 l-p:0.19798633456230164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0015, 1.9101, 1.3390],
        [3.0015, 2.5451, 2.7062],
        [3.0015, 1.9011, 1.4226],
        [3.0015, 3.0015, 3.0015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.15579213201999664 
model_pd.l_d.mean(): -25.229076385498047 
model_pd.lagr.mean(): -25.073284149169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0138], device='cuda:0')), ('power', tensor([-25.2153], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.15579213201999664
epoch£º457	 i:1 	 global-step:9141	 l-p:0.12008512020111084
epoch£º457	 i:2 	 global-step:9142	 l-p:0.1330169141292572
epoch£º457	 i:3 	 global-step:9143	 l-p:0.1379406750202179
epoch£º457	 i:4 	 global-step:9144	 l-p:0.2507261633872986
epoch£º457	 i:5 	 global-step:9145	 l-p:0.1570272594690323
epoch£º457	 i:6 	 global-step:9146	 l-p:0.08058694750070572
epoch£º457	 i:7 	 global-step:9147	 l-p:0.12209505587816238
epoch£º457	 i:8 	 global-step:9148	 l-p:0.0464278981089592
epoch£º457	 i:9 	 global-step:9149	 l-p:0.2786044478416443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8747, 2.8744, 2.8747],
        [2.8747, 2.6276, 2.7783],
        [2.8747, 2.8747, 2.8747],
        [2.8747, 2.8742, 2.8747]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.23693057894706726 
model_pd.l_d.mean(): -25.191997528076172 
model_pd.lagr.mean(): -24.955066680908203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0407], device='cuda:0')), ('power', tensor([-25.2327], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.23693057894706726
epoch£º458	 i:1 	 global-step:9161	 l-p:0.13018834590911865
epoch£º458	 i:2 	 global-step:9162	 l-p:0.15223675966262817
epoch£º458	 i:3 	 global-step:9163	 l-p:0.2702368199825287
epoch£º458	 i:4 	 global-step:9164	 l-p:0.12333814054727554
epoch£º458	 i:5 	 global-step:9165	 l-p:0.14280597865581512
epoch£º458	 i:6 	 global-step:9166	 l-p:0.0721539556980133
epoch£º458	 i:7 	 global-step:9167	 l-p:0.16736751794815063
epoch£º458	 i:8 	 global-step:9168	 l-p:0.15512534976005554
epoch£º458	 i:9 	 global-step:9169	 l-p:5.286262512207031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9328, 2.9328, 2.9328],
        [2.9328, 1.8184, 1.2835],
        [2.9328, 2.1512, 1.5314],
        [2.9328, 2.9327, 2.9328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.1525866985321045 
model_pd.l_d.mean(): -24.991207122802734 
model_pd.lagr.mean(): -24.838621139526367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0032], device='cuda:0')), ('power', tensor([-24.9945], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.1525866985321045
epoch£º459	 i:1 	 global-step:9181	 l-p:0.15388508141040802
epoch£º459	 i:2 	 global-step:9182	 l-p:0.10968492180109024
epoch£º459	 i:3 	 global-step:9183	 l-p:0.16531972587108612
epoch£º459	 i:4 	 global-step:9184	 l-p:0.1454770416021347
epoch£º459	 i:5 	 global-step:9185	 l-p:0.15425549447536469
epoch£º459	 i:6 	 global-step:9186	 l-p:0.13016277551651
epoch£º459	 i:7 	 global-step:9187	 l-p:0.10085395723581314
epoch£º459	 i:8 	 global-step:9188	 l-p:0.1340109258890152
epoch£º459	 i:9 	 global-step:9189	 l-p:0.13043099641799927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1219, 3.0739, 3.1160],
        [3.1219, 2.2720, 2.1568],
        [3.1219, 2.3200, 1.6803],
        [3.1219, 2.0183, 1.5048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.0979561135172844 
model_pd.l_d.mean(): -24.440032958984375 
model_pd.lagr.mean(): -24.342077255249023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0899], device='cuda:0')), ('power', tensor([-24.5299], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.0979561135172844
epoch£º460	 i:1 	 global-step:9201	 l-p:0.12154518067836761
epoch£º460	 i:2 	 global-step:9202	 l-p:0.12376226484775543
epoch£º460	 i:3 	 global-step:9203	 l-p:0.20128095149993896
epoch£º460	 i:4 	 global-step:9204	 l-p:0.08641922473907471
epoch£º460	 i:5 	 global-step:9205	 l-p:0.1490001678466797
epoch£º460	 i:6 	 global-step:9206	 l-p:0.148899108171463
epoch£º460	 i:7 	 global-step:9207	 l-p:0.1451166570186615
epoch£º460	 i:8 	 global-step:9208	 l-p:0.11528396606445312
epoch£º460	 i:9 	 global-step:9209	 l-p:0.2465238869190216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7816, 2.7717, 2.7812],
        [2.7816, 1.9329, 1.8525],
        [2.7816, 2.5190, 2.6751],
        [2.7816, 1.6722, 1.1745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.4452599287033081 
model_pd.l_d.mean(): -24.407318115234375 
model_pd.lagr.mean(): -23.962059020996094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2253], device='cuda:0')), ('power', tensor([-24.6326], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.4452599287033081
epoch£º461	 i:1 	 global-step:9221	 l-p:0.0918436050415039
epoch£º461	 i:2 	 global-step:9222	 l-p:0.1260576993227005
epoch£º461	 i:3 	 global-step:9223	 l-p:0.25394207239151
epoch£º461	 i:4 	 global-step:9224	 l-p:0.15921077132225037
epoch£º461	 i:5 	 global-step:9225	 l-p:0.14255282282829285
epoch£º461	 i:6 	 global-step:9226	 l-p:0.1712813526391983
epoch£º461	 i:7 	 global-step:9227	 l-p:0.21869298815727234
epoch£º461	 i:8 	 global-step:9228	 l-p:4.998967170715332
epoch£º461	 i:9 	 global-step:9229	 l-p:0.17438174784183502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0405, 2.0101, 1.6425],
        [3.0405, 2.0980, 1.8689],
        [3.0405, 2.8118, 2.9558],
        [3.0405, 2.8152, 2.9580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.3011518120765686 
model_pd.l_d.mean(): -25.0665340423584 
model_pd.lagr.mean(): -24.765382766723633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0169], device='cuda:0')), ('power', tensor([-25.0834], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.3011518120765686
epoch£º462	 i:1 	 global-step:9241	 l-p:0.12535974383354187
epoch£º462	 i:2 	 global-step:9242	 l-p:0.1280674934387207
epoch£º462	 i:3 	 global-step:9243	 l-p:0.11732009798288345
epoch£º462	 i:4 	 global-step:9244	 l-p:0.12292880564928055
epoch£º462	 i:5 	 global-step:9245	 l-p:0.10291805118322372
epoch£º462	 i:6 	 global-step:9246	 l-p:0.11762358993291855
epoch£º462	 i:7 	 global-step:9247	 l-p:0.13134445250034332
epoch£º462	 i:8 	 global-step:9248	 l-p:0.13797779381275177
epoch£º462	 i:9 	 global-step:9249	 l-p:0.13619279861450195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9350, 2.9274, 2.9347],
        [2.9350, 2.8794, 2.9276],
        [2.9350, 2.8855, 2.9289],
        [2.9350, 2.9343, 2.9350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): -1.8082964420318604 
model_pd.l_d.mean(): -24.266286849975586 
model_pd.lagr.mean(): -26.074583053588867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2193], device='cuda:0')), ('power', tensor([-24.4855], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:-1.8082964420318604
epoch£º463	 i:1 	 global-step:9261	 l-p:-0.08846496045589447
epoch£º463	 i:2 	 global-step:9262	 l-p:0.19861629605293274
epoch£º463	 i:3 	 global-step:9263	 l-p:0.15262527763843536
epoch£º463	 i:4 	 global-step:9264	 l-p:0.14453724026679993
epoch£º463	 i:5 	 global-step:9265	 l-p:0.13160915672779083
epoch£º463	 i:6 	 global-step:9266	 l-p:0.14676795899868011
epoch£º463	 i:7 	 global-step:9267	 l-p:0.17870208621025085
epoch£º463	 i:8 	 global-step:9268	 l-p:0.1197161003947258
epoch£º463	 i:9 	 global-step:9269	 l-p:0.14385366439819336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7656, 2.7475, 2.7645],
        [2.7656, 1.6537, 1.1702],
        [2.7656, 1.6792, 1.1547],
        [2.7656, 2.7657, 2.7657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.11355199664831161 
model_pd.l_d.mean(): -25.164125442504883 
model_pd.lagr.mean(): -25.050573348999023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1091], device='cuda:0')), ('power', tensor([-25.2732], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.11355199664831161
epoch£º464	 i:1 	 global-step:9281	 l-p:0.03441893681883812
epoch£º464	 i:2 	 global-step:9282	 l-p:0.4103924334049225
epoch£º464	 i:3 	 global-step:9283	 l-p:0.3142603933811188
epoch£º464	 i:4 	 global-step:9284	 l-p:0.152696430683136
epoch£º464	 i:5 	 global-step:9285	 l-p:0.14308683574199677
epoch£º464	 i:6 	 global-step:9286	 l-p:-0.0706193596124649
epoch£º464	 i:7 	 global-step:9287	 l-p:0.15956398844718933
epoch£º464	 i:8 	 global-step:9288	 l-p:0.16834348440170288
epoch£º464	 i:9 	 global-step:9289	 l-p:0.13870416581630707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9973, 2.9973, 2.9973],
        [2.9973, 2.9902, 2.9970],
        [2.9973, 2.8899, 2.9747],
        [2.9973, 2.2497, 2.2589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.13118575513362885 
model_pd.l_d.mean(): -24.94325828552246 
model_pd.lagr.mean(): -24.81207275390625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0845], device='cuda:0')), ('power', tensor([-25.0278], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.13118575513362885
epoch£º465	 i:1 	 global-step:9301	 l-p:0.22482819855213165
epoch£º465	 i:2 	 global-step:9302	 l-p:0.12643611431121826
epoch£º465	 i:3 	 global-step:9303	 l-p:0.13541273772716522
epoch£º465	 i:4 	 global-step:9304	 l-p:0.1218709945678711
epoch£º465	 i:5 	 global-step:9305	 l-p:0.13842728734016418
epoch£º465	 i:6 	 global-step:9306	 l-p:0.1948264241218567
epoch£º465	 i:7 	 global-step:9307	 l-p:0.19535522162914276
epoch£º465	 i:8 	 global-step:9308	 l-p:0.16655941307544708
epoch£º465	 i:9 	 global-step:9309	 l-p:0.11898460239171982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8217, 2.2100, 2.3254],
        [2.8217, 2.0617, 2.0731],
        [2.8217, 2.7411, 2.8080],
        [2.8217, 1.7248, 1.2982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.20326553285121918 
model_pd.l_d.mean(): -24.62763023376465 
model_pd.lagr.mean(): -24.42436408996582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2249], device='cuda:0')), ('power', tensor([-24.8525], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.20326553285121918
epoch£º466	 i:1 	 global-step:9321	 l-p:0.1529555320739746
epoch£º466	 i:2 	 global-step:9322	 l-p:0.15706518292427063
epoch£º466	 i:3 	 global-step:9323	 l-p:0.1373884528875351
epoch£º466	 i:4 	 global-step:9324	 l-p:0.16825821995735168
epoch£º466	 i:5 	 global-step:9325	 l-p:0.1807963103055954
epoch£º466	 i:6 	 global-step:9326	 l-p:0.17347997426986694
epoch£º466	 i:7 	 global-step:9327	 l-p:0.1441076695919037
epoch£º466	 i:8 	 global-step:9328	 l-p:0.13955768942832947
epoch£º466	 i:9 	 global-step:9329	 l-p:0.08520172536373138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1080, 3.1078, 3.1080],
        [3.1080, 1.9871, 1.4096],
        [3.1080, 2.9853, 3.0795],
        [3.1080, 2.4274, 2.4854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.11630475521087646 
model_pd.l_d.mean(): -24.97822380065918 
model_pd.lagr.mean(): -24.861919403076172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0271], device='cuda:0')), ('power', tensor([-24.9511], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.11630475521087646
epoch£º467	 i:1 	 global-step:9341	 l-p:0.16336189210414886
epoch£º467	 i:2 	 global-step:9342	 l-p:0.12478059530258179
epoch£º467	 i:3 	 global-step:9343	 l-p:-0.10075812041759491
epoch£º467	 i:4 	 global-step:9344	 l-p:0.1371033489704132
epoch£º467	 i:5 	 global-step:9345	 l-p:0.16854426264762878
epoch£º467	 i:6 	 global-step:9346	 l-p:0.29732927680015564
epoch£º467	 i:7 	 global-step:9347	 l-p:0.44702059030532837
epoch£º467	 i:8 	 global-step:9348	 l-p:0.12471043318510056
epoch£º467	 i:9 	 global-step:9349	 l-p:0.11908191442489624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9469, 2.7558, 2.8859],
        [2.9469, 1.8284, 1.2816],
        [2.9469, 2.6159, 2.7847],
        [2.9469, 2.1034, 1.4875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.5345950126647949 
model_pd.l_d.mean(): -25.156099319458008 
model_pd.lagr.mean(): -24.621503829956055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0663], device='cuda:0')), ('power', tensor([-25.2224], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.5345950126647949
epoch£º468	 i:1 	 global-step:9361	 l-p:0.1202264353632927
epoch£º468	 i:2 	 global-step:9362	 l-p:0.1450471729040146
epoch£º468	 i:3 	 global-step:9363	 l-p:0.09215528517961502
epoch£º468	 i:4 	 global-step:9364	 l-p:0.23165901005268097
epoch£º468	 i:5 	 global-step:9365	 l-p:0.2673160433769226
epoch£º468	 i:6 	 global-step:9366	 l-p:0.14782588183879852
epoch£º468	 i:7 	 global-step:9367	 l-p:0.1295379102230072
epoch£º468	 i:8 	 global-step:9368	 l-p:-51.36545181274414
epoch£º468	 i:9 	 global-step:9369	 l-p:0.11984071135520935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9912, 2.7434, 2.8944],
        [2.9912, 2.9913, 2.9913],
        [2.9912, 2.9769, 2.9905],
        [2.9912, 2.9912, 2.9912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.054891061037778854 
model_pd.l_d.mean(): -24.98017120361328 
model_pd.lagr.mean(): -24.92527961730957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0764], device='cuda:0')), ('power', tensor([-24.9038], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.054891061037778854
epoch£º469	 i:1 	 global-step:9381	 l-p:0.1753372699022293
epoch£º469	 i:2 	 global-step:9382	 l-p:0.12291695177555084
epoch£º469	 i:3 	 global-step:9383	 l-p:0.13094785809516907
epoch£º469	 i:4 	 global-step:9384	 l-p:0.14148937165737152
epoch£º469	 i:5 	 global-step:9385	 l-p:0.1433166116476059
epoch£º469	 i:6 	 global-step:9386	 l-p:0.1559239774942398
epoch£º469	 i:7 	 global-step:9387	 l-p:0.13271629810333252
epoch£º469	 i:8 	 global-step:9388	 l-p:0.15837234258651733
epoch£º469	 i:9 	 global-step:9389	 l-p:0.11618287861347198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9904, 2.9892, 2.9904],
        [2.9904, 2.3641, 2.4655],
        [2.9904, 2.6721, 2.8390],
        [2.9904, 2.9896, 2.9904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.11956466734409332 
model_pd.l_d.mean(): -25.073274612426758 
model_pd.lagr.mean(): -24.953710556030273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0413], device='cuda:0')), ('power', tensor([-25.0320], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.11956466734409332
epoch£º470	 i:1 	 global-step:9401	 l-p:0.3187978267669678
epoch£º470	 i:2 	 global-step:9402	 l-p:2.6456375122070312
epoch£º470	 i:3 	 global-step:9403	 l-p:0.1867816150188446
epoch£º470	 i:4 	 global-step:9404	 l-p:0.15483781695365906
epoch£º470	 i:5 	 global-step:9405	 l-p:0.19028401374816895
epoch£º470	 i:6 	 global-step:9406	 l-p:0.1541813313961029
epoch£º470	 i:7 	 global-step:9407	 l-p:0.10626113414764404
epoch£º470	 i:8 	 global-step:9408	 l-p:0.14915244281291962
epoch£º470	 i:9 	 global-step:9409	 l-p:0.1274578869342804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0257, 3.0257, 3.0257],
        [3.0257, 3.0257, 3.0257],
        [3.0257, 3.0257, 3.0257],
        [3.0257, 3.0257, 3.0257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.1332881897687912 
model_pd.l_d.mean(): -24.898475646972656 
model_pd.lagr.mean(): -24.765188217163086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0510], device='cuda:0')), ('power', tensor([-24.9495], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.1332881897687912
epoch£º471	 i:1 	 global-step:9421	 l-p:0.13491962850093842
epoch£º471	 i:2 	 global-step:9422	 l-p:0.21284815669059753
epoch£º471	 i:3 	 global-step:9423	 l-p:0.13942472636699677
epoch£º471	 i:4 	 global-step:9424	 l-p:0.21037456393241882
epoch£º471	 i:5 	 global-step:9425	 l-p:0.13945887982845306
epoch£º471	 i:6 	 global-step:9426	 l-p:0.13630197942256927
epoch£º471	 i:7 	 global-step:9427	 l-p:0.14654755592346191
epoch£º471	 i:8 	 global-step:9428	 l-p:0.09139872342348099
epoch£º471	 i:9 	 global-step:9429	 l-p:0.16796860098838806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8356, 1.8626, 1.6262],
        [2.8356, 2.7261, 2.8126],
        [2.8356, 1.8129, 1.2465],
        [2.8356, 2.8356, 2.8356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.1510596126317978 
model_pd.l_d.mean(): -25.186067581176758 
model_pd.lagr.mean(): -25.03500747680664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0796], device='cuda:0')), ('power', tensor([-25.2657], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.1510596126317978
epoch£º472	 i:1 	 global-step:9441	 l-p:0.10686516761779785
epoch£º472	 i:2 	 global-step:9442	 l-p:0.1830981969833374
epoch£º472	 i:3 	 global-step:9443	 l-p:0.20363502204418182
epoch£º472	 i:4 	 global-step:9444	 l-p:0.1878829151391983
epoch£º472	 i:5 	 global-step:9445	 l-p:0.13460125029087067
epoch£º472	 i:6 	 global-step:9446	 l-p:0.03504189848899841
epoch£º472	 i:7 	 global-step:9447	 l-p:0.07972145080566406
epoch£º472	 i:8 	 global-step:9448	 l-p:0.16151517629623413
epoch£º472	 i:9 	 global-step:9449	 l-p:0.1450120061635971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0229, 2.2339, 2.2081],
        [3.0229, 3.0228, 3.0229],
        [3.0229, 3.0227, 3.0229],
        [3.0229, 2.9088, 2.9980]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.15087805688381195 
model_pd.l_d.mean(): -24.838449478149414 
model_pd.lagr.mean(): -24.687570571899414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1995], device='cuda:0')), ('power', tensor([-25.0379], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.15087805688381195
epoch£º473	 i:1 	 global-step:9461	 l-p:0.1186617761850357
epoch£º473	 i:2 	 global-step:9462	 l-p:0.17417682707309723
epoch£º473	 i:3 	 global-step:9463	 l-p:0.12665338814258575
epoch£º473	 i:4 	 global-step:9464	 l-p:0.11358827352523804
epoch£º473	 i:5 	 global-step:9465	 l-p:0.14554618299007416
epoch£º473	 i:6 	 global-step:9466	 l-p:0.11462590843439102
epoch£º473	 i:7 	 global-step:9467	 l-p:0.11308048665523529
epoch£º473	 i:8 	 global-step:9468	 l-p:0.1336410641670227
epoch£º473	 i:9 	 global-step:9469	 l-p:0.15062133967876434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9685, 2.9588, 2.9681],
        [2.9685, 2.9685, 2.9685],
        [2.9685, 2.9685, 2.9685],
        [2.9685, 2.6099, 2.7817]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.11914394050836563 
model_pd.l_d.mean(): -25.006175994873047 
model_pd.lagr.mean(): -24.88703155517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0845], device='cuda:0')), ('power', tensor([-25.0907], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.11914394050836563
epoch£º474	 i:1 	 global-step:9481	 l-p:0.1143513172864914
epoch£º474	 i:2 	 global-step:9482	 l-p:0.14392876625061035
epoch£º474	 i:3 	 global-step:9483	 l-p:0.18562674522399902
epoch£º474	 i:4 	 global-step:9484	 l-p:0.17693611979484558
epoch£º474	 i:5 	 global-step:9485	 l-p:0.1364264041185379
epoch£º474	 i:6 	 global-step:9486	 l-p:0.1366363912820816
epoch£º474	 i:7 	 global-step:9487	 l-p:0.19311359524726868
epoch£º474	 i:8 	 global-step:9488	 l-p:-0.008379259146749973
epoch£º474	 i:9 	 global-step:9489	 l-p:0.196293905377388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9596, 2.4929, 2.6580],
        [2.9596, 2.3234, 2.4234],
        [2.9596, 2.9596, 2.9596],
        [2.9596, 2.9596, 2.9596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.1382327824831009 
model_pd.l_d.mean(): -24.96761131286621 
model_pd.lagr.mean(): -24.829378128051758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1336], device='cuda:0')), ('power', tensor([-25.1012], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.1382327824831009
epoch£º475	 i:1 	 global-step:9501	 l-p:0.14279769361019135
epoch£º475	 i:2 	 global-step:9502	 l-p:0.13533180952072144
epoch£º475	 i:3 	 global-step:9503	 l-p:0.16206328570842743
epoch£º475	 i:4 	 global-step:9504	 l-p:0.1690630316734314
epoch£º475	 i:5 	 global-step:9505	 l-p:0.18726839125156403
epoch£º475	 i:6 	 global-step:9506	 l-p:0.14067314565181732
epoch£º475	 i:7 	 global-step:9507	 l-p:0.12698926031589508
epoch£º475	 i:8 	 global-step:9508	 l-p:0.1336633265018463
epoch£º475	 i:9 	 global-step:9509	 l-p:0.17775435745716095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0715, 2.0526, 1.4407],
        [3.0715, 3.0714, 3.0715],
        [3.0715, 2.0140, 1.4094],
        [3.0715, 2.7796, 2.9418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.11886800825595856 
model_pd.l_d.mean(): -25.025774002075195 
model_pd.lagr.mean(): -24.906906127929688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0441], device='cuda:0')), ('power', tensor([-24.9817], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.11886800825595856
epoch£º476	 i:1 	 global-step:9521	 l-p:0.1314125806093216
epoch£º476	 i:2 	 global-step:9522	 l-p:0.16849926114082336
epoch£º476	 i:3 	 global-step:9523	 l-p:0.1434699296951294
epoch£º476	 i:4 	 global-step:9524	 l-p:0.17402087152004242
epoch£º476	 i:5 	 global-step:9525	 l-p:0.18420378863811493
epoch£º476	 i:6 	 global-step:9526	 l-p:0.03403257951140404
epoch£º476	 i:7 	 global-step:9527	 l-p:0.1370716243982315
epoch£º476	 i:8 	 global-step:9528	 l-p:0.10664041340351105
epoch£º476	 i:9 	 global-step:9529	 l-p:0.14205989241600037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9161, 2.9118, 2.9160],
        [2.9161, 1.7838, 1.2561],
        [2.9161, 2.6666, 2.8192],
        [2.9161, 2.1276, 2.1143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.14569956064224243 
model_pd.l_d.mean(): -24.6606388092041 
model_pd.lagr.mean(): -24.514938354492188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1171], device='cuda:0')), ('power', tensor([-24.7778], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.14569956064224243
epoch£º477	 i:1 	 global-step:9541	 l-p:0.14605948328971863
epoch£º477	 i:2 	 global-step:9542	 l-p:0.114311084151268
epoch£º477	 i:3 	 global-step:9543	 l-p:0.14272022247314453
epoch£º477	 i:4 	 global-step:9544	 l-p:0.11406848579645157
epoch£º477	 i:5 	 global-step:9545	 l-p:1.8571351766586304
epoch£º477	 i:6 	 global-step:9546	 l-p:0.505715548992157
epoch£º477	 i:7 	 global-step:9547	 l-p:0.12357348948717117
epoch£º477	 i:8 	 global-step:9548	 l-p:0.1715434193611145
epoch£º477	 i:9 	 global-step:9549	 l-p:0.16795097291469574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8887, 2.8887, 2.8887],
        [2.8887, 1.9203, 1.3303],
        [2.8887, 2.8887, 2.8887],
        [2.8887, 1.8782, 1.5905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.055206298828125 
model_pd.l_d.mean(): -24.52910041809082 
model_pd.lagr.mean(): -24.473894119262695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2339], device='cuda:0')), ('power', tensor([-24.7630], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.055206298828125
epoch£º478	 i:1 	 global-step:9561	 l-p:0.15660442411899567
epoch£º478	 i:2 	 global-step:9562	 l-p:0.465255469083786
epoch£º478	 i:3 	 global-step:9563	 l-p:0.14576976001262665
epoch£º478	 i:4 	 global-step:9564	 l-p:0.1027873232960701
epoch£º478	 i:5 	 global-step:9565	 l-p:0.1264999508857727
epoch£º478	 i:6 	 global-step:9566	 l-p:0.19714568555355072
epoch£º478	 i:7 	 global-step:9567	 l-p:0.13501247763633728
epoch£º478	 i:8 	 global-step:9568	 l-p:0.11159961670637131
epoch£º478	 i:9 	 global-step:9569	 l-p:0.14941518008708954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0139, 2.0383, 1.4282],
        [3.0139, 2.9331, 3.0002],
        [3.0139, 2.0103, 1.7168],
        [3.0139, 2.6048, 2.7770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.15742480754852295 
model_pd.l_d.mean(): -24.820512771606445 
model_pd.lagr.mean(): -24.663087844848633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0630], device='cuda:0')), ('power', tensor([-24.8835], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.15742480754852295
epoch£º479	 i:1 	 global-step:9581	 l-p:0.169826477766037
epoch£º479	 i:2 	 global-step:9582	 l-p:0.1338004320859909
epoch£º479	 i:3 	 global-step:9583	 l-p:0.056110817939043045
epoch£º479	 i:4 	 global-step:9584	 l-p:0.13420583307743073
epoch£º479	 i:5 	 global-step:9585	 l-p:0.1321697235107422
epoch£º479	 i:6 	 global-step:9586	 l-p:0.18180100619792938
epoch£º479	 i:7 	 global-step:9587	 l-p:0.13491949439048767
epoch£º479	 i:8 	 global-step:9588	 l-p:0.15036500990390778
epoch£º479	 i:9 	 global-step:9589	 l-p:0.18557913601398468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9309, 2.9307, 2.9309],
        [2.9309, 2.3093, 2.4219],
        [2.9309, 2.9307, 2.9309],
        [2.9309, 2.5855, 2.7579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): -0.031771574169397354 
model_pd.l_d.mean(): -24.694751739501953 
model_pd.lagr.mean(): -24.72652244567871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0990], device='cuda:0')), ('power', tensor([-24.7937], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:-0.031771574169397354
epoch£º480	 i:1 	 global-step:9601	 l-p:0.1327558159828186
epoch£º480	 i:2 	 global-step:9602	 l-p:0.36807844042778015
epoch£º480	 i:3 	 global-step:9603	 l-p:0.12084583938121796
epoch£º480	 i:4 	 global-step:9604	 l-p:0.2116764783859253
epoch£º480	 i:5 	 global-step:9605	 l-p:0.15361547470092773
epoch£º480	 i:6 	 global-step:9606	 l-p:0.13666360080242157
epoch£º480	 i:7 	 global-step:9607	 l-p:0.14022836089134216
epoch£º480	 i:8 	 global-step:9608	 l-p:0.23802390694618225
epoch£º480	 i:9 	 global-step:9609	 l-p:0.21545077860355377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9904, 2.8084, 2.9348],
        [2.9904, 2.9903, 2.9904],
        [2.9904, 2.9702, 2.9890],
        [2.9904, 2.9902, 2.9904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.10569685697555542 
model_pd.l_d.mean(): -25.24597930908203 
model_pd.lagr.mean(): -25.140281677246094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0063], device='cuda:0')), ('power', tensor([-25.2523], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.10569685697555542
epoch£º481	 i:1 	 global-step:9621	 l-p:0.12804074585437775
epoch£º481	 i:2 	 global-step:9622	 l-p:0.13463012874126434
epoch£º481	 i:3 	 global-step:9623	 l-p:0.1400049477815628
epoch£º481	 i:4 	 global-step:9624	 l-p:0.21150708198547363
epoch£º481	 i:5 	 global-step:9625	 l-p:0.13617359101772308
epoch£º481	 i:6 	 global-step:9626	 l-p:0.12086506932973862
epoch£º481	 i:7 	 global-step:9627	 l-p:0.13171587884426117
epoch£º481	 i:8 	 global-step:9628	 l-p:0.245225191116333
epoch£º481	 i:9 	 global-step:9629	 l-p:0.13326039910316467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8759, 2.7624, 2.8516],
        [2.8759, 2.6089, 2.7675],
        [2.8759, 1.9244, 1.7284],
        [2.8759, 1.7398, 1.2213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.9775677919387817 
model_pd.l_d.mean(): -25.25872039794922 
model_pd.lagr.mean(): -24.281152725219727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0082], device='cuda:0')), ('power', tensor([-25.2669], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.9775677919387817
epoch£º482	 i:1 	 global-step:9641	 l-p:0.10603681206703186
epoch£º482	 i:2 	 global-step:9642	 l-p:0.24205809831619263
epoch£º482	 i:3 	 global-step:9643	 l-p:0.1825772076845169
epoch£º482	 i:4 	 global-step:9644	 l-p:0.16221652925014496
epoch£º482	 i:5 	 global-step:9645	 l-p:0.1148911863565445
epoch£º482	 i:6 	 global-step:9646	 l-p:-0.37289199233055115
epoch£º482	 i:7 	 global-step:9647	 l-p:0.12473234534263611
epoch£º482	 i:8 	 global-step:9648	 l-p:0.11875928938388824
epoch£º482	 i:9 	 global-step:9649	 l-p:0.13451772928237915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0385, 3.0384, 3.0385],
        [3.0385, 1.8956, 1.3444],
        [3.0385, 3.0364, 3.0384],
        [3.0385, 3.0385, 3.0385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.132432222366333 
model_pd.l_d.mean(): -24.400920867919922 
model_pd.lagr.mean(): -24.26848793029785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1031], device='cuda:0')), ('power', tensor([-24.5040], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.132432222366333
epoch£º483	 i:1 	 global-step:9661	 l-p:0.14813277125358582
epoch£º483	 i:2 	 global-step:9662	 l-p:0.15811669826507568
epoch£º483	 i:3 	 global-step:9663	 l-p:0.13089965283870697
epoch£º483	 i:4 	 global-step:9664	 l-p:0.15456199645996094
epoch£º483	 i:5 	 global-step:9665	 l-p:0.09417954087257385
epoch£º483	 i:6 	 global-step:9666	 l-p:0.16270692646503448
epoch£º483	 i:7 	 global-step:9667	 l-p:0.14277760684490204
epoch£º483	 i:8 	 global-step:9668	 l-p:-0.7878186702728271
epoch£º483	 i:9 	 global-step:9669	 l-p:0.16969604790210724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[2.7761, 1.6902, 1.1508],
        [2.7761, 1.8366, 1.6679],
        [2.7761, 1.6990, 1.3377],
        [2.7761, 1.7018, 1.1577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.2408018559217453 
model_pd.l_d.mean(): -24.80777359008789 
model_pd.lagr.mean(): -24.566970825195312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2606], device='cuda:0')), ('power', tensor([-25.0684], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.2408018559217453
epoch£º484	 i:1 	 global-step:9681	 l-p:0.481283038854599
epoch£º484	 i:2 	 global-step:9682	 l-p:0.14553694427013397
epoch£º484	 i:3 	 global-step:9683	 l-p:0.058759402483701706
epoch£º484	 i:4 	 global-step:9684	 l-p:-0.2659860849380493
epoch£º484	 i:5 	 global-step:9685	 l-p:0.20791159570217133
epoch£º484	 i:6 	 global-step:9686	 l-p:0.11813777685165405
epoch£º484	 i:7 	 global-step:9687	 l-p:0.12276528030633926
epoch£º484	 i:8 	 global-step:9688	 l-p:0.11770953238010406
epoch£º484	 i:9 	 global-step:9689	 l-p:0.11658183485269547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1797, 3.0159, 3.1331],
        [3.1797, 2.6945, 2.8527],
        [3.1797, 3.1797, 3.1797],
        [3.1797, 3.1797, 3.1797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.12294042110443115 
model_pd.l_d.mean(): -25.229021072387695 
model_pd.lagr.mean(): -25.106081008911133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2079], device='cuda:0')), ('power', tensor([-25.0212], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.12294042110443115
epoch£º485	 i:1 	 global-step:9701	 l-p:0.11833787709474564
epoch£º485	 i:2 	 global-step:9702	 l-p:0.10087861865758896
epoch£º485	 i:3 	 global-step:9703	 l-p:0.1802387833595276
epoch£º485	 i:4 	 global-step:9704	 l-p:0.15345071256160736
epoch£º485	 i:5 	 global-step:9705	 l-p:0.12172983586788177
epoch£º485	 i:6 	 global-step:9706	 l-p:0.14731426537036896
epoch£º485	 i:7 	 global-step:9707	 l-p:0.17923304438591003
epoch£º485	 i:8 	 global-step:9708	 l-p:0.12620434165000916
epoch£º485	 i:9 	 global-step:9709	 l-p:0.18189413845539093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8531, 1.7547, 1.3530],
        [2.8531, 2.1121, 2.1516],
        [2.8531, 1.7516, 1.2014],
        [2.8531, 2.5316, 2.7025]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.14767077565193176 
model_pd.l_d.mean(): -24.976533889770508 
model_pd.lagr.mean(): -24.8288631439209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1869], device='cuda:0')), ('power', tensor([-25.1634], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.14767077565193176
epoch£º486	 i:1 	 global-step:9721	 l-p:0.09335514158010483
epoch£º486	 i:2 	 global-step:9722	 l-p:0.13870427012443542
epoch£º486	 i:3 	 global-step:9723	 l-p:0.17528632283210754
epoch£º486	 i:4 	 global-step:9724	 l-p:0.08141382783651352
epoch£º486	 i:5 	 global-step:9725	 l-p:0.12129079550504684
epoch£º486	 i:6 	 global-step:9726	 l-p:0.15600807964801788
epoch£º486	 i:7 	 global-step:9727	 l-p:0.2481488734483719
epoch£º486	 i:8 	 global-step:9728	 l-p:0.20411574840545654
epoch£º486	 i:9 	 global-step:9729	 l-p:0.05801016837358475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9629, 2.2254, 2.2615],
        [2.9629, 2.9621, 2.9629],
        [2.9629, 2.9629, 2.9629],
        [2.9629, 1.8185, 1.3041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.1767227202653885 
model_pd.l_d.mean(): -24.94522476196289 
model_pd.lagr.mean(): -24.76850128173828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0645], device='cuda:0')), ('power', tensor([-25.0097], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.1767227202653885
epoch£º487	 i:1 	 global-step:9741	 l-p:0.13527269661426544
epoch£º487	 i:2 	 global-step:9742	 l-p:0.14150066673755646
epoch£º487	 i:3 	 global-step:9743	 l-p:-0.07119665294885635
epoch£º487	 i:4 	 global-step:9744	 l-p:0.1371874362230301
epoch£º487	 i:5 	 global-step:9745	 l-p:0.1450457125902176
epoch£º487	 i:6 	 global-step:9746	 l-p:0.13609616458415985
epoch£º487	 i:7 	 global-step:9747	 l-p:0.1417628973722458
epoch£º487	 i:8 	 global-step:9748	 l-p:0.13457022607326508
epoch£º487	 i:9 	 global-step:9749	 l-p:0.13686862587928772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8680, 2.8625, 2.8678],
        [2.8680, 1.8143, 1.4799],
        [2.8680, 2.7570, 2.8447],
        [2.8680, 1.8491, 1.2704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.1588088721036911 
model_pd.l_d.mean(): -25.09169578552246 
model_pd.lagr.mean(): -24.932886123657227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1146], device='cuda:0')), ('power', tensor([-25.2062], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.1588088721036911
epoch£º488	 i:1 	 global-step:9761	 l-p:0.17505021393299103
epoch£º488	 i:2 	 global-step:9762	 l-p:-0.0465957447886467
epoch£º488	 i:3 	 global-step:9763	 l-p:0.16807490587234497
epoch£º488	 i:4 	 global-step:9764	 l-p:0.052277106791734695
epoch£º488	 i:5 	 global-step:9765	 l-p:0.08488849550485611
epoch£º488	 i:6 	 global-step:9766	 l-p:0.09295223653316498
epoch£º488	 i:7 	 global-step:9767	 l-p:0.12062770873308182
epoch£º488	 i:8 	 global-step:9768	 l-p:0.1747804582118988
epoch£º488	 i:9 	 global-step:9769	 l-p:0.22365811467170715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8807, 2.7130, 2.8331],
        [2.8807, 2.8791, 2.8807],
        [2.8807, 2.6962, 2.8245],
        [2.8807, 2.8807, 2.8807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.11355406790971756 
model_pd.l_d.mean(): -25.153278350830078 
model_pd.lagr.mean(): -25.039724349975586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0021], device='cuda:0')), ('power', tensor([-25.1512], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.11355406790971756
epoch£º489	 i:1 	 global-step:9781	 l-p:0.1236850693821907
epoch£º489	 i:2 	 global-step:9782	 l-p:0.45454856753349304
epoch£º489	 i:3 	 global-step:9783	 l-p:0.15599022805690765
epoch£º489	 i:4 	 global-step:9784	 l-p:0.13202832639217377
epoch£º489	 i:5 	 global-step:9785	 l-p:0.13787971436977386
epoch£º489	 i:6 	 global-step:9786	 l-p:0.12759917974472046
epoch£º489	 i:7 	 global-step:9787	 l-p:0.13331112265586853
epoch£º489	 i:8 	 global-step:9788	 l-p:0.15885771811008453
epoch£º489	 i:9 	 global-step:9789	 l-p:0.10647302120923996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9830, 2.9831, 2.9830],
        [2.9830, 2.9809, 2.9830],
        [2.9830, 2.9796, 2.9830],
        [2.9830, 2.9831, 2.9830]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.13903702795505524 
model_pd.l_d.mean(): -25.19228744506836 
model_pd.lagr.mean(): -25.053251266479492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0446], device='cuda:0')), ('power', tensor([-25.1477], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.13903702795505524
epoch£º490	 i:1 	 global-step:9801	 l-p:0.09627778828144073
epoch£º490	 i:2 	 global-step:9802	 l-p:0.14570854604244232
epoch£º490	 i:3 	 global-step:9803	 l-p:0.33348509669303894
epoch£º490	 i:4 	 global-step:9804	 l-p:0.18333080410957336
epoch£º490	 i:5 	 global-step:9805	 l-p:0.17206965386867523
epoch£º490	 i:6 	 global-step:9806	 l-p:0.15556679666042328
epoch£º490	 i:7 	 global-step:9807	 l-p:0.15222014486789703
epoch£º490	 i:8 	 global-step:9808	 l-p:0.14334720373153687
epoch£º490	 i:9 	 global-step:9809	 l-p:0.12303424626588821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0885, 2.1030, 1.4792],
        [3.0885, 2.4064, 2.4783],
        [3.0885, 2.8912, 3.0245],
        [3.0885, 1.9677, 1.3740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.15099166333675385 
model_pd.l_d.mean(): -24.68275260925293 
model_pd.lagr.mean(): -24.531761169433594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0157], device='cuda:0')), ('power', tensor([-24.6984], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.15099166333675385
epoch£º491	 i:1 	 global-step:9821	 l-p:0.12797950208187103
epoch£º491	 i:2 	 global-step:9822	 l-p:0.1252361238002777
epoch£º491	 i:3 	 global-step:9823	 l-p:0.14729589223861694
epoch£º491	 i:4 	 global-step:9824	 l-p:0.09339803457260132
epoch£º491	 i:5 	 global-step:9825	 l-p:0.8575521111488342
epoch£º491	 i:6 	 global-step:9826	 l-p:0.18166902661323547
epoch£º491	 i:7 	 global-step:9827	 l-p:0.09890537708997726
epoch£º491	 i:8 	 global-step:9828	 l-p:0.1464112102985382
epoch£º491	 i:9 	 global-step:9829	 l-p:0.1471700370311737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8580, 2.8579, 2.8580],
        [2.8580, 1.8220, 1.5185],
        [2.8580, 2.8220, 2.8544],
        [2.8580, 2.8316, 2.8558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.18348833918571472 
model_pd.l_d.mean(): -25.2420711517334 
model_pd.lagr.mean(): -25.058582305908203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1164], device='cuda:0')), ('power', tensor([-25.3585], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.18348833918571472
epoch£º492	 i:1 	 global-step:9841	 l-p:0.16899655759334564
epoch£º492	 i:2 	 global-step:9842	 l-p:0.12003582715988159
epoch£º492	 i:3 	 global-step:9843	 l-p:-0.2517426609992981
epoch£º492	 i:4 	 global-step:9844	 l-p:0.16312003135681152
epoch£º492	 i:5 	 global-step:9845	 l-p:0.0673082247376442
epoch£º492	 i:6 	 global-step:9846	 l-p:0.15770207345485687
epoch£º492	 i:7 	 global-step:9847	 l-p:0.13342593610286713
epoch£º492	 i:8 	 global-step:9848	 l-p:0.15686100721359253
epoch£º492	 i:9 	 global-step:9849	 l-p:0.1458013355731964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0105, 2.2195, 1.5812],
        [3.0105, 3.0093, 3.0105],
        [3.0105, 2.3810, 2.4910],
        [3.0105, 2.1057, 1.4822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.09858732670545578 
model_pd.l_d.mean(): -23.990360260009766 
model_pd.lagr.mean(): -23.891773223876953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2185], device='cuda:0')), ('power', tensor([-24.2088], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.09858732670545578
epoch£º493	 i:1 	 global-step:9861	 l-p:0.11956514418125153
epoch£º493	 i:2 	 global-step:9862	 l-p:0.14517654478549957
epoch£º493	 i:3 	 global-step:9863	 l-p:0.22259074449539185
epoch£º493	 i:4 	 global-step:9864	 l-p:0.1589283049106598
epoch£º493	 i:5 	 global-step:9865	 l-p:0.1267925202846527
epoch£º493	 i:6 	 global-step:9866	 l-p:0.132902130484581
epoch£º493	 i:7 	 global-step:9867	 l-p:0.1710716336965561
epoch£º493	 i:8 	 global-step:9868	 l-p:0.13468539714813232
epoch£º493	 i:9 	 global-step:9869	 l-p:0.13382336497306824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8494, 1.8091, 1.5059],
        [2.8494, 2.0014, 1.3899],
        [2.8494, 2.3464, 2.5123],
        [2.8494, 2.7634, 2.8344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.14496345818042755 
model_pd.l_d.mean(): -24.3574275970459 
model_pd.lagr.mean(): -24.21246337890625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3325], device='cuda:0')), ('power', tensor([-24.6899], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.14496345818042755
epoch£º494	 i:1 	 global-step:9881	 l-p:0.19308893382549286
epoch£º494	 i:2 	 global-step:9882	 l-p:-0.02088778465986252
epoch£º494	 i:3 	 global-step:9883	 l-p:0.3934078812599182
epoch£º494	 i:4 	 global-step:9884	 l-p:0.1413426399230957
epoch£º494	 i:5 	 global-step:9885	 l-p:0.13929857313632965
epoch£º494	 i:6 	 global-step:9886	 l-p:0.12166374176740646
epoch£º494	 i:7 	 global-step:9887	 l-p:0.13316495716571808
epoch£º494	 i:8 	 global-step:9888	 l-p:0.13990207016468048
epoch£º494	 i:9 	 global-step:9889	 l-p:0.14642535150051117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0022, 2.4014, 2.5286],
        [3.0022, 2.9996, 3.0022],
        [3.0022, 1.9477, 1.3483],
        [3.0022, 2.9484, 2.9953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.13721317052841187 
model_pd.l_d.mean(): -25.057876586914062 
model_pd.lagr.mean(): -24.920663833618164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0024], device='cuda:0')), ('power', tensor([-25.0603], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.13721317052841187
epoch£º495	 i:1 	 global-step:9901	 l-p:0.14019407331943512
epoch£º495	 i:2 	 global-step:9902	 l-p:0.16703295707702637
epoch£º495	 i:3 	 global-step:9903	 l-p:0.15094289183616638
epoch£º495	 i:4 	 global-step:9904	 l-p:0.2690306305885315
epoch£º495	 i:5 	 global-step:9905	 l-p:0.03352474048733711
epoch£º495	 i:6 	 global-step:9906	 l-p:0.15950550138950348
epoch£º495	 i:7 	 global-step:9907	 l-p:0.15437757968902588
epoch£º495	 i:8 	 global-step:9908	 l-p:0.6726243495941162
epoch£º495	 i:9 	 global-step:9909	 l-p:0.2705303430557251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7691, 2.7691, 2.7691],
        [2.7691, 2.4294, 2.6057],
        [2.7691, 2.7690, 2.7691],
        [2.7691, 1.7243, 1.1664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.23792392015457153 
model_pd.l_d.mean(): -25.156850814819336 
model_pd.lagr.mean(): -24.918926239013672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1067], device='cuda:0')), ('power', tensor([-25.2635], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.23792392015457153
epoch£º496	 i:1 	 global-step:9921	 l-p:0.15460224449634552
epoch£º496	 i:2 	 global-step:9922	 l-p:0.11308787763118744
epoch£º496	 i:3 	 global-step:9923	 l-p:0.22518233954906464
epoch£º496	 i:4 	 global-step:9924	 l-p:0.17689639329910278
epoch£º496	 i:5 	 global-step:9925	 l-p:0.16120918095111847
epoch£º496	 i:6 	 global-step:9926	 l-p:0.12648145854473114
epoch£º496	 i:7 	 global-step:9927	 l-p:0.13776282966136932
epoch£º496	 i:8 	 global-step:9928	 l-p:0.12656418979167938
epoch£º496	 i:9 	 global-step:9929	 l-p:0.1402367204427719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1715, 2.1913, 1.9386],
        [3.1715, 3.1715, 3.1715],
        [3.1715, 3.1714, 3.1715],
        [3.1715, 3.1643, 3.1713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.11543720960617065 
model_pd.l_d.mean(): -24.969669342041016 
model_pd.lagr.mean(): -24.854232788085938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0040], device='cuda:0')), ('power', tensor([-24.9657], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.11543720960617065
epoch£º497	 i:1 	 global-step:9941	 l-p:0.14916683733463287
epoch£º497	 i:2 	 global-step:9942	 l-p:0.13662849366664886
epoch£º497	 i:3 	 global-step:9943	 l-p:0.15343919396400452
epoch£º497	 i:4 	 global-step:9944	 l-p:0.12307865172624588
epoch£º497	 i:5 	 global-step:9945	 l-p:0.1424163579940796
epoch£º497	 i:6 	 global-step:9946	 l-p:0.09976841509342194
epoch£º497	 i:7 	 global-step:9947	 l-p:0.15493224561214447
epoch£º497	 i:8 	 global-step:9948	 l-p:0.10966737568378448
epoch£º497	 i:9 	 global-step:9949	 l-p:-0.7473770976066589
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8072, 2.5709, 2.7217],
        [2.8072, 1.8138, 1.2351],
        [2.8072, 1.6844, 1.1469],
        [2.8072, 2.7724, 2.8039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.34403833746910095 
model_pd.l_d.mean(): -25.092145919799805 
model_pd.lagr.mean(): -24.74810791015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1433], device='cuda:0')), ('power', tensor([-25.2355], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.34403833746910095
epoch£º498	 i:1 	 global-step:9961	 l-p:0.20433193445205688
epoch£º498	 i:2 	 global-step:9962	 l-p:0.1279047429561615
epoch£º498	 i:3 	 global-step:9963	 l-p:0.18155090510845184
epoch£º498	 i:4 	 global-step:9964	 l-p:0.17607887089252472
epoch£º498	 i:5 	 global-step:9965	 l-p:0.1326480209827423
epoch£º498	 i:6 	 global-step:9966	 l-p:0.1622757911682129
epoch£º498	 i:7 	 global-step:9967	 l-p:0.2323409914970398
epoch£º498	 i:8 	 global-step:9968	 l-p:0.5383371114730835
epoch£º498	 i:9 	 global-step:9969	 l-p:1.093475580215454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0055, 3.0020, 3.0054],
        [3.0055, 1.9872, 1.3785],
        [3.0055, 3.0055, 3.0055],
        [3.0055, 3.0055, 3.0055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.14798517525196075 
model_pd.l_d.mean(): -25.231029510498047 
model_pd.lagr.mean(): -25.083044052124023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1002], device='cuda:0')), ('power', tensor([-25.1309], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.14798517525196075
epoch£º499	 i:1 	 global-step:9981	 l-p:0.039213936775922775
epoch£º499	 i:2 	 global-step:9982	 l-p:0.15144504606723785
epoch£º499	 i:3 	 global-step:9983	 l-p:0.13381360471248627
epoch£º499	 i:4 	 global-step:9984	 l-p:0.1419917643070221
epoch£º499	 i:5 	 global-step:9985	 l-p:0.18218578398227692
epoch£º499	 i:6 	 global-step:9986	 l-p:0.11788108199834824
epoch£º499	 i:7 	 global-step:9987	 l-p:0.15474605560302734
epoch£º499	 i:8 	 global-step:9988	 l-p:0.131987527012825
epoch£º499	 i:9 	 global-step:9989	 l-p:0.1284521520137787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0082, 2.5542, 2.7277],
        [3.0082, 2.0206, 1.7845],
        [3.0082, 2.7746, 2.9234],
        [3.0082, 1.8503, 1.2914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.20999304950237274 
model_pd.l_d.mean(): -25.2590274810791 
model_pd.lagr.mean(): -25.049034118652344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0133], device='cuda:0')), ('power', tensor([-25.2724], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.20999304950237274
epoch£º500	 i:1 	 global-step:10001	 l-p:0.13147997856140137
epoch£º500	 i:2 	 global-step:10002	 l-p:0.5313782095909119
epoch£º500	 i:3 	 global-step:10003	 l-p:0.1303894966840744
epoch£º500	 i:4 	 global-step:10004	 l-p:0.12122704088687897
epoch£º500	 i:5 	 global-step:10005	 l-p:0.129898339509964
epoch£º500	 i:6 	 global-step:10006	 l-p:0.1241326853632927
epoch£º500	 i:7 	 global-step:10007	 l-p:0.1721026599407196
epoch£º500	 i:8 	 global-step:10008	 l-p:-0.18367092311382294
epoch£º500	 i:9 	 global-step:10009	 l-p:0.15549807250499725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8883, 2.3079, 2.4519],
        [2.8883, 1.8721, 1.6102],
        [2.8883, 1.8166, 1.4771],
        [2.8883, 2.7575, 2.8578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.009507031179964542 
model_pd.l_d.mean(): -24.932212829589844 
model_pd.lagr.mean(): -24.922706604003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1340], device='cuda:0')), ('power', tensor([-25.0662], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:0.009507031179964542
epoch£º501	 i:1 	 global-step:10021	 l-p:0.2054048478603363
epoch£º501	 i:2 	 global-step:10022	 l-p:0.1530420035123825
epoch£º501	 i:3 	 global-step:10023	 l-p:0.11278928816318512
epoch£º501	 i:4 	 global-step:10024	 l-p:0.14766086637973785
epoch£º501	 i:5 	 global-step:10025	 l-p:-0.04861528426408768
epoch£º501	 i:6 	 global-step:10026	 l-p:0.11871471256017685
epoch£º501	 i:7 	 global-step:10027	 l-p:0.1399300992488861
epoch£º501	 i:8 	 global-step:10028	 l-p:0.33873823285102844
epoch£º501	 i:9 	 global-step:10029	 l-p:0.1406010240316391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9225, 2.0416, 1.9566],
        [2.9225, 2.9225, 2.9225],
        [2.9225, 1.8970, 1.3033],
        [2.9225, 2.9214, 2.9225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.1496381014585495 
model_pd.l_d.mean(): -24.72654151916504 
model_pd.lagr.mean(): -24.576904296875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0884], device='cuda:0')), ('power', tensor([-24.8149], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.1496381014585495
epoch£º502	 i:1 	 global-step:10041	 l-p:0.032573409378528595
epoch£º502	 i:2 	 global-step:10042	 l-p:0.1806720644235611
epoch£º502	 i:3 	 global-step:10043	 l-p:0.14874345064163208
epoch£º502	 i:4 	 global-step:10044	 l-p:0.10473981499671936
epoch£º502	 i:5 	 global-step:10045	 l-p:0.1530746966600418
epoch£º502	 i:6 	 global-step:10046	 l-p:0.20405036211013794
epoch£º502	 i:7 	 global-step:10047	 l-p:0.07810036092996597
epoch£º502	 i:8 	 global-step:10048	 l-p:0.13748089969158173
epoch£º502	 i:9 	 global-step:10049	 l-p:0.10566592216491699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9986, 2.9982, 2.9986],
        [2.9986, 1.9584, 1.3533],
        [2.9986, 2.9986, 2.9986],
        [2.9986, 2.8860, 2.9749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.1101643517613411 
model_pd.l_d.mean(): -24.74565315246582 
model_pd.lagr.mean(): -24.635488510131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0034], device='cuda:0')), ('power', tensor([-24.7423], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.1101643517613411
epoch£º503	 i:1 	 global-step:10061	 l-p:0.18886420130729675
epoch£º503	 i:2 	 global-step:10062	 l-p:0.14414091408252716
epoch£º503	 i:3 	 global-step:10063	 l-p:0.12167598307132721
epoch£º503	 i:4 	 global-step:10064	 l-p:0.11692425608634949
epoch£º503	 i:5 	 global-step:10065	 l-p:0.1338103711605072
epoch£º503	 i:6 	 global-step:10066	 l-p:0.13002480566501617
epoch£º503	 i:7 	 global-step:10067	 l-p:0.14734520018100739
epoch£º503	 i:8 	 global-step:10068	 l-p:0.12358209490776062
epoch£º503	 i:9 	 global-step:10069	 l-p:-0.0020389629062265158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9150, 2.8638, 2.9087],
        [2.9150, 1.8956, 1.6311],
        [2.9150, 2.7703, 2.8787],
        [2.9150, 2.3439, 2.4921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): -0.02194838970899582 
model_pd.l_d.mean(): -25.056901931762695 
model_pd.lagr.mean(): -25.07884979248047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0869], device='cuda:0')), ('power', tensor([-25.1438], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:-0.02194838970899582
epoch£º504	 i:1 	 global-step:10081	 l-p:0.11440578103065491
epoch£º504	 i:2 	 global-step:10082	 l-p:0.1872120201587677
epoch£º504	 i:3 	 global-step:10083	 l-p:0.22159941494464874
epoch£º504	 i:4 	 global-step:10084	 l-p:0.18137134611606598
epoch£º504	 i:5 	 global-step:10085	 l-p:0.11215860396623611
epoch£º504	 i:6 	 global-step:10086	 l-p:0.1351427435874939
epoch£º504	 i:7 	 global-step:10087	 l-p:0.026018761098384857
epoch£º504	 i:8 	 global-step:10088	 l-p:0.1636025458574295
epoch£º504	 i:9 	 global-step:10089	 l-p:0.14899291098117828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9566, 2.2762, 2.3686],
        [2.9566, 2.7051, 2.8608],
        [2.9566, 2.9364, 2.9552],
        [2.9566, 2.5995, 2.7772]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.17113925516605377 
model_pd.l_d.mean(): -25.125837326049805 
model_pd.lagr.mean(): -24.95469856262207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0222], device='cuda:0')), ('power', tensor([-25.1480], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.17113925516605377
epoch£º505	 i:1 	 global-step:10101	 l-p:-0.1313154101371765
epoch£º505	 i:2 	 global-step:10102	 l-p:0.30306345224380493
epoch£º505	 i:3 	 global-step:10103	 l-p:0.1481579840183258
epoch£º505	 i:4 	 global-step:10104	 l-p:0.12932173907756805
epoch£º505	 i:5 	 global-step:10105	 l-p:0.13086341321468353
epoch£º505	 i:6 	 global-step:10106	 l-p:0.149128258228302
epoch£º505	 i:7 	 global-step:10107	 l-p:0.13897180557250977
epoch£º505	 i:8 	 global-step:10108	 l-p:0.07729995250701904
epoch£º505	 i:9 	 global-step:10109	 l-p:0.16833540797233582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0422, 2.9912, 3.0360],
        [3.0422, 2.2150, 1.5704],
        [3.0422, 2.1150, 1.4835],
        [3.0422, 1.9781, 1.6326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.14710424840450287 
model_pd.l_d.mean(): -24.765478134155273 
model_pd.lagr.mean(): -24.61837387084961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0633], device='cuda:0')), ('power', tensor([-24.8288], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.14710424840450287
epoch£º506	 i:1 	 global-step:10121	 l-p:0.1569790095090866
epoch£º506	 i:2 	 global-step:10122	 l-p:0.14930659532546997
epoch£º506	 i:3 	 global-step:10123	 l-p:0.24664226174354553
epoch£º506	 i:4 	 global-step:10124	 l-p:0.164667010307312
epoch£º506	 i:5 	 global-step:10125	 l-p:0.11113950610160828
epoch£º506	 i:6 	 global-step:10126	 l-p:0.1449013203382492
epoch£º506	 i:7 	 global-step:10127	 l-p:0.11197914183139801
epoch£º506	 i:8 	 global-step:10128	 l-p:0.1259520947933197
epoch£º506	 i:9 	 global-step:10129	 l-p:0.19160769879817963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9013, 2.6709, 2.8198],
        [2.9013, 2.1050, 2.1144],
        [2.9013, 1.9677, 1.8285],
        [2.9013, 2.0216, 1.4021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.19440200924873352 
model_pd.l_d.mean(): -25.123228073120117 
model_pd.lagr.mean(): -24.92882537841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1180], device='cuda:0')), ('power', tensor([-25.2413], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.19440200924873352
epoch£º507	 i:1 	 global-step:10141	 l-p:-0.03128630667924881
epoch£º507	 i:2 	 global-step:10142	 l-p:0.15318405628204346
epoch£º507	 i:3 	 global-step:10143	 l-p:0.07827244699001312
epoch£º507	 i:4 	 global-step:10144	 l-p:0.1486540585756302
epoch£º507	 i:5 	 global-step:10145	 l-p:228.58421325683594
epoch£º507	 i:6 	 global-step:10146	 l-p:0.16422894597053528
epoch£º507	 i:7 	 global-step:10147	 l-p:0.14265328645706177
epoch£º507	 i:8 	 global-step:10148	 l-p:0.11676675826311111
epoch£º507	 i:9 	 global-step:10149	 l-p:0.11918497085571289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1768, 3.0643, 3.1530],
        [3.1768, 3.1668, 3.1764],
        [3.1768, 3.1742, 3.1768],
        [3.1768, 2.0114, 1.4732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.12659713625907898 
model_pd.l_d.mean(): -24.67542839050293 
model_pd.lagr.mean(): -24.548831939697266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0243], device='cuda:0')), ('power', tensor([-24.6997], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.12659713625907898
epoch£º508	 i:1 	 global-step:10161	 l-p:0.12516461312770844
epoch£º508	 i:2 	 global-step:10162	 l-p:0.14307640492916107
epoch£º508	 i:3 	 global-step:10163	 l-p:0.1491849422454834
epoch£º508	 i:4 	 global-step:10164	 l-p:0.1629961133003235
epoch£º508	 i:5 	 global-step:10165	 l-p:0.14126452803611755
epoch£º508	 i:6 	 global-step:10166	 l-p:-0.21773661673069
epoch£º508	 i:7 	 global-step:10167	 l-p:0.11792447417974472
epoch£º508	 i:8 	 global-step:10168	 l-p:0.19591611623764038
epoch£º508	 i:9 	 global-step:10169	 l-p:-15.495410919189453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8996, 1.7886, 1.2181],
        [2.8996, 2.8975, 2.8996],
        [2.8996, 2.0749, 1.4457],
        [2.8996, 2.6981, 2.8353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.058249395340681076 
model_pd.l_d.mean(): -24.91645622253418 
model_pd.lagr.mean(): -24.85820770263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1199], device='cuda:0')), ('power', tensor([-25.0364], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.058249395340681076
epoch£º509	 i:1 	 global-step:10181	 l-p:0.1348622739315033
epoch£º509	 i:2 	 global-step:10182	 l-p:0.13388897478580475
epoch£º509	 i:3 	 global-step:10183	 l-p:0.18723700940608978
epoch£º509	 i:4 	 global-step:10184	 l-p:0.10390918701887131
epoch£º509	 i:5 	 global-step:10185	 l-p:0.13070596754550934
epoch£º509	 i:6 	 global-step:10186	 l-p:0.11954982578754425
epoch£º509	 i:7 	 global-step:10187	 l-p:0.19432541728019714
epoch£º509	 i:8 	 global-step:10188	 l-p:0.10620152205228806
epoch£º509	 i:9 	 global-step:10189	 l-p:0.13997255265712738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0534, 3.0472, 3.0532],
        [3.0534, 1.8925, 1.3120],
        [3.0534, 1.9066, 1.3173],
        [3.0534, 1.8815, 1.3602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.1181349903345108 
model_pd.l_d.mean(): -24.398908615112305 
model_pd.lagr.mean(): -24.280773162841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0160], device='cuda:0')), ('power', tensor([-24.3829], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.1181349903345108
epoch£º510	 i:1 	 global-step:10201	 l-p:0.24607668817043304
epoch£º510	 i:2 	 global-step:10202	 l-p:0.1367722600698471
epoch£º510	 i:3 	 global-step:10203	 l-p:0.16166144609451294
epoch£º510	 i:4 	 global-step:10204	 l-p:0.13294272124767303
epoch£º510	 i:5 	 global-step:10205	 l-p:0.14626392722129822
epoch£º510	 i:6 	 global-step:10206	 l-p:0.16039004921913147
epoch£º510	 i:7 	 global-step:10207	 l-p:0.15709266066551208
epoch£º510	 i:8 	 global-step:10208	 l-p:0.21896113455295563
epoch£º510	 i:9 	 global-step:10209	 l-p:0.48856672644615173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8428, 2.4592, 2.6411],
        [2.8428, 1.6821, 1.1578],
        [2.8428, 2.6802, 2.7987],
        [2.8428, 2.8427, 2.8428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.25035640597343445 
model_pd.l_d.mean(): -25.036056518554688 
model_pd.lagr.mean(): -24.78569984436035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2231], device='cuda:0')), ('power', tensor([-25.2591], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.25035640597343445
epoch£º511	 i:1 	 global-step:10221	 l-p:0.14099033176898956
epoch£º511	 i:2 	 global-step:10222	 l-p:0.12878145277500153
epoch£º511	 i:3 	 global-step:10223	 l-p:0.14190366864204407
epoch£º511	 i:4 	 global-step:10224	 l-p:0.13292990624904633
epoch£º511	 i:5 	 global-step:10225	 l-p:0.15483607351779938
epoch£º511	 i:6 	 global-step:10226	 l-p:0.12650640308856964
epoch£º511	 i:7 	 global-step:10227	 l-p:0.1438957303762436
epoch£º511	 i:8 	 global-step:10228	 l-p:-4.00046443939209
epoch£º511	 i:9 	 global-step:10229	 l-p:0.08738797903060913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9826, 2.9304, 2.9762],
        [2.9826, 2.7819, 2.9185],
        [2.9826, 1.8082, 1.2642],
        [2.9826, 2.4030, 2.5486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.14856590330600739 
model_pd.l_d.mean(): -25.159385681152344 
model_pd.lagr.mean(): -25.010820388793945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([4.8165e-05], device='cuda:0')), ('power', tensor([-25.1594], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.14856590330600739
epoch£º512	 i:1 	 global-step:10241	 l-p:0.3186526596546173
epoch£º512	 i:2 	 global-step:10242	 l-p:0.1513921469449997
epoch£º512	 i:3 	 global-step:10243	 l-p:0.13719442486763
epoch£º512	 i:4 	 global-step:10244	 l-p:0.14325521886348724
epoch£º512	 i:5 	 global-step:10245	 l-p:0.18888762593269348
epoch£º512	 i:6 	 global-step:10246	 l-p:0.11721381545066833
epoch£º512	 i:7 	 global-step:10247	 l-p:0.13302356004714966
epoch£º512	 i:8 	 global-step:10248	 l-p:0.22413018345832825
epoch£º512	 i:9 	 global-step:10249	 l-p:0.160938560962677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0252, 2.2570, 2.2858],
        [3.0252, 1.9893, 1.3757],
        [3.0252, 2.3431, 2.4351],
        [3.0252, 3.0157, 3.0248]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.1690138280391693 
model_pd.l_d.mean(): -25.062166213989258 
model_pd.lagr.mean(): -24.893152236938477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0304], device='cuda:0')), ('power', tensor([-25.0926], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.1690138280391693
epoch£º513	 i:1 	 global-step:10261	 l-p:0.17034225165843964
epoch£º513	 i:2 	 global-step:10262	 l-p:0.11999748647212982
epoch£º513	 i:3 	 global-step:10263	 l-p:0.1541995257139206
epoch£º513	 i:4 	 global-step:10264	 l-p:0.12213517725467682
epoch£º513	 i:5 	 global-step:10265	 l-p:0.13407926261425018
epoch£º513	 i:6 	 global-step:10266	 l-p:0.2484494149684906
epoch£º513	 i:7 	 global-step:10267	 l-p:0.12371376901865005
epoch£º513	 i:8 	 global-step:10268	 l-p:0.12952160835266113
epoch£º513	 i:9 	 global-step:10269	 l-p:0.15741457045078278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0043, 2.0984, 1.4673],
        [3.0043, 1.9547, 1.3471],
        [3.0043, 2.3894, 2.5198],
        [3.0043, 3.0034, 3.0042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.13475234806537628 
model_pd.l_d.mean(): -25.015335083007812 
model_pd.lagr.mean(): -24.880582809448242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0240], device='cuda:0')), ('power', tensor([-24.9914], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.13475234806537628
epoch£º514	 i:1 	 global-step:10281	 l-p:0.15588051080703735
epoch£º514	 i:2 	 global-step:10282	 l-p:0.17948009073734283
epoch£º514	 i:3 	 global-step:10283	 l-p:-0.18893425166606903
epoch£º514	 i:4 	 global-step:10284	 l-p:0.1298471838235855
epoch£º514	 i:5 	 global-step:10285	 l-p:0.16227315366268158
epoch£º514	 i:6 	 global-step:10286	 l-p:0.126008540391922
epoch£º514	 i:7 	 global-step:10287	 l-p:0.16703887283802032
epoch£º514	 i:8 	 global-step:10288	 l-p:0.13957753777503967
epoch£º514	 i:9 	 global-step:10289	 l-p:0.1460881531238556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9416, 1.8744, 1.2819],
        [2.9416, 1.8460, 1.2605],
        [2.9416, 2.6810, 2.8405],
        [2.9416, 1.7777, 1.3017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.1342920958995819 
model_pd.l_d.mean(): -24.622617721557617 
model_pd.lagr.mean(): -24.488325119018555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1985], device='cuda:0')), ('power', tensor([-24.8211], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.1342920958995819
epoch£º515	 i:1 	 global-step:10301	 l-p:-0.0012314128689467907
epoch£º515	 i:2 	 global-step:10302	 l-p:0.11571438610553741
epoch£º515	 i:3 	 global-step:10303	 l-p:0.14588512480258942
epoch£º515	 i:4 	 global-step:10304	 l-p:0.13048036396503448
epoch£º515	 i:5 	 global-step:10305	 l-p:0.13219259679317474
epoch£º515	 i:6 	 global-step:10306	 l-p:0.14863881468772888
epoch£º515	 i:7 	 global-step:10307	 l-p:0.1374298334121704
epoch£º515	 i:8 	 global-step:10308	 l-p:0.15878434479236603
epoch£º515	 i:9 	 global-step:10309	 l-p:0.23336462676525116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9750, 2.9336, 2.9706],
        [2.9750, 2.3082, 2.4133],
        [2.9750, 1.9044, 1.3061],
        [2.9750, 2.8972, 2.9625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.21890513598918915 
model_pd.l_d.mean(): -24.939861297607422 
model_pd.lagr.mean(): -24.720956802368164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1461], device='cuda:0')), ('power', tensor([-25.0860], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.21890513598918915
epoch£º516	 i:1 	 global-step:10321	 l-p:0.1476311832666397
epoch£º516	 i:2 	 global-step:10322	 l-p:0.13401664793491364
epoch£º516	 i:3 	 global-step:10323	 l-p:0.27775225043296814
epoch£º516	 i:4 	 global-step:10324	 l-p:0.3252270519733429
epoch£º516	 i:5 	 global-step:10325	 l-p:0.1278102993965149
epoch£º516	 i:6 	 global-step:10326	 l-p:0.20613883435726166
epoch£º516	 i:7 	 global-step:10327	 l-p:0.06038283556699753
epoch£º516	 i:8 	 global-step:10328	 l-p:0.12526020407676697
epoch£º516	 i:9 	 global-step:10329	 l-p:0.13203009963035583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0787, 3.0465, 3.0758],
        [3.0787, 2.0455, 1.7539],
        [3.0787, 2.0989, 1.8871],
        [3.0787, 3.0786, 3.0787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.1410711258649826 
model_pd.l_d.mean(): -24.86649513244629 
model_pd.lagr.mean(): -24.72542381286621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0132], device='cuda:0')), ('power', tensor([-24.8533], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.1410711258649826
epoch£º517	 i:1 	 global-step:10341	 l-p:0.1253465861082077
epoch£º517	 i:2 	 global-step:10342	 l-p:0.14488443732261658
epoch£º517	 i:3 	 global-step:10343	 l-p:0.748932421207428
epoch£º517	 i:4 	 global-step:10344	 l-p:0.3852439820766449
epoch£º517	 i:5 	 global-step:10345	 l-p:0.13643203675746918
epoch£º517	 i:6 	 global-step:10346	 l-p:0.09631597995758057
epoch£º517	 i:7 	 global-step:10347	 l-p:-0.41017988324165344
epoch£º517	 i:8 	 global-step:10348	 l-p:0.0774044319987297
epoch£º517	 i:9 	 global-step:10349	 l-p:0.1561174988746643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[2.9921, 1.8430, 1.2653],
        [2.9921, 1.8560, 1.4251],
        [2.9921, 2.5613, 2.7415],
        [2.9921, 2.1975, 2.2080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.12268418818712234 
model_pd.l_d.mean(): -25.03655433654785 
model_pd.lagr.mean(): -24.913869857788086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0766], device='cuda:0')), ('power', tensor([-25.1132], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.12268418818712234
epoch£º518	 i:1 	 global-step:10361	 l-p:0.2129754275083542
epoch£º518	 i:2 	 global-step:10362	 l-p:0.17484210431575775
epoch£º518	 i:3 	 global-step:10363	 l-p:0.12336190789937973
epoch£º518	 i:4 	 global-step:10364	 l-p:0.1660185158252716
epoch£º518	 i:5 	 global-step:10365	 l-p:0.13892054557800293
epoch£º518	 i:6 	 global-step:10366	 l-p:0.2578563392162323
epoch£º518	 i:7 	 global-step:10367	 l-p:0.1303088217973709
epoch£º518	 i:8 	 global-step:10368	 l-p:0.12636598944664001
epoch£º518	 i:9 	 global-step:10369	 l-p:0.17687486112117767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0456, 3.0333, 3.0450],
        [3.0456, 2.1896, 1.5452],
        [3.0456, 3.0322, 3.0449],
        [3.0456, 2.6976, 2.8750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.1358354091644287 
model_pd.l_d.mean(): -25.05083656311035 
model_pd.lagr.mean(): -24.915000915527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0143], device='cuda:0')), ('power', tensor([-25.0365], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.1358354091644287
epoch£º519	 i:1 	 global-step:10381	 l-p:0.17918258905410767
epoch£º519	 i:2 	 global-step:10382	 l-p:0.11606596410274506
epoch£º519	 i:3 	 global-step:10383	 l-p:0.16678376495838165
epoch£º519	 i:4 	 global-step:10384	 l-p:0.32449182868003845
epoch£º519	 i:5 	 global-step:10385	 l-p:0.20591166615486145
epoch£º519	 i:6 	 global-step:10386	 l-p:0.12644042074680328
epoch£º519	 i:7 	 global-step:10387	 l-p:0.17710280418395996
epoch£º519	 i:8 	 global-step:10388	 l-p:0.115366131067276
epoch£º519	 i:9 	 global-step:10389	 l-p:0.45543715357780457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1029, 3.1029, 3.1029],
        [3.1029, 2.1678, 1.5254],
        [3.1029, 3.1028, 3.1029],
        [3.1029, 3.0707, 3.1000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.12926924228668213 
model_pd.l_d.mean(): -24.728403091430664 
model_pd.lagr.mean(): -24.59913444519043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0260], device='cuda:0')), ('power', tensor([-24.7024], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.12926924228668213
epoch£º520	 i:1 	 global-step:10401	 l-p:0.1201571598649025
epoch£º520	 i:2 	 global-step:10402	 l-p:0.11449195444583893
epoch£º520	 i:3 	 global-step:10403	 l-p:0.13112449645996094
epoch£º520	 i:4 	 global-step:10404	 l-p:-3.718996524810791
epoch£º520	 i:5 	 global-step:10405	 l-p:0.19740520417690277
epoch£º520	 i:6 	 global-step:10406	 l-p:0.13160158693790436
epoch£º520	 i:7 	 global-step:10407	 l-p:0.12717711925506592
epoch£º520	 i:8 	 global-step:10408	 l-p:0.14000166952610016
epoch£º520	 i:9 	 global-step:10409	 l-p:0.16847945749759674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9573, 2.0591, 1.9690],
        [2.9573, 2.8294, 2.9282],
        [2.9573, 2.0722, 1.4429],
        [2.9573, 2.1978, 2.2424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.14553861320018768 
model_pd.l_d.mean(): -25.17873764038086 
model_pd.lagr.mean(): -25.033199310302734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0111], device='cuda:0')), ('power', tensor([-25.1677], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.14553861320018768
epoch£º521	 i:1 	 global-step:10421	 l-p:0.14580044150352478
epoch£º521	 i:2 	 global-step:10422	 l-p:0.08523053675889969
epoch£º521	 i:3 	 global-step:10423	 l-p:0.21035923063755035
epoch£º521	 i:4 	 global-step:10424	 l-p:0.14156675338745117
epoch£º521	 i:5 	 global-step:10425	 l-p:0.19476836919784546
epoch£º521	 i:6 	 global-step:10426	 l-p:0.14460410177707672
epoch£º521	 i:7 	 global-step:10427	 l-p:0.14634574949741364
epoch£º521	 i:8 	 global-step:10428	 l-p:0.21882353723049164
epoch£º521	 i:9 	 global-step:10429	 l-p:0.157078355550766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0079, 2.9923, 3.0070],
        [3.0079, 3.0078, 3.0079],
        [3.0079, 2.9710, 3.0043],
        [3.0079, 2.9664, 3.0035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.2339654117822647 
model_pd.l_d.mean(): -25.195104598999023 
model_pd.lagr.mean(): -24.961139678955078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0196], device='cuda:0')), ('power', tensor([-25.2147], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.2339654117822647
epoch£º522	 i:1 	 global-step:10441	 l-p:0.13614411652088165
epoch£º522	 i:2 	 global-step:10442	 l-p:0.15510684251785278
epoch£º522	 i:3 	 global-step:10443	 l-p:0.04662151262164116
epoch£º522	 i:4 	 global-step:10444	 l-p:0.1366998851299286
epoch£º522	 i:5 	 global-step:10445	 l-p:0.14214704930782318
epoch£º522	 i:6 	 global-step:10446	 l-p:0.13229718804359436
epoch£º522	 i:7 	 global-step:10447	 l-p:0.29025885462760925
epoch£º522	 i:8 	 global-step:10448	 l-p:0.133326455950737
epoch£º522	 i:9 	 global-step:10449	 l-p:0.07576088607311249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9416, 2.8610, 2.9285],
        [2.9416, 2.9417, 2.9417],
        [2.9416, 2.0553, 1.4278],
        [2.9416, 2.9409, 2.9416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.08040957152843475 
model_pd.l_d.mean(): -24.32577133178711 
model_pd.lagr.mean(): -24.245361328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2530], device='cuda:0')), ('power', tensor([-24.5788], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.08040957152843475
epoch£º523	 i:1 	 global-step:10461	 l-p:0.04535287618637085
epoch£º523	 i:2 	 global-step:10462	 l-p:0.04177141189575195
epoch£º523	 i:3 	 global-step:10463	 l-p:0.14690783619880676
epoch£º523	 i:4 	 global-step:10464	 l-p:0.1322382539510727
epoch£º523	 i:5 	 global-step:10465	 l-p:0.18101800978183746
epoch£º523	 i:6 	 global-step:10466	 l-p:0.14474497735500336
epoch£º523	 i:7 	 global-step:10467	 l-p:0.1417151242494583
epoch£º523	 i:8 	 global-step:10468	 l-p:0.14494186639785767
epoch£º523	 i:9 	 global-step:10469	 l-p:0.11952710151672363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9684, 2.9201, 2.9628],
        [2.9684, 1.8939, 1.2962],
        [2.9684, 2.9672, 2.9684],
        [2.9684, 2.9227, 2.9633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.18026825785636902 
model_pd.l_d.mean(): -25.03849220275879 
model_pd.lagr.mean(): -24.858224868774414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0579], device='cuda:0')), ('power', tensor([-25.0964], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.18026825785636902
epoch£º524	 i:1 	 global-step:10481	 l-p:-0.0025544739328324795
epoch£º524	 i:2 	 global-step:10482	 l-p:0.17363019287586212
epoch£º524	 i:3 	 global-step:10483	 l-p:0.21175052225589752
epoch£º524	 i:4 	 global-step:10484	 l-p:0.125494584441185
epoch£º524	 i:5 	 global-step:10485	 l-p:0.12288874387741089
epoch£º524	 i:6 	 global-step:10486	 l-p:0.10970214009284973
epoch£º524	 i:7 	 global-step:10487	 l-p:0.11904523521661758
epoch£º524	 i:8 	 global-step:10488	 l-p:0.12566417455673218
epoch£º524	 i:9 	 global-step:10489	 l-p:0.17142315208911896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0188, 3.0090, 3.0184],
        [3.0188, 3.0187, 3.0188],
        [3.0188, 3.0054, 3.0181],
        [3.0188, 2.1943, 2.1796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.15010303258895874 
model_pd.l_d.mean(): -24.82986831665039 
model_pd.lagr.mean(): -24.679765701293945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0810], device='cuda:0')), ('power', tensor([-24.9108], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.15010303258895874
epoch£º525	 i:1 	 global-step:10501	 l-p:-0.5426931381225586
epoch£º525	 i:2 	 global-step:10502	 l-p:0.12482786178588867
epoch£º525	 i:3 	 global-step:10503	 l-p:0.2609926164150238
epoch£º525	 i:4 	 global-step:10504	 l-p:0.12499722838401794
epoch£º525	 i:5 	 global-step:10505	 l-p:0.12863706052303314
epoch£º525	 i:6 	 global-step:10506	 l-p:0.21569859981536865
epoch£º525	 i:7 	 global-step:10507	 l-p:0.22202825546264648
epoch£º525	 i:8 	 global-step:10508	 l-p:0.16102685034275055
epoch£º525	 i:9 	 global-step:10509	 l-p:0.11597820371389389
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9030, 2.8553, 2.8975],
        [2.9030, 2.2325, 2.3425],
        [2.9030, 1.7341, 1.1886],
        [2.9030, 1.7630, 1.1981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.1575235277414322 
model_pd.l_d.mean(): -25.215471267700195 
model_pd.lagr.mean(): -25.057947158813477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1008], device='cuda:0')), ('power', tensor([-25.3163], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.1575235277414322
epoch£º526	 i:1 	 global-step:10521	 l-p:0.1528090387582779
epoch£º526	 i:2 	 global-step:10522	 l-p:0.17161057889461517
epoch£º526	 i:3 	 global-step:10523	 l-p:0.12019505351781845
epoch£º526	 i:4 	 global-step:10524	 l-p:0.19366860389709473
epoch£º526	 i:5 	 global-step:10525	 l-p:0.13902023434638977
epoch£º526	 i:6 	 global-step:10526	 l-p:0.13932400941848755
epoch£º526	 i:7 	 global-step:10527	 l-p:0.13491149246692657
epoch£º526	 i:8 	 global-step:10528	 l-p:0.100981205701828
epoch£º526	 i:9 	 global-step:10529	 l-p:0.13970069587230682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0250, 3.0250, 3.0250],
        [3.0250, 2.0056, 1.7519],
        [3.0250, 3.0250, 3.0250],
        [3.0250, 3.0223, 3.0249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.13427631556987762 
model_pd.l_d.mean(): -25.077472686767578 
model_pd.lagr.mean(): -24.94319725036621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0175], device='cuda:0')), ('power', tensor([-25.0600], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.13427631556987762
epoch£º527	 i:1 	 global-step:10541	 l-p:0.14915208518505096
epoch£º527	 i:2 	 global-step:10542	 l-p:0.18894951045513153
epoch£º527	 i:3 	 global-step:10543	 l-p:0.17294013500213623
epoch£º527	 i:4 	 global-step:10544	 l-p:0.07637922465801239
epoch£º527	 i:5 	 global-step:10545	 l-p:0.15582522749900818
epoch£º527	 i:6 	 global-step:10546	 l-p:0.1469397097826004
epoch£º527	 i:7 	 global-step:10547	 l-p:-0.08443327993154526
epoch£º527	 i:8 	 global-step:10548	 l-p:0.12730561196804047
epoch£º527	 i:9 	 global-step:10549	 l-p:0.13545431196689606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9758, 2.7672, 2.9079],
        [2.9758, 2.9477, 2.9735],
        [2.9758, 1.9200, 1.3162],
        [2.9758, 1.9201, 1.3163]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): -0.5554744005203247 
model_pd.l_d.mean(): -24.83585548400879 
model_pd.lagr.mean(): -25.39133071899414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1196], device='cuda:0')), ('power', tensor([-24.9555], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:-0.5554744005203247
epoch£º528	 i:1 	 global-step:10561	 l-p:0.12447425723075867
epoch£º528	 i:2 	 global-step:10562	 l-p:0.13519081473350525
epoch£º528	 i:3 	 global-step:10563	 l-p:0.11166039854288101
epoch£º528	 i:4 	 global-step:10564	 l-p:0.12975871562957764
epoch£º528	 i:5 	 global-step:10565	 l-p:0.7106596827507019
epoch£º528	 i:6 	 global-step:10566	 l-p:0.15900738537311554
epoch£º528	 i:7 	 global-step:10567	 l-p:0.08913639187812805
epoch£º528	 i:8 	 global-step:10568	 l-p:0.12693987786769867
epoch£º528	 i:9 	 global-step:10569	 l-p:0.14727850258350372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[2.9478, 1.8889, 1.5883],
        [2.9478, 1.7704, 1.2809],
        [2.9478, 2.5672, 2.7497],
        [2.9478, 1.8472, 1.2587]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): -2.417987585067749 
model_pd.l_d.mean(): -25.194414138793945 
model_pd.lagr.mean(): -27.612401962280273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0612], device='cuda:0')), ('power', tensor([-25.2556], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:-2.417987585067749
epoch£º529	 i:1 	 global-step:10581	 l-p:0.011951579712331295
epoch£º529	 i:2 	 global-step:10582	 l-p:0.14370138943195343
epoch£º529	 i:3 	 global-step:10583	 l-p:0.13451562821865082
epoch£º529	 i:4 	 global-step:10584	 l-p:0.17292895913124084
epoch£º529	 i:5 	 global-step:10585	 l-p:0.12870438396930695
epoch£º529	 i:6 	 global-step:10586	 l-p:0.15114344656467438
epoch£º529	 i:7 	 global-step:10587	 l-p:0.1588464379310608
epoch£º529	 i:8 	 global-step:10588	 l-p:0.16239112615585327
epoch£º529	 i:9 	 global-step:10589	 l-p:0.16155093908309937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8868, 2.8868, 2.8868],
        [2.8868, 1.9788, 1.8886],
        [2.8868, 2.8868, 2.8868],
        [2.8868, 1.7348, 1.1783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.15167270600795746 
model_pd.l_d.mean(): -25.08329963684082 
model_pd.lagr.mean(): -24.93162727355957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1266], device='cuda:0')), ('power', tensor([-25.2099], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.15167270600795746
epoch£º530	 i:1 	 global-step:10601	 l-p:0.18143047392368317
epoch£º530	 i:2 	 global-step:10602	 l-p:0.1314193159341812
epoch£º530	 i:3 	 global-step:10603	 l-p:0.11375825852155685
epoch£º530	 i:4 	 global-step:10604	 l-p:0.056776005774736404
epoch£º530	 i:5 	 global-step:10605	 l-p:0.15114201605319977
epoch£º530	 i:6 	 global-step:10606	 l-p:0.17400707304477692
epoch£º530	 i:7 	 global-step:10607	 l-p:0.2912161648273468
epoch£º530	 i:8 	 global-step:10608	 l-p:0.12687543034553528
epoch£º530	 i:9 	 global-step:10609	 l-p:0.27200716733932495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1485, 3.1272, 3.1470],
        [3.1485, 3.1485, 3.1485],
        [3.1485, 3.1485, 3.1485],
        [3.1485, 3.1485, 3.1485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.1305915117263794 
model_pd.l_d.mean(): -24.66444206237793 
model_pd.lagr.mean(): -24.533849716186523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0275], device='cuda:0')), ('power', tensor([-24.6370], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.1305915117263794
epoch£º531	 i:1 	 global-step:10621	 l-p:0.11202707141637802
epoch£º531	 i:2 	 global-step:10622	 l-p:0.15276551246643066
epoch£º531	 i:3 	 global-step:10623	 l-p:0.11335083097219467
epoch£º531	 i:4 	 global-step:10624	 l-p:0.11561226844787598
epoch£º531	 i:5 	 global-step:10625	 l-p:0.14554980397224426
epoch£º531	 i:6 	 global-step:10626	 l-p:0.14603985846042633
epoch£º531	 i:7 	 global-step:10627	 l-p:0.14232705533504486
epoch£º531	 i:8 	 global-step:10628	 l-p:0.16016212105751038
epoch£º531	 i:9 	 global-step:10629	 l-p:0.16160251200199127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8425, 1.6624, 1.1631],
        [2.8425, 2.6222, 2.7686],
        [2.8425, 1.6725, 1.1405],
        [2.8425, 2.8306, 2.8419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.17795482277870178 
model_pd.l_d.mean(): -24.787742614746094 
model_pd.lagr.mean(): -24.609786987304688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1852], device='cuda:0')), ('power', tensor([-24.9729], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:0.17795482277870178
epoch£º532	 i:1 	 global-step:10641	 l-p:0.05240553617477417
epoch£º532	 i:2 	 global-step:10642	 l-p:0.16676324605941772
epoch£º532	 i:3 	 global-step:10643	 l-p:0.16432437300682068
epoch£º532	 i:4 	 global-step:10644	 l-p:0.1486523598432541
epoch£º532	 i:5 	 global-step:10645	 l-p:0.12383747845888138
epoch£º532	 i:6 	 global-step:10646	 l-p:0.10754132270812988
epoch£º532	 i:7 	 global-step:10647	 l-p:0.15797482430934906
epoch£º532	 i:8 	 global-step:10648	 l-p:-0.011384406127035618
epoch£º532	 i:9 	 global-step:10649	 l-p:-0.015073861926794052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7719, 1.7039, 1.1399],
        [2.7719, 2.7580, 2.7711],
        [2.7719, 2.6600, 2.7491],
        [2.7719, 1.9552, 1.9704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.1879643201828003 
model_pd.l_d.mean(): -24.935375213623047 
model_pd.lagr.mean(): -24.747411727905273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1971], device='cuda:0')), ('power', tensor([-25.1324], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.1879643201828003
epoch£º533	 i:1 	 global-step:10661	 l-p:0.16064201295375824
epoch£º533	 i:2 	 global-step:10662	 l-p:0.13736028969287872
epoch£º533	 i:3 	 global-step:10663	 l-p:0.11293775588274002
epoch£º533	 i:4 	 global-step:10664	 l-p:0.18373362720012665
epoch£º533	 i:5 	 global-step:10665	 l-p:0.21098293364048004
epoch£º533	 i:6 	 global-step:10666	 l-p:0.01187443733215332
epoch£º533	 i:7 	 global-step:10667	 l-p:0.10300037264823914
epoch£º533	 i:8 	 global-step:10668	 l-p:0.14610780775547028
epoch£º533	 i:9 	 global-step:10669	 l-p:0.14793603122234344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0830, 2.3167, 2.3556],
        [3.0830, 1.9475, 1.5189],
        [3.0830, 3.0489, 3.0799],
        [3.0830, 2.1606, 2.0413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.1446010321378708 
model_pd.l_d.mean(): -24.91346549987793 
model_pd.lagr.mean(): -24.768863677978516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0909], device='cuda:0')), ('power', tensor([-25.0044], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.1446010321378708
epoch£º534	 i:1 	 global-step:10681	 l-p:0.13162532448768616
epoch£º534	 i:2 	 global-step:10682	 l-p:0.12264629453420639
epoch£º534	 i:3 	 global-step:10683	 l-p:0.12685592472553253
epoch£º534	 i:4 	 global-step:10684	 l-p:0.10681560635566711
epoch£º534	 i:5 	 global-step:10685	 l-p:-0.4626466631889343
epoch£º534	 i:6 	 global-step:10686	 l-p:0.19652137160301208
epoch£º534	 i:7 	 global-step:10687	 l-p:0.11062172055244446
epoch£º534	 i:8 	 global-step:10688	 l-p:0.1370532065629959
epoch£º534	 i:9 	 global-step:10689	 l-p:0.14686523377895355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9895, 2.9885, 2.9895],
        [2.9895, 2.8991, 2.9736],
        [2.9895, 2.9893, 2.9895],
        [2.9895, 2.7849, 2.9242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.14897634088993073 
model_pd.l_d.mean(): -24.971426010131836 
model_pd.lagr.mean(): -24.82244873046875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0149], device='cuda:0')), ('power', tensor([-24.9566], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:0.14897634088993073
epoch£º535	 i:1 	 global-step:10701	 l-p:0.14860984683036804
epoch£º535	 i:2 	 global-step:10702	 l-p:0.16285140812397003
epoch£º535	 i:3 	 global-step:10703	 l-p:0.17562425136566162
epoch£º535	 i:4 	 global-step:10704	 l-p:0.12877054512500763
epoch£º535	 i:5 	 global-step:10705	 l-p:0.16416260600090027
epoch£º535	 i:6 	 global-step:10706	 l-p:0.13234342634677887
epoch£º535	 i:7 	 global-step:10707	 l-p:0.09212072193622589
epoch£º535	 i:8 	 global-step:10708	 l-p:0.12610968947410583
epoch£º535	 i:9 	 global-step:10709	 l-p:0.16420993208885193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0745, 3.0285, 3.0693],
        [3.0745, 3.0522, 3.0729],
        [3.0745, 3.0326, 3.0701],
        [3.0745, 1.9043, 1.3104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.13428092002868652 
model_pd.l_d.mean(): -24.788299560546875 
model_pd.lagr.mean(): -24.65401840209961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0929], device='cuda:0')), ('power', tensor([-24.8812], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.13428092002868652
epoch£º536	 i:1 	 global-step:10721	 l-p:0.20289379358291626
epoch£º536	 i:2 	 global-step:10722	 l-p:0.13030730187892914
epoch£º536	 i:3 	 global-step:10723	 l-p:0.12806637585163116
epoch£º536	 i:4 	 global-step:10724	 l-p:0.11106696724891663
epoch£º536	 i:5 	 global-step:10725	 l-p:0.1358100026845932
epoch£º536	 i:6 	 global-step:10726	 l-p:0.11611856520175934
epoch£º536	 i:7 	 global-step:10727	 l-p:0.13356783986091614
epoch£º536	 i:8 	 global-step:10728	 l-p:0.12685054540634155
epoch£º536	 i:9 	 global-step:10729	 l-p:0.2144351750612259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9777, 2.9775, 2.9777],
        [2.9777, 2.1288, 2.1027],
        [2.9777, 2.1659, 2.1749],
        [2.9777, 2.9777, 2.9777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): -0.1109781563282013 
model_pd.l_d.mean(): -25.14097785949707 
model_pd.lagr.mean(): -25.251956939697266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0950], device='cuda:0')), ('power', tensor([-25.0460], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:-0.1109781563282013
epoch£º537	 i:1 	 global-step:10741	 l-p:0.1360468864440918
epoch£º537	 i:2 	 global-step:10742	 l-p:0.4051470160484314
epoch£º537	 i:3 	 global-step:10743	 l-p:0.10839521139860153
epoch£º537	 i:4 	 global-step:10744	 l-p:0.16141033172607422
epoch£º537	 i:5 	 global-step:10745	 l-p:0.04186946898698807
epoch£º537	 i:6 	 global-step:10746	 l-p:0.17049440741539001
epoch£º537	 i:7 	 global-step:10747	 l-p:0.12293513119220734
epoch£º537	 i:8 	 global-step:10748	 l-p:0.15669259428977966
epoch£º537	 i:9 	 global-step:10749	 l-p:0.07229277491569519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9233, 2.7744, 2.8860],
        [2.9233, 2.9212, 2.9233],
        [2.9233, 2.9231, 2.9233],
        [2.9233, 2.6750, 2.8320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.16300787031650543 
model_pd.l_d.mean(): -25.181289672851562 
model_pd.lagr.mean(): -25.018281936645508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0321], device='cuda:0')), ('power', tensor([-25.2133], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.16300787031650543
epoch£º538	 i:1 	 global-step:10761	 l-p:0.08006025850772858
epoch£º538	 i:2 	 global-step:10762	 l-p:0.15071235597133636
epoch£º538	 i:3 	 global-step:10763	 l-p:0.12148351222276688
epoch£º538	 i:4 	 global-step:10764	 l-p:0.9243660569190979
epoch£º538	 i:5 	 global-step:10765	 l-p:0.16873550415039062
epoch£º538	 i:6 	 global-step:10766	 l-p:0.14277885854244232
epoch£º538	 i:7 	 global-step:10767	 l-p:0.12985171377658844
epoch£º538	 i:8 	 global-step:10768	 l-p:0.13445542752742767
epoch£º538	 i:9 	 global-step:10769	 l-p:0.14151692390441895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0950, 2.6624, 2.8444],
        [3.0950, 3.0949, 3.0950],
        [3.0950, 3.0391, 3.0879],
        [3.0950, 3.0947, 3.0950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.35472410917282104 
model_pd.l_d.mean(): -25.038095474243164 
model_pd.lagr.mean(): -24.68337059020996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0291], device='cuda:0')), ('power', tensor([-25.0090], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.35472410917282104
epoch£º539	 i:1 	 global-step:10781	 l-p:0.1628950834274292
epoch£º539	 i:2 	 global-step:10782	 l-p:0.1278778463602066
epoch£º539	 i:3 	 global-step:10783	 l-p:0.13170011341571808
epoch£º539	 i:4 	 global-step:10784	 l-p:0.11402450501918793
epoch£º539	 i:5 	 global-step:10785	 l-p:0.1293536275625229
epoch£º539	 i:6 	 global-step:10786	 l-p:0.1202099546790123
epoch£º539	 i:7 	 global-step:10787	 l-p:0.13254399597644806
epoch£º539	 i:8 	 global-step:10788	 l-p:0.0038712595123797655
epoch£º539	 i:9 	 global-step:10789	 l-p:0.13105408847332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8556, 1.7538, 1.4182],
        [2.8556, 2.8557, 2.8557],
        [2.8556, 2.2525, 2.4025],
        [2.8556, 1.6990, 1.1475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.3144422769546509 
model_pd.l_d.mean(): -24.7110538482666 
model_pd.lagr.mean(): -24.3966121673584 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2650], device='cuda:0')), ('power', tensor([-24.9760], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.3144422769546509
epoch£º540	 i:1 	 global-step:10801	 l-p:0.14630603790283203
epoch£º540	 i:2 	 global-step:10802	 l-p:0.215457022190094
epoch£º540	 i:3 	 global-step:10803	 l-p:0.15502184629440308
epoch£º540	 i:4 	 global-step:10804	 l-p:0.11692384630441666
epoch£º540	 i:5 	 global-step:10805	 l-p:0.13368865847587585
epoch£º540	 i:6 	 global-step:10806	 l-p:0.1499057561159134
epoch£º540	 i:7 	 global-step:10807	 l-p:0.17273306846618652
epoch£º540	 i:8 	 global-step:10808	 l-p:0.16718441247940063
epoch£º540	 i:9 	 global-step:10809	 l-p:0.4008805751800537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9799, 2.8146, 2.9351],
        [2.9799, 2.8500, 2.9504],
        [2.9799, 2.0310, 1.4040],
        [2.9799, 2.9799, 2.9799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.11939067393541336 
model_pd.l_d.mean(): -24.57959747314453 
model_pd.lagr.mean(): -24.460206985473633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0362], device='cuda:0')), ('power', tensor([-24.6158], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.11939067393541336
epoch£º541	 i:1 	 global-step:10821	 l-p:0.13953924179077148
epoch£º541	 i:2 	 global-step:10822	 l-p:0.1941995918750763
epoch£º541	 i:3 	 global-step:10823	 l-p:0.15171252191066742
epoch£º541	 i:4 	 global-step:10824	 l-p:0.14303819835186005
epoch£º541	 i:5 	 global-step:10825	 l-p:0.17962075769901276
epoch£º541	 i:6 	 global-step:10826	 l-p:0.15236015617847443
epoch£º541	 i:7 	 global-step:10827	 l-p:0.13326159119606018
epoch£º541	 i:8 	 global-step:10828	 l-p:0.1598789393901825
epoch£º541	 i:9 	 global-step:10829	 l-p:0.13791310787200928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1842, 3.1842, 3.1842],
        [3.1842, 2.1489, 1.8682],
        [3.1842, 2.3774, 1.7060],
        [3.1842, 2.0482, 1.6041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.10551027208566666 
model_pd.l_d.mean(): -24.318256378173828 
model_pd.lagr.mean(): -24.212745666503906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0197], device='cuda:0')), ('power', tensor([-24.3379], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.10551027208566666
epoch£º542	 i:1 	 global-step:10841	 l-p:0.1199975535273552
epoch£º542	 i:2 	 global-step:10842	 l-p:0.11980800330638885
epoch£º542	 i:3 	 global-step:10843	 l-p:0.18751391768455505
epoch£º542	 i:4 	 global-step:10844	 l-p:0.12636002898216248
epoch£º542	 i:5 	 global-step:10845	 l-p:0.12462230771780014
epoch£º542	 i:6 	 global-step:10846	 l-p:0.18600960075855255
epoch£º542	 i:7 	 global-step:10847	 l-p:0.3628254532814026
epoch£º542	 i:8 	 global-step:10848	 l-p:0.1731676608324051
epoch£º542	 i:9 	 global-step:10849	 l-p:0.12490415573120117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0249, 1.8913, 1.4871],
        [3.0249, 2.1594, 1.5131],
        [3.0249, 2.6577, 2.8403],
        [3.0249, 3.0194, 3.0247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.1286994367837906 
model_pd.l_d.mean(): -24.94063949584961 
model_pd.lagr.mean(): -24.811939239501953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0496], device='cuda:0')), ('power', tensor([-24.9902], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.1286994367837906
epoch£º543	 i:1 	 global-step:10861	 l-p:0.6445754766464233
epoch£º543	 i:2 	 global-step:10862	 l-p:0.19381311535835266
epoch£º543	 i:3 	 global-step:10863	 l-p:0.316220223903656
epoch£º543	 i:4 	 global-step:10864	 l-p:0.12007405608892441
epoch£º543	 i:5 	 global-step:10865	 l-p:0.11009935289621353
epoch£º543	 i:6 	 global-step:10866	 l-p:0.1481633484363556
epoch£º543	 i:7 	 global-step:10867	 l-p:0.16841939091682434
epoch£º543	 i:8 	 global-step:10868	 l-p:0.13498730957508087
epoch£º543	 i:9 	 global-step:10869	 l-p:0.1609228551387787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9139, 2.9093, 2.9138],
        [2.9139, 1.7447, 1.3000],
        [2.9139, 2.9139, 2.9139],
        [2.9139, 1.8979, 1.2931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.10248127579689026 
model_pd.l_d.mean(): -25.146289825439453 
model_pd.lagr.mean(): -25.043807983398438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0391], device='cuda:0')), ('power', tensor([-25.1854], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.10248127579689026
epoch£º544	 i:1 	 global-step:10881	 l-p:0.14343859255313873
epoch£º544	 i:2 	 global-step:10882	 l-p:0.11264363676309586
epoch£º544	 i:3 	 global-step:10883	 l-p:0.1892944872379303
epoch£º544	 i:4 	 global-step:10884	 l-p:0.15529511868953705
epoch£º544	 i:5 	 global-step:10885	 l-p:0.16528040170669556
epoch£º544	 i:6 	 global-step:10886	 l-p:0.09645219892263412
epoch£º544	 i:7 	 global-step:10887	 l-p:1.4783755540847778
epoch£º544	 i:8 	 global-step:10888	 l-p:0.11014063656330109
epoch£º544	 i:9 	 global-step:10889	 l-p:0.12939998507499695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0451, 2.9959, 3.0394],
        [3.0451, 2.8396, 2.9795],
        [3.0451, 2.3658, 2.4725],
        [3.0451, 2.3874, 2.5061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.16110417246818542 
model_pd.l_d.mean(): -25.202585220336914 
model_pd.lagr.mean(): -25.041481018066406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0037], device='cuda:0')), ('power', tensor([-25.1989], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.16110417246818542
epoch£º545	 i:1 	 global-step:10901	 l-p:0.13304518163204193
epoch£º545	 i:2 	 global-step:10902	 l-p:0.12977220118045807
epoch£º545	 i:3 	 global-step:10903	 l-p:0.24297289550304413
epoch£º545	 i:4 	 global-step:10904	 l-p:0.159373477101326
epoch£º545	 i:5 	 global-step:10905	 l-p:0.13811556994915009
epoch£º545	 i:6 	 global-step:10906	 l-p:0.14161626994609833
epoch£º545	 i:7 	 global-step:10907	 l-p:0.1600686013698578
epoch£º545	 i:8 	 global-step:10908	 l-p:0.1264757513999939
epoch£º545	 i:9 	 global-step:10909	 l-p:0.11701013147830963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9953, 2.8784, 2.9707],
        [2.9953, 2.8838, 2.9726],
        [2.9953, 2.9947, 2.9953],
        [2.9953, 1.9669, 1.7241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.13709987699985504 
model_pd.l_d.mean(): -24.848953247070312 
model_pd.lagr.mean(): -24.71185302734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0319], device='cuda:0')), ('power', tensor([-24.8171], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.13709987699985504
epoch£º546	 i:1 	 global-step:10921	 l-p:0.1581958532333374
epoch£º546	 i:2 	 global-step:10922	 l-p:0.1713002771139145
epoch£º546	 i:3 	 global-step:10923	 l-p:0.2762395441532135
epoch£º546	 i:4 	 global-step:10924	 l-p:0.10669519007205963
epoch£º546	 i:5 	 global-step:10925	 l-p:0.06928002834320068
epoch£º546	 i:6 	 global-step:10926	 l-p:0.3683728575706482
epoch£º546	 i:7 	 global-step:10927	 l-p:-0.3059588372707367
epoch£º546	 i:8 	 global-step:10928	 l-p:0.2065315544605255
epoch£º546	 i:9 	 global-step:10929	 l-p:0.0899806097149849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9123, 2.4083, 2.5871],
        [2.9123, 1.9741, 1.3542],
        [2.9123, 2.9082, 2.9122],
        [2.9123, 2.9123, 2.9123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.16281995177268982 
model_pd.l_d.mean(): -25.129844665527344 
model_pd.lagr.mean(): -24.967023849487305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0670], device='cuda:0')), ('power', tensor([-25.1968], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.16281995177268982
epoch£º547	 i:1 	 global-step:10941	 l-p:0.12051208317279816
epoch£º547	 i:2 	 global-step:10942	 l-p:0.11611545085906982
epoch£º547	 i:3 	 global-step:10943	 l-p:0.15568219125270844
epoch£º547	 i:4 	 global-step:10944	 l-p:0.12475897371768951
epoch£º547	 i:5 	 global-step:10945	 l-p:0.13589927554130554
epoch£º547	 i:6 	 global-step:10946	 l-p:0.1241549625992775
epoch£º547	 i:7 	 global-step:10947	 l-p:0.13267138600349426
epoch£º547	 i:8 	 global-step:10948	 l-p:0.13702392578125
epoch£º547	 i:9 	 global-step:10949	 l-p:0.4296664595603943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9762, 2.9753, 2.9762],
        [2.9762, 2.5338, 2.7190],
        [2.9762, 2.8384, 2.9437],
        [2.9762, 2.7695, 2.9103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.011983747594058514 
model_pd.l_d.mean(): -24.68780517578125 
model_pd.lagr.mean(): -24.67582130432129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0960], device='cuda:0')), ('power', tensor([-24.7838], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.011983747594058514
epoch£º548	 i:1 	 global-step:10961	 l-p:0.1347964107990265
epoch£º548	 i:2 	 global-step:10962	 l-p:0.1677526980638504
epoch£º548	 i:3 	 global-step:10963	 l-p:0.1314755231142044
epoch£º548	 i:4 	 global-step:10964	 l-p:0.19000747799873352
epoch£º548	 i:5 	 global-step:10965	 l-p:0.12709854543209076
epoch£º548	 i:6 	 global-step:10966	 l-p:0.14068487286567688
epoch£º548	 i:7 	 global-step:10967	 l-p:-0.03090575337409973
epoch£º548	 i:8 	 global-step:10968	 l-p:0.12302787601947784
epoch£º548	 i:9 	 global-step:10969	 l-p:0.2953948974609375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9509, 1.9196, 1.3102],
        [2.9509, 2.9300, 2.9495],
        [2.9509, 2.9180, 2.9479],
        [2.9509, 2.1413, 2.1614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.36838361620903015 
model_pd.l_d.mean(): -25.089630126953125 
model_pd.lagr.mean(): -24.72124671936035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0013], device='cuda:0')), ('power', tensor([-25.0909], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.36838361620903015
epoch£º549	 i:1 	 global-step:10981	 l-p:0.1223742812871933
epoch£º549	 i:2 	 global-step:10982	 l-p:0.1448136270046234
epoch£º549	 i:3 	 global-step:10983	 l-p:0.18591248989105225
epoch£º549	 i:4 	 global-step:10984	 l-p:0.13669389486312866
epoch£º549	 i:5 	 global-step:10985	 l-p:0.09840761125087738
epoch£º549	 i:6 	 global-step:10986	 l-p:0.13817894458770752
epoch£º549	 i:7 	 global-step:10987	 l-p:0.1293746531009674
epoch£º549	 i:8 	 global-step:10988	 l-p:0.1425807774066925
epoch£º549	 i:9 	 global-step:10989	 l-p:0.12813790142536163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1025, 2.9581, 3.0670],
        [3.1025, 2.0505, 1.7651],
        [3.1025, 2.1948, 2.1048],
        [3.1025, 2.8578, 3.0132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.12815295159816742 
model_pd.l_d.mean(): -24.401580810546875 
model_pd.lagr.mean(): -24.273427963256836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1191], device='cuda:0')), ('power', tensor([-24.5206], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.12815295159816742
epoch£º550	 i:1 	 global-step:11001	 l-p:0.16239972412586212
epoch£º550	 i:2 	 global-step:11002	 l-p:0.15029376745224
epoch£º550	 i:3 	 global-step:11003	 l-p:0.043773047626018524
epoch£º550	 i:4 	 global-step:11004	 l-p:0.12204000353813171
epoch£º550	 i:5 	 global-step:11005	 l-p:0.1506689488887787
epoch£º550	 i:6 	 global-step:11006	 l-p:0.24641868472099304
epoch£º550	 i:7 	 global-step:11007	 l-p:0.154510498046875
epoch£º550	 i:8 	 global-step:11008	 l-p:0.21869224309921265
epoch£º550	 i:9 	 global-step:11009	 l-p:0.37271589040756226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7812, 2.2023, 2.3673],
        [2.7812, 1.6250, 1.0864],
        [2.7812, 1.8990, 1.2845],
        [2.7812, 2.7812, 2.7812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.07267137616872787 
model_pd.l_d.mean(): -24.276405334472656 
model_pd.lagr.mean(): -24.203733444213867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2657], device='cuda:0')), ('power', tensor([-24.5421], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.07267137616872787
epoch£º551	 i:1 	 global-step:11021	 l-p:0.04230738431215286
epoch£º551	 i:2 	 global-step:11022	 l-p:0.15250010788440704
epoch£º551	 i:3 	 global-step:11023	 l-p:0.21211913228034973
epoch£º551	 i:4 	 global-step:11024	 l-p:0.055784471333026886
epoch£º551	 i:5 	 global-step:11025	 l-p:0.13863778114318848
epoch£º551	 i:6 	 global-step:11026	 l-p:0.20898950099945068
epoch£º551	 i:7 	 global-step:11027	 l-p:0.15459387004375458
epoch£º551	 i:8 	 global-step:11028	 l-p:0.12521690130233765
epoch£º551	 i:9 	 global-step:11029	 l-p:0.1114281415939331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1948, 2.1393, 1.8378],
        [3.1948, 3.1947, 3.1948],
        [3.1948, 3.1948, 3.1948],
        [3.1948, 3.1948, 3.1948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.11542705446481705 
model_pd.l_d.mean(): -24.775901794433594 
model_pd.lagr.mean(): -24.66047477722168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0508], device='cuda:0')), ('power', tensor([-24.7251], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.11542705446481705
epoch£º552	 i:1 	 global-step:11041	 l-p:0.13751310110092163
epoch£º552	 i:2 	 global-step:11042	 l-p:0.12662194669246674
epoch£º552	 i:3 	 global-step:11043	 l-p:0.17885443568229675
epoch£º552	 i:4 	 global-step:11044	 l-p:0.12888328731060028
epoch£º552	 i:5 	 global-step:11045	 l-p:0.14666546881198883
epoch£º552	 i:6 	 global-step:11046	 l-p:0.15656964480876923
epoch£º552	 i:7 	 global-step:11047	 l-p:0.14744098484516144
epoch£º552	 i:8 	 global-step:11048	 l-p:0.12927848100662231
epoch£º552	 i:9 	 global-step:11049	 l-p:0.13231301307678223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0140, 3.0140, 3.0140],
        [3.0140, 1.9384, 1.6318],
        [3.0140, 2.1105, 1.4685],
        [3.0140, 2.8189, 2.9545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.1726682037115097 
model_pd.l_d.mean(): -25.055477142333984 
model_pd.lagr.mean(): -24.882808685302734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1238], device='cuda:0')), ('power', tensor([-25.1793], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.1726682037115097
epoch£º553	 i:1 	 global-step:11061	 l-p:-7.748199939727783
epoch£º553	 i:2 	 global-step:11062	 l-p:-0.09941207617521286
epoch£º553	 i:3 	 global-step:11063	 l-p:0.15482476353645325
epoch£º553	 i:4 	 global-step:11064	 l-p:0.1342974752187729
epoch£º553	 i:5 	 global-step:11065	 l-p:-0.036418914794921875
epoch£º553	 i:6 	 global-step:11066	 l-p:0.1369091123342514
epoch£º553	 i:7 	 global-step:11067	 l-p:0.1302735060453415
epoch£º553	 i:8 	 global-step:11068	 l-p:0.026105578988790512
epoch£º553	 i:9 	 global-step:11069	 l-p:0.19367046654224396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9398, 2.8492, 2.9240],
        [2.9398, 2.9361, 2.9397],
        [2.9398, 2.8265, 2.9167],
        [2.9398, 1.7427, 1.2522]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): -0.3629409372806549 
model_pd.l_d.mean(): -24.88190460205078 
model_pd.lagr.mean(): -25.24484634399414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0082], device='cuda:0')), ('power', tensor([-24.8901], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:-0.3629409372806549
epoch£º554	 i:1 	 global-step:11081	 l-p:0.13599248230457306
epoch£º554	 i:2 	 global-step:11082	 l-p:1.8189057111740112
epoch£º554	 i:3 	 global-step:11083	 l-p:0.02916296012699604
epoch£º554	 i:4 	 global-step:11084	 l-p:0.2901654541492462
epoch£º554	 i:5 	 global-step:11085	 l-p:0.0612163245677948
epoch£º554	 i:6 	 global-step:11086	 l-p:0.11297236382961273
epoch£º554	 i:7 	 global-step:11087	 l-p:0.13348479568958282
epoch£º554	 i:8 	 global-step:11088	 l-p:0.11413821578025818
epoch£º554	 i:9 	 global-step:11089	 l-p:0.12568336725234985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1612, 3.1610, 3.1612],
        [3.1612, 2.8044, 2.9858],
        [3.1612, 3.1077, 3.1546],
        [3.1612, 2.7185, 2.9015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.12721137702465057 
model_pd.l_d.mean(): -24.989839553833008 
model_pd.lagr.mean(): -24.862628936767578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0776], device='cuda:0')), ('power', tensor([-24.9122], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.12721137702465057
epoch£º555	 i:1 	 global-step:11101	 l-p:0.15001994371414185
epoch£º555	 i:2 	 global-step:11102	 l-p:0.10630311071872711
epoch£º555	 i:3 	 global-step:11103	 l-p:0.14443178474903107
epoch£º555	 i:4 	 global-step:11104	 l-p:0.12855352461338043
epoch£º555	 i:5 	 global-step:11105	 l-p:0.06658738106489182
epoch£º555	 i:6 	 global-step:11106	 l-p:0.1726260781288147
epoch£º555	 i:7 	 global-step:11107	 l-p:0.1814078539609909
epoch£º555	 i:8 	 global-step:11108	 l-p:0.12425526976585388
epoch£º555	 i:9 	 global-step:11109	 l-p:0.16389110684394836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9345, 1.9489, 1.7813],
        [2.9345, 2.9345, 2.9345],
        [2.9345, 1.7384, 1.1889],
        [2.9345, 2.9269, 2.9342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): -0.017736634239554405 
model_pd.l_d.mean(): -25.176111221313477 
model_pd.lagr.mean(): -25.19384765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0658], device='cuda:0')), ('power', tensor([-25.2419], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:-0.017736634239554405
epoch£º556	 i:1 	 global-step:11121	 l-p:0.1019902303814888
epoch£º556	 i:2 	 global-step:11122	 l-p:0.08605246245861053
epoch£º556	 i:3 	 global-step:11123	 l-p:0.1376621276140213
epoch£º556	 i:4 	 global-step:11124	 l-p:0.1566983312368393
epoch£º556	 i:5 	 global-step:11125	 l-p:0.17713302373886108
epoch£º556	 i:6 	 global-step:11126	 l-p:-0.17645308375358582
epoch£º556	 i:7 	 global-step:11127	 l-p:0.1384560614824295
epoch£º556	 i:8 	 global-step:11128	 l-p:0.12244369089603424
epoch£º556	 i:9 	 global-step:11129	 l-p:0.14763310551643372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0005, 2.9577, 2.9960],
        [3.0005, 1.8450, 1.4269],
        [3.0005, 2.7301, 2.8950],
        [3.0005, 1.8018, 1.2356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.8072047233581543 
model_pd.l_d.mean(): -24.97540283203125 
model_pd.lagr.mean(): -24.168197631835938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1580], device='cuda:0')), ('power', tensor([-25.1334], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.8072047233581543
epoch£º557	 i:1 	 global-step:11141	 l-p:0.13934551179409027
epoch£º557	 i:2 	 global-step:11142	 l-p:0.4753882884979248
epoch£º557	 i:3 	 global-step:11143	 l-p:0.14033450186252594
epoch£º557	 i:4 	 global-step:11144	 l-p:0.14110516011714935
epoch£º557	 i:5 	 global-step:11145	 l-p:0.13925449550151825
epoch£º557	 i:6 	 global-step:11146	 l-p:0.16014701128005981
epoch£º557	 i:7 	 global-step:11147	 l-p:0.13035273551940918
epoch£º557	 i:8 	 global-step:11148	 l-p:0.11744725704193115
epoch£º557	 i:9 	 global-step:11149	 l-p:0.12348588556051254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9786, 2.0286, 1.9047],
        [2.9786, 1.7724, 1.2245],
        [2.9786, 2.8541, 2.9514],
        [2.9786, 2.9507, 2.9764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): -0.07211746275424957 
model_pd.l_d.mean(): -24.892120361328125 
model_pd.lagr.mean(): -24.964237213134766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0618], device='cuda:0')), ('power', tensor([-24.9539], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:-0.07211746275424957
epoch£º558	 i:1 	 global-step:11161	 l-p:0.4522630572319031
epoch£º558	 i:2 	 global-step:11162	 l-p:0.12791785597801208
epoch£º558	 i:3 	 global-step:11163	 l-p:0.13655920326709747
epoch£º558	 i:4 	 global-step:11164	 l-p:0.08833036571741104
epoch£º558	 i:5 	 global-step:11165	 l-p:0.14942823350429535
epoch£º558	 i:6 	 global-step:11166	 l-p:0.12992344796657562
epoch£º558	 i:7 	 global-step:11167	 l-p:0.18372440338134766
epoch£º558	 i:8 	 global-step:11168	 l-p:0.16017383337020874
epoch£º558	 i:9 	 global-step:11169	 l-p:0.16918379068374634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9241, 1.7260, 1.2403],
        [2.9241, 2.8868, 2.9205],
        [2.9241, 2.9240, 2.9241],
        [2.9241, 2.9214, 2.9241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.15355974435806274 
model_pd.l_d.mean(): -24.94693374633789 
model_pd.lagr.mean(): -24.793373107910156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2296], device='cuda:0')), ('power', tensor([-25.1765], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.15355974435806274
epoch£º559	 i:1 	 global-step:11181	 l-p:0.15267084538936615
epoch£º559	 i:2 	 global-step:11182	 l-p:0.15720731019973755
epoch£º559	 i:3 	 global-step:11183	 l-p:0.153160959482193
epoch£º559	 i:4 	 global-step:11184	 l-p:0.14637057483196259
epoch£º559	 i:5 	 global-step:11185	 l-p:0.7779425382614136
epoch£º559	 i:6 	 global-step:11186	 l-p:0.26439368724823
epoch£º559	 i:7 	 global-step:11187	 l-p:0.133051797747612
epoch£º559	 i:8 	 global-step:11188	 l-p:0.21362873911857605
epoch£º559	 i:9 	 global-step:11189	 l-p:0.11328621953725815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[3.0858, 2.2500, 1.5882],
        [3.0858, 1.9935, 1.6587],
        [3.0858, 2.2548, 1.5924],
        [3.0858, 1.8765, 1.2965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): -0.07132454961538315 
model_pd.l_d.mean(): -24.409645080566406 
model_pd.lagr.mean(): -24.48097038269043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0688], device='cuda:0')), ('power', tensor([-24.4784], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:-0.07132454961538315
epoch£º560	 i:1 	 global-step:11201	 l-p:0.16442660987377167
epoch£º560	 i:2 	 global-step:11202	 l-p:0.12415410578250885
epoch£º560	 i:3 	 global-step:11203	 l-p:0.1166258379817009
epoch£º560	 i:4 	 global-step:11204	 l-p:0.12393929064273834
epoch£º560	 i:5 	 global-step:11205	 l-p:0.11141494661569595
epoch£º560	 i:6 	 global-step:11206	 l-p:0.14364537596702576
epoch£º560	 i:7 	 global-step:11207	 l-p:0.1794614940881729
epoch£º560	 i:8 	 global-step:11208	 l-p:0.1733250766992569
epoch£º560	 i:9 	 global-step:11209	 l-p:0.14056384563446045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9341, 2.9341, 2.9342],
        [2.9341, 1.8731, 1.6054],
        [2.9341, 2.8958, 2.9304],
        [2.9341, 1.7378, 1.1832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): -0.05591671168804169 
model_pd.l_d.mean(): -24.834514617919922 
model_pd.lagr.mean(): -24.890430450439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0695], device='cuda:0')), ('power', tensor([-24.9040], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:-0.05591671168804169
epoch£º561	 i:1 	 global-step:11221	 l-p:0.138447105884552
epoch£º561	 i:2 	 global-step:11222	 l-p:0.23083803057670593
epoch£º561	 i:3 	 global-step:11223	 l-p:0.12932486832141876
epoch£º561	 i:4 	 global-step:11224	 l-p:0.24159982800483704
epoch£º561	 i:5 	 global-step:11225	 l-p:0.15609773993492126
epoch£º561	 i:6 	 global-step:11226	 l-p:0.13678866624832153
epoch£º561	 i:7 	 global-step:11227	 l-p:0.2714392840862274
epoch£º561	 i:8 	 global-step:11228	 l-p:0.08810988813638687
epoch£º561	 i:9 	 global-step:11229	 l-p:0.1574094295501709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8921, 2.7230, 2.8463],
        [2.8921, 2.8899, 2.8921],
        [2.8921, 2.8919, 2.8921],
        [2.8921, 1.7037, 1.2479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.22889569401741028 
model_pd.l_d.mean(): -24.869760513305664 
model_pd.lagr.mean(): -24.640865325927734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0478], device='cuda:0')), ('power', tensor([-24.9175], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.22889569401741028
epoch£º562	 i:1 	 global-step:11241	 l-p:0.148313969373703
epoch£º562	 i:2 	 global-step:11242	 l-p:0.23157842457294464
epoch£º562	 i:3 	 global-step:11243	 l-p:-0.0339924693107605
epoch£º562	 i:4 	 global-step:11244	 l-p:0.09979833662509918
epoch£º562	 i:5 	 global-step:11245	 l-p:0.12844747304916382
epoch£º562	 i:6 	 global-step:11246	 l-p:0.13047567009925842
epoch£º562	 i:7 	 global-step:11247	 l-p:0.11254481971263885
epoch£º562	 i:8 	 global-step:11248	 l-p:0.14241153001785278
epoch£º562	 i:9 	 global-step:11249	 l-p:0.15267960727214813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0969, 3.0681, 3.0945],
        [3.0969, 2.6159, 2.7977],
        [3.0969, 2.7776, 2.9546],
        [3.0969, 2.9575, 3.0638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): -0.9291889071464539 
model_pd.l_d.mean(): -25.115320205688477 
model_pd.lagr.mean(): -26.044509887695312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0062], device='cuda:0')), ('power', tensor([-25.1091], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:-0.9291889071464539
epoch£º563	 i:1 	 global-step:11261	 l-p:0.13128133118152618
epoch£º563	 i:2 	 global-step:11262	 l-p:0.15269744396209717
epoch£º563	 i:3 	 global-step:11263	 l-p:0.1440138965845108
epoch£º563	 i:4 	 global-step:11264	 l-p:0.15730735659599304
epoch£º563	 i:5 	 global-step:11265	 l-p:0.5266159772872925
epoch£º563	 i:6 	 global-step:11266	 l-p:0.13715814054012299
epoch£º563	 i:7 	 global-step:11267	 l-p:0.14650477468967438
epoch£º563	 i:8 	 global-step:11268	 l-p:-0.25103670358657837
epoch£º563	 i:9 	 global-step:11269	 l-p:0.12219563871622086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[2.9796, 1.8088, 1.3777],
        [2.9796, 1.7743, 1.2155],
        [2.9796, 2.0252, 1.9007],
        [2.9796, 2.4199, 2.5879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.17466044425964355 
model_pd.l_d.mean(): -24.816076278686523 
model_pd.lagr.mean(): -24.641416549682617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0468], device='cuda:0')), ('power', tensor([-24.8629], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.17466044425964355
epoch£º564	 i:1 	 global-step:11281	 l-p:0.17676810920238495
epoch£º564	 i:2 	 global-step:11282	 l-p:0.1359083354473114
epoch£º564	 i:3 	 global-step:11283	 l-p:0.132573664188385
epoch£º564	 i:4 	 global-step:11284	 l-p:0.11053638905286789
epoch£º564	 i:5 	 global-step:11285	 l-p:0.22178590297698975
epoch£º564	 i:6 	 global-step:11286	 l-p:0.11736757308244705
epoch£º564	 i:7 	 global-step:11287	 l-p:0.12721483409404755
epoch£º564	 i:8 	 global-step:11288	 l-p:1.3503504991531372
epoch£º564	 i:9 	 global-step:11289	 l-p:0.003341579344123602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9894, 2.9697, 2.9882],
        [2.9894, 2.5059, 2.6901],
        [2.9894, 2.0983, 1.4551],
        [2.9894, 2.1019, 1.4582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.13907530903816223 
model_pd.l_d.mean(): -24.946380615234375 
model_pd.lagr.mean(): -24.80730438232422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0846], device='cuda:0')), ('power', tensor([-25.0310], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.13907530903816223
epoch£º565	 i:1 	 global-step:11301	 l-p:-0.057472046464681625
epoch£º565	 i:2 	 global-step:11302	 l-p:0.1379956752061844
epoch£º565	 i:3 	 global-step:11303	 l-p:0.13700084388256073
epoch£º565	 i:4 	 global-step:11304	 l-p:0.029310040175914764
epoch£º565	 i:5 	 global-step:11305	 l-p:0.13080763816833496
epoch£º565	 i:6 	 global-step:11306	 l-p:0.14427797496318817
epoch£º565	 i:7 	 global-step:11307	 l-p:0.14282390475273132
epoch£º565	 i:8 	 global-step:11308	 l-p:0.4382776618003845
epoch£º565	 i:9 	 global-step:11309	 l-p:0.21013294160366058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9964, 2.9427, 2.9898],
        [2.9964, 2.9898, 2.9962],
        [2.9964, 2.5339, 2.7205],
        [2.9964, 2.7828, 2.9273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.019060159102082253 
model_pd.l_d.mean(): -25.002901077270508 
model_pd.lagr.mean(): -24.983840942382812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0292], device='cuda:0')), ('power', tensor([-25.0321], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.019060159102082253
epoch£º566	 i:1 	 global-step:11321	 l-p:0.11840666830539703
epoch£º566	 i:2 	 global-step:11322	 l-p:0.1832369565963745
epoch£º566	 i:3 	 global-step:11323	 l-p:0.1447576880455017
epoch£º566	 i:4 	 global-step:11324	 l-p:0.14481084048748016
epoch£º566	 i:5 	 global-step:11325	 l-p:0.11561453342437744
epoch£º566	 i:6 	 global-step:11326	 l-p:0.1516038328409195
epoch£º566	 i:7 	 global-step:11327	 l-p:0.22456572949886322
epoch£º566	 i:8 	 global-step:11328	 l-p:0.04751018434762955
epoch£º566	 i:9 	 global-step:11329	 l-p:0.14372532069683075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[2.9684, 2.0876, 1.4450],
        [2.9684, 2.0436, 1.4084],
        [2.9684, 2.3542, 2.5044],
        [2.9684, 1.9225, 1.3089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.1853976845741272 
model_pd.l_d.mean(): -24.402088165283203 
model_pd.lagr.mean(): -24.216690063476562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1935], device='cuda:0')), ('power', tensor([-24.5956], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.1853976845741272
epoch£º567	 i:1 	 global-step:11341	 l-p:0.1595108062028885
epoch£º567	 i:2 	 global-step:11342	 l-p:0.6567492485046387
epoch£º567	 i:3 	 global-step:11343	 l-p:0.1557338386774063
epoch£º567	 i:4 	 global-step:11344	 l-p:0.08958613872528076
epoch£º567	 i:5 	 global-step:11345	 l-p:0.14862751960754395
epoch£º567	 i:6 	 global-step:11346	 l-p:0.04966728016734123
epoch£º567	 i:7 	 global-step:11347	 l-p:0.18130503594875336
epoch£º567	 i:8 	 global-step:11348	 l-p:0.15716616809368134
epoch£º567	 i:9 	 global-step:11349	 l-p:0.07221246510744095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9866, 1.8975, 1.5889],
        [2.9866, 2.9496, 2.9831],
        [2.9866, 1.8500, 1.2528],
        [2.9866, 1.8618, 1.5028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.08101846277713776 
model_pd.l_d.mean(): -24.99396324157715 
model_pd.lagr.mean(): -24.912944793701172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1317], device='cuda:0')), ('power', tensor([-25.1256], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.08101846277713776
epoch£º568	 i:1 	 global-step:11361	 l-p:0.16418389976024628
epoch£º568	 i:2 	 global-step:11362	 l-p:0.11786818504333496
epoch£º568	 i:3 	 global-step:11363	 l-p:0.11716331541538239
epoch£º568	 i:4 	 global-step:11364	 l-p:0.20755164325237274
epoch£º568	 i:5 	 global-step:11365	 l-p:0.12626996636390686
epoch£º568	 i:6 	 global-step:11366	 l-p:0.12426614761352539
epoch£º568	 i:7 	 global-step:11367	 l-p:0.04865000769495964
epoch£º568	 i:8 	 global-step:11368	 l-p:0.192331463098526
epoch£º568	 i:9 	 global-step:11369	 l-p:0.1797647476196289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0868, 3.0867, 3.0868],
        [3.0868, 2.9536, 3.0563],
        [3.0868, 2.1982, 1.5412],
        [3.0868, 3.0868, 3.0868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.12579560279846191 
model_pd.l_d.mean(): -24.728784561157227 
model_pd.lagr.mean(): -24.602989196777344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0282], device='cuda:0')), ('power', tensor([-24.7569], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.12579560279846191
epoch£º569	 i:1 	 global-step:11381	 l-p:0.39949554204940796
epoch£º569	 i:2 	 global-step:11382	 l-p:0.14908038079738617
epoch£º569	 i:3 	 global-step:11383	 l-p:0.11576349288225174
epoch£º569	 i:4 	 global-step:11384	 l-p:0.12050176411867142
epoch£º569	 i:5 	 global-step:11385	 l-p:0.12680788338184357
epoch£º569	 i:6 	 global-step:11386	 l-p:0.1280374675989151
epoch£º569	 i:7 	 global-step:11387	 l-p:0.14703655242919922
epoch£º569	 i:8 	 global-step:11388	 l-p:0.16863422095775604
epoch£º569	 i:9 	 global-step:11389	 l-p:0.13424545526504517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9319, 2.8902, 2.9276],
        [2.9319, 2.9319, 2.9319],
        [2.9319, 1.8743, 1.6186],
        [2.9319, 1.8626, 1.2598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.1527792513370514 
model_pd.l_d.mean(): -24.888301849365234 
model_pd.lagr.mean(): -24.735523223876953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0321], device='cuda:0')), ('power', tensor([-24.9204], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.1527792513370514
epoch£º570	 i:1 	 global-step:11401	 l-p:0.16631421446800232
epoch£º570	 i:2 	 global-step:11402	 l-p:0.09219514578580856
epoch£º570	 i:3 	 global-step:11403	 l-p:0.050115685909986496
epoch£º570	 i:4 	 global-step:11404	 l-p:0.1447005271911621
epoch£º570	 i:5 	 global-step:11405	 l-p:0.17378093302249908
epoch£º570	 i:6 	 global-step:11406	 l-p:0.14761002361774445
epoch£º570	 i:7 	 global-step:11407	 l-p:0.14178906381130219
epoch£º570	 i:8 	 global-step:11408	 l-p:0.07908838987350464
epoch£º570	 i:9 	 global-step:11409	 l-p:0.1593925654888153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9837, 2.1252, 2.1115],
        [2.9837, 2.9835, 2.9837],
        [2.9837, 2.8579, 2.9562],
        [2.9837, 2.9624, 2.9823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.1701233685016632 
model_pd.l_d.mean(): -24.690250396728516 
model_pd.lagr.mean(): -24.520126342773438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0708], device='cuda:0')), ('power', tensor([-24.7611], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.1701233685016632
epoch£º571	 i:1 	 global-step:11421	 l-p:0.4028388261795044
epoch£º571	 i:2 	 global-step:11422	 l-p:0.1075778380036354
epoch£º571	 i:3 	 global-step:11423	 l-p:0.186122864484787
epoch£º571	 i:4 	 global-step:11424	 l-p:0.19975702464580536
epoch£º571	 i:5 	 global-step:11425	 l-p:0.11354110389947891
epoch£º571	 i:6 	 global-step:11426	 l-p:0.1166064441204071
epoch£º571	 i:7 	 global-step:11427	 l-p:0.13039694726467133
epoch£º571	 i:8 	 global-step:11428	 l-p:0.11674465239048004
epoch£º571	 i:9 	 global-step:11429	 l-p:0.1103019118309021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1750, 3.1713, 3.1749],
        [3.1750, 2.3416, 1.6669],
        [3.1750, 2.2704, 1.6031],
        [3.1750, 3.1250, 3.1692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.1158512681722641 
model_pd.l_d.mean(): -24.714900970458984 
model_pd.lagr.mean(): -24.599050521850586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0234], device='cuda:0')), ('power', tensor([-24.7383], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.1158512681722641
epoch£º572	 i:1 	 global-step:11441	 l-p:0.1359845995903015
epoch£º572	 i:2 	 global-step:11442	 l-p:0.1534820944070816
epoch£º572	 i:3 	 global-step:11443	 l-p:-0.02402574010193348
epoch£º572	 i:4 	 global-step:11444	 l-p:0.138039693236351
epoch£º572	 i:5 	 global-step:11445	 l-p:0.1430392563343048
epoch£º572	 i:6 	 global-step:11446	 l-p:0.08845268934965134
epoch£º572	 i:7 	 global-step:11447	 l-p:0.36775699257850647
epoch£º572	 i:8 	 global-step:11448	 l-p:-0.29206356406211853
epoch£º572	 i:9 	 global-step:11449	 l-p:-0.04810451716184616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7943, 2.5495, 2.7079],
        [2.7943, 2.0484, 2.1387],
        [2.7943, 2.4438, 2.6305],
        [2.7943, 2.7933, 2.7943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.4323911964893341 
model_pd.l_d.mean(): -24.82793617248535 
model_pd.lagr.mean(): -24.395544052124023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1363], device='cuda:0')), ('power', tensor([-24.9643], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.4323911964893341
epoch£º573	 i:1 	 global-step:11461	 l-p:1.1452929973602295
epoch£º573	 i:2 	 global-step:11462	 l-p:0.12085767835378647
epoch£º573	 i:3 	 global-step:11463	 l-p:0.09025175124406815
epoch£º573	 i:4 	 global-step:11464	 l-p:-0.004412698559463024
epoch£º573	 i:5 	 global-step:11465	 l-p:0.13819704949855804
epoch£º573	 i:6 	 global-step:11466	 l-p:0.15057478845119476
epoch£º573	 i:7 	 global-step:11467	 l-p:0.12518003582954407
epoch£º573	 i:8 	 global-step:11468	 l-p:0.1334676891565323
epoch£º573	 i:9 	 global-step:11469	 l-p:0.08620449155569077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1528, 2.1354, 1.9196],
        [3.1528, 1.9458, 1.3377],
        [3.1528, 3.1529, 3.1528],
        [3.1528, 3.1376, 3.1520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.12706606090068817 
model_pd.l_d.mean(): -24.97770118713379 
model_pd.lagr.mean(): -24.850635528564453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0462], device='cuda:0')), ('power', tensor([-24.9315], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.12706606090068817
epoch£º574	 i:1 	 global-step:11481	 l-p:0.13130556046962738
epoch£º574	 i:2 	 global-step:11482	 l-p:0.13529722392559052
epoch£º574	 i:3 	 global-step:11483	 l-p:-0.06780814379453659
epoch£º574	 i:4 	 global-step:11484	 l-p:-0.11151216924190521
epoch£º574	 i:5 	 global-step:11485	 l-p:0.1711847335100174
epoch£º574	 i:6 	 global-step:11486	 l-p:0.16499659419059753
epoch£º574	 i:7 	 global-step:11487	 l-p:0.08909922093153
epoch£º574	 i:8 	 global-step:11488	 l-p:0.1916988343000412
epoch£º574	 i:9 	 global-step:11489	 l-p:0.12548042833805084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9436, 2.8958, 2.9383],
        [2.9436, 2.2434, 2.3559],
        [2.9436, 2.9436, 2.9436],
        [2.9436, 2.8893, 2.9370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.1341015249490738 
model_pd.l_d.mean(): -25.09840202331543 
model_pd.lagr.mean(): -24.96430015563965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1396], device='cuda:0')), ('power', tensor([-25.2380], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.1341015249490738
epoch£º575	 i:1 	 global-step:11501	 l-p:0.10664030909538269
epoch£º575	 i:2 	 global-step:11502	 l-p:0.007453298196196556
epoch£º575	 i:3 	 global-step:11503	 l-p:0.11389452964067459
epoch£º575	 i:4 	 global-step:11504	 l-p:0.1452292948961258
epoch£º575	 i:5 	 global-step:11505	 l-p:0.12914520502090454
epoch£º575	 i:6 	 global-step:11506	 l-p:0.12611123919487
epoch£º575	 i:7 	 global-step:11507	 l-p:0.13794812560081482
epoch£º575	 i:8 	 global-step:11508	 l-p:0.17584247887134552
epoch£º575	 i:9 	 global-step:11509	 l-p:0.14567913115024567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0473, 3.0461, 3.0473],
        [3.0473, 3.0337, 3.0466],
        [3.0473, 2.1085, 1.4623],
        [3.0473, 1.8279, 1.3098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.18502111732959747 
model_pd.l_d.mean(): -25.020187377929688 
model_pd.lagr.mean(): -24.835166931152344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0371], device='cuda:0')), ('power', tensor([-25.0573], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.18502111732959747
epoch£º576	 i:1 	 global-step:11521	 l-p:0.1417430192232132
epoch£º576	 i:2 	 global-step:11522	 l-p:0.13934314250946045
epoch£º576	 i:3 	 global-step:11523	 l-p:0.41128066182136536
epoch£º576	 i:4 	 global-step:11524	 l-p:0.12752117216587067
epoch£º576	 i:5 	 global-step:11525	 l-p:0.13311322033405304
epoch£º576	 i:6 	 global-step:11526	 l-p:0.11893846839666367
epoch£º576	 i:7 	 global-step:11527	 l-p:0.2382180094718933
epoch£º576	 i:8 	 global-step:11528	 l-p:0.013292293064296246
epoch£º576	 i:9 	 global-step:11529	 l-p:0.15930643677711487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0064, 2.5476, 2.7364],
        [3.0064, 3.0064, 3.0064],
        [3.0064, 1.8146, 1.2305],
        [3.0064, 3.0064, 3.0064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): -1.404075264930725 
model_pd.l_d.mean(): -25.18140411376953 
model_pd.lagr.mean(): -26.585479736328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0245], device='cuda:0')), ('power', tensor([-25.2059], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:-1.404075264930725
epoch£º577	 i:1 	 global-step:11541	 l-p:0.15661007165908813
epoch£º577	 i:2 	 global-step:11542	 l-p:0.07617136836051941
epoch£º577	 i:3 	 global-step:11543	 l-p:0.11086093634366989
epoch£º577	 i:4 	 global-step:11544	 l-p:0.12726520001888275
epoch£º577	 i:5 	 global-step:11545	 l-p:0.13378456234931946
epoch£º577	 i:6 	 global-step:11546	 l-p:0.1606384664773941
epoch£º577	 i:7 	 global-step:11547	 l-p:0.12267761677503586
epoch£º577	 i:8 	 global-step:11548	 l-p:0.12254064530134201
epoch£º577	 i:9 	 global-step:11549	 l-p:0.16816124320030212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0425, 3.0425, 3.0425],
        [3.0425, 1.8383, 1.2523],
        [3.0425, 2.2426, 2.2818],
        [3.0425, 1.9238, 1.5743]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.17405469715595245 
model_pd.l_d.mean(): -25.099613189697266 
model_pd.lagr.mean(): -24.92555809020996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0041], device='cuda:0')), ('power', tensor([-25.1037], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.17405469715595245
epoch£º578	 i:1 	 global-step:11561	 l-p:0.2210194319486618
epoch£º578	 i:2 	 global-step:11562	 l-p:0.12025965750217438
epoch£º578	 i:3 	 global-step:11563	 l-p:0.10345809906721115
epoch£º578	 i:4 	 global-step:11564	 l-p:5.251130104064941
epoch£º578	 i:5 	 global-step:11565	 l-p:0.15308421850204468
epoch£º578	 i:6 	 global-step:11566	 l-p:0.1252897083759308
epoch£º578	 i:7 	 global-step:11567	 l-p:0.12577958405017853
epoch£º578	 i:8 	 global-step:11568	 l-p:0.13621200621128082
epoch£º578	 i:9 	 global-step:11569	 l-p:0.17030540108680725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0352, 2.1500, 1.4961],
        [3.0352, 2.7099, 2.8904],
        [3.0352, 1.9374, 1.6233],
        [3.0352, 2.8057, 2.9573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.14081250131130219 
model_pd.l_d.mean(): -25.082374572753906 
model_pd.lagr.mean(): -24.94156265258789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0134], device='cuda:0')), ('power', tensor([-25.0958], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.14081250131130219
epoch£º579	 i:1 	 global-step:11581	 l-p:-0.02980194054543972
epoch£º579	 i:2 	 global-step:11582	 l-p:0.13655801117420197
epoch£º579	 i:3 	 global-step:11583	 l-p:0.043947529047727585
epoch£º579	 i:4 	 global-step:11584	 l-p:-0.01978004351258278
epoch£º579	 i:5 	 global-step:11585	 l-p:0.1543302685022354
epoch£º579	 i:6 	 global-step:11586	 l-p:0.16193075478076935
epoch£º579	 i:7 	 global-step:11587	 l-p:0.10263082385063171
epoch£º579	 i:8 	 global-step:11588	 l-p:0.14103081822395325
epoch£º579	 i:9 	 global-step:11589	 l-p:0.15821492671966553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0685, 2.9867, 3.0553],
        [3.0685, 3.0461, 3.0670],
        [3.0685, 1.8386, 1.3057],
        [3.0685, 2.9634, 3.0484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.18390138447284698 
model_pd.l_d.mean(): -25.24982452392578 
model_pd.lagr.mean(): -25.0659236907959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0715], device='cuda:0')), ('power', tensor([-25.1783], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.18390138447284698
epoch£º580	 i:1 	 global-step:11601	 l-p:0.14018748700618744
epoch£º580	 i:2 	 global-step:11602	 l-p:0.1320561170578003
epoch£º580	 i:3 	 global-step:11603	 l-p:-0.2356186956167221
epoch£º580	 i:4 	 global-step:11604	 l-p:0.41309791803359985
epoch£º580	 i:5 	 global-step:11605	 l-p:0.16327516734600067
epoch£º580	 i:6 	 global-step:11606	 l-p:0.15063221752643585
epoch£º580	 i:7 	 global-step:11607	 l-p:0.23043780028820038
epoch£º580	 i:8 	 global-step:11608	 l-p:-0.0008052539778873324
epoch£º580	 i:9 	 global-step:11609	 l-p:0.11552716791629791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1908, 3.1907, 3.1908],
        [3.1908, 2.0578, 1.4147],
        [3.1908, 2.1197, 1.8318],
        [3.1908, 2.7641, 2.9525]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.1269873082637787 
model_pd.l_d.mean(): -24.930788040161133 
model_pd.lagr.mean(): -24.803800582885742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0295], device='cuda:0')), ('power', tensor([-24.9013], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.1269873082637787
epoch£º581	 i:1 	 global-step:11621	 l-p:0.09400352090597153
epoch£º581	 i:2 	 global-step:11622	 l-p:0.11777643114328384
epoch£º581	 i:3 	 global-step:11623	 l-p:0.12010408192873001
epoch£º581	 i:4 	 global-step:11624	 l-p:0.1396459937095642
epoch£º581	 i:5 	 global-step:11625	 l-p:0.1751336306333542
epoch£º581	 i:6 	 global-step:11626	 l-p:0.19277232885360718
epoch£º581	 i:7 	 global-step:11627	 l-p:0.13708828389644623
epoch£º581	 i:8 	 global-step:11628	 l-p:0.10414138436317444
epoch£º581	 i:9 	 global-step:11629	 l-p:0.13794700801372528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0239, 2.8141, 2.9577],
        [3.0239, 2.1043, 1.4554],
        [3.0239, 3.0225, 3.0239],
        [3.0239, 2.9818, 3.0196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.1404392272233963 
model_pd.l_d.mean(): -24.37224006652832 
model_pd.lagr.mean(): -24.231800079345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1304], device='cuda:0')), ('power', tensor([-24.5026], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.1404392272233963
epoch£º582	 i:1 	 global-step:11641	 l-p:0.09177300333976746
epoch£º582	 i:2 	 global-step:11642	 l-p:0.126839742064476
epoch£º582	 i:3 	 global-step:11643	 l-p:0.15038087964057922
epoch£º582	 i:4 	 global-step:11644	 l-p:0.17854106426239014
epoch£º582	 i:5 	 global-step:11645	 l-p:0.523887038230896
epoch£º582	 i:6 	 global-step:11646	 l-p:0.13937701284885406
epoch£º582	 i:7 	 global-step:11647	 l-p:0.14413608610630035
epoch£º582	 i:8 	 global-step:11648	 l-p:0.12175074964761734
epoch£º582	 i:9 	 global-step:11649	 l-p:0.14696601033210754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9857, 2.8470, 2.9535],
        [2.9857, 1.8072, 1.2171],
        [2.9857, 1.8190, 1.2249],
        [2.9857, 1.8336, 1.4572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.1417863965034485 
model_pd.l_d.mean(): -25.111722946166992 
model_pd.lagr.mean(): -24.96993637084961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0949], device='cuda:0')), ('power', tensor([-25.2066], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.1417863965034485
epoch£º583	 i:1 	 global-step:11661	 l-p:0.13901866972446442
epoch£º583	 i:2 	 global-step:11662	 l-p:0.3502948582172394
epoch£º583	 i:3 	 global-step:11663	 l-p:-0.33014869689941406
epoch£º583	 i:4 	 global-step:11664	 l-p:0.017850475385785103
epoch£º583	 i:5 	 global-step:11665	 l-p:0.1441550850868225
epoch£º583	 i:6 	 global-step:11666	 l-p:0.10794674605131149
epoch£º583	 i:7 	 global-step:11667	 l-p:0.17106059193611145
epoch£º583	 i:8 	 global-step:11668	 l-p:0.1466740369796753
epoch£º583	 i:9 	 global-step:11669	 l-p:0.12122558057308197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1551, 3.0733, 3.1419],
        [3.1551, 3.1551, 3.1551],
        [3.1551, 2.9288, 3.0789],
        [3.1551, 2.8803, 3.0477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.20589086413383484 
model_pd.l_d.mean(): -25.045269012451172 
model_pd.lagr.mean(): -24.839378356933594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0802], device='cuda:0')), ('power', tensor([-24.9651], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.20589086413383484
epoch£º584	 i:1 	 global-step:11681	 l-p:0.11972077190876007
epoch£º584	 i:2 	 global-step:11682	 l-p:0.14764894545078278
epoch£º584	 i:3 	 global-step:11683	 l-p:0.14745593070983887
epoch£º584	 i:4 	 global-step:11684	 l-p:0.10755422711372375
epoch£º584	 i:5 	 global-step:11685	 l-p:0.15788018703460693
epoch£º584	 i:6 	 global-step:11686	 l-p:0.1437073051929474
epoch£º584	 i:7 	 global-step:11687	 l-p:0.2542400062084198
epoch£º584	 i:8 	 global-step:11688	 l-p:0.1143808588385582
epoch£º584	 i:9 	 global-step:11689	 l-p:0.12630900740623474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7911, 2.4637, 2.6477],
        [2.7911, 2.7623, 2.7888],
        [2.7911, 2.7851, 2.7910],
        [2.7911, 2.7531, 2.7875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.1473061591386795 
model_pd.l_d.mean(): -25.09674644470215 
model_pd.lagr.mean(): -24.949440002441406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0855], device='cuda:0')), ('power', tensor([-25.1823], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.1473061591386795
epoch£º585	 i:1 	 global-step:11701	 l-p:0.07804558426141739
epoch£º585	 i:2 	 global-step:11702	 l-p:-1.0964893102645874
epoch£º585	 i:3 	 global-step:11703	 l-p:2.0895462036132812
epoch£º585	 i:4 	 global-step:11704	 l-p:0.227205291390419
epoch£º585	 i:5 	 global-step:11705	 l-p:0.13348299264907837
epoch£º585	 i:6 	 global-step:11706	 l-p:0.3057604134082794
epoch£º585	 i:7 	 global-step:11707	 l-p:0.14777599275112152
epoch£º585	 i:8 	 global-step:11708	 l-p:0.21671317517757416
epoch£º585	 i:9 	 global-step:11709	 l-p:0.10642043501138687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0054, 3.0030, 3.0053],
        [3.0054, 2.0810, 1.4345],
        [3.0054, 2.3506, 2.4900],
        [3.0054, 2.8765, 2.9770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.14892363548278809 
model_pd.l_d.mean(): -24.984195709228516 
model_pd.lagr.mean(): -24.83527183532715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0299], device='cuda:0')), ('power', tensor([-25.0141], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.14892363548278809
epoch£º586	 i:1 	 global-step:11721	 l-p:0.15303856134414673
epoch£º586	 i:2 	 global-step:11722	 l-p:0.18891790509223938
epoch£º586	 i:3 	 global-step:11723	 l-p:0.14281398057937622
epoch£º586	 i:4 	 global-step:11724	 l-p:0.19595550000667572
epoch£º586	 i:5 	 global-step:11725	 l-p:0.10484892874956131
epoch£º586	 i:6 	 global-step:11726	 l-p:0.14302800595760345
epoch£º586	 i:7 	 global-step:11727	 l-p:0.12675482034683228
epoch£º586	 i:8 	 global-step:11728	 l-p:-0.027848724275827408
epoch£º586	 i:9 	 global-step:11729	 l-p:0.13813944160938263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0716, 1.9855, 1.6951],
        [3.0716, 1.8690, 1.2686],
        [3.0716, 1.8440, 1.2605],
        [3.0716, 2.8481, 2.9976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.12069617956876755 
model_pd.l_d.mean(): -24.966463088989258 
model_pd.lagr.mean(): -24.845766067504883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0541], device='cuda:0')), ('power', tensor([-25.0205], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.12069617956876755
epoch£º587	 i:1 	 global-step:11741	 l-p:0.2918066084384918
epoch£º587	 i:2 	 global-step:11742	 l-p:-0.12926986813545227
epoch£º587	 i:3 	 global-step:11743	 l-p:0.13167403638362885
epoch£º587	 i:4 	 global-step:11744	 l-p:0.1359282284975052
epoch£º587	 i:5 	 global-step:11745	 l-p:0.18554534018039703
epoch£º587	 i:6 	 global-step:11746	 l-p:0.17607063055038452
epoch£º587	 i:7 	 global-step:11747	 l-p:0.1561848372220993
epoch£º587	 i:8 	 global-step:11748	 l-p:0.13224588334560394
epoch£º587	 i:9 	 global-step:11749	 l-p:0.23817643523216248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9031, 2.9031, 2.9031],
        [2.9031, 2.5372, 2.7269],
        [2.9031, 1.9882, 1.9364],
        [2.9031, 2.5949, 2.7736]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.1380676031112671 
model_pd.l_d.mean(): -25.067960739135742 
model_pd.lagr.mean(): -24.929893493652344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0746], device='cuda:0')), ('power', tensor([-25.1425], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.1380676031112671
epoch£º588	 i:1 	 global-step:11761	 l-p:0.09847254306077957
epoch£º588	 i:2 	 global-step:11762	 l-p:0.08118803799152374
epoch£º588	 i:3 	 global-step:11763	 l-p:0.10906482487916946
epoch£º588	 i:4 	 global-step:11764	 l-p:0.226984441280365
epoch£º588	 i:5 	 global-step:11765	 l-p:0.1456555873155594
epoch£º588	 i:6 	 global-step:11766	 l-p:0.13448172807693481
epoch£º588	 i:7 	 global-step:11767	 l-p:0.12219635397195816
epoch£º588	 i:8 	 global-step:11768	 l-p:0.13467657566070557
epoch£º588	 i:9 	 global-step:11769	 l-p:0.19622471928596497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9969, 2.9412, 2.9900],
        [2.9969, 2.9938, 2.9968],
        [2.9969, 2.8248, 2.9502],
        [2.9969, 2.9968, 2.9969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.2691005766391754 
model_pd.l_d.mean(): -24.965557098388672 
model_pd.lagr.mean(): -24.696456909179688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0780], device='cuda:0')), ('power', tensor([-25.0435], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.2691005766391754
epoch£º589	 i:1 	 global-step:11781	 l-p:0.12443556636571884
epoch£º589	 i:2 	 global-step:11782	 l-p:0.08019130676984787
epoch£º589	 i:3 	 global-step:11783	 l-p:0.2503278851509094
epoch£º589	 i:4 	 global-step:11784	 l-p:0.14355182647705078
epoch£º589	 i:5 	 global-step:11785	 l-p:0.15551747381687164
epoch£º589	 i:6 	 global-step:11786	 l-p:0.1403522491455078
epoch£º589	 i:7 	 global-step:11787	 l-p:0.14456190168857574
epoch£º589	 i:8 	 global-step:11788	 l-p:0.15732862055301666
epoch£º589	 i:9 	 global-step:11789	 l-p:0.15166722238063812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9063, 2.7704, 2.8755],
        [2.9063, 1.6805, 1.1470],
        [2.9063, 2.6739, 2.8279],
        [2.9063, 2.8685, 2.9027]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.201546773314476 
model_pd.l_d.mean(): -25.103473663330078 
model_pd.lagr.mean(): -24.901926040649414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1273], device='cuda:0')), ('power', tensor([-25.2308], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.201546773314476
epoch£º590	 i:1 	 global-step:11801	 l-p:0.16299621760845184
epoch£º590	 i:2 	 global-step:11802	 l-p:0.13480733335018158
epoch£º590	 i:3 	 global-step:11803	 l-p:0.1446748524904251
epoch£º590	 i:4 	 global-step:11804	 l-p:0.15927942097187042
epoch£º590	 i:5 	 global-step:11805	 l-p:0.14907778799533844
epoch£º590	 i:6 	 global-step:11806	 l-p:0.13891516625881195
epoch£º590	 i:7 	 global-step:11807	 l-p:0.08417610079050064
epoch£º590	 i:8 	 global-step:11808	 l-p:-0.008682461455464363
epoch£º590	 i:9 	 global-step:11809	 l-p:0.15126875042915344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9676, 1.7779, 1.1944],
        [2.9676, 1.8826, 1.6089],
        [2.9676, 2.9672, 2.9676],
        [2.9676, 2.9663, 2.9676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.13008926808834076 
model_pd.l_d.mean(): -24.49100112915039 
model_pd.lagr.mean(): -24.360912322998047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0995], device='cuda:0')), ('power', tensor([-24.5905], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.13008926808834076
epoch£º591	 i:1 	 global-step:11821	 l-p:0.14368730783462524
epoch£º591	 i:2 	 global-step:11822	 l-p:0.11182858049869537
epoch£º591	 i:3 	 global-step:11823	 l-p:0.20156916975975037
epoch£º591	 i:4 	 global-step:11824	 l-p:0.1575222909450531
epoch£º591	 i:5 	 global-step:11825	 l-p:0.15594536066055298
epoch£º591	 i:6 	 global-step:11826	 l-p:0.13583800196647644
epoch£º591	 i:7 	 global-step:11827	 l-p:0.14031369984149933
epoch£º591	 i:8 	 global-step:11828	 l-p:0.12111292779445648
epoch£º591	 i:9 	 global-step:11829	 l-p:0.12761269509792328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[3.1217, 2.3061, 2.3371],
        [3.1217, 2.3053, 2.3358],
        [3.1217, 2.2648, 1.5929],
        [3.1217, 2.2182, 2.1656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.12546057999134064 
model_pd.l_d.mean(): -24.942750930786133 
model_pd.lagr.mean(): -24.817291259765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0391], device='cuda:0')), ('power', tensor([-24.9037], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.12546057999134064
epoch£º592	 i:1 	 global-step:11841	 l-p:0.18639567494392395
epoch£º592	 i:2 	 global-step:11842	 l-p:0.13429950177669525
epoch£º592	 i:3 	 global-step:11843	 l-p:0.14118967950344086
epoch£º592	 i:4 	 global-step:11844	 l-p:0.24222077429294586
epoch£º592	 i:5 	 global-step:11845	 l-p:0.13493530452251434
epoch£º592	 i:6 	 global-step:11846	 l-p:0.15857316553592682
epoch£º592	 i:7 	 global-step:11847	 l-p:0.15389947593212128
epoch£º592	 i:8 	 global-step:11848	 l-p:0.3565162718296051
epoch£º592	 i:9 	 global-step:11849	 l-p:0.15650847554206848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8784, 2.5820, 2.7582],
        [2.8784, 2.3935, 2.5863],
        [2.8784, 2.8783, 2.8784],
        [2.8784, 2.8784, 2.8784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.12054695934057236 
model_pd.l_d.mean(): -25.2637996673584 
model_pd.lagr.mean(): -25.143253326416016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0828], device='cuda:0')), ('power', tensor([-25.3466], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.12054695934057236
epoch£º593	 i:1 	 global-step:11861	 l-p:0.12279784679412842
epoch£º593	 i:2 	 global-step:11862	 l-p:0.13351412117481232
epoch£º593	 i:3 	 global-step:11863	 l-p:0.2276565283536911
epoch£º593	 i:4 	 global-step:11864	 l-p:0.15154708921909332
epoch£º593	 i:5 	 global-step:11865	 l-p:-0.23326189815998077
epoch£º593	 i:6 	 global-step:11866	 l-p:0.11943689733743668
epoch£º593	 i:7 	 global-step:11867	 l-p:0.13626642525196075
epoch£º593	 i:8 	 global-step:11868	 l-p:0.13933640718460083
epoch£º593	 i:9 	 global-step:11869	 l-p:0.8977771401405334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0535, 2.6144, 2.8070],
        [3.0535, 3.0534, 3.0535],
        [3.0535, 2.8058, 2.9652],
        [3.0535, 1.9609, 1.3338]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.2428748458623886 
model_pd.l_d.mean(): -25.064319610595703 
model_pd.lagr.mean(): -24.82144546508789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0983], device='cuda:0')), ('power', tensor([-25.1626], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.2428748458623886
epoch£º594	 i:1 	 global-step:11881	 l-p:0.12786494195461273
epoch£º594	 i:2 	 global-step:11882	 l-p:0.15559853613376617
epoch£º594	 i:3 	 global-step:11883	 l-p:0.12418628484010696
epoch£º594	 i:4 	 global-step:11884	 l-p:0.13335543870925903
epoch£º594	 i:5 	 global-step:11885	 l-p:0.1433633714914322
epoch£º594	 i:6 	 global-step:11886	 l-p:0.13522489368915558
epoch£º594	 i:7 	 global-step:11887	 l-p:0.1449858397245407
epoch£º594	 i:8 	 global-step:11888	 l-p:0.100505530834198
epoch£º594	 i:9 	 global-step:11889	 l-p:0.14063213765621185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9599, 1.8735, 1.2622],
        [2.9599, 2.1313, 2.1638],
        [2.9599, 2.9598, 2.9599],
        [2.9599, 2.9539, 2.9597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.15390419960021973 
model_pd.l_d.mean(): -25.142465591430664 
model_pd.lagr.mean(): -24.988561630249023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0291], device='cuda:0')), ('power', tensor([-25.1716], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.15390419960021973
epoch£º595	 i:1 	 global-step:11901	 l-p:0.16331802308559418
epoch£º595	 i:2 	 global-step:11902	 l-p:0.25336530804634094
epoch£º595	 i:3 	 global-step:11903	 l-p:-0.14583979547023773
epoch£º595	 i:4 	 global-step:11904	 l-p:0.15784625709056854
epoch£º595	 i:5 	 global-step:11905	 l-p:0.15063568949699402
epoch£º595	 i:6 	 global-step:11906	 l-p:0.12752804160118103
epoch£º595	 i:7 	 global-step:11907	 l-p:0.5456267595291138
epoch£º595	 i:8 	 global-step:11908	 l-p:0.11650996655225754
epoch£º595	 i:9 	 global-step:11909	 l-p:0.10719546675682068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0965, 2.9530, 3.0624],
        [3.0965, 3.0936, 3.0965],
        [3.0965, 1.8569, 1.3271],
        [3.0965, 3.0964, 3.0965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.06220000609755516 
model_pd.l_d.mean(): -25.06969451904297 
model_pd.lagr.mean(): -25.00749397277832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0433], device='cuda:0')), ('power', tensor([-25.0264], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.06220000609755516
epoch£º596	 i:1 	 global-step:11921	 l-p:0.1255771368741989
epoch£º596	 i:2 	 global-step:11922	 l-p:0.1278754621744156
epoch£º596	 i:3 	 global-step:11923	 l-p:0.11827333271503448
epoch£º596	 i:4 	 global-step:11924	 l-p:0.13469071686267853
epoch£º596	 i:5 	 global-step:11925	 l-p:0.21456289291381836
epoch£º596	 i:6 	 global-step:11926	 l-p:0.17330695688724518
epoch£º596	 i:7 	 global-step:11927	 l-p:0.16146428883075714
epoch£º596	 i:8 	 global-step:11928	 l-p:0.13869188725948334
epoch£º596	 i:9 	 global-step:11929	 l-p:0.12231466919183731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0716, 2.3616, 2.4733],
        [3.0716, 2.9782, 3.0553],
        [3.0716, 3.0714, 3.0716],
        [3.0716, 3.0656, 3.0714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.18391910195350647 
model_pd.l_d.mean(): -25.00241470336914 
model_pd.lagr.mean(): -24.81849479675293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0421], device='cuda:0')), ('power', tensor([-24.9604], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.18391910195350647
epoch£º597	 i:1 	 global-step:11941	 l-p:0.12894678115844727
epoch£º597	 i:2 	 global-step:11942	 l-p:0.19507206976413727
epoch£º597	 i:3 	 global-step:11943	 l-p:0.10000072419643402
epoch£º597	 i:4 	 global-step:11944	 l-p:0.17770858108997345
epoch£º597	 i:5 	 global-step:11945	 l-p:0.14540930092334747
epoch£º597	 i:6 	 global-step:11946	 l-p:0.14091695845127106
epoch£º597	 i:7 	 global-step:11947	 l-p:0.12863369286060333
epoch£º597	 i:8 	 global-step:11948	 l-p:0.11164199560880661
epoch£º597	 i:9 	 global-step:11949	 l-p:0.11202370375394821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1439, 3.1411, 3.1439],
        [3.1439, 1.9748, 1.3453],
        [3.1439, 2.5624, 2.7293],
        [3.1439, 2.1818, 1.5190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.10127949714660645 
model_pd.l_d.mean(): -24.602529525756836 
model_pd.lagr.mean(): -24.501249313354492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0175], device='cuda:0')), ('power', tensor([-24.6200], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.10127949714660645
epoch£º598	 i:1 	 global-step:11961	 l-p:0.037103183567523956
epoch£º598	 i:2 	 global-step:11962	 l-p:0.15368345379829407
epoch£º598	 i:3 	 global-step:11963	 l-p:0.1272878795862198
epoch£º598	 i:4 	 global-step:11964	 l-p:0.23118789494037628
epoch£º598	 i:5 	 global-step:11965	 l-p:0.15913473069667816
epoch£º598	 i:6 	 global-step:11966	 l-p:0.13208827376365662
epoch£º598	 i:7 	 global-step:11967	 l-p:0.14173568785190582
epoch£º598	 i:8 	 global-step:11968	 l-p:0.16426457464694977
epoch£º598	 i:9 	 global-step:11969	 l-p:0.14730609953403473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8622, 2.8622, 2.8622],
        [2.8622, 2.2321, 2.3917],
        [2.8622, 1.7803, 1.1854],
        [2.8622, 2.8481, 2.8614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.48698171973228455 
model_pd.l_d.mean(): -24.855934143066406 
model_pd.lagr.mean(): -24.36895179748535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2035], device='cuda:0')), ('power', tensor([-25.0594], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.48698171973228455
epoch£º599	 i:1 	 global-step:11981	 l-p:0.12651562690734863
epoch£º599	 i:2 	 global-step:11982	 l-p:0.1492035686969757
epoch£º599	 i:3 	 global-step:11983	 l-p:0.134139284491539
epoch£º599	 i:4 	 global-step:11984	 l-p:0.08515022695064545
epoch£º599	 i:5 	 global-step:11985	 l-p:0.13593238592147827
epoch£º599	 i:6 	 global-step:11986	 l-p:1.0189706087112427
epoch£º599	 i:7 	 global-step:11987	 l-p:0.18411999940872192
epoch£º599	 i:8 	 global-step:11988	 l-p:0.15223945677280426
epoch£º599	 i:9 	 global-step:11989	 l-p:0.12888360023498535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0166, 1.8129, 1.2204],
        [3.0166, 3.0166, 3.0166],
        [3.0166, 3.0166, 3.0166],
        [3.0166, 3.0166, 3.0166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.13577744364738464 
model_pd.l_d.mean(): -24.78181266784668 
model_pd.lagr.mean(): -24.64603614807129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0276], device='cuda:0')), ('power', tensor([-24.8095], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.13577744364738464
epoch£º600	 i:1 	 global-step:12001	 l-p:0.12078263610601425
epoch£º600	 i:2 	 global-step:12002	 l-p:0.8387343883514404
epoch£º600	 i:3 	 global-step:12003	 l-p:0.18723037838935852
epoch£º600	 i:4 	 global-step:12004	 l-p:0.147627055644989
epoch£º600	 i:5 	 global-step:12005	 l-p:0.14948919415473938
epoch£º600	 i:6 	 global-step:12006	 l-p:0.15403713285923004
epoch£º600	 i:7 	 global-step:12007	 l-p:0.14241889119148254
epoch£º600	 i:8 	 global-step:12008	 l-p:0.10813020169734955
epoch£º600	 i:9 	 global-step:12009	 l-p:0.11906073242425919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1192, 1.8734, 1.2861],
        [3.1192, 2.0049, 1.6814],
        [3.1192, 3.0433, 3.1077],
        [3.1192, 3.1169, 3.1191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.1434970498085022 
model_pd.l_d.mean(): -25.092575073242188 
model_pd.lagr.mean(): -24.949077606201172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0805], device='cuda:0')), ('power', tensor([-25.0120], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.1434970498085022
epoch£º601	 i:1 	 global-step:12021	 l-p:0.1226564571261406
epoch£º601	 i:2 	 global-step:12022	 l-p:0.19770510494709015
epoch£º601	 i:3 	 global-step:12023	 l-p:0.13763470947742462
epoch£º601	 i:4 	 global-step:12024	 l-p:0.13027063012123108
epoch£º601	 i:5 	 global-step:12025	 l-p:0.17195360362529755
epoch£º601	 i:6 	 global-step:12026	 l-p:0.16019824147224426
epoch£º601	 i:7 	 global-step:12027	 l-p:0.06802379339933395
epoch£º601	 i:8 	 global-step:12028	 l-p:-0.20217818021774292
epoch£º601	 i:9 	 global-step:12029	 l-p:0.11857697367668152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9903, 2.9903, 2.9903],
        [2.9903, 1.7781, 1.1953],
        [2.9903, 2.8991, 2.9747],
        [2.9903, 1.7597, 1.1898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): -0.40395715832710266 
model_pd.l_d.mean(): -25.060060501098633 
model_pd.lagr.mean(): -25.464017868041992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0610], device='cuda:0')), ('power', tensor([-25.1211], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:-0.40395715832710266
epoch£º602	 i:1 	 global-step:12041	 l-p:0.16996848583221436
epoch£º602	 i:2 	 global-step:12042	 l-p:0.11861104518175125
epoch£º602	 i:3 	 global-step:12043	 l-p:0.11085176467895508
epoch£º602	 i:4 	 global-step:12044	 l-p:0.06631934642791748
epoch£º602	 i:5 	 global-step:12045	 l-p:0.16276904940605164
epoch£º602	 i:6 	 global-step:12046	 l-p:1.211359977722168
epoch£º602	 i:7 	 global-step:12047	 l-p:0.16094890236854553
epoch£º602	 i:8 	 global-step:12048	 l-p:0.1460280567407608
epoch£º602	 i:9 	 global-step:12049	 l-p:0.14910094439983368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0662, 2.5426, 2.7279],
        [3.0662, 3.0257, 3.0622],
        [3.0662, 3.0659, 3.0662],
        [3.0662, 2.5834, 2.7746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.1576695591211319 
model_pd.l_d.mean(): -25.180212020874023 
model_pd.lagr.mean(): -25.02254295349121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0570], device='cuda:0')), ('power', tensor([-25.1232], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.1576695591211319
epoch£º603	 i:1 	 global-step:12061	 l-p:0.1511824131011963
epoch£º603	 i:2 	 global-step:12062	 l-p:0.13012704253196716
epoch£º603	 i:3 	 global-step:12063	 l-p:0.13381806015968323
epoch£º603	 i:4 	 global-step:12064	 l-p:0.557881772518158
epoch£º603	 i:5 	 global-step:12065	 l-p:0.07220081984996796
epoch£º603	 i:6 	 global-step:12066	 l-p:0.15473033487796783
epoch£º603	 i:7 	 global-step:12067	 l-p:0.32714182138442993
epoch£º603	 i:8 	 global-step:12068	 l-p:0.12591926753520966
epoch£º603	 i:9 	 global-step:12069	 l-p:0.1317615509033203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9323, 1.8641, 1.2518],
        [2.9323, 2.9323, 2.9323],
        [2.9323, 2.6698, 2.8358],
        [2.9323, 2.0428, 2.0267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.030049189925193787 
model_pd.l_d.mean(): -25.105514526367188 
model_pd.lagr.mean(): -25.07546615600586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0424], device='cuda:0')), ('power', tensor([-25.1479], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:0.030049189925193787
epoch£º604	 i:1 	 global-step:12081	 l-p:0.2012055665254593
epoch£º604	 i:2 	 global-step:12082	 l-p:0.15415696799755096
epoch£º604	 i:3 	 global-step:12083	 l-p:0.1351887583732605
epoch£º604	 i:4 	 global-step:12084	 l-p:0.1685742735862732
epoch£º604	 i:5 	 global-step:12085	 l-p:0.12132318317890167
epoch£º604	 i:6 	 global-step:12086	 l-p:-0.3365274667739868
epoch£º604	 i:7 	 global-step:12087	 l-p:0.11131362617015839
epoch£º604	 i:8 	 global-step:12088	 l-p:0.15023821592330933
epoch£º604	 i:9 	 global-step:12089	 l-p:0.1268976926803589
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0226, 1.7777, 1.2167],
        [3.0226, 2.5626, 2.7566],
        [3.0226, 2.8062, 2.9536],
        [3.0226, 2.8486, 2.9754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.10865875333547592 
model_pd.l_d.mean(): -25.055397033691406 
model_pd.lagr.mean(): -24.946739196777344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0045], device='cuda:0')), ('power', tensor([-25.0599], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.10865875333547592
epoch£º605	 i:1 	 global-step:12101	 l-p:-0.12329423427581787
epoch£º605	 i:2 	 global-step:12102	 l-p:0.14102058112621307
epoch£º605	 i:3 	 global-step:12103	 l-p:0.1350051909685135
epoch£º605	 i:4 	 global-step:12104	 l-p:0.28804585337638855
epoch£º605	 i:5 	 global-step:12105	 l-p:0.15467225015163422
epoch£º605	 i:6 	 global-step:12106	 l-p:0.1160954162478447
epoch£º605	 i:7 	 global-step:12107	 l-p:0.24224083125591278
epoch£º605	 i:8 	 global-step:12108	 l-p:0.13847079873085022
epoch£º605	 i:9 	 global-step:12109	 l-p:-0.1333724409341812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9607, 2.9608, 2.9608],
        [2.9607, 1.8722, 1.2589],
        [2.9607, 2.0751, 2.0613],
        [2.9607, 2.9595, 2.9607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.10406877100467682 
model_pd.l_d.mean(): -25.20066261291504 
model_pd.lagr.mean(): -25.096593856811523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0235], device='cuda:0')), ('power', tensor([-25.2241], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.10406877100467682
epoch£º606	 i:1 	 global-step:12121	 l-p:0.15267862379550934
epoch£º606	 i:2 	 global-step:12122	 l-p:0.14025016129016876
epoch£º606	 i:3 	 global-step:12123	 l-p:0.076725535094738
epoch£º606	 i:4 	 global-step:12124	 l-p:0.13689526915550232
epoch£º606	 i:5 	 global-step:12125	 l-p:0.16360078752040863
epoch£º606	 i:6 	 global-step:12126	 l-p:0.14988067746162415
epoch£º606	 i:7 	 global-step:12127	 l-p:0.1429445743560791
epoch£º606	 i:8 	 global-step:12128	 l-p:0.1336836963891983
epoch£º606	 i:9 	 global-step:12129	 l-p:0.15679776668548584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0312, 2.9969, 3.0282],
        [3.0312, 2.0761, 1.9832],
        [3.0312, 3.0263, 3.0311],
        [3.0312, 2.2017, 2.2348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.14697258174419403 
model_pd.l_d.mean(): -25.06308364868164 
model_pd.lagr.mean(): -24.91611099243164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0255], device='cuda:0')), ('power', tensor([-25.0885], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.14697258174419403
epoch£º607	 i:1 	 global-step:12141	 l-p:0.132674440741539
epoch£º607	 i:2 	 global-step:12142	 l-p:0.09814222157001495
epoch£º607	 i:3 	 global-step:12143	 l-p:0.1395561248064041
epoch£º607	 i:4 	 global-step:12144	 l-p:0.22024041414260864
epoch£º607	 i:5 	 global-step:12145	 l-p:0.03446014225482941
epoch£º607	 i:6 	 global-step:12146	 l-p:0.2716224193572998
epoch£º607	 i:7 	 global-step:12147	 l-p:0.11138379573822021
epoch£º607	 i:8 	 global-step:12148	 l-p:-0.798987865447998
epoch£º607	 i:9 	 global-step:12149	 l-p:0.14013764262199402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0550, 3.0498, 3.0549],
        [3.0550, 3.0346, 3.0537],
        [3.0550, 2.8072, 2.9673],
        [3.0550, 1.9459, 1.3192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.14262597262859344 
model_pd.l_d.mean(): -24.77884292602539 
model_pd.lagr.mean(): -24.63621711730957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1284], device='cuda:0')), ('power', tensor([-24.9072], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.14262597262859344
epoch£º608	 i:1 	 global-step:12161	 l-p:0.13792921602725983
epoch£º608	 i:2 	 global-step:12162	 l-p:0.18368752300739288
epoch£º608	 i:3 	 global-step:12163	 l-p:0.14228680729866028
epoch£º608	 i:4 	 global-step:12164	 l-p:0.476252019405365
epoch£º608	 i:5 	 global-step:12165	 l-p:0.5923053622245789
epoch£º608	 i:6 	 global-step:12166	 l-p:0.1364685744047165
epoch£º608	 i:7 	 global-step:12167	 l-p:0.21830274164676666
epoch£º608	 i:8 	 global-step:12168	 l-p:0.1375114768743515
epoch£º608	 i:9 	 global-step:12169	 l-p:0.09184184670448303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0646, 3.0646, 3.0646],
        [3.0646, 2.9998, 3.0558],
        [3.0646, 1.9548, 1.3263],
        [3.0646, 2.5953, 2.7883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.29293134808540344 
model_pd.l_d.mean(): -24.59300994873047 
model_pd.lagr.mean(): -24.300079345703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1056], device='cuda:0')), ('power', tensor([-24.6986], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.29293134808540344
epoch£º609	 i:1 	 global-step:12181	 l-p:0.13578547537326813
epoch£º609	 i:2 	 global-step:12182	 l-p:0.12336190789937973
epoch£º609	 i:3 	 global-step:12183	 l-p:0.12813079357147217
epoch£º609	 i:4 	 global-step:12184	 l-p:0.08025829493999481
epoch£º609	 i:5 	 global-step:12185	 l-p:0.13767312467098236
epoch£º609	 i:6 	 global-step:12186	 l-p:0.14201803505420685
epoch£º609	 i:7 	 global-step:12187	 l-p:0.13504311442375183
epoch£º609	 i:8 	 global-step:12188	 l-p:0.25704431533813477
epoch£º609	 i:9 	 global-step:12189	 l-p:0.014111017808318138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0122, 2.5411, 2.7350],
        [3.0122, 3.0109, 3.0121],
        [3.0122, 3.0120, 3.0122],
        [3.0122, 2.8826, 2.9839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.3284717798233032 
model_pd.l_d.mean(): -25.029014587402344 
model_pd.lagr.mean(): -24.700542449951172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0140], device='cuda:0')), ('power', tensor([-25.0430], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.3284717798233032
epoch£º610	 i:1 	 global-step:12201	 l-p:0.14646092057228088
epoch£º610	 i:2 	 global-step:12202	 l-p:0.10865577310323715
epoch£º610	 i:3 	 global-step:12203	 l-p:0.03624236583709717
epoch£º610	 i:4 	 global-step:12204	 l-p:4.6614556312561035
epoch£º610	 i:5 	 global-step:12205	 l-p:0.3473873734474182
epoch£º610	 i:6 	 global-step:12206	 l-p:0.12976135313510895
epoch£º610	 i:7 	 global-step:12207	 l-p:0.15458473563194275
epoch£º610	 i:8 	 global-step:12208	 l-p:0.1473987102508545
epoch£º610	 i:9 	 global-step:12209	 l-p:0.1383511871099472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0726, 2.1715, 2.1360],
        [3.0726, 2.0060, 1.3681],
        [3.0726, 2.1665, 1.5035],
        [3.0726, 3.0688, 3.0726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.14926311373710632 
model_pd.l_d.mean(): -25.00770378112793 
model_pd.lagr.mean(): -24.858440399169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1173], device='cuda:0')), ('power', tensor([-24.8904], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.14926311373710632
epoch£º611	 i:1 	 global-step:12221	 l-p:0.11930707842111588
epoch£º611	 i:2 	 global-step:12222	 l-p:0.12503425776958466
epoch£º611	 i:3 	 global-step:12223	 l-p:0.14737461507320404
epoch£º611	 i:4 	 global-step:12224	 l-p:0.13470622897148132
epoch£º611	 i:5 	 global-step:12225	 l-p:0.29091182351112366
epoch£º611	 i:6 	 global-step:12226	 l-p:0.1323605179786682
epoch£º611	 i:7 	 global-step:12227	 l-p:0.13167442381381989
epoch£º611	 i:8 	 global-step:12228	 l-p:0.05176619440317154
epoch£º611	 i:9 	 global-step:12229	 l-p:0.16295495629310608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9841, 1.7404, 1.1870],
        [2.9841, 2.9840, 2.9841],
        [2.9841, 2.1677, 2.2155],
        [2.9841, 2.9809, 2.9841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.15538296103477478 
model_pd.l_d.mean(): -24.77610969543457 
model_pd.lagr.mean(): -24.6207275390625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0915], device='cuda:0')), ('power', tensor([-24.8676], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.15538296103477478
epoch£º612	 i:1 	 global-step:12241	 l-p:0.14237608015537262
epoch£º612	 i:2 	 global-step:12242	 l-p:0.09859587252140045
epoch£º612	 i:3 	 global-step:12243	 l-p:0.1442776918411255
epoch£º612	 i:4 	 global-step:12244	 l-p:0.18276739120483398
epoch£º612	 i:5 	 global-step:12245	 l-p:0.4674730598926544
epoch£º612	 i:6 	 global-step:12246	 l-p:0.09581400454044342
epoch£º612	 i:7 	 global-step:12247	 l-p:0.11116944998502731
epoch£º612	 i:8 	 global-step:12248	 l-p:0.02435862459242344
epoch£º612	 i:9 	 global-step:12249	 l-p:0.15844900906085968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8596, 2.8592, 2.8597],
        [2.8596, 2.7674, 2.8439],
        [2.8596, 2.8480, 2.8591],
        [2.8596, 2.6363, 2.7875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 1.805106282234192 
model_pd.l_d.mean(): -25.18286895751953 
model_pd.lagr.mean(): -23.377761840820312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1099], device='cuda:0')), ('power', tensor([-25.2927], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:1.805106282234192
epoch£º613	 i:1 	 global-step:12261	 l-p:0.49395468831062317
epoch£º613	 i:2 	 global-step:12262	 l-p:0.13150419294834137
epoch£º613	 i:3 	 global-step:12263	 l-p:0.06822909414768219
epoch£º613	 i:4 	 global-step:12264	 l-p:0.11660148203372955
epoch£º613	 i:5 	 global-step:12265	 l-p:0.12981899082660675
epoch£º613	 i:6 	 global-step:12266	 l-p:0.1501692682504654
epoch£º613	 i:7 	 global-step:12267	 l-p:-0.12151186913251877
epoch£º613	 i:8 	 global-step:12268	 l-p:0.06404215842485428
epoch£º613	 i:9 	 global-step:12269	 l-p:0.11231648921966553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0500, 1.8007, 1.2713],
        [3.0500, 2.0467, 1.8953],
        [3.0500, 3.0378, 3.0494],
        [3.0500, 3.0440, 3.0498]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.13397172093391418 
model_pd.l_d.mean(): -25.053773880004883 
model_pd.lagr.mean(): -24.919801712036133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0208], device='cuda:0')), ('power', tensor([-25.0745], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.13397172093391418
epoch£º614	 i:1 	 global-step:12281	 l-p:0.14052563905715942
epoch£º614	 i:2 	 global-step:12282	 l-p:0.20740218460559845
epoch£º614	 i:3 	 global-step:12283	 l-p:0.12619875371456146
epoch£º614	 i:4 	 global-step:12284	 l-p:0.12152726948261261
epoch£º614	 i:5 	 global-step:12285	 l-p:0.1304829865694046
epoch£º614	 i:6 	 global-step:12286	 l-p:0.14970555901527405
epoch£º614	 i:7 	 global-step:12287	 l-p:0.15662753582000732
epoch£º614	 i:8 	 global-step:12288	 l-p:0.25427141785621643
epoch£º614	 i:9 	 global-step:12289	 l-p:0.2154453545808792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0645, 2.0142, 1.7986],
        [3.0645, 2.0792, 1.9496],
        [3.0645, 1.8516, 1.2497],
        [3.0645, 3.0645, 3.0645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.13168810307979584 
model_pd.l_d.mean(): -24.571674346923828 
model_pd.lagr.mean(): -24.439987182617188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0444], device='cuda:0')), ('power', tensor([-24.6161], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.13168810307979584
epoch£º615	 i:1 	 global-step:12301	 l-p:0.14078707993030548
epoch£º615	 i:2 	 global-step:12302	 l-p:0.20067709684371948
epoch£º615	 i:3 	 global-step:12303	 l-p:0.16040335595607758
epoch£º615	 i:4 	 global-step:12304	 l-p:0.12986035645008087
epoch£º615	 i:5 	 global-step:12305	 l-p:0.13029544055461884
epoch£º615	 i:6 	 global-step:12306	 l-p:0.14261335134506226
epoch£º615	 i:7 	 global-step:12307	 l-p:0.15154987573623657
epoch£º615	 i:8 	 global-step:12308	 l-p:0.20930157601833344
epoch£º615	 i:9 	 global-step:12309	 l-p:1.137529730796814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0419, 3.0345, 3.0416],
        [3.0419, 2.9185, 3.0159],
        [3.0419, 1.8967, 1.2794],
        [3.0419, 1.8196, 1.3516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.1264631748199463 
model_pd.l_d.mean(): -24.448074340820312 
model_pd.lagr.mean(): -24.321611404418945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1795], device='cuda:0')), ('power', tensor([-24.6276], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.1264631748199463
epoch£º616	 i:1 	 global-step:12321	 l-p:0.14252696931362152
epoch£º616	 i:2 	 global-step:12322	 l-p:0.13289085030555725
epoch£º616	 i:3 	 global-step:12323	 l-p:0.2491486668586731
epoch£º616	 i:4 	 global-step:12324	 l-p:0.15891607105731964
epoch£º616	 i:5 	 global-step:12325	 l-p:-0.19593445956707
epoch£º616	 i:6 	 global-step:12326	 l-p:0.12929585576057434
epoch£º616	 i:7 	 global-step:12327	 l-p:0.09507545083761215
epoch£º616	 i:8 	 global-step:12328	 l-p:0.08342137187719345
epoch£º616	 i:9 	 global-step:12329	 l-p:0.12123867124319077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0133, 2.7806, 2.9351],
        [3.0133, 2.3779, 2.5332],
        [3.0133, 2.1258, 1.4670],
        [3.0133, 2.7851, 2.9377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.13665345311164856 
model_pd.l_d.mean(): -25.018814086914062 
model_pd.lagr.mean(): -24.882160186767578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0436], device='cuda:0')), ('power', tensor([-25.0624], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.13665345311164856
epoch£º617	 i:1 	 global-step:12341	 l-p:0.20693781971931458
epoch£º617	 i:2 	 global-step:12342	 l-p:0.15607070922851562
epoch£º617	 i:3 	 global-step:12343	 l-p:0.0548149049282074
epoch£º617	 i:4 	 global-step:12344	 l-p:0.1300085186958313
epoch£º617	 i:5 	 global-step:12345	 l-p:0.13571175932884216
epoch£º617	 i:6 	 global-step:12346	 l-p:0.2922440469264984
epoch£º617	 i:7 	 global-step:12347	 l-p:0.15894043445587158
epoch£º617	 i:8 	 global-step:12348	 l-p:0.11891911923885345
epoch£º617	 i:9 	 global-step:12349	 l-p:-0.10765542089939117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0221, 1.9871, 1.7974],
        [3.0221, 2.7840, 2.9407],
        [3.0221, 2.1188, 1.4613],
        [3.0221, 2.2011, 2.2442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.11550366878509521 
model_pd.l_d.mean(): -25.020273208618164 
model_pd.lagr.mean(): -24.904769897460938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0384], device='cuda:0')), ('power', tensor([-25.0587], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.11550366878509521
epoch£º618	 i:1 	 global-step:12361	 l-p:0.1664254367351532
epoch£º618	 i:2 	 global-step:12362	 l-p:0.047601211816072464
epoch£º618	 i:3 	 global-step:12363	 l-p:0.1820075511932373
epoch£º618	 i:4 	 global-step:12364	 l-p:0.24978573620319366
epoch£º618	 i:5 	 global-step:12365	 l-p:0.1318313181400299
epoch£º618	 i:6 	 global-step:12366	 l-p:0.0008640098385512829
epoch£º618	 i:7 	 global-step:12367	 l-p:0.15374380350112915
epoch£º618	 i:8 	 global-step:12368	 l-p:0.145167738199234
epoch£º618	 i:9 	 global-step:12369	 l-p:0.17006805539131165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0552, 2.8274, 2.9798],
        [3.0552, 2.5766, 2.7696],
        [3.0552, 2.5565, 2.7473],
        [3.0552, 1.8703, 1.2601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.267804890871048 
model_pd.l_d.mean(): -25.076274871826172 
model_pd.lagr.mean(): -24.808469772338867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0841], device='cuda:0')), ('power', tensor([-25.1604], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.267804890871048
epoch£º619	 i:1 	 global-step:12381	 l-p:0.07879681885242462
epoch£º619	 i:2 	 global-step:12382	 l-p:0.11471286416053772
epoch£º619	 i:3 	 global-step:12383	 l-p:0.11442579329013824
epoch£º619	 i:4 	 global-step:12384	 l-p:0.14829164743423462
epoch£º619	 i:5 	 global-step:12385	 l-p:0.17999641597270966
epoch£º619	 i:6 	 global-step:12386	 l-p:0.12313970178365707
epoch£º619	 i:7 	 global-step:12387	 l-p:0.14849920570850372
epoch£º619	 i:8 	 global-step:12388	 l-p:0.14934086799621582
epoch£º619	 i:9 	 global-step:12389	 l-p:0.1350032240152359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0858, 3.0021, 3.0723],
        [3.0858, 2.4678, 2.6276],
        [3.0858, 3.0858, 3.0858],
        [3.0858, 1.8385, 1.2543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.19271183013916016 
model_pd.l_d.mean(): -25.262611389160156 
model_pd.lagr.mean(): -25.069900512695312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0081], device='cuda:0')), ('power', tensor([-25.2545], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.19271183013916016
epoch£º620	 i:1 	 global-step:12401	 l-p:0.19584757089614868
epoch£º620	 i:2 	 global-step:12402	 l-p:0.12428662925958633
epoch£º620	 i:3 	 global-step:12403	 l-p:0.14656925201416016
epoch£º620	 i:4 	 global-step:12404	 l-p:0.12383932620286942
epoch£º620	 i:5 	 global-step:12405	 l-p:0.16549214720726013
epoch£º620	 i:6 	 global-step:12406	 l-p:0.1344873160123825
epoch£º620	 i:7 	 global-step:12407	 l-p:0.10641458630561829
epoch£º620	 i:8 	 global-step:12408	 l-p:0.1311408430337906
epoch£º620	 i:9 	 global-step:12409	 l-p:0.4335481822490692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0408, 1.8329, 1.2336],
        [3.0408, 3.0377, 3.0407],
        [3.0408, 3.0408, 3.0408],
        [3.0408, 3.0407, 3.0408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.1395104080438614 
model_pd.l_d.mean(): -25.203777313232422 
model_pd.lagr.mean(): -25.064266204833984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0718], device='cuda:0')), ('power', tensor([-25.1320], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.1395104080438614
epoch£º621	 i:1 	 global-step:12421	 l-p:0.5942619442939758
epoch£º621	 i:2 	 global-step:12422	 l-p:0.2020711898803711
epoch£º621	 i:3 	 global-step:12423	 l-p:0.7303900718688965
epoch£º621	 i:4 	 global-step:12424	 l-p:0.15042045712471008
epoch£º621	 i:5 	 global-step:12425	 l-p:0.13005433976650238
epoch£º621	 i:6 	 global-step:12426	 l-p:0.1064143180847168
epoch£º621	 i:7 	 global-step:12427	 l-p:0.12462024390697479
epoch£º621	 i:8 	 global-step:12428	 l-p:0.13940921425819397
epoch£º621	 i:9 	 global-step:12429	 l-p:0.21710887551307678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0677, 3.0677, 3.0677],
        [3.0677, 2.1818, 2.1645],
        [3.0677, 1.8156, 1.2826],
        [3.0677, 2.3727, 2.4984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.1465885490179062 
model_pd.l_d.mean(): -25.17311668395996 
model_pd.lagr.mean(): -25.026527404785156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0406], device='cuda:0')), ('power', tensor([-25.1325], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.1465885490179062
epoch£º622	 i:1 	 global-step:12441	 l-p:0.12929297983646393
epoch£º622	 i:2 	 global-step:12442	 l-p:0.42724621295928955
epoch£º622	 i:3 	 global-step:12443	 l-p:0.170266255736351
epoch£º622	 i:4 	 global-step:12444	 l-p:0.12093552947044373
epoch£º622	 i:5 	 global-step:12445	 l-p:0.2093750387430191
epoch£º622	 i:6 	 global-step:12446	 l-p:0.14153948426246643
epoch£º622	 i:7 	 global-step:12447	 l-p:0.14814864099025726
epoch£º622	 i:8 	 global-step:12448	 l-p:-0.05401230603456497
epoch£º622	 i:9 	 global-step:12449	 l-p:0.04592778906226158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0193, 2.8926, 2.9922],
        [3.0193, 2.5468, 2.7413],
        [3.0193, 2.9963, 3.0177],
        [3.0193, 3.0188, 3.0193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.1475740671157837 
model_pd.l_d.mean(): -24.96625518798828 
model_pd.lagr.mean(): -24.818681716918945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0411], device='cuda:0')), ('power', tensor([-24.9251], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.1475740671157837
epoch£º623	 i:1 	 global-step:12461	 l-p:0.05985094979405403
epoch£º623	 i:2 	 global-step:12462	 l-p:0.15343299508094788
epoch£º623	 i:3 	 global-step:12463	 l-p:0.14942137897014618
epoch£º623	 i:4 	 global-step:12464	 l-p:-0.06707841902971268
epoch£º623	 i:5 	 global-step:12465	 l-p:0.18219928443431854
epoch£º623	 i:6 	 global-step:12466	 l-p:0.11581473797559738
epoch£º623	 i:7 	 global-step:12467	 l-p:0.1645556390285492
epoch£º623	 i:8 	 global-step:12468	 l-p:0.13815471529960632
epoch£º623	 i:9 	 global-step:12469	 l-p:0.191647469997406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0859, 3.0559, 3.0834],
        [3.0859, 2.0820, 1.4308],
        [3.0859, 3.0798, 3.0857],
        [3.0859, 3.0859, 3.0859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.14162412285804749 
model_pd.l_d.mean(): -25.039167404174805 
model_pd.lagr.mean(): -24.89754295349121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0212], device='cuda:0')), ('power', tensor([-25.0180], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.14162412285804749
epoch£º624	 i:1 	 global-step:12481	 l-p:0.1270262449979782
epoch£º624	 i:2 	 global-step:12482	 l-p:0.13864350318908691
epoch£º624	 i:3 	 global-step:12483	 l-p:0.22569753229618073
epoch£º624	 i:4 	 global-step:12484	 l-p:0.10144772380590439
epoch£º624	 i:5 	 global-step:12485	 l-p:0.121223084628582
epoch£º624	 i:6 	 global-step:12486	 l-p:0.1315581053495407
epoch£º624	 i:7 	 global-step:12487	 l-p:0.14501775801181793
epoch£º624	 i:8 	 global-step:12488	 l-p:0.19409915804862976
epoch£º624	 i:9 	 global-step:12489	 l-p:0.21890142560005188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0851, 3.0851, 3.0851],
        [3.0851, 3.0405, 3.0804],
        [3.0851, 3.0808, 3.0850],
        [3.0851, 1.9486, 1.6066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.16558575630187988 
model_pd.l_d.mean(): -25.00728988647461 
model_pd.lagr.mean(): -24.841703414916992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0252], device='cuda:0')), ('power', tensor([-25.0325], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.16558575630187988
epoch£º625	 i:1 	 global-step:12501	 l-p:0.01247047632932663
epoch£º625	 i:2 	 global-step:12502	 l-p:0.14731499552726746
epoch£º625	 i:3 	 global-step:12503	 l-p:0.1316334307193756
epoch£º625	 i:4 	 global-step:12504	 l-p:0.15066120028495789
epoch£º625	 i:5 	 global-step:12505	 l-p:0.13007239997386932
epoch£º625	 i:6 	 global-step:12506	 l-p:0.16708560287952423
epoch£º625	 i:7 	 global-step:12507	 l-p:0.11389341205358505
epoch£º625	 i:8 	 global-step:12508	 l-p:0.12572547793388367
epoch£º625	 i:9 	 global-step:12509	 l-p:0.15763063728809357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1077, 2.9509, 3.0683],
        [3.1077, 2.2123, 2.1835],
        [3.1077, 3.1077, 3.1077],
        [3.1077, 2.1177, 1.4610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.13990256190299988 
model_pd.l_d.mean(): -25.15777015686035 
model_pd.lagr.mean(): -25.017868041992188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0956], device='cuda:0')), ('power', tensor([-25.0622], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.13990256190299988
epoch£º626	 i:1 	 global-step:12521	 l-p:0.1718856394290924
epoch£º626	 i:2 	 global-step:12522	 l-p:0.10387910902500153
epoch£º626	 i:3 	 global-step:12523	 l-p:0.18099485337734222
epoch£º626	 i:4 	 global-step:12524	 l-p:0.15310649573802948
epoch£º626	 i:5 	 global-step:12525	 l-p:0.10230105370283127
epoch£º626	 i:6 	 global-step:12526	 l-p:0.13709357380867004
epoch£º626	 i:7 	 global-step:12527	 l-p:0.1416923850774765
epoch£º626	 i:8 	 global-step:12528	 l-p:0.11757749319076538
epoch£º626	 i:9 	 global-step:12529	 l-p:0.1248599961400032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1020, 2.7160, 2.9086],
        [3.1020, 1.9589, 1.6057],
        [3.1020, 2.7493, 2.9377],
        [3.1020, 2.9841, 3.0779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.14818264544010162 
model_pd.l_d.mean(): -25.099424362182617 
model_pd.lagr.mean(): -24.951242446899414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1136], device='cuda:0')), ('power', tensor([-24.9858], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.14818264544010162
epoch£º627	 i:1 	 global-step:12541	 l-p:0.14654035866260529
epoch£º627	 i:2 	 global-step:12542	 l-p:0.15136998891830444
epoch£º627	 i:3 	 global-step:12543	 l-p:0.09318932145833969
epoch£º627	 i:4 	 global-step:12544	 l-p:0.12573356926441193
epoch£º627	 i:5 	 global-step:12545	 l-p:-0.107660211622715
epoch£º627	 i:6 	 global-step:12546	 l-p:0.1455552577972412
epoch£º627	 i:7 	 global-step:12547	 l-p:0.292375773191452
epoch£º627	 i:8 	 global-step:12548	 l-p:0.15236037969589233
epoch£º627	 i:9 	 global-step:12549	 l-p:0.15326376259326935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9762, 1.9599, 1.8024],
        [2.9762, 2.9750, 2.9762],
        [2.9762, 2.9731, 2.9761],
        [2.9762, 2.7236, 2.8863]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.19022242724895477 
model_pd.l_d.mean(): -25.133394241333008 
model_pd.lagr.mean(): -24.943172454833984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0413], device='cuda:0')), ('power', tensor([-25.0920], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.19022242724895477
epoch£º628	 i:1 	 global-step:12561	 l-p:0.15174371004104614
epoch£º628	 i:2 	 global-step:12562	 l-p:0.1935243159532547
epoch£º628	 i:3 	 global-step:12563	 l-p:0.18732769787311554
epoch£º628	 i:4 	 global-step:12564	 l-p:0.09801483154296875
epoch£º628	 i:5 	 global-step:12565	 l-p:0.14735065400600433
epoch£º628	 i:6 	 global-step:12566	 l-p:0.14133475720882416
epoch£º628	 i:7 	 global-step:12567	 l-p:0.12185357511043549
epoch£º628	 i:8 	 global-step:12568	 l-p:0.1394309550523758
epoch£º628	 i:9 	 global-step:12569	 l-p:0.05115688219666481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9715, 2.9684, 2.9714],
        [2.9715, 2.0789, 2.0639],
        [2.9715, 2.9555, 2.9706],
        [2.9715, 1.8613, 1.2484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.1508631408214569 
model_pd.l_d.mean(): -25.313615798950195 
model_pd.lagr.mean(): -25.162752151489258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0551], device='cuda:0')), ('power', tensor([-25.2585], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.1508631408214569
epoch£º629	 i:1 	 global-step:12581	 l-p:-0.4622153043746948
epoch£º629	 i:2 	 global-step:12582	 l-p:0.1350860446691513
epoch£º629	 i:3 	 global-step:12583	 l-p:0.13434843719005585
epoch£º629	 i:4 	 global-step:12584	 l-p:0.16173633933067322
epoch£º629	 i:5 	 global-step:12585	 l-p:0.04670865088701248
epoch£º629	 i:6 	 global-step:12586	 l-p:-0.0879676416516304
epoch£º629	 i:7 	 global-step:12587	 l-p:0.1211635172367096
epoch£º629	 i:8 	 global-step:12588	 l-p:0.16680757701396942
epoch£º629	 i:9 	 global-step:12589	 l-p:0.20574058592319489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0216, 3.0215, 3.0216],
        [3.0216, 2.1324, 2.1178],
        [3.0216, 1.9644, 1.7491],
        [3.0216, 3.0216, 3.0216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.15145334601402283 
model_pd.l_d.mean(): -24.95844078063965 
model_pd.lagr.mean(): -24.806987762451172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0672], device='cuda:0')), ('power', tensor([-25.0256], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.15145334601402283
epoch£º630	 i:1 	 global-step:12601	 l-p:0.2715722918510437
epoch£º630	 i:2 	 global-step:12602	 l-p:0.1681114286184311
epoch£º630	 i:3 	 global-step:12603	 l-p:2.4498074054718018
epoch£º630	 i:4 	 global-step:12604	 l-p:0.16028594970703125
epoch£º630	 i:5 	 global-step:12605	 l-p:0.13424834609031677
epoch£º630	 i:6 	 global-step:12606	 l-p:0.12145625799894333
epoch£º630	 i:7 	 global-step:12607	 l-p:0.11810123175382614
epoch£º630	 i:8 	 global-step:12608	 l-p:0.13771957159042358
epoch£º630	 i:9 	 global-step:12609	 l-p:0.16322185099124908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1044, 3.0448, 3.0968],
        [3.1044, 1.8466, 1.2685],
        [3.1044, 3.1044, 3.1044],
        [3.1044, 2.5774, 2.7638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.16134129464626312 
model_pd.l_d.mean(): -25.192026138305664 
model_pd.lagr.mean(): -25.030685424804688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1353], device='cuda:0')), ('power', tensor([-25.0568], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.16134129464626312
epoch£º631	 i:1 	 global-step:12621	 l-p:0.1381971389055252
epoch£º631	 i:2 	 global-step:12622	 l-p:0.1473832130432129
epoch£º631	 i:3 	 global-step:12623	 l-p:0.12251004576683044
epoch£º631	 i:4 	 global-step:12624	 l-p:0.1853436380624771
epoch£º631	 i:5 	 global-step:12625	 l-p:0.13138319551944733
epoch£º631	 i:6 	 global-step:12626	 l-p:0.12506859004497528
epoch£º631	 i:7 	 global-step:12627	 l-p:0.08809962868690491
epoch£º631	 i:8 	 global-step:12628	 l-p:0.1286427527666092
epoch£º631	 i:9 	 global-step:12629	 l-p:0.25887298583984375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0572, 2.8522, 2.9947],
        [3.0572, 2.9967, 3.0494],
        [3.0572, 3.0389, 3.0561],
        [3.0572, 3.0428, 3.0564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.2682928740978241 
model_pd.l_d.mean(): -24.700407028198242 
model_pd.lagr.mean(): -24.432113647460938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0193], device='cuda:0')), ('power', tensor([-24.7197], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.2682928740978241
epoch£º632	 i:1 	 global-step:12641	 l-p:0.13378597795963287
epoch£º632	 i:2 	 global-step:12642	 l-p:0.131633922457695
epoch£º632	 i:3 	 global-step:12643	 l-p:0.14221204817295074
epoch£º632	 i:4 	 global-step:12644	 l-p:0.14664967358112335
epoch£º632	 i:5 	 global-step:12645	 l-p:0.13873395323753357
epoch£º632	 i:6 	 global-step:12646	 l-p:0.12390246987342834
epoch£º632	 i:7 	 global-step:12647	 l-p:0.0784681886434555
epoch£º632	 i:8 	 global-step:12648	 l-p:-0.2350328266620636
epoch£º632	 i:9 	 global-step:12649	 l-p:0.17563945055007935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9773, 2.9260, 2.9714],
        [2.9773, 1.8677, 1.5825],
        [2.9773, 2.5283, 2.7256],
        [2.9773, 2.1033, 1.4450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.14014142751693726 
model_pd.l_d.mean(): -24.915653228759766 
model_pd.lagr.mean(): -24.7755126953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1015], device='cuda:0')), ('power', tensor([-25.0171], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.14014142751693726
epoch£º633	 i:1 	 global-step:12661	 l-p:0.1298404484987259
epoch£º633	 i:2 	 global-step:12662	 l-p:0.27748754620552063
epoch£º633	 i:3 	 global-step:12663	 l-p:-0.06759311258792877
epoch£º633	 i:4 	 global-step:12664	 l-p:0.12920545041561127
epoch£º633	 i:5 	 global-step:12665	 l-p:0.14028121531009674
epoch£º633	 i:6 	 global-step:12666	 l-p:0.14397777616977692
epoch£º633	 i:7 	 global-step:12667	 l-p:0.156434565782547
epoch£º633	 i:8 	 global-step:12668	 l-p:0.04886180907487869
epoch£º633	 i:9 	 global-step:12669	 l-p:0.27737951278686523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0005, 3.0005, 3.0005],
        [3.0005, 2.9870, 2.9998],
        [3.0005, 2.5917, 2.7878],
        [3.0005, 2.6355, 2.8272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.12583084404468536 
model_pd.l_d.mean(): -24.904008865356445 
model_pd.lagr.mean(): -24.77817726135254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0481], device='cuda:0')), ('power', tensor([-24.9522], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.12583084404468536
epoch£º634	 i:1 	 global-step:12681	 l-p:0.15957219898700714
epoch£º634	 i:2 	 global-step:12682	 l-p:0.15737594664096832
epoch£º634	 i:3 	 global-step:12683	 l-p:0.17316122353076935
epoch£º634	 i:4 	 global-step:12684	 l-p:0.1535879671573639
epoch£º634	 i:5 	 global-step:12685	 l-p:0.14518652856349945
epoch£º634	 i:6 	 global-step:12686	 l-p:0.04924849793314934
epoch£º634	 i:7 	 global-step:12687	 l-p:-1.0922170877456665
epoch£º634	 i:8 	 global-step:12688	 l-p:0.08672083914279938
epoch£º634	 i:9 	 global-step:12689	 l-p:0.1488213837146759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0039, 2.3641, 2.5206],
        [3.0039, 2.4192, 2.5949],
        [3.0039, 3.0007, 3.0038],
        [3.0039, 1.7596, 1.2628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.15126755833625793 
model_pd.l_d.mean(): -25.151315689086914 
model_pd.lagr.mean(): -25.00004768371582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0431], device='cuda:0')), ('power', tensor([-25.1944], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.15126755833625793
epoch£º635	 i:1 	 global-step:12701	 l-p:0.13982708752155304
epoch£º635	 i:2 	 global-step:12702	 l-p:0.09172055870294571
epoch£º635	 i:3 	 global-step:12703	 l-p:0.1462162435054779
epoch£º635	 i:4 	 global-step:12704	 l-p:0.25711384415626526
epoch£º635	 i:5 	 global-step:12705	 l-p:5.291393756866455
epoch£º635	 i:6 	 global-step:12706	 l-p:0.05389271676540375
epoch£º635	 i:7 	 global-step:12707	 l-p:0.1298341304063797
epoch£º635	 i:8 	 global-step:12708	 l-p:-0.5676641464233398
epoch£º635	 i:9 	 global-step:12709	 l-p:0.15974397957324982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0720, 2.8430, 2.9961],
        [3.0720, 3.0083, 3.0635],
        [3.0720, 1.8140, 1.2420],
        [3.0720, 3.0716, 3.0720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.08816386759281158 
model_pd.l_d.mean(): -25.002059936523438 
model_pd.lagr.mean(): -24.913896560668945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0774], device='cuda:0')), ('power', tensor([-24.9247], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.08816386759281158
epoch£º636	 i:1 	 global-step:12721	 l-p:0.11831936240196228
epoch£º636	 i:2 	 global-step:12722	 l-p:0.15277718007564545
epoch£º636	 i:3 	 global-step:12723	 l-p:0.11285470426082611
epoch£º636	 i:4 	 global-step:12724	 l-p:0.16358867287635803
epoch£º636	 i:5 	 global-step:12725	 l-p:0.15799939632415771
epoch£º636	 i:6 	 global-step:12726	 l-p:0.12697167694568634
epoch£º636	 i:7 	 global-step:12727	 l-p:0.1347937434911728
epoch£º636	 i:8 	 global-step:12728	 l-p:0.14014658331871033
epoch£º636	 i:9 	 global-step:12729	 l-p:0.14359572529792786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1131, 2.2905, 2.3322],
        [3.1131, 2.2402, 1.5647],
        [3.1131, 2.5781, 2.7636],
        [3.1131, 3.0526, 3.1053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.1390332728624344 
model_pd.l_d.mean(): -24.70551300048828 
model_pd.lagr.mean(): -24.56648063659668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0101], device='cuda:0')), ('power', tensor([-24.7156], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.1390332728624344
epoch£º637	 i:1 	 global-step:12741	 l-p:0.11764427274465561
epoch£º637	 i:2 	 global-step:12742	 l-p:0.2591734528541565
epoch£º637	 i:3 	 global-step:12743	 l-p:0.14110754430294037
epoch£º637	 i:4 	 global-step:12744	 l-p:0.12291073799133301
epoch£º637	 i:5 	 global-step:12745	 l-p:0.13227081298828125
epoch£º637	 i:6 	 global-step:12746	 l-p:-0.12908218801021576
epoch£º637	 i:7 	 global-step:12747	 l-p:0.15275456011295319
epoch£º637	 i:8 	 global-step:12748	 l-p:0.23610176146030426
epoch£º637	 i:9 	 global-step:12749	 l-p:0.15327782928943634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9162, 2.9151, 2.9162],
        [2.9162, 2.9162, 2.9162],
        [2.9162, 1.8064, 1.2027],
        [2.9162, 1.9373, 1.3058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.18680037558078766 
model_pd.l_d.mean(): -25.180410385131836 
model_pd.lagr.mean(): -24.993610382080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2057], device='cuda:0')), ('power', tensor([-25.3861], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.18680037558078766
epoch£º638	 i:1 	 global-step:12761	 l-p:0.09973380714654922
epoch£º638	 i:2 	 global-step:12762	 l-p:0.043710868805646896
epoch£º638	 i:3 	 global-step:12763	 l-p:0.13163180649280548
epoch£º638	 i:4 	 global-step:12764	 l-p:0.15399731695652008
epoch£º638	 i:5 	 global-step:12765	 l-p:-0.1562110185623169
epoch£º638	 i:6 	 global-step:12766	 l-p:0.1487855613231659
epoch£º638	 i:7 	 global-step:12767	 l-p:0.14924322068691254
epoch£º638	 i:8 	 global-step:12768	 l-p:0.09157832711935043
epoch£º638	 i:9 	 global-step:12769	 l-p:0.12192878127098083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0306, 2.0751, 1.4223],
        [3.0306, 3.0172, 3.0300],
        [3.0306, 2.1002, 2.0464],
        [3.0306, 2.1764, 2.1972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.09199576079845428 
model_pd.l_d.mean(): -24.329978942871094 
model_pd.lagr.mean(): -24.23798370361328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2371], device='cuda:0')), ('power', tensor([-24.5671], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.09199576079845428
epoch£º639	 i:1 	 global-step:12781	 l-p:0.16491252183914185
epoch£º639	 i:2 	 global-step:12782	 l-p:0.1952894777059555
epoch£º639	 i:3 	 global-step:12783	 l-p:0.1614042967557907
epoch£º639	 i:4 	 global-step:12784	 l-p:0.1407872438430786
epoch£º639	 i:5 	 global-step:12785	 l-p:0.2854223847389221
epoch£º639	 i:6 	 global-step:12786	 l-p:0.13068358600139618
epoch£º639	 i:7 	 global-step:12787	 l-p:0.14169755578041077
epoch£º639	 i:8 	 global-step:12788	 l-p:0.14071965217590332
epoch£º639	 i:9 	 global-step:12789	 l-p:0.12952405214309692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0765, 2.1840, 2.1666],
        [3.0765, 1.9680, 1.6787],
        [3.0765, 3.0765, 3.0765],
        [3.0765, 3.0127, 3.0680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.17218732833862305 
model_pd.l_d.mean(): -24.945228576660156 
model_pd.lagr.mean(): -24.773040771484375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1244], device='cuda:0')), ('power', tensor([-25.0697], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.17218732833862305
epoch£º640	 i:1 	 global-step:12801	 l-p:0.13990449905395508
epoch£º640	 i:2 	 global-step:12802	 l-p:0.18437862396240234
epoch£º640	 i:3 	 global-step:12803	 l-p:0.17439185082912445
epoch£º640	 i:4 	 global-step:12804	 l-p:0.3435157835483551
epoch£º640	 i:5 	 global-step:12805	 l-p:0.1192307099699974
epoch£º640	 i:6 	 global-step:12806	 l-p:0.09810656309127808
epoch£º640	 i:7 	 global-step:12807	 l-p:0.12185271829366684
epoch£º640	 i:8 	 global-step:12808	 l-p:0.2137330174446106
epoch£º640	 i:9 	 global-step:12809	 l-p:0.13357849419116974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0852, 3.0803, 3.0851],
        [3.0852, 1.8287, 1.2448],
        [3.0852, 3.0852, 3.0852],
        [3.0852, 2.8558, 3.0093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.13080933690071106 
model_pd.l_d.mean(): -25.157154083251953 
model_pd.lagr.mean(): -25.026344299316406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0183], device='cuda:0')), ('power', tensor([-25.1754], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.13080933690071106
epoch£º641	 i:1 	 global-step:12821	 l-p:0.15269958972930908
epoch£º641	 i:2 	 global-step:12822	 l-p:0.09820615500211716
epoch£º641	 i:3 	 global-step:12823	 l-p:0.15876607596874237
epoch£º641	 i:4 	 global-step:12824	 l-p:0.13320723176002502
epoch£º641	 i:5 	 global-step:12825	 l-p:0.14805464446544647
epoch£º641	 i:6 	 global-step:12826	 l-p:0.1295015811920166
epoch£º641	 i:7 	 global-step:12827	 l-p:0.15716412663459778
epoch£º641	 i:8 	 global-step:12828	 l-p:0.18688903748989105
epoch£º641	 i:9 	 global-step:12829	 l-p:0.13304968178272247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0726, 3.0718, 3.0726],
        [3.0726, 2.2198, 2.2402],
        [3.0726, 3.0726, 3.0726],
        [3.0726, 2.1332, 1.4716]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.19920212030410767 
model_pd.l_d.mean(): -24.65628433227539 
model_pd.lagr.mean(): -24.457082748413086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1670], device='cuda:0')), ('power', tensor([-24.8233], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:0.19920212030410767
epoch£º642	 i:1 	 global-step:12841	 l-p:0.15192219614982605
epoch£º642	 i:2 	 global-step:12842	 l-p:0.36968910694122314
epoch£º642	 i:3 	 global-step:12843	 l-p:0.1337045133113861
epoch£º642	 i:4 	 global-step:12844	 l-p:0.18314532935619354
epoch£º642	 i:5 	 global-step:12845	 l-p:0.12125323712825775
epoch£º642	 i:6 	 global-step:12846	 l-p:0.13030271232128143
epoch£º642	 i:7 	 global-step:12847	 l-p:0.11003942042589188
epoch£º642	 i:8 	 global-step:12848	 l-p:0.046692658215761185
epoch£º642	 i:9 	 global-step:12849	 l-p:0.15587806701660156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0218, 2.7682, 2.9315],
        [3.0218, 3.0218, 3.0218],
        [3.0218, 1.7656, 1.2473],
        [3.0218, 3.0217, 3.0218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.17477495968341827 
model_pd.l_d.mean(): -24.53116226196289 
model_pd.lagr.mean(): -24.356388092041016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1227], device='cuda:0')), ('power', tensor([-24.6539], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.17477495968341827
epoch£º643	 i:1 	 global-step:12861	 l-p:0.12412725389003754
epoch£º643	 i:2 	 global-step:12862	 l-p:0.1347561925649643
epoch£º643	 i:3 	 global-step:12863	 l-p:-0.1573556661605835
epoch£º643	 i:4 	 global-step:12864	 l-p:0.198191836476326
epoch£º643	 i:5 	 global-step:12865	 l-p:0.0700962170958519
epoch£º643	 i:6 	 global-step:12866	 l-p:0.10193727165460587
epoch£º643	 i:7 	 global-step:12867	 l-p:0.04854302480816841
epoch£º643	 i:8 	 global-step:12868	 l-p:0.12127351760864258
epoch£º643	 i:9 	 global-step:12869	 l-p:0.13450512290000916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[3.0579, 2.0441, 1.8912],
        [3.0579, 1.7993, 1.2742],
        [3.0579, 2.1913, 2.2006],
        [3.0579, 2.6672, 2.8620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.3087308406829834 
model_pd.l_d.mean(): -25.034461975097656 
model_pd.lagr.mean(): -24.725730895996094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0791], device='cuda:0')), ('power', tensor([-25.1136], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.3087308406829834
epoch£º644	 i:1 	 global-step:12881	 l-p:0.149865984916687
epoch£º644	 i:2 	 global-step:12882	 l-p:0.13337361812591553
epoch£º644	 i:3 	 global-step:12883	 l-p:0.12455827742815018
epoch£º644	 i:4 	 global-step:12884	 l-p:0.15527288615703583
epoch£º644	 i:5 	 global-step:12885	 l-p:0.1419290006160736
epoch£º644	 i:6 	 global-step:12886	 l-p:0.14003311097621918
epoch£º644	 i:7 	 global-step:12887	 l-p:0.23864257335662842
epoch£º644	 i:8 	 global-step:12888	 l-p:0.033190350979566574
epoch£º644	 i:9 	 global-step:12889	 l-p:0.028658313676714897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9590, 2.7387, 2.8888],
        [2.9590, 2.1825, 2.2694],
        [2.9590, 1.7180, 1.1527],
        [2.9590, 2.9476, 2.9584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.16731086373329163 
model_pd.l_d.mean(): -24.955806732177734 
model_pd.lagr.mean(): -24.788496017456055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1268], device='cuda:0')), ('power', tensor([-25.0826], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.16731086373329163
epoch£º645	 i:1 	 global-step:12901	 l-p:-0.5037427544593811
epoch£º645	 i:2 	 global-step:12902	 l-p:0.12021563947200775
epoch£º645	 i:3 	 global-step:12903	 l-p:0.11757440119981766
epoch£º645	 i:4 	 global-step:12904	 l-p:0.13025815784931183
epoch£º645	 i:5 	 global-step:12905	 l-p:0.36084887385368347
epoch£º645	 i:6 	 global-step:12906	 l-p:0.15140393376350403
epoch£º645	 i:7 	 global-step:12907	 l-p:0.3747517168521881
epoch£º645	 i:8 	 global-step:12908	 l-p:-0.08410052955150604
epoch£º645	 i:9 	 global-step:12909	 l-p:0.13151724636554718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[3.0728, 1.8078, 1.2483],
        [3.0728, 2.1295, 1.4680],
        [3.0728, 2.1572, 1.4916],
        [3.0728, 2.2457, 2.2888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.21107730269432068 
model_pd.l_d.mean(): -24.68875503540039 
model_pd.lagr.mean(): -24.477678298950195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1127], device='cuda:0')), ('power', tensor([-24.8015], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.21107730269432068
epoch£º646	 i:1 	 global-step:12921	 l-p:0.16025863587856293
epoch£º646	 i:2 	 global-step:12922	 l-p:0.12280614674091339
epoch£º646	 i:3 	 global-step:12923	 l-p:0.13214191794395447
epoch£º646	 i:4 	 global-step:12924	 l-p:0.08474422246217728
epoch£º646	 i:5 	 global-step:12925	 l-p:0.19593241810798645
epoch£º646	 i:6 	 global-step:12926	 l-p:0.12058252096176147
epoch£º646	 i:7 	 global-step:12927	 l-p:0.1266230046749115
epoch£º646	 i:8 	 global-step:12928	 l-p:0.13472270965576172
epoch£º646	 i:9 	 global-step:12929	 l-p:0.1548706591129303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0960, 3.0960, 3.0960],
        [3.0960, 3.0950, 3.0960],
        [3.0960, 1.8371, 1.3109],
        [3.0960, 3.0948, 3.0960]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.19450032711029053 
model_pd.l_d.mean(): -24.920225143432617 
model_pd.lagr.mean(): -24.725725173950195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0800], device='cuda:0')), ('power', tensor([-25.0002], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.19450032711029053
epoch£º647	 i:1 	 global-step:12941	 l-p:0.1251169741153717
epoch£º647	 i:2 	 global-step:12942	 l-p:0.11933092772960663
epoch£º647	 i:3 	 global-step:12943	 l-p:0.12338417023420334
epoch£º647	 i:4 	 global-step:12944	 l-p:0.22107604146003723
epoch£º647	 i:5 	 global-step:12945	 l-p:0.24465234577655792
epoch£º647	 i:6 	 global-step:12946	 l-p:0.15090902149677277
epoch£º647	 i:7 	 global-step:12947	 l-p:0.16399893164634705
epoch£º647	 i:8 	 global-step:12948	 l-p:0.13422292470932007
epoch£º647	 i:9 	 global-step:12949	 l-p:0.10636447370052338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0998, 1.8482, 1.2529],
        [3.0998, 3.0997, 3.0998],
        [3.0998, 2.9617, 3.0684],
        [3.0998, 3.0839, 3.0990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.22662001848220825 
model_pd.l_d.mean(): -25.1939754486084 
model_pd.lagr.mean(): -24.967355728149414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0164], device='cuda:0')), ('power', tensor([-25.1776], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.22662001848220825
epoch£º648	 i:1 	 global-step:12961	 l-p:0.011346936225891113
epoch£º648	 i:2 	 global-step:12962	 l-p:0.13256621360778809
epoch£º648	 i:3 	 global-step:12963	 l-p:0.11505954712629318
epoch£º648	 i:4 	 global-step:12964	 l-p:0.14582504332065582
epoch£º648	 i:5 	 global-step:12965	 l-p:0.13332737982273102
epoch£º648	 i:6 	 global-step:12966	 l-p:0.1258835345506668
epoch£º648	 i:7 	 global-step:12967	 l-p:0.14628057181835175
epoch£º648	 i:8 	 global-step:12968	 l-p:0.10766761749982834
epoch£º648	 i:9 	 global-step:12969	 l-p:0.1864638477563858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0698, 2.9343, 3.0395],
        [3.0698, 3.0595, 3.0694],
        [3.0698, 3.0619, 3.0695],
        [3.0698, 2.2028, 2.2120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.12107158452272415 
model_pd.l_d.mean(): -24.931190490722656 
model_pd.lagr.mean(): -24.81011962890625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0118], device='cuda:0')), ('power', tensor([-24.9430], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.12107158452272415
epoch£º649	 i:1 	 global-step:12981	 l-p:0.11932655423879623
epoch£º649	 i:2 	 global-step:12982	 l-p:0.012872838415205479
epoch£º649	 i:3 	 global-step:12983	 l-p:0.15326045453548431
epoch£º649	 i:4 	 global-step:12984	 l-p:0.17095007002353668
epoch£º649	 i:5 	 global-step:12985	 l-p:0.1262856274843216
epoch£º649	 i:6 	 global-step:12986	 l-p:0.13671326637268066
epoch£º649	 i:7 	 global-step:12987	 l-p:0.14894333481788635
epoch£º649	 i:8 	 global-step:12988	 l-p:-0.11249099671840668
epoch£º649	 i:9 	 global-step:12989	 l-p:0.17532077431678772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9680, 2.5999, 2.7933],
        [2.9680, 2.7900, 2.9197],
        [2.9680, 1.7366, 1.2795],
        [2.9680, 2.3260, 2.4854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 1.719024419784546 
model_pd.l_d.mean(): -25.184186935424805 
model_pd.lagr.mean(): -23.46516227722168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0234], device='cuda:0')), ('power', tensor([-25.2076], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:1.719024419784546
epoch£º650	 i:1 	 global-step:13001	 l-p:0.07520516216754913
epoch£º650	 i:2 	 global-step:13002	 l-p:0.16250360012054443
epoch£º650	 i:3 	 global-step:13003	 l-p:0.13064612448215485
epoch£º650	 i:4 	 global-step:13004	 l-p:0.1378587931394577
epoch£º650	 i:5 	 global-step:13005	 l-p:0.13606193661689758
epoch£º650	 i:6 	 global-step:13006	 l-p:0.13572277128696442
epoch£º650	 i:7 	 global-step:13007	 l-p:0.15157455205917358
epoch£º650	 i:8 	 global-step:13008	 l-p:0.14788606762886047
epoch£º650	 i:9 	 global-step:13009	 l-p:0.13287393748760223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9936, 2.9936, 2.9936],
        [2.9936, 2.0378, 1.3892],
        [2.9936, 2.9936, 2.9936],
        [2.9936, 2.2177, 2.3043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.1594695746898651 
model_pd.l_d.mean(): -25.0441951751709 
model_pd.lagr.mean(): -24.88472557067871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0969], device='cuda:0')), ('power', tensor([-25.1411], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.1594695746898651
epoch£º651	 i:1 	 global-step:13021	 l-p:0.10743460059165955
epoch£º651	 i:2 	 global-step:13022	 l-p:0.015431957319378853
epoch£º651	 i:3 	 global-step:13023	 l-p:0.12303724139928818
epoch£º651	 i:4 	 global-step:13024	 l-p:0.13578087091445923
epoch£º651	 i:5 	 global-step:13025	 l-p:0.15214233100414276
epoch£º651	 i:6 	 global-step:13026	 l-p:0.15947633981704712
epoch£º651	 i:7 	 global-step:13027	 l-p:0.17602749168872833
epoch£º651	 i:8 	 global-step:13028	 l-p:0.18486569821834564
epoch£º651	 i:9 	 global-step:13029	 l-p:-0.15829487144947052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0025, 2.5552, 2.7535],
        [3.0025, 2.9611, 2.9983],
        [3.0025, 3.0025, 3.0025],
        [3.0025, 2.9980, 3.0023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.2700164318084717 
model_pd.l_d.mean(): -25.070751190185547 
model_pd.lagr.mean(): -24.800735473632812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0297], device='cuda:0')), ('power', tensor([-25.0410], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.2700164318084717
epoch£º652	 i:1 	 global-step:13041	 l-p:0.1391400843858719
epoch£º652	 i:2 	 global-step:13042	 l-p:0.176411435008049
epoch£º652	 i:3 	 global-step:13043	 l-p:0.229719340801239
epoch£º652	 i:4 	 global-step:13044	 l-p:0.17885497212409973
epoch£º652	 i:5 	 global-step:13045	 l-p:0.13709397614002228
epoch£º652	 i:6 	 global-step:13046	 l-p:0.12646014988422394
epoch£º652	 i:7 	 global-step:13047	 l-p:0.12639504671096802
epoch£º652	 i:8 	 global-step:13048	 l-p:0.17862646281719208
epoch£º652	 i:9 	 global-step:13049	 l-p:0.13102038204669952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1855, 2.0091, 1.3660],
        [3.1855, 1.9145, 1.3397],
        [3.1855, 2.0141, 1.3700],
        [3.1855, 2.9339, 3.0958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.1265752911567688 
model_pd.l_d.mean(): -24.882204055786133 
model_pd.lagr.mean(): -24.75562858581543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0458], device='cuda:0')), ('power', tensor([-24.8364], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.1265752911567688
epoch£º653	 i:1 	 global-step:13061	 l-p:0.10854092240333557
epoch£º653	 i:2 	 global-step:13062	 l-p:0.1199064627289772
epoch£º653	 i:3 	 global-step:13063	 l-p:0.12202148884534836
epoch£º653	 i:4 	 global-step:13064	 l-p:0.13221170008182526
epoch£º653	 i:5 	 global-step:13065	 l-p:0.15697833895683289
epoch£º653	 i:6 	 global-step:13066	 l-p:0.2890265882015228
epoch£º653	 i:7 	 global-step:13067	 l-p:0.2904927134513855
epoch£º653	 i:8 	 global-step:13068	 l-p:0.2200022041797638
epoch£º653	 i:9 	 global-step:13069	 l-p:0.16787829995155334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0401, 1.8579, 1.2458],
        [3.0401, 2.3418, 2.4730],
        [3.0401, 3.0401, 3.0402],
        [3.0401, 2.2159, 2.2644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): -0.2466862052679062 
model_pd.l_d.mean(): -24.820253372192383 
model_pd.lagr.mean(): -25.066940307617188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0790], device='cuda:0')), ('power', tensor([-24.8993], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:-0.2466862052679062
epoch£º654	 i:1 	 global-step:13081	 l-p:0.16647382080554962
epoch£º654	 i:2 	 global-step:13082	 l-p:0.14224399626255035
epoch£º654	 i:3 	 global-step:13083	 l-p:0.15267258882522583
epoch£º654	 i:4 	 global-step:13084	 l-p:0.13620464503765106
epoch£º654	 i:5 	 global-step:13085	 l-p:0.15178687870502472
epoch£º654	 i:6 	 global-step:13086	 l-p:0.125258207321167
epoch£º654	 i:7 	 global-step:13087	 l-p:0.16529874503612518
epoch£º654	 i:8 	 global-step:13088	 l-p:0.14572995901107788
epoch£º654	 i:9 	 global-step:13089	 l-p:0.12299061566591263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[3.1178, 2.0034, 1.7054],
        [3.1178, 1.8905, 1.4237],
        [3.1178, 2.2553, 2.2663],
        [3.1178, 2.5871, 2.7748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.16532880067825317 
model_pd.l_d.mean(): -25.133758544921875 
model_pd.lagr.mean(): -24.968429565429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0083], device='cuda:0')), ('power', tensor([-25.1254], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.16532880067825317
epoch£º655	 i:1 	 global-step:13101	 l-p:0.18186508119106293
epoch£º655	 i:2 	 global-step:13102	 l-p:0.12845271825790405
epoch£º655	 i:3 	 global-step:13103	 l-p:0.1472233682870865
epoch£º655	 i:4 	 global-step:13104	 l-p:0.1329318732023239
epoch£º655	 i:5 	 global-step:13105	 l-p:0.11999215930700302
epoch£º655	 i:6 	 global-step:13106	 l-p:0.12560661137104034
epoch£º655	 i:7 	 global-step:13107	 l-p:0.21011029183864594
epoch£º655	 i:8 	 global-step:13108	 l-p:0.12536613643169403
epoch£º655	 i:9 	 global-step:13109	 l-p:0.12668411433696747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0425, 2.9688, 3.0317],
        [3.0425, 2.9236, 3.0183],
        [3.0425, 2.9898, 3.0363],
        [3.0425, 1.7786, 1.2409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.12052842974662781 
model_pd.l_d.mean(): -25.089948654174805 
model_pd.lagr.mean(): -24.969419479370117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0242], device='cuda:0')), ('power', tensor([-25.0658], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.12052842974662781
epoch£º656	 i:1 	 global-step:13121	 l-p:0.1327281892299652
epoch£º656	 i:2 	 global-step:13122	 l-p:0.19568319618701935
epoch£º656	 i:3 	 global-step:13123	 l-p:0.13942210376262665
epoch£º656	 i:4 	 global-step:13124	 l-p:0.14672930538654327
epoch£º656	 i:5 	 global-step:13125	 l-p:0.2268742173910141
epoch£º656	 i:6 	 global-step:13126	 l-p:0.15347275137901306
epoch£º656	 i:7 	 global-step:13127	 l-p:-0.039474476128816605
epoch£º656	 i:8 	 global-step:13128	 l-p:0.12399622052907944
epoch£º656	 i:9 	 global-step:13129	 l-p:0.11295340955257416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[2.9967, 2.1630, 2.2071],
        [2.9967, 1.9109, 1.6682],
        [2.9967, 1.7581, 1.1782],
        [2.9967, 1.7350, 1.1927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.1530926376581192 
model_pd.l_d.mean(): -25.147153854370117 
model_pd.lagr.mean(): -24.994060516357422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1133], device='cuda:0')), ('power', tensor([-25.2604], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.1530926376581192
epoch£º657	 i:1 	 global-step:13141	 l-p:0.20458051562309265
epoch£º657	 i:2 	 global-step:13142	 l-p:0.1586657464504242
epoch£º657	 i:3 	 global-step:13143	 l-p:0.13961729407310486
epoch£º657	 i:4 	 global-step:13144	 l-p:0.2899981141090393
epoch£º657	 i:5 	 global-step:13145	 l-p:0.0007416915614157915
epoch£º657	 i:6 	 global-step:13146	 l-p:-0.07226059585809708
epoch£º657	 i:7 	 global-step:13147	 l-p:0.10601629316806793
epoch£º657	 i:8 	 global-step:13148	 l-p:0.1418563276529312
epoch£º657	 i:9 	 global-step:13149	 l-p:0.13848954439163208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1004, 3.1003, 3.1004],
        [3.1004, 1.9670, 1.6439],
        [3.1004, 2.1867, 2.1504],
        [3.1004, 2.1334, 2.0385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.19203071296215057 
model_pd.l_d.mean(): -24.922197341918945 
model_pd.lagr.mean(): -24.730167388916016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0021], device='cuda:0')), ('power', tensor([-24.9201], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.19203071296215057
epoch£º658	 i:1 	 global-step:13161	 l-p:0.13779287040233612
epoch£º658	 i:2 	 global-step:13162	 l-p:0.13754332065582275
epoch£º658	 i:3 	 global-step:13163	 l-p:0.15265563130378723
epoch£º658	 i:4 	 global-step:13164	 l-p:0.15979915857315063
epoch£º658	 i:5 	 global-step:13165	 l-p:0.1601373255252838
epoch£º658	 i:6 	 global-step:13166	 l-p:0.1363089382648468
epoch£º658	 i:7 	 global-step:13167	 l-p:0.12854143977165222
epoch£º658	 i:8 	 global-step:13168	 l-p:0.14528551697731018
epoch£º658	 i:9 	 global-step:13169	 l-p:0.09212984144687653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0708, 1.8189, 1.2292],
        [3.0708, 3.0697, 3.0708],
        [3.0708, 2.0038, 1.7827],
        [3.0708, 3.0676, 3.0707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.12378202378749847 
model_pd.l_d.mean(): -25.040128707885742 
model_pd.lagr.mean(): -24.91634750366211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0570], device='cuda:0')), ('power', tensor([-24.9832], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.12378202378749847
epoch£º659	 i:1 	 global-step:13181	 l-p:0.33008092641830444
epoch£º659	 i:2 	 global-step:13182	 l-p:0.15057331323623657
epoch£º659	 i:3 	 global-step:13183	 l-p:0.11821792274713516
epoch£º659	 i:4 	 global-step:13184	 l-p:-0.043174952268600464
epoch£º659	 i:5 	 global-step:13185	 l-p:0.1343131810426712
epoch£º659	 i:6 	 global-step:13186	 l-p:0.19470477104187012
epoch£º659	 i:7 	 global-step:13187	 l-p:0.2196989804506302
epoch£º659	 i:8 	 global-step:13188	 l-p:0.30054619908332825
epoch£º659	 i:9 	 global-step:13189	 l-p:0.13188515603542328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0797, 1.9553, 1.3220],
        [3.0797, 2.6064, 2.8028],
        [3.0797, 1.8355, 1.2378],
        [3.0797, 3.0794, 3.0797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.2127533107995987 
model_pd.l_d.mean(): -24.829952239990234 
model_pd.lagr.mean(): -24.617198944091797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1574], device='cuda:0')), ('power', tensor([-24.9874], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.2127533107995987
epoch£º660	 i:1 	 global-step:13201	 l-p:0.12994323670864105
epoch£º660	 i:2 	 global-step:13202	 l-p:0.13262240588665009
epoch£º660	 i:3 	 global-step:13203	 l-p:0.1723777800798416
epoch£º660	 i:4 	 global-step:13204	 l-p:0.22883757948875427
epoch£º660	 i:5 	 global-step:13205	 l-p:0.0168360136449337
epoch£º660	 i:6 	 global-step:13206	 l-p:0.1426820605993271
epoch£º660	 i:7 	 global-step:13207	 l-p:0.11342209577560425
epoch£º660	 i:8 	 global-step:13208	 l-p:0.11886461079120636
epoch£º660	 i:9 	 global-step:13209	 l-p:0.13279971480369568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1250, 3.1218, 3.1249],
        [3.1250, 2.7333, 2.9281],
        [3.1250, 3.0297, 3.1084],
        [3.1250, 2.5167, 2.6842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.13231386244297028 
model_pd.l_d.mean(): -24.90165901184082 
model_pd.lagr.mean(): -24.769344329833984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0180], device='cuda:0')), ('power', tensor([-24.9196], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.13231386244297028
epoch£º661	 i:1 	 global-step:13221	 l-p:0.12887050211429596
epoch£º661	 i:2 	 global-step:13222	 l-p:0.19716189801692963
epoch£º661	 i:3 	 global-step:13223	 l-p:0.1569535732269287
epoch£º661	 i:4 	 global-step:13224	 l-p:0.3552851676940918
epoch£º661	 i:5 	 global-step:13225	 l-p:0.10432878136634827
epoch£º661	 i:6 	 global-step:13226	 l-p:0.1371816247701645
epoch£º661	 i:7 	 global-step:13227	 l-p:0.07577639818191528
epoch£º661	 i:8 	 global-step:13228	 l-p:0.24791404604911804
epoch£º661	 i:9 	 global-step:13229	 l-p:0.13612498342990875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9863, 2.9863, 2.9863],
        [2.9863, 2.9863, 2.9863],
        [2.9863, 2.9082, 2.9745],
        [2.9863, 2.2079, 2.2948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.1399940699338913 
model_pd.l_d.mean(): -25.155086517333984 
model_pd.lagr.mean(): -25.015092849731445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0895], device='cuda:0')), ('power', tensor([-25.0656], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.1399940699338913
epoch£º662	 i:1 	 global-step:13241	 l-p:0.3056226968765259
epoch£º662	 i:2 	 global-step:13242	 l-p:0.14619575440883636
epoch£º662	 i:3 	 global-step:13243	 l-p:-0.04066881164908409
epoch£º662	 i:4 	 global-step:13244	 l-p:-0.006709177512675524
epoch£º662	 i:5 	 global-step:13245	 l-p:0.12568257749080658
epoch£º662	 i:6 	 global-step:13246	 l-p:-0.18505389988422394
epoch£º662	 i:7 	 global-step:13247	 l-p:0.11207620054483414
epoch£º662	 i:8 	 global-step:13248	 l-p:0.13593202829360962
epoch£º662	 i:9 	 global-step:13249	 l-p:0.1729489266872406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1058, 1.9355, 1.3064],
        [3.1058, 2.8822, 3.0334],
        [3.1058, 2.0219, 1.7740],
        [3.1058, 3.0607, 3.1010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.12452956289052963 
model_pd.l_d.mean(): -25.03153419494629 
model_pd.lagr.mean(): -24.907005310058594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0946], device='cuda:0')), ('power', tensor([-24.9370], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.12452956289052963
epoch£º663	 i:1 	 global-step:13261	 l-p:0.11106893420219421
epoch£º663	 i:2 	 global-step:13262	 l-p:0.14748793840408325
epoch£º663	 i:3 	 global-step:13263	 l-p:0.1620817929506302
epoch£º663	 i:4 	 global-step:13264	 l-p:0.2785419821739197
epoch£º663	 i:5 	 global-step:13265	 l-p:0.14020031690597534
epoch£º663	 i:6 	 global-step:13266	 l-p:0.13867723941802979
epoch£º663	 i:7 	 global-step:13267	 l-p:0.17165298759937286
epoch£º663	 i:8 	 global-step:13268	 l-p:0.22299493849277496
epoch£º663	 i:9 	 global-step:13269	 l-p:0.22209428250789642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0885, 3.0884, 3.0885],
        [3.0885, 1.9532, 1.6308],
        [3.0885, 3.0825, 3.0883],
        [3.0885, 2.3632, 2.4789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.17223550379276276 
model_pd.l_d.mean(): -25.013208389282227 
model_pd.lagr.mean(): -24.840972900390625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0006], device='cuda:0')), ('power', tensor([-25.0138], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.17223550379276276
epoch£º664	 i:1 	 global-step:13281	 l-p:0.14404605329036713
epoch£º664	 i:2 	 global-step:13282	 l-p:0.12031232565641403
epoch£º664	 i:3 	 global-step:13283	 l-p:0.14955951273441315
epoch£º664	 i:4 	 global-step:13284	 l-p:0.125594362616539
epoch£º664	 i:5 	 global-step:13285	 l-p:0.1250912994146347
epoch£º664	 i:6 	 global-step:13286	 l-p:0.1429770141839981
epoch£º664	 i:7 	 global-step:13287	 l-p:0.08584548532962799
epoch£º664	 i:8 	 global-step:13288	 l-p:0.37703070044517517
epoch£º664	 i:9 	 global-step:13289	 l-p:0.4360341429710388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0606, 3.0526, 3.0603],
        [3.0606, 3.0524, 3.0603],
        [3.0606, 1.9409, 1.6455],
        [3.0606, 2.7832, 2.9550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.4628370702266693 
model_pd.l_d.mean(): -25.091724395751953 
model_pd.lagr.mean(): -24.628887176513672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0843], device='cuda:0')), ('power', tensor([-25.1761], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.4628370702266693
epoch£º665	 i:1 	 global-step:13301	 l-p:0.13772717118263245
epoch£º665	 i:2 	 global-step:13302	 l-p:0.15803475677967072
epoch£º665	 i:3 	 global-step:13303	 l-p:0.20975112915039062
epoch£º665	 i:4 	 global-step:13304	 l-p:0.23181083798408508
epoch£º665	 i:5 	 global-step:13305	 l-p:0.127718985080719
epoch£º665	 i:6 	 global-step:13306	 l-p:0.12770621478557587
epoch£º665	 i:7 	 global-step:13307	 l-p:0.1333501636981964
epoch£º665	 i:8 	 global-step:13308	 l-p:0.1424112617969513
epoch£º665	 i:9 	 global-step:13309	 l-p:0.12146030366420746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0470, 3.0470, 3.0470],
        [3.0470, 1.7821, 1.2153],
        [3.0470, 1.7862, 1.2115],
        [3.0470, 2.3570, 2.4938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.12613268196582794 
model_pd.l_d.mean(): -24.535633087158203 
model_pd.lagr.mean(): -24.409500122070312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1259], device='cuda:0')), ('power', tensor([-24.6615], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.12613268196582794
epoch£º666	 i:1 	 global-step:13321	 l-p:0.29210010170936584
epoch£º666	 i:2 	 global-step:13322	 l-p:0.03403030335903168
epoch£º666	 i:3 	 global-step:13323	 l-p:0.13790227472782135
epoch£º666	 i:4 	 global-step:13324	 l-p:0.05554356426000595
epoch£º666	 i:5 	 global-step:13325	 l-p:0.14589808881282806
epoch£º666	 i:6 	 global-step:13326	 l-p:0.20347502827644348
epoch£º666	 i:7 	 global-step:13327	 l-p:0.09019044041633606
epoch£º666	 i:8 	 global-step:13328	 l-p:0.1299930363893509
epoch£º666	 i:9 	 global-step:13329	 l-p:0.16771167516708374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[3.0128, 2.0980, 1.4385],
        [3.0128, 2.0624, 1.9953],
        [3.0128, 1.9343, 1.3039],
        [3.0128, 1.8593, 1.5226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.8697401881217957 
model_pd.l_d.mean(): -25.082687377929688 
model_pd.lagr.mean(): -24.212947845458984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1385], device='cuda:0')), ('power', tensor([-25.2212], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.8697401881217957
epoch£º667	 i:1 	 global-step:13341	 l-p:0.11438476294279099
epoch£º667	 i:2 	 global-step:13342	 l-p:0.14055268466472626
epoch£º667	 i:3 	 global-step:13343	 l-p:0.1319657415151596
epoch£º667	 i:4 	 global-step:13344	 l-p:0.1638229489326477
epoch£º667	 i:5 	 global-step:13345	 l-p:0.17084522545337677
epoch£º667	 i:6 	 global-step:13346	 l-p:0.08709355443716049
epoch£º667	 i:7 	 global-step:13347	 l-p:0.07341720163822174
epoch£º667	 i:8 	 global-step:13348	 l-p:0.12651772797107697
epoch£º667	 i:9 	 global-step:13349	 l-p:0.2129143625497818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0578, 2.9527, 3.0383],
        [3.0578, 3.0577, 3.0578],
        [3.0578, 2.2129, 2.2465],
        [3.0578, 1.7885, 1.2315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.09492847323417664 
model_pd.l_d.mean(): -24.83783531188965 
model_pd.lagr.mean(): -24.74290657043457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0524], device='cuda:0')), ('power', tensor([-24.8902], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.09492847323417664
epoch£º668	 i:1 	 global-step:13361	 l-p:0.131826251745224
epoch£º668	 i:2 	 global-step:13362	 l-p:0.15811827778816223
epoch£º668	 i:3 	 global-step:13363	 l-p:0.13867683708667755
epoch£º668	 i:4 	 global-step:13364	 l-p:0.15191970765590668
epoch£º668	 i:5 	 global-step:13365	 l-p:0.22946596145629883
epoch£º668	 i:6 	 global-step:13366	 l-p:0.20795553922653198
epoch£º668	 i:7 	 global-step:13367	 l-p:0.11556213349103928
epoch£º668	 i:8 	 global-step:13368	 l-p:0.18446467816829681
epoch£º668	 i:9 	 global-step:13369	 l-p:0.14429987967014313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0876, 2.4412, 2.5968],
        [3.0876, 3.0873, 3.0876],
        [3.0876, 3.0876, 3.0876],
        [3.0876, 1.9112, 1.5323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.13080273568630219 
model_pd.l_d.mean(): -24.892261505126953 
model_pd.lagr.mean(): -24.761459350585938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0987], device='cuda:0')), ('power', tensor([-24.7935], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.13080273568630219
epoch£º669	 i:1 	 global-step:13381	 l-p:0.14372830092906952
epoch£º669	 i:2 	 global-step:13382	 l-p:0.3495625853538513
epoch£º669	 i:3 	 global-step:13383	 l-p:0.447325199842453
epoch£º669	 i:4 	 global-step:13384	 l-p:0.1460787057876587
epoch£º669	 i:5 	 global-step:13385	 l-p:0.2098110467195511
epoch£º669	 i:6 	 global-step:13386	 l-p:0.14651519060134888
epoch£º669	 i:7 	 global-step:13387	 l-p:0.16558483242988586
epoch£º669	 i:8 	 global-step:13388	 l-p:0.14823827147483826
epoch£º669	 i:9 	 global-step:13389	 l-p:3.297534465789795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0459, 3.0459, 3.0459],
        [3.0459, 2.7849, 2.9513],
        [3.0459, 3.0410, 3.0458],
        [3.0459, 3.0459, 3.0459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): -0.115336112678051 
model_pd.l_d.mean(): -25.154808044433594 
model_pd.lagr.mean(): -25.270143508911133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0628], device='cuda:0')), ('power', tensor([-25.2177], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:-0.115336112678051
epoch£º670	 i:1 	 global-step:13401	 l-p:0.1111525297164917
epoch£º670	 i:2 	 global-step:13402	 l-p:0.13545425236225128
epoch£º670	 i:3 	 global-step:13403	 l-p:0.12541481852531433
epoch£º670	 i:4 	 global-step:13404	 l-p:0.19684810936450958
epoch£º670	 i:5 	 global-step:13405	 l-p:0.21328668296337128
epoch£º670	 i:6 	 global-step:13406	 l-p:-0.22648054361343384
epoch£º670	 i:7 	 global-step:13407	 l-p:-0.7512820363044739
epoch£º670	 i:8 	 global-step:13408	 l-p:0.13670207560062408
epoch£º670	 i:9 	 global-step:13409	 l-p:0.13219448924064636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0973, 3.0602, 3.0939],
        [3.0973, 3.0849, 3.0967],
        [3.0973, 2.1981, 2.1803],
        [3.0973, 3.0845, 3.0967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.13481780886650085 
model_pd.l_d.mean(): -25.150028228759766 
model_pd.lagr.mean(): -25.01521110534668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0644], device='cuda:0')), ('power', tensor([-25.0856], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.13481780886650085
epoch£º671	 i:1 	 global-step:13421	 l-p:0.15544778108596802
epoch£º671	 i:2 	 global-step:13422	 l-p:0.15037716925144196
epoch£º671	 i:3 	 global-step:13423	 l-p:0.16446639597415924
epoch£º671	 i:4 	 global-step:13424	 l-p:0.13812899589538574
epoch£º671	 i:5 	 global-step:13425	 l-p:0.14564065635204315
epoch£º671	 i:6 	 global-step:13426	 l-p:0.11767692118883133
epoch£º671	 i:7 	 global-step:13427	 l-p:0.2142142653465271
epoch£º671	 i:8 	 global-step:13428	 l-p:0.12939195334911346
epoch£º671	 i:9 	 global-step:13429	 l-p:0.137287437915802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1550, 3.0815, 3.1443],
        [3.1550, 1.8828, 1.3308],
        [3.1550, 3.1550, 3.1550],
        [3.1550, 2.3268, 2.3692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.12918829917907715 
model_pd.l_d.mean(): -24.90105628967285 
model_pd.lagr.mean(): -24.771867752075195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0107], device='cuda:0')), ('power', tensor([-24.9118], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.12918829917907715
epoch£º672	 i:1 	 global-step:13441	 l-p:0.13053737580776215
epoch£º672	 i:2 	 global-step:13442	 l-p:0.12827590107917786
epoch£º672	 i:3 	 global-step:13443	 l-p:0.25298061966896057
epoch£º672	 i:4 	 global-step:13444	 l-p:0.1504281610250473
epoch£º672	 i:5 	 global-step:13445	 l-p:0.15116892755031586
epoch£º672	 i:6 	 global-step:13446	 l-p:0.17443592846393585
epoch£º672	 i:7 	 global-step:13447	 l-p:0.12677767872810364
epoch£º672	 i:8 	 global-step:13448	 l-p:0.11606929451227188
epoch£º672	 i:9 	 global-step:13449	 l-p:-0.15623952448368073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0320, 3.0311, 3.0320],
        [3.0320, 3.0320, 3.0320],
        [3.0320, 1.7982, 1.2041],
        [3.0320, 2.9959, 3.0287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.13234487175941467 
model_pd.l_d.mean(): -24.989749908447266 
model_pd.lagr.mean(): -24.857404708862305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0015], device='cuda:0')), ('power', tensor([-24.9913], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.13234487175941467
epoch£º673	 i:1 	 global-step:13461	 l-p:0.14287099242210388
epoch£º673	 i:2 	 global-step:13462	 l-p:0.16573339700698853
epoch£º673	 i:3 	 global-step:13463	 l-p:0.1831219494342804
epoch£º673	 i:4 	 global-step:13464	 l-p:0.06421351432800293
epoch£º673	 i:5 	 global-step:13465	 l-p:0.15585575997829437
epoch£º673	 i:6 	 global-step:13466	 l-p:0.3494846224784851
epoch£º673	 i:7 	 global-step:13467	 l-p:0.06529837101697922
epoch£º673	 i:8 	 global-step:13468	 l-p:0.591997504234314
epoch£º673	 i:9 	 global-step:13469	 l-p:0.035327911376953125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0328, 2.6015, 2.8006],
        [3.0328, 2.6636, 2.8575],
        [3.0328, 2.4299, 2.6039],
        [3.0328, 1.9561, 1.7300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.14587615430355072 
model_pd.l_d.mean(): -24.774688720703125 
model_pd.lagr.mean(): -24.628812789916992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1447], device='cuda:0')), ('power', tensor([-24.9194], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.14587615430355072
epoch£º674	 i:1 	 global-step:13481	 l-p:0.12918610870838165
epoch£º674	 i:2 	 global-step:13482	 l-p:0.15716923773288727
epoch£º674	 i:3 	 global-step:13483	 l-p:0.12338253110647202
epoch£º674	 i:4 	 global-step:13484	 l-p:0.5892351269721985
epoch£º674	 i:5 	 global-step:13485	 l-p:0.16766805946826935
epoch£º674	 i:6 	 global-step:13486	 l-p:0.15547166764736176
epoch£º674	 i:7 	 global-step:13487	 l-p:0.13835959136486053
epoch£º674	 i:8 	 global-step:13488	 l-p:0.14303092658519745
epoch£º674	 i:9 	 global-step:13489	 l-p:0.13403373956680298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0967, 2.3850, 2.5100],
        [3.0967, 2.0968, 1.9682],
        [3.0967, 2.7380, 2.9296],
        [3.0967, 2.8293, 2.9978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.1557053029537201 
model_pd.l_d.mean(): -24.92557144165039 
model_pd.lagr.mean(): -24.769866943359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0262], device='cuda:0')), ('power', tensor([-24.8993], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.1557053029537201
epoch£º675	 i:1 	 global-step:13501	 l-p:0.17516565322875977
epoch£º675	 i:2 	 global-step:13502	 l-p:0.1457267701625824
epoch£º675	 i:3 	 global-step:13503	 l-p:0.15682661533355713
epoch£º675	 i:4 	 global-step:13504	 l-p:0.13468395173549652
epoch£º675	 i:5 	 global-step:13505	 l-p:0.13877004384994507
epoch£º675	 i:6 	 global-step:13506	 l-p:0.08051671832799911
epoch£º675	 i:7 	 global-step:13507	 l-p:0.12338471412658691
epoch£º675	 i:8 	 global-step:13508	 l-p:0.15043482184410095
epoch£º675	 i:9 	 global-step:13509	 l-p:-0.19732311367988586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0380, 3.0379, 3.0380],
        [3.0380, 1.8462, 1.4556],
        [3.0380, 2.9965, 3.0339],
        [3.0380, 3.0244, 3.0374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): -0.05772489309310913 
model_pd.l_d.mean(): -25.00021743774414 
model_pd.lagr.mean(): -25.057941436767578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0435], device='cuda:0')), ('power', tensor([-24.9567], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:-0.05772489309310913
epoch£º676	 i:1 	 global-step:13521	 l-p:0.15576386451721191
epoch£º676	 i:2 	 global-step:13522	 l-p:0.1709420382976532
epoch£º676	 i:3 	 global-step:13523	 l-p:0.13092997670173645
epoch£º676	 i:4 	 global-step:13524	 l-p:0.14823995530605316
epoch£º676	 i:5 	 global-step:13525	 l-p:-0.22857680916786194
epoch£º676	 i:6 	 global-step:13526	 l-p:0.19814056158065796
epoch£º676	 i:7 	 global-step:13527	 l-p:0.12612393498420715
epoch£º676	 i:8 	 global-step:13528	 l-p:0.12837670743465424
epoch£º676	 i:9 	 global-step:13529	 l-p:0.13428519666194916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9225, 2.9225, 2.9225],
        [2.9225, 2.5434, 2.7400],
        [2.9225, 2.0446, 2.0593],
        [2.9225, 2.8654, 2.9155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.14527422189712524 
model_pd.l_d.mean(): -25.142719268798828 
model_pd.lagr.mean(): -24.99744415283203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0599], device='cuda:0')), ('power', tensor([-25.2026], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.14527422189712524
epoch£º677	 i:1 	 global-step:13541	 l-p:0.17248797416687012
epoch£º677	 i:2 	 global-step:13542	 l-p:0.11809217184782028
epoch£º677	 i:3 	 global-step:13543	 l-p:0.1595621109008789
epoch£º677	 i:4 	 global-step:13544	 l-p:0.20082354545593262
epoch£º677	 i:5 	 global-step:13545	 l-p:0.1418323516845703
epoch£º677	 i:6 	 global-step:13546	 l-p:0.2786860167980194
epoch£º677	 i:7 	 global-step:13547	 l-p:0.11202052980661392
epoch£º677	 i:8 	 global-step:13548	 l-p:0.06153951212763786
epoch£º677	 i:9 	 global-step:13549	 l-p:0.09578028321266174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0014, 2.8913, 2.9803],
        [3.0014, 2.9961, 3.0013],
        [3.0014, 2.9049, 2.9846],
        [3.0014, 2.8613, 2.9696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): 0.42011532187461853 
model_pd.l_d.mean(): -24.870956420898438 
model_pd.lagr.mean(): -24.450841903686523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1446], device='cuda:0')), ('power', tensor([-25.0155], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:0.42011532187461853
epoch£º678	 i:1 	 global-step:13561	 l-p:-0.018944930285215378
epoch£º678	 i:2 	 global-step:13562	 l-p:0.28281018137931824
epoch£º678	 i:3 	 global-step:13563	 l-p:0.17956441640853882
epoch£º678	 i:4 	 global-step:13564	 l-p:0.12052319943904877
epoch£º678	 i:5 	 global-step:13565	 l-p:0.1102517694234848
epoch£º678	 i:6 	 global-step:13566	 l-p:0.17002420127391815
epoch£º678	 i:7 	 global-step:13567	 l-p:0.14541272819042206
epoch£º678	 i:8 	 global-step:13568	 l-p:0.12789258360862732
epoch£º678	 i:9 	 global-step:13569	 l-p:0.10603572428226471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2408, 3.2405, 3.2408],
        [3.2408, 1.9831, 1.3517],
        [3.2408, 3.1490, 3.2252],
        [3.2408, 3.0649, 3.1930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.14386513829231262 
model_pd.l_d.mean(): -24.957324981689453 
model_pd.lagr.mean(): -24.813459396362305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0861], device='cuda:0')), ('power', tensor([-24.8712], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.14386513829231262
epoch£º679	 i:1 	 global-step:13581	 l-p:0.1291978806257248
epoch£º679	 i:2 	 global-step:13582	 l-p:0.13670220971107483
epoch£º679	 i:3 	 global-step:13583	 l-p:0.14853689074516296
epoch£º679	 i:4 	 global-step:13584	 l-p:0.15365661680698395
epoch£º679	 i:5 	 global-step:13585	 l-p:0.0979546308517456
epoch£º679	 i:6 	 global-step:13586	 l-p:0.17162024974822998
epoch£º679	 i:7 	 global-step:13587	 l-p:0.17455469071865082
epoch£º679	 i:8 	 global-step:13588	 l-p:0.2199946790933609
epoch£º679	 i:9 	 global-step:13589	 l-p:0.11035556346178055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0569, 3.0339, 3.0554],
        [3.0569, 3.0487, 3.0566],
        [3.0569, 2.9248, 3.0281],
        [3.0569, 1.9458, 1.3125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.15796437859535217 
model_pd.l_d.mean(): -25.267154693603516 
model_pd.lagr.mean(): -25.109189987182617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0378], device='cuda:0')), ('power', tensor([-25.2294], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.15796437859535217
epoch£º680	 i:1 	 global-step:13601	 l-p:-0.2598784565925598
epoch£º680	 i:2 	 global-step:13602	 l-p:0.27354690432548523
epoch£º680	 i:3 	 global-step:13603	 l-p:0.1443721055984497
epoch£º680	 i:4 	 global-step:13604	 l-p:0.05531613156199455
epoch£º680	 i:5 	 global-step:13605	 l-p:0.12573115527629852
epoch£º680	 i:6 	 global-step:13606	 l-p:0.1316649615764618
epoch£º680	 i:7 	 global-step:13607	 l-p:0.39400291442871094
epoch£º680	 i:8 	 global-step:13608	 l-p:0.12721896171569824
epoch£º680	 i:9 	 global-step:13609	 l-p:0.12024427205324173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9683, 2.1813, 2.2672],
        [2.9683, 1.9823, 1.3404],
        [2.9683, 2.9653, 2.9683],
        [2.9683, 2.9651, 2.9682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.05385008081793785 
model_pd.l_d.mean(): -25.002761840820312 
model_pd.lagr.mean(): -24.948911666870117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0190], device='cuda:0')), ('power', tensor([-25.0218], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.05385008081793785
epoch£º681	 i:1 	 global-step:13621	 l-p:0.14781907200813293
epoch£º681	 i:2 	 global-step:13622	 l-p:0.20861034095287323
epoch£º681	 i:3 	 global-step:13623	 l-p:0.17049601674079895
epoch£º681	 i:4 	 global-step:13624	 l-p:0.1804254800081253
epoch£º681	 i:5 	 global-step:13625	 l-p:0.08226503431797028
epoch£º681	 i:6 	 global-step:13626	 l-p:-0.103062704205513
epoch£º681	 i:7 	 global-step:13627	 l-p:0.16012713313102722
epoch£º681	 i:8 	 global-step:13628	 l-p:0.16282758116722107
epoch£º681	 i:9 	 global-step:13629	 l-p:-0.008699235506355762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0086, 2.6472, 2.8404],
        [3.0086, 2.5252, 2.7238],
        [3.0086, 3.0003, 3.0083],
        [3.0086, 2.9566, 3.0026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.13876527547836304 
model_pd.l_d.mean(): -25.1028995513916 
model_pd.lagr.mean(): -24.964134216308594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0335], device='cuda:0')), ('power', tensor([-25.1364], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.13876527547836304
epoch£º682	 i:1 	 global-step:13641	 l-p:0.08593153953552246
epoch£º682	 i:2 	 global-step:13642	 l-p:0.1110716238617897
epoch£º682	 i:3 	 global-step:13643	 l-p:0.08616357296705246
epoch£º682	 i:4 	 global-step:13644	 l-p:0.14110825955867767
epoch£º682	 i:5 	 global-step:13645	 l-p:0.12554019689559937
epoch£º682	 i:6 	 global-step:13646	 l-p:0.13869118690490723
epoch£º682	 i:7 	 global-step:13647	 l-p:0.14100585877895355
epoch£º682	 i:8 	 global-step:13648	 l-p:-0.15250831842422485
epoch£º682	 i:9 	 global-step:13649	 l-p:0.1376878172159195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9760, 2.5196, 2.7201],
        [2.9760, 2.8494, 2.9493],
        [2.9760, 2.9760, 2.9760],
        [2.9760, 2.3635, 2.5373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.15107958018779755 
model_pd.l_d.mean(): -25.29157066345215 
model_pd.lagr.mean(): -25.140491485595703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0526], device='cuda:0')), ('power', tensor([-25.3441], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.15107958018779755
epoch£º683	 i:1 	 global-step:13661	 l-p:0.22852420806884766
epoch£º683	 i:2 	 global-step:13662	 l-p:0.09474186599254608
epoch£º683	 i:3 	 global-step:13663	 l-p:0.10391252487897873
epoch£º683	 i:4 	 global-step:13664	 l-p:0.05788416787981987
epoch£º683	 i:5 	 global-step:13665	 l-p:0.12186180055141449
epoch£º683	 i:6 	 global-step:13666	 l-p:0.2785058319568634
epoch£º683	 i:7 	 global-step:13667	 l-p:0.043870486319065094
epoch£º683	 i:8 	 global-step:13668	 l-p:0.14237982034683228
epoch£º683	 i:9 	 global-step:13669	 l-p:0.15413352847099304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0685, 1.7988, 1.2705],
        [3.0685, 3.0685, 3.0685],
        [3.0685, 1.9919, 1.3500],
        [3.0685, 3.0610, 3.0683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.25701504945755005 
model_pd.l_d.mean(): -24.9879150390625 
model_pd.lagr.mean(): -24.730899810791016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0043], device='cuda:0')), ('power', tensor([-24.9922], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.25701504945755005
epoch£º684	 i:1 	 global-step:13681	 l-p:0.1615200936794281
epoch£º684	 i:2 	 global-step:13682	 l-p:0.12354544550180435
epoch£º684	 i:3 	 global-step:13683	 l-p:0.13202811777591705
epoch£º684	 i:4 	 global-step:13684	 l-p:0.13898038864135742
epoch£º684	 i:5 	 global-step:13685	 l-p:0.13623060286045074
epoch£º684	 i:6 	 global-step:13686	 l-p:0.09654712677001953
epoch£º684	 i:7 	 global-step:13687	 l-p:0.2682151794433594
epoch£º684	 i:8 	 global-step:13688	 l-p:0.12153840810060501
epoch£º684	 i:9 	 global-step:13689	 l-p:0.16104067862033844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0920, 2.8046, 2.9798],
        [3.0920, 2.1022, 1.4421],
        [3.0920, 2.0003, 1.7516],
        [3.0920, 3.0918, 3.0921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.14084668457508087 
model_pd.l_d.mean(): -25.119552612304688 
model_pd.lagr.mean(): -24.97870635986328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0046], device='cuda:0')), ('power', tensor([-25.1241], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.14084668457508087
epoch£º685	 i:1 	 global-step:13701	 l-p:0.22214530408382416
epoch£º685	 i:2 	 global-step:13702	 l-p:0.15945009887218475
epoch£º685	 i:3 	 global-step:13703	 l-p:0.1501631736755371
epoch£º685	 i:4 	 global-step:13704	 l-p:0.06723041832447052
epoch£º685	 i:5 	 global-step:13705	 l-p:0.16043208539485931
epoch£º685	 i:6 	 global-step:13706	 l-p:0.15808044373989105
epoch£º685	 i:7 	 global-step:13707	 l-p:0.11770662665367126
epoch£º685	 i:8 	 global-step:13708	 l-p:0.1330500990152359
epoch£º685	 i:9 	 global-step:13709	 l-p:0.11261764168739319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1404, 3.1393, 3.1404],
        [3.1404, 3.1385, 3.1403],
        [3.1404, 3.0328, 3.1200],
        [3.1404, 2.0452, 1.3936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.13874544203281403 
model_pd.l_d.mean(): -24.51093101501465 
model_pd.lagr.mean(): -24.37218475341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1202], device='cuda:0')), ('power', tensor([-24.6312], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.13874544203281403
epoch£º686	 i:1 	 global-step:13721	 l-p:-0.057010717689991
epoch£º686	 i:2 	 global-step:13722	 l-p:0.14131924510002136
epoch£º686	 i:3 	 global-step:13723	 l-p:0.1359608918428421
epoch£º686	 i:4 	 global-step:13724	 l-p:0.16252116858959198
epoch£º686	 i:5 	 global-step:13725	 l-p:0.1557827889919281
epoch£º686	 i:6 	 global-step:13726	 l-p:0.15492691099643707
epoch£º686	 i:7 	 global-step:13727	 l-p:0.17748886346817017
epoch£º686	 i:8 	 global-step:13728	 l-p:0.12996353209018707
epoch£º686	 i:9 	 global-step:13729	 l-p:0.13599897921085358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[3.1185, 1.9977, 1.3544],
        [3.1185, 1.8799, 1.4096],
        [3.1185, 2.2471, 2.2583],
        [3.1185, 1.9236, 1.2954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.04772535338997841 
model_pd.l_d.mean(): -24.775114059448242 
model_pd.lagr.mean(): -24.727388381958008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1082], device='cuda:0')), ('power', tensor([-24.8833], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.04772535338997841
epoch£º687	 i:1 	 global-step:13741	 l-p:0.15424904227256775
epoch£º687	 i:2 	 global-step:13742	 l-p:0.1316397488117218
epoch£º687	 i:3 	 global-step:13743	 l-p:0.13415025174617767
epoch£º687	 i:4 	 global-step:13744	 l-p:0.14288052916526794
epoch£º687	 i:5 	 global-step:13745	 l-p:0.10248765349388123
epoch£º687	 i:6 	 global-step:13746	 l-p:0.027152203023433685
epoch£º687	 i:7 	 global-step:13747	 l-p:0.05137425661087036
epoch£º687	 i:8 	 global-step:13748	 l-p:0.14936767518520355
epoch£º687	 i:9 	 global-step:13749	 l-p:0.1639104187488556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9949, 2.9949, 2.9950],
        [2.9949, 1.8121, 1.4444],
        [2.9949, 2.3434, 2.5029],
        [2.9949, 2.9947, 2.9949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 3.2130613327026367 
model_pd.l_d.mean(): -24.721086502075195 
model_pd.lagr.mean(): -21.508026123046875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1376], device='cuda:0')), ('power', tensor([-24.8587], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:3.2130613327026367
epoch£º688	 i:1 	 global-step:13761	 l-p:0.13409580290317535
epoch£º688	 i:2 	 global-step:13762	 l-p:0.1615414172410965
epoch£º688	 i:3 	 global-step:13763	 l-p:0.187042236328125
epoch£º688	 i:4 	 global-step:13764	 l-p:0.1576588749885559
epoch£º688	 i:5 	 global-step:13765	 l-p:-0.044709887355566025
epoch£º688	 i:6 	 global-step:13766	 l-p:0.18008150160312653
epoch£º688	 i:7 	 global-step:13767	 l-p:0.1335865706205368
epoch£º688	 i:8 	 global-step:13768	 l-p:0.12678343057632446
epoch£º688	 i:9 	 global-step:13769	 l-p:0.05319004878401756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0354, 2.7976, 2.9555],
        [3.0354, 3.0111, 3.0337],
        [3.0354, 3.0353, 3.0354],
        [3.0354, 3.0354, 3.0355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.14539389312267303 
model_pd.l_d.mean(): -25.12702178955078 
model_pd.lagr.mean(): -24.98162841796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0072], device='cuda:0')), ('power', tensor([-25.1198], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.14539389312267303
epoch£º689	 i:1 	 global-step:13781	 l-p:0.13221584260463715
epoch£º689	 i:2 	 global-step:13782	 l-p:0.12195819616317749
epoch£º689	 i:3 	 global-step:13783	 l-p:2.2427730560302734
epoch£º689	 i:4 	 global-step:13784	 l-p:-1.0136966705322266
epoch£º689	 i:5 	 global-step:13785	 l-p:0.6104170680046082
epoch£º689	 i:6 	 global-step:13786	 l-p:0.17549096047878265
epoch£º689	 i:7 	 global-step:13787	 l-p:0.12145265191793442
epoch£º689	 i:8 	 global-step:13788	 l-p:0.14094890654087067
epoch£º689	 i:9 	 global-step:13789	 l-p:0.12202455848455429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1146, 3.1143, 3.1146],
        [3.1146, 2.1641, 1.4944],
        [3.1146, 3.1127, 3.1145],
        [3.1146, 3.1146, 3.1146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.13238388299942017 
model_pd.l_d.mean(): -25.127933502197266 
model_pd.lagr.mean(): -24.99555015563965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0048], device='cuda:0')), ('power', tensor([-25.1327], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.13238388299942017
epoch£º690	 i:1 	 global-step:13801	 l-p:0.0660904198884964
epoch£º690	 i:2 	 global-step:13802	 l-p:0.11904052644968033
epoch£º690	 i:3 	 global-step:13803	 l-p:0.19033357501029968
epoch£º690	 i:4 	 global-step:13804	 l-p:0.12268956005573273
epoch£º690	 i:5 	 global-step:13805	 l-p:0.2314513921737671
epoch£º690	 i:6 	 global-step:13806	 l-p:0.1580185890197754
epoch£º690	 i:7 	 global-step:13807	 l-p:0.2670409679412842
epoch£º690	 i:8 	 global-step:13808	 l-p:0.13380718231201172
epoch£º690	 i:9 	 global-step:13809	 l-p:0.1314561367034912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0607, 3.0606, 3.0607],
        [3.0607, 2.6653, 2.8628],
        [3.0607, 1.7880, 1.2177],
        [3.0607, 3.0532, 3.0604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.16453580558300018 
model_pd.l_d.mean(): -24.76675796508789 
model_pd.lagr.mean(): -24.602222442626953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0759], device='cuda:0')), ('power', tensor([-24.8427], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.16453580558300018
epoch£º691	 i:1 	 global-step:13821	 l-p:-1.2903834581375122
epoch£º691	 i:2 	 global-step:13822	 l-p:0.1457432061433792
epoch£º691	 i:3 	 global-step:13823	 l-p:0.14792436361312866
epoch£º691	 i:4 	 global-step:13824	 l-p:0.16264598071575165
epoch£º691	 i:5 	 global-step:13825	 l-p:0.18368123471736908
epoch£º691	 i:6 	 global-step:13826	 l-p:0.16325441002845764
epoch£º691	 i:7 	 global-step:13827	 l-p:0.6557061672210693
epoch£º691	 i:8 	 global-step:13828	 l-p:0.050664205104112625
epoch£º691	 i:9 	 global-step:13829	 l-p:0.11813407391309738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0448, 3.0438, 3.0448],
        [3.0448, 3.0430, 3.0448],
        [3.0448, 2.8166, 2.9706],
        [3.0448, 3.0448, 3.0448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.16460064053535461 
model_pd.l_d.mean(): -25.159526824951172 
model_pd.lagr.mean(): -24.99492645263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0799], device='cuda:0')), ('power', tensor([-25.0796], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.16460064053535461
epoch£º692	 i:1 	 global-step:13841	 l-p:0.37254175543785095
epoch£º692	 i:2 	 global-step:13842	 l-p:0.2554541230201721
epoch£º692	 i:3 	 global-step:13843	 l-p:0.12848994135856628
epoch£º692	 i:4 	 global-step:13844	 l-p:0.14692136645317078
epoch£º692	 i:5 	 global-step:13845	 l-p:0.1592719405889511
epoch£º692	 i:6 	 global-step:13846	 l-p:0.1334066241979599
epoch£º692	 i:7 	 global-step:13847	 l-p:0.1130591481924057
epoch£º692	 i:8 	 global-step:13848	 l-p:0.12992019951343536
epoch£º692	 i:9 	 global-step:13849	 l-p:-0.1455400437116623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1388, 3.1264, 3.1383],
        [3.1388, 1.9690, 1.3306],
        [3.1388, 2.1482, 1.4807],
        [3.1388, 2.1777, 1.5060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.1296505331993103 
model_pd.l_d.mean(): -24.78036880493164 
model_pd.lagr.mean(): -24.650718688964844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1605], device='cuda:0')), ('power', tensor([-24.9409], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.1296505331993103
epoch£º693	 i:1 	 global-step:13861	 l-p:-0.010917081497609615
epoch£º693	 i:2 	 global-step:13862	 l-p:0.13564005494117737
epoch£º693	 i:3 	 global-step:13863	 l-p:0.13792099058628082
epoch£º693	 i:4 	 global-step:13864	 l-p:0.12887413799762726
epoch£º693	 i:5 	 global-step:13865	 l-p:0.13058438897132874
epoch£º693	 i:6 	 global-step:13866	 l-p:0.26465925574302673
epoch£º693	 i:7 	 global-step:13867	 l-p:0.16356170177459717
epoch£º693	 i:8 	 global-step:13868	 l-p:-0.009322967380285263
epoch£º693	 i:9 	 global-step:13869	 l-p:0.2929898202419281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0266, 2.7971, 2.9517],
        [3.0266, 1.8540, 1.4989],
        [3.0266, 3.0265, 3.0266],
        [3.0266, 3.0266, 3.0266]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.19647449254989624 
model_pd.l_d.mean(): -24.588579177856445 
model_pd.lagr.mean(): -24.392105102539062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1127], device='cuda:0')), ('power', tensor([-24.7012], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.19647449254989624
epoch£º694	 i:1 	 global-step:13881	 l-p:0.31700950860977173
epoch£º694	 i:2 	 global-step:13882	 l-p:-0.00702868914231658
epoch£º694	 i:3 	 global-step:13883	 l-p:0.12626272439956665
epoch£º694	 i:4 	 global-step:13884	 l-p:0.1257425993680954
epoch£º694	 i:5 	 global-step:13885	 l-p:0.1411716789007187
epoch£º694	 i:6 	 global-step:13886	 l-p:0.12893226742744446
epoch£º694	 i:7 	 global-step:13887	 l-p:0.18275229632854462
epoch£º694	 i:8 	 global-step:13888	 l-p:0.12902893126010895
epoch£º694	 i:9 	 global-step:13889	 l-p:0.1824062317609787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0981, 2.0929, 1.9637],
        [3.0981, 1.8243, 1.2904],
        [3.0981, 1.8214, 1.2777],
        [3.0981, 3.0971, 3.0981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.12342243641614914 
model_pd.l_d.mean(): -24.769855499267578 
model_pd.lagr.mean(): -24.646432876586914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0394], device='cuda:0')), ('power', tensor([-24.8092], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.12342243641614914
epoch£º695	 i:1 	 global-step:13901	 l-p:0.14117105305194855
epoch£º695	 i:2 	 global-step:13902	 l-p:0.20513123273849487
epoch£º695	 i:3 	 global-step:13903	 l-p:0.09942315518856049
epoch£º695	 i:4 	 global-step:13904	 l-p:0.17540870606899261
epoch£º695	 i:5 	 global-step:13905	 l-p:0.11710517853498459
epoch£º695	 i:6 	 global-step:13906	 l-p:0.12475358694791794
epoch£º695	 i:7 	 global-step:13907	 l-p:0.19031040370464325
epoch£º695	 i:8 	 global-step:13908	 l-p:0.13001421093940735
epoch£º695	 i:9 	 global-step:13909	 l-p:0.17890384793281555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1109, 3.0813, 3.1086],
        [3.1109, 1.8364, 1.3001],
        [3.1109, 2.9555, 3.0728],
        [3.1109, 2.1043, 1.4432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.1439608335494995 
model_pd.l_d.mean(): -25.173418045043945 
model_pd.lagr.mean(): -25.029457092285156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0484], device='cuda:0')), ('power', tensor([-25.1250], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.1439608335494995
epoch£º696	 i:1 	 global-step:13921	 l-p:0.06264069676399231
epoch£º696	 i:2 	 global-step:13922	 l-p:0.1431410163640976
epoch£º696	 i:3 	 global-step:13923	 l-p:0.11520665884017944
epoch£º696	 i:4 	 global-step:13924	 l-p:0.13308463990688324
epoch£º696	 i:5 	 global-step:13925	 l-p:0.26634153723716736
epoch£º696	 i:6 	 global-step:13926	 l-p:0.14909975230693817
epoch£º696	 i:7 	 global-step:13927	 l-p:0.2630559504032135
epoch£º696	 i:8 	 global-step:13928	 l-p:0.13503912091255188
epoch£º696	 i:9 	 global-step:13929	 l-p:0.21064135432243347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0885, 3.0885, 3.0885],
        [3.0885, 3.0884, 3.0885],
        [3.0885, 1.9112, 1.2842],
        [3.0885, 3.0856, 3.0885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.12662270665168762 
model_pd.l_d.mean(): -24.889516830444336 
model_pd.lagr.mean(): -24.762893676757812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0580], device='cuda:0')), ('power', tensor([-24.8315], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.12662270665168762
epoch£º697	 i:1 	 global-step:13941	 l-p:0.14894361793994904
epoch£º697	 i:2 	 global-step:13942	 l-p:0.09891317039728165
epoch£º697	 i:3 	 global-step:13943	 l-p:0.1558506041765213
epoch£º697	 i:4 	 global-step:13944	 l-p:0.17648100852966309
epoch£º697	 i:5 	 global-step:13945	 l-p:0.13462333381175995
epoch£º697	 i:6 	 global-step:13946	 l-p:0.13499705493450165
epoch£º697	 i:7 	 global-step:13947	 l-p:0.506458044052124
epoch£º697	 i:8 	 global-step:13948	 l-p:0.13123878836631775
epoch£º697	 i:9 	 global-step:13949	 l-p:-0.039964333176612854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0350, 2.9089, 3.0085],
        [3.0350, 3.0267, 3.0347],
        [3.0350, 2.2508, 2.3381],
        [3.0350, 1.9524, 1.7257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.13908371329307556 
model_pd.l_d.mean(): -25.095117568969727 
model_pd.lagr.mean(): -24.95603370666504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0049], device='cuda:0')), ('power', tensor([-25.0903], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.13908371329307556
epoch£º698	 i:1 	 global-step:13961	 l-p:0.06458400934934616
epoch£º698	 i:2 	 global-step:13962	 l-p:0.25320154428482056
epoch£º698	 i:3 	 global-step:13963	 l-p:0.12953193485736847
epoch£º698	 i:4 	 global-step:13964	 l-p:0.12931805849075317
epoch£º698	 i:5 	 global-step:13965	 l-p:0.1435735523700714
epoch£º698	 i:6 	 global-step:13966	 l-p:0.1788187325000763
epoch£º698	 i:7 	 global-step:13967	 l-p:-0.9360955357551575
epoch£º698	 i:8 	 global-step:13968	 l-p:1.271849274635315
epoch£º698	 i:9 	 global-step:13969	 l-p:0.10776282846927643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0814, 3.0580, 3.0798],
        [3.0814, 2.8551, 3.0081],
        [3.0814, 1.9567, 1.6644],
        [3.0814, 3.0814, 3.0814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.19045867025852203 
model_pd.l_d.mean(): -25.06436538696289 
model_pd.lagr.mean(): -24.8739070892334 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0209], device='cuda:0')), ('power', tensor([-25.0853], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.19045867025852203
epoch£º699	 i:1 	 global-step:13981	 l-p:0.11871608346700668
epoch£º699	 i:2 	 global-step:13982	 l-p:0.1313524842262268
epoch£º699	 i:3 	 global-step:13983	 l-p:0.19291193783283234
epoch£º699	 i:4 	 global-step:13984	 l-p:0.14210212230682373
epoch£º699	 i:5 	 global-step:13985	 l-p:0.16132132709026337
epoch£º699	 i:6 	 global-step:13986	 l-p:0.19506341218948364
epoch£º699	 i:7 	 global-step:13987	 l-p:0.13571427762508392
epoch£º699	 i:8 	 global-step:13988	 l-p:0.038113754242658615
epoch£º699	 i:9 	 global-step:13989	 l-p:0.12472188472747803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1354, 3.1188, 3.1345],
        [3.1354, 2.1587, 2.0620],
        [3.1354, 2.2985, 2.3409],
        [3.1354, 2.9029, 3.0584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): -0.08189495652914047 
model_pd.l_d.mean(): -25.025047302246094 
model_pd.lagr.mean(): -25.106943130493164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0776], device='cuda:0')), ('power', tensor([-24.9475], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:-0.08189495652914047
epoch£º700	 i:1 	 global-step:14001	 l-p:0.14608903229236603
epoch£º700	 i:2 	 global-step:14002	 l-p:0.11463835090398788
epoch£º700	 i:3 	 global-step:14003	 l-p:0.11658550798892975
epoch£º700	 i:4 	 global-step:14004	 l-p:0.13137319684028625
epoch£º700	 i:5 	 global-step:14005	 l-p:0.13945481181144714
epoch£º700	 i:6 	 global-step:14006	 l-p:0.15431679785251617
epoch£º700	 i:7 	 global-step:14007	 l-p:0.16526122391223907
epoch£º700	 i:8 	 global-step:14008	 l-p:0.17216961085796356
epoch£º700	 i:9 	 global-step:14009	 l-p:0.14774441719055176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0620, 3.0377, 3.0603],
        [3.0620, 2.7727, 2.9492],
        [3.0620, 3.0620, 3.0620],
        [3.0620, 2.3799, 2.5251]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 1.014264702796936 
model_pd.l_d.mean(): -24.920045852661133 
model_pd.lagr.mean(): -23.905780792236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0226], device='cuda:0')), ('power', tensor([-24.9427], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:1.014264702796936
epoch£º701	 i:1 	 global-step:14021	 l-p:0.14482450485229492
epoch£º701	 i:2 	 global-step:14022	 l-p:0.11841125041246414
epoch£º701	 i:3 	 global-step:14023	 l-p:0.15035423636436462
epoch£º701	 i:4 	 global-step:14024	 l-p:0.3968310058116913
epoch£º701	 i:5 	 global-step:14025	 l-p:0.19381648302078247
epoch£º701	 i:6 	 global-step:14026	 l-p:0.13358275592327118
epoch£º701	 i:7 	 global-step:14027	 l-p:-0.10787574201822281
epoch£º701	 i:8 	 global-step:14028	 l-p:0.12998925149440765
epoch£º701	 i:9 	 global-step:14029	 l-p:0.1384197175502777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0580, 2.8310, 2.9845],
        [3.0580, 1.9259, 1.2948],
        [3.0580, 2.8384, 2.9886],
        [3.0580, 3.0580, 3.0580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.1061871349811554 
model_pd.l_d.mean(): -24.989913940429688 
model_pd.lagr.mean(): -24.883726119995117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0292], device='cuda:0')), ('power', tensor([-25.0191], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.1061871349811554
epoch£º702	 i:1 	 global-step:14041	 l-p:0.1532231569290161
epoch£º702	 i:2 	 global-step:14042	 l-p:0.18478870391845703
epoch£º702	 i:3 	 global-step:14043	 l-p:0.7878533601760864
epoch£º702	 i:4 	 global-step:14044	 l-p:0.1353296935558319
epoch£º702	 i:5 	 global-step:14045	 l-p:0.14056767523288727
epoch£º702	 i:6 	 global-step:14046	 l-p:0.1731332689523697
epoch£º702	 i:7 	 global-step:14047	 l-p:0.1573941707611084
epoch£º702	 i:8 	 global-step:14048	 l-p:0.5833491683006287
epoch£º702	 i:9 	 global-step:14049	 l-p:0.12014956027269363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0631, 3.0536, 3.0627],
        [3.0631, 1.9827, 1.3409],
        [3.0631, 3.0213, 3.0590],
        [3.0631, 1.9997, 1.3549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.4309225082397461 
model_pd.l_d.mean(): -25.058067321777344 
model_pd.lagr.mean(): -24.62714385986328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0535], device='cuda:0')), ('power', tensor([-25.1115], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.4309225082397461
epoch£º703	 i:1 	 global-step:14061	 l-p:0.2606354355812073
epoch£º703	 i:2 	 global-step:14062	 l-p:0.13697810471057892
epoch£º703	 i:3 	 global-step:14063	 l-p:0.1340855211019516
epoch£º703	 i:4 	 global-step:14064	 l-p:0.1517518162727356
epoch£º703	 i:5 	 global-step:14065	 l-p:0.15079493820667267
epoch£º703	 i:6 	 global-step:14066	 l-p:0.19804006814956665
epoch£º703	 i:7 	 global-step:14067	 l-p:0.16377070546150208
epoch£º703	 i:8 	 global-step:14068	 l-p:0.12639793753623962
epoch£º703	 i:9 	 global-step:14069	 l-p:0.1478550136089325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0763, 3.0764, 3.0764],
        [3.0763, 2.9202, 3.0380],
        [3.0763, 2.7836, 2.9611],
        [3.0763, 1.8128, 1.3114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.13416706025600433 
model_pd.l_d.mean(): -25.255268096923828 
model_pd.lagr.mean(): -25.12110137939453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1284], device='cuda:0')), ('power', tensor([-25.1268], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.13416706025600433
epoch£º704	 i:1 	 global-step:14081	 l-p:1.070816993713379
epoch£º704	 i:2 	 global-step:14082	 l-p:0.15240927040576935
epoch£º704	 i:3 	 global-step:14083	 l-p:0.18021155893802643
epoch£º704	 i:4 	 global-step:14084	 l-p:0.13900834321975708
epoch£º704	 i:5 	 global-step:14085	 l-p:0.27505621314048767
epoch£º704	 i:6 	 global-step:14086	 l-p:0.07347099483013153
epoch£º704	 i:7 	 global-step:14087	 l-p:0.1505422294139862
epoch£º704	 i:8 	 global-step:14088	 l-p:0.09434280544519424
epoch£º704	 i:9 	 global-step:14089	 l-p:0.13232222199440002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0244, 3.0244, 3.0244],
        [3.0244, 3.0096, 3.0237],
        [3.0244, 1.7563, 1.2485],
        [3.0244, 1.8441, 1.2300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 0.5297962427139282 
model_pd.l_d.mean(): -25.00284194946289 
model_pd.lagr.mean(): -24.473045349121094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1370], device='cuda:0')), ('power', tensor([-25.1398], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:0.5297962427139282
epoch£º705	 i:1 	 global-step:14101	 l-p:0.15302178263664246
epoch£º705	 i:2 	 global-step:14102	 l-p:-0.02519381418824196
epoch£º705	 i:3 	 global-step:14103	 l-p:0.1417885273694992
epoch£º705	 i:4 	 global-step:14104	 l-p:0.2664584517478943
epoch£º705	 i:5 	 global-step:14105	 l-p:0.11511224508285522
epoch£º705	 i:6 	 global-step:14106	 l-p:0.12809611856937408
epoch£º705	 i:7 	 global-step:14107	 l-p:0.13695089519023895
epoch£º705	 i:8 	 global-step:14108	 l-p:0.33643144369125366
epoch£º705	 i:9 	 global-step:14109	 l-p:0.10260951519012451
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0721, 2.8018, 2.9721],
        [3.0721, 1.9478, 1.3121],
        [3.0721, 2.9458, 3.0455],
        [3.0721, 1.9678, 1.7106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.13716335594654083 
model_pd.l_d.mean(): -25.078760147094727 
model_pd.lagr.mean(): -24.94159698486328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0237], device='cuda:0')), ('power', tensor([-25.1025], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.13716335594654083
epoch£º706	 i:1 	 global-step:14121	 l-p:0.1270676553249359
epoch£º706	 i:2 	 global-step:14122	 l-p:0.15001368522644043
epoch£º706	 i:3 	 global-step:14123	 l-p:-0.015147036872804165
epoch£º706	 i:4 	 global-step:14124	 l-p:-0.001441364292986691
epoch£º706	 i:5 	 global-step:14125	 l-p:0.1634121537208557
epoch£º706	 i:6 	 global-step:14126	 l-p:0.10735368728637695
epoch£º706	 i:7 	 global-step:14127	 l-p:1.7447385787963867
epoch£º706	 i:8 	 global-step:14128	 l-p:0.1435537040233612
epoch£º706	 i:9 	 global-step:14129	 l-p:0.10211830586194992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0027, 3.0018, 3.0027],
        [3.0027, 3.0027, 3.0028],
        [3.0027, 3.0026, 3.0027],
        [3.0027, 2.6031, 2.8026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.1838260293006897 
model_pd.l_d.mean(): -25.251178741455078 
model_pd.lagr.mean(): -25.067352294921875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0026], device='cuda:0')), ('power', tensor([-25.2485], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.1838260293006897
epoch£º707	 i:1 	 global-step:14141	 l-p:0.0819719135761261
epoch£º707	 i:2 	 global-step:14142	 l-p:0.1640508621931076
epoch£º707	 i:3 	 global-step:14143	 l-p:0.28798311948776245
epoch£º707	 i:4 	 global-step:14144	 l-p:0.12778694927692413
epoch£º707	 i:5 	 global-step:14145	 l-p:0.14848646521568298
epoch£º707	 i:6 	 global-step:14146	 l-p:0.056481242179870605
epoch£º707	 i:7 	 global-step:14147	 l-p:0.1340254247188568
epoch£º707	 i:8 	 global-step:14148	 l-p:0.34778067469596863
epoch£º707	 i:9 	 global-step:14149	 l-p:0.10244202613830566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0248, 2.7618, 2.9299],
        [3.0248, 2.3387, 2.4850],
        [3.0248, 1.7732, 1.1845],
        [3.0248, 2.7586, 2.9279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.13626568019390106 
model_pd.l_d.mean(): -25.070354461669922 
model_pd.lagr.mean(): -24.93408966064453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0776], device='cuda:0')), ('power', tensor([-25.1479], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.13626568019390106
epoch£º708	 i:1 	 global-step:14161	 l-p:-0.2551133930683136
epoch£º708	 i:2 	 global-step:14162	 l-p:0.1513095498085022
epoch£º708	 i:3 	 global-step:14163	 l-p:0.11540639400482178
epoch£º708	 i:4 	 global-step:14164	 l-p:0.1281493604183197
epoch£º708	 i:5 	 global-step:14165	 l-p:0.02911788783967495
epoch£º708	 i:6 	 global-step:14166	 l-p:0.17466503381729126
epoch£º708	 i:7 	 global-step:14167	 l-p:0.15462200343608856
epoch£º708	 i:8 	 global-step:14168	 l-p:0.1451292186975479
epoch£º708	 i:9 	 global-step:14169	 l-p:0.16681763529777527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9877, 2.0455, 2.0020],
        [2.9877, 2.8571, 2.9597],
        [2.9877, 2.0625, 1.4037],
        [2.9877, 1.7228, 1.2286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): -0.039777908474206924 
model_pd.l_d.mean(): -24.998287200927734 
model_pd.lagr.mean(): -25.03806495666504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0622], device='cuda:0')), ('power', tensor([-25.0605], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:-0.039777908474206924
epoch£º709	 i:1 	 global-step:14181	 l-p:0.16257014870643616
epoch£º709	 i:2 	 global-step:14182	 l-p:0.27665579319000244
epoch£º709	 i:3 	 global-step:14183	 l-p:0.07591807097196579
epoch£º709	 i:4 	 global-step:14184	 l-p:0.15468521416187286
epoch£º709	 i:5 	 global-step:14185	 l-p:-1.2272224426269531
epoch£º709	 i:6 	 global-step:14186	 l-p:0.054299335926771164
epoch£º709	 i:7 	 global-step:14187	 l-p:0.002259483328089118
epoch£º709	 i:8 	 global-step:14188	 l-p:0.09998226165771484
epoch£º709	 i:9 	 global-step:14189	 l-p:0.15221689641475677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1156, 3.0577, 3.1085],
        [3.1156, 2.1944, 2.1648],
        [3.1156, 2.4079, 2.5404],
        [3.1156, 3.0222, 3.0997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.05998224765062332 
model_pd.l_d.mean(): -24.995454788208008 
model_pd.lagr.mean(): -24.93547248840332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0943], device='cuda:0')), ('power', tensor([-24.9011], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.05998224765062332
epoch£º710	 i:1 	 global-step:14201	 l-p:0.10763330012559891
epoch£º710	 i:2 	 global-step:14202	 l-p:0.11910320818424225
epoch£º710	 i:3 	 global-step:14203	 l-p:0.1472780406475067
epoch£º710	 i:4 	 global-step:14204	 l-p:0.13885542750358582
epoch£º710	 i:5 	 global-step:14205	 l-p:0.20092707872390747
epoch£º710	 i:6 	 global-step:14206	 l-p:0.14941617846488953
epoch£º710	 i:7 	 global-step:14207	 l-p:0.1311260163784027
epoch£º710	 i:8 	 global-step:14208	 l-p:0.12028272449970245
epoch£º710	 i:9 	 global-step:14209	 l-p:0.1598934680223465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0911, 3.0546, 3.0878],
        [3.0911, 1.8939, 1.2692],
        [3.0911, 1.8505, 1.2394],
        [3.0911, 3.0911, 3.0911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.12580181658267975 
model_pd.l_d.mean(): -24.75594711303711 
model_pd.lagr.mean(): -24.630146026611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0685], device='cuda:0')), ('power', tensor([-24.8244], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.12580181658267975
epoch£º711	 i:1 	 global-step:14221	 l-p:0.14892682433128357
epoch£º711	 i:2 	 global-step:14222	 l-p:0.17050132155418396
epoch£º711	 i:3 	 global-step:14223	 l-p:0.14737564325332642
epoch£º711	 i:4 	 global-step:14224	 l-p:0.1469048261642456
epoch£º711	 i:5 	 global-step:14225	 l-p:0.16977939009666443
epoch£º711	 i:6 	 global-step:14226	 l-p:-0.03732403740286827
epoch£º711	 i:7 	 global-step:14227	 l-p:0.13328716158866882
epoch£º711	 i:8 	 global-step:14228	 l-p:0.10745061933994293
epoch£º711	 i:9 	 global-step:14229	 l-p:0.13571546971797943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9363, 1.6824, 1.1176],
        [2.9363, 1.7603, 1.1619],
        [2.9363, 2.6712, 2.8407],
        [2.9363, 1.8423, 1.2242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 0.14158199727535248 
model_pd.l_d.mean(): -25.094202041625977 
model_pd.lagr.mean(): -24.952619552612305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2285], device='cuda:0')), ('power', tensor([-25.3227], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:0.14158199727535248
epoch£º712	 i:1 	 global-step:14241	 l-p:0.13273189961910248
epoch£º712	 i:2 	 global-step:14242	 l-p:0.16827037930488586
epoch£º712	 i:3 	 global-step:14243	 l-p:0.45379891991615295
epoch£º712	 i:4 	 global-step:14244	 l-p:0.1705862581729889
epoch£º712	 i:5 	 global-step:14245	 l-p:0.14120595157146454
epoch£º712	 i:6 	 global-step:14246	 l-p:0.17112460732460022
epoch£º712	 i:7 	 global-step:14247	 l-p:0.12160618603229523
epoch£º712	 i:8 	 global-step:14248	 l-p:0.09842874109745026
epoch£º712	 i:9 	 global-step:14249	 l-p:0.13385002315044403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9985, 2.9751, 2.9969],
        [2.9985, 1.8602, 1.5666],
        [2.9985, 2.9985, 2.9985],
        [2.9985, 1.7393, 1.2623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.01863035187125206 
model_pd.l_d.mean(): -25.206193923950195 
model_pd.lagr.mean(): -25.187562942504883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0682], device='cuda:0')), ('power', tensor([-25.1380], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.01863035187125206
epoch£º713	 i:1 	 global-step:14261	 l-p:0.20996864140033722
epoch£º713	 i:2 	 global-step:14262	 l-p:0.14464163780212402
epoch£º713	 i:3 	 global-step:14263	 l-p:0.12047781050205231
epoch£º713	 i:4 	 global-step:14264	 l-p:0.15407361090183258
epoch£º713	 i:5 	 global-step:14265	 l-p:0.13730314373970032
epoch£º713	 i:6 	 global-step:14266	 l-p:0.11657851934432983
epoch£º713	 i:7 	 global-step:14267	 l-p:0.24639476835727692
epoch£º713	 i:8 	 global-step:14268	 l-p:0.1435275375843048
epoch£º713	 i:9 	 global-step:14269	 l-p:0.14498233795166016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0708, 3.0708, 3.0708],
        [3.0708, 1.8024, 1.2109],
        [3.0708, 1.7946, 1.2777],
        [3.0708, 3.0571, 3.0701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.15100516378879547 
model_pd.l_d.mean(): -25.12299919128418 
model_pd.lagr.mean(): -24.971994400024414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0102], device='cuda:0')), ('power', tensor([-25.1332], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.15100516378879547
epoch£º714	 i:1 	 global-step:14281	 l-p:0.15523645281791687
epoch£º714	 i:2 	 global-step:14282	 l-p:0.11583239585161209
epoch£º714	 i:3 	 global-step:14283	 l-p:0.14506155252456665
epoch£º714	 i:4 	 global-step:14284	 l-p:0.07782366871833801
epoch£º714	 i:5 	 global-step:14285	 l-p:0.1461828202009201
epoch£º714	 i:6 	 global-step:14286	 l-p:-0.007569632492959499
epoch£º714	 i:7 	 global-step:14287	 l-p:0.33928102254867554
epoch£º714	 i:8 	 global-step:14288	 l-p:0.12791714072227478
epoch£º714	 i:9 	 global-step:14289	 l-p:0.03932853043079376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0197, 2.9796, 3.0159],
        [3.0197, 2.0355, 1.3817],
        [3.0197, 1.8886, 1.6042],
        [3.0197, 3.0197, 3.0197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.0616626963019371 
model_pd.l_d.mean(): -25.036039352416992 
model_pd.lagr.mean(): -24.974376678466797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0342], device='cuda:0')), ('power', tensor([-25.0703], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.0616626963019371
epoch£º715	 i:1 	 global-step:14301	 l-p:0.15418334305286407
epoch£º715	 i:2 	 global-step:14302	 l-p:0.2367345541715622
epoch£º715	 i:3 	 global-step:14303	 l-p:0.14119930565357208
epoch£º715	 i:4 	 global-step:14304	 l-p:0.15035833418369293
epoch£º715	 i:5 	 global-step:14305	 l-p:0.1373351663351059
epoch£º715	 i:6 	 global-step:14306	 l-p:0.1278381645679474
epoch£º715	 i:7 	 global-step:14307	 l-p:0.050891708582639694
epoch£º715	 i:8 	 global-step:14308	 l-p:0.21824568510055542
epoch£º715	 i:9 	 global-step:14309	 l-p:0.09582486003637314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0416, 1.7611, 1.1937],
        [3.0416, 3.0333, 3.0413],
        [3.0416, 3.0416, 3.0416],
        [3.0416, 2.0437, 1.3889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.203831747174263 
model_pd.l_d.mean(): -25.037954330444336 
model_pd.lagr.mean(): -24.834121704101562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0650], device='cuda:0')), ('power', tensor([-25.1029], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.203831747174263
epoch£º716	 i:1 	 global-step:14321	 l-p:0.12930259108543396
epoch£º716	 i:2 	 global-step:14322	 l-p:0.13588908314704895
epoch£º716	 i:3 	 global-step:14323	 l-p:0.1292860209941864
epoch£º716	 i:4 	 global-step:14324	 l-p:0.15625722706317902
epoch£º716	 i:5 	 global-step:14325	 l-p:0.1894446462392807
epoch£º716	 i:6 	 global-step:14326	 l-p:0.12101239711046219
epoch£º716	 i:7 	 global-step:14327	 l-p:0.15412823855876923
epoch£º716	 i:8 	 global-step:14328	 l-p:0.2309144288301468
epoch£º716	 i:9 	 global-step:14329	 l-p:0.1259007751941681
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9891, 2.6194, 2.8160],
        [2.9891, 2.5031, 2.7051],
        [2.9891, 2.9717, 2.9881],
        [2.9891, 1.8326, 1.2177]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.1541934758424759 
model_pd.l_d.mean(): -25.222389221191406 
model_pd.lagr.mean(): -25.068195343017578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0299], device='cuda:0')), ('power', tensor([-25.1925], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.1541934758424759
epoch£º717	 i:1 	 global-step:14341	 l-p:0.11046943068504333
epoch£º717	 i:2 	 global-step:14342	 l-p:0.20344328880310059
epoch£º717	 i:3 	 global-step:14343	 l-p:0.09600768238306046
epoch£º717	 i:4 	 global-step:14344	 l-p:0.12339837104082108
epoch£º717	 i:5 	 global-step:14345	 l-p:-0.11935708671808243
epoch£º717	 i:6 	 global-step:14346	 l-p:0.15909714996814728
epoch£º717	 i:7 	 global-step:14347	 l-p:0.16598521173000336
epoch£º717	 i:8 	 global-step:14348	 l-p:0.16563566029071808
epoch£º717	 i:9 	 global-step:14349	 l-p:0.11990310251712799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[3.0340, 2.1723, 2.2068],
        [3.0340, 2.3765, 2.5374],
        [3.0340, 2.0739, 1.4134],
        [3.0340, 2.5079, 2.7048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.1341148018836975 
model_pd.l_d.mean(): -25.228715896606445 
model_pd.lagr.mean(): -25.094600677490234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0494], device='cuda:0')), ('power', tensor([-25.1793], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.1341148018836975
epoch£º718	 i:1 	 global-step:14361	 l-p:0.19442200660705566
epoch£º718	 i:2 	 global-step:14362	 l-p:0.044669073075056076
epoch£º718	 i:3 	 global-step:14363	 l-p:0.12509368360042572
epoch£º718	 i:4 	 global-step:14364	 l-p:0.37455645203590393
epoch£º718	 i:5 	 global-step:14365	 l-p:0.1569577157497406
epoch£º718	 i:6 	 global-step:14366	 l-p:0.12095577269792557
epoch£º718	 i:7 	 global-step:14367	 l-p:0.1407725065946579
epoch£º718	 i:8 	 global-step:14368	 l-p:0.16386260092258453
epoch£º718	 i:9 	 global-step:14369	 l-p:0.14097033441066742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1059, 2.5349, 2.7215],
        [3.1059, 1.8255, 1.2338],
        [3.1059, 1.9187, 1.2872],
        [3.1059, 3.0308, 3.0949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.10416001826524734 
model_pd.l_d.mean(): -24.433658599853516 
model_pd.lagr.mean(): -24.329498291015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1692], device='cuda:0')), ('power', tensor([-24.6029], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.10416001826524734
epoch£º719	 i:1 	 global-step:14381	 l-p:0.17514602839946747
epoch£º719	 i:2 	 global-step:14382	 l-p:0.19399045407772064
epoch£º719	 i:3 	 global-step:14383	 l-p:0.1339707225561142
epoch£º719	 i:4 	 global-step:14384	 l-p:0.1309749335050583
epoch£º719	 i:5 	 global-step:14385	 l-p:0.13480955362319946
epoch£º719	 i:6 	 global-step:14386	 l-p:0.1282486617565155
epoch£º719	 i:7 	 global-step:14387	 l-p:0.12978890538215637
epoch£º719	 i:8 	 global-step:14388	 l-p:0.12501797080039978
epoch£º719	 i:9 	 global-step:14389	 l-p:0.1296301633119583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0855, 2.8396, 3.0014],
        [3.0855, 2.2245, 2.2575],
        [3.0855, 1.8089, 1.2963],
        [3.0855, 2.0426, 1.8791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.1669377237558365 
model_pd.l_d.mean(): -25.047958374023438 
model_pd.lagr.mean(): -24.88102149963379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0158], device='cuda:0')), ('power', tensor([-25.0637], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.1669377237558365
epoch£º720	 i:1 	 global-step:14401	 l-p:0.5531694889068604
epoch£º720	 i:2 	 global-step:14402	 l-p:0.1999247819185257
epoch£º720	 i:3 	 global-step:14403	 l-p:-0.1386459469795227
epoch£º720	 i:4 	 global-step:14404	 l-p:0.12419606000185013
epoch£º720	 i:5 	 global-step:14405	 l-p:0.13151134550571442
epoch£º720	 i:6 	 global-step:14406	 l-p:0.14422383904457092
epoch£º720	 i:7 	 global-step:14407	 l-p:0.1589631736278534
epoch£º720	 i:8 	 global-step:14408	 l-p:0.14984536170959473
epoch£º720	 i:9 	 global-step:14409	 l-p:0.055011287331581116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[3.0202, 1.7361, 1.1905],
        [3.0202, 2.0585, 1.3998],
        [3.0202, 1.9318, 1.7115],
        [3.0202, 1.8356, 1.4797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.11549906432628632 
model_pd.l_d.mean(): -24.828323364257812 
model_pd.lagr.mean(): -24.71282386779785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1169], device='cuda:0')), ('power', tensor([-24.9453], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.11549906432628632
epoch£º721	 i:1 	 global-step:14421	 l-p:0.22054420411586761
epoch£º721	 i:2 	 global-step:14422	 l-p:0.11976031959056854
epoch£º721	 i:3 	 global-step:14423	 l-p:0.14237432181835175
epoch£º721	 i:4 	 global-step:14424	 l-p:-0.007545385044068098
epoch£º721	 i:5 	 global-step:14425	 l-p:0.3789987862110138
epoch£º721	 i:6 	 global-step:14426	 l-p:0.16160638630390167
epoch£º721	 i:7 	 global-step:14427	 l-p:0.005006837658584118
epoch£º721	 i:8 	 global-step:14428	 l-p:0.13575126230716705
epoch£º721	 i:9 	 global-step:14429	 l-p:0.14378242194652557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0362, 3.0362, 3.0362],
        [3.0362, 3.0280, 3.0359],
        [3.0362, 3.0362, 3.0362],
        [3.0362, 2.8743, 2.9958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.3050447702407837 
model_pd.l_d.mean(): -24.95701026916504 
model_pd.lagr.mean(): -24.651966094970703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0563], device='cuda:0')), ('power', tensor([-25.0133], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.3050447702407837
epoch£º722	 i:1 	 global-step:14441	 l-p:0.06431126594543457
epoch£º722	 i:2 	 global-step:14442	 l-p:0.02070167474448681
epoch£º722	 i:3 	 global-step:14443	 l-p:0.1344810277223587
epoch£º722	 i:4 	 global-step:14444	 l-p:0.12912189960479736
epoch£º722	 i:5 	 global-step:14445	 l-p:0.15293465554714203
epoch£º722	 i:6 	 global-step:14446	 l-p:0.24877741932868958
epoch£º722	 i:7 	 global-step:14447	 l-p:0.1280863881111145
epoch£º722	 i:8 	 global-step:14448	 l-p:0.16126158833503723
epoch£º722	 i:9 	 global-step:14449	 l-p:0.1423410326242447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1074, 2.8830, 3.0357],
        [3.1074, 1.8276, 1.2327],
        [3.1074, 3.1068, 3.1074],
        [3.1074, 3.1074, 3.1074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.169328510761261 
model_pd.l_d.mean(): -25.08379364013672 
model_pd.lagr.mean(): -24.914464950561523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0106], device='cuda:0')), ('power', tensor([-25.0944], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.169328510761261
epoch£º723	 i:1 	 global-step:14461	 l-p:0.130619615316391
epoch£º723	 i:2 	 global-step:14462	 l-p:0.14128535985946655
epoch£º723	 i:3 	 global-step:14463	 l-p:0.08754720538854599
epoch£º723	 i:4 	 global-step:14464	 l-p:0.13935455679893494
epoch£º723	 i:5 	 global-step:14465	 l-p:0.14466682076454163
epoch£º723	 i:6 	 global-step:14466	 l-p:0.1399005502462387
epoch£º723	 i:7 	 global-step:14467	 l-p:0.12349994480609894
epoch£º723	 i:8 	 global-step:14468	 l-p:0.16639655828475952
epoch£º723	 i:9 	 global-step:14469	 l-p:0.5243293046951294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0118, 3.0028, 3.0114],
        [3.0118, 1.7541, 1.1672],
        [3.0118, 2.9725, 3.0080],
        [3.0118, 2.0642, 2.0204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.08461315929889679 
model_pd.l_d.mean(): -24.836746215820312 
model_pd.lagr.mean(): -24.752132415771484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0664], device='cuda:0')), ('power', tensor([-24.9032], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.08461315929889679
epoch£º724	 i:1 	 global-step:14481	 l-p:0.23616404831409454
epoch£º724	 i:2 	 global-step:14482	 l-p:0.16297230124473572
epoch£º724	 i:3 	 global-step:14483	 l-p:0.13727231323719025
epoch£º724	 i:4 	 global-step:14484	 l-p:0.11720821261405945
epoch£º724	 i:5 	 global-step:14485	 l-p:0.1525757610797882
epoch£º724	 i:6 	 global-step:14486	 l-p:0.13183431327342987
epoch£º724	 i:7 	 global-step:14487	 l-p:0.09215502440929413
epoch£º724	 i:8 	 global-step:14488	 l-p:0.19808587431907654
epoch£º724	 i:9 	 global-step:14489	 l-p:0.07185322791337967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0033, 2.9894, 3.0026],
        [3.0033, 1.7190, 1.1766],
        [3.0033, 1.7287, 1.2283],
        [3.0033, 2.5149, 2.7173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.1353161633014679 
model_pd.l_d.mean(): -25.05316734313965 
model_pd.lagr.mean(): -24.917850494384766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0534], device='cuda:0')), ('power', tensor([-24.9998], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.1353161633014679
epoch£º725	 i:1 	 global-step:14501	 l-p:0.12526631355285645
epoch£º725	 i:2 	 global-step:14502	 l-p:0.13782405853271484
epoch£º725	 i:3 	 global-step:14503	 l-p:-0.1865379810333252
epoch£º725	 i:4 	 global-step:14504	 l-p:0.13190685212612152
epoch£º725	 i:5 	 global-step:14505	 l-p:0.16483880579471588
epoch£º725	 i:6 	 global-step:14506	 l-p:0.5258092880249023
epoch£º725	 i:7 	 global-step:14507	 l-p:0.03593634068965912
epoch£º725	 i:8 	 global-step:14508	 l-p:0.0844498872756958
epoch£º725	 i:9 	 global-step:14509	 l-p:0.12751010060310364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[3.0444, 2.5867, 2.7896],
        [3.0444, 1.9291, 1.6710],
        [3.0444, 1.7923, 1.1938],
        [3.0444, 1.9482, 1.3093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.2367955595254898 
model_pd.l_d.mean(): -24.594263076782227 
model_pd.lagr.mean(): -24.357467651367188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1186], device='cuda:0')), ('power', tensor([-24.7128], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.2367955595254898
epoch£º726	 i:1 	 global-step:14521	 l-p:-0.7706231474876404
epoch£º726	 i:2 	 global-step:14522	 l-p:0.14131587743759155
epoch£º726	 i:3 	 global-step:14523	 l-p:0.14136390388011932
epoch£º726	 i:4 	 global-step:14524	 l-p:0.16449248790740967
epoch£º726	 i:5 	 global-step:14525	 l-p:0.1276368647813797
epoch£º726	 i:6 	 global-step:14526	 l-p:0.1349792182445526
epoch£º726	 i:7 	 global-step:14527	 l-p:0.161058247089386
epoch£º726	 i:8 	 global-step:14528	 l-p:0.14160102605819702
epoch£º726	 i:9 	 global-step:14529	 l-p:0.1192229613661766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0981, 3.0981, 3.0981],
        [3.0981, 1.9546, 1.3147],
        [3.0981, 2.1391, 1.4684],
        [3.0981, 3.0981, 3.0981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.14488059282302856 
model_pd.l_d.mean(): -25.116819381713867 
model_pd.lagr.mean(): -24.971939086914062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0301], device='cuda:0')), ('power', tensor([-25.1469], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.14488059282302856
epoch£º727	 i:1 	 global-step:14541	 l-p:0.12118405848741531
epoch£º727	 i:2 	 global-step:14542	 l-p:0.1915484219789505
epoch£º727	 i:3 	 global-step:14543	 l-p:0.015447878278791904
epoch£º727	 i:4 	 global-step:14544	 l-p:0.2200944423675537
epoch£º727	 i:5 	 global-step:14545	 l-p:0.16199088096618652
epoch£º727	 i:6 	 global-step:14546	 l-p:0.10356494784355164
epoch£º727	 i:7 	 global-step:14547	 l-p:0.037502773106098175
epoch£º727	 i:8 	 global-step:14548	 l-p:0.14623671770095825
epoch£º727	 i:9 	 global-step:14549	 l-p:0.15714941918849945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0480, 3.0033, 3.0434],
        [3.0480, 3.0455, 3.0479],
        [3.0480, 1.7702, 1.2641],
        [3.0480, 2.1087, 1.4414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.23856528103351593 
model_pd.l_d.mean(): -25.035675048828125 
model_pd.lagr.mean(): -24.797109603881836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0043], device='cuda:0')), ('power', tensor([-25.0400], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.23856528103351593
epoch£º728	 i:1 	 global-step:14561	 l-p:0.175481379032135
epoch£º728	 i:2 	 global-step:14562	 l-p:0.12349575012922287
epoch£º728	 i:3 	 global-step:14563	 l-p:0.1331242471933365
epoch£º728	 i:4 	 global-step:14564	 l-p:0.21927687525749207
epoch£º728	 i:5 	 global-step:14565	 l-p:0.13783712685108185
epoch£º728	 i:6 	 global-step:14566	 l-p:0.14691226184368134
epoch£º728	 i:7 	 global-step:14567	 l-p:0.13939552009105682
epoch£º728	 i:8 	 global-step:14568	 l-p:0.600240170955658
epoch£º728	 i:9 	 global-step:14569	 l-p:0.7236316800117493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0723, 2.5520, 2.7502],
        [3.0723, 2.8119, 2.9796],
        [3.0723, 2.4166, 2.5789],
        [3.0723, 3.0723, 3.0724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.17099791765213013 
model_pd.l_d.mean(): -24.70113754272461 
model_pd.lagr.mean(): -24.530139923095703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2052], device='cuda:0')), ('power', tensor([-24.9063], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:0.17099791765213013
epoch£º729	 i:1 	 global-step:14581	 l-p:0.32377445697784424
epoch£º729	 i:2 	 global-step:14582	 l-p:0.14737896621227264
epoch£º729	 i:3 	 global-step:14583	 l-p:0.13100993633270264
epoch£º729	 i:4 	 global-step:14584	 l-p:0.12742918729782104
epoch£º729	 i:5 	 global-step:14585	 l-p:0.20965667068958282
epoch£º729	 i:6 	 global-step:14586	 l-p:0.12741360068321228
epoch£º729	 i:7 	 global-step:14587	 l-p:0.2588111460208893
epoch£º729	 i:8 	 global-step:14588	 l-p:0.13558442890644073
epoch£º729	 i:9 	 global-step:14589	 l-p:0.09466781467199326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[3.0856, 1.8064, 1.2134],
        [3.0856, 1.9772, 1.3324],
        [3.0856, 1.8011, 1.2805],
        [3.0856, 1.7965, 1.2633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.25801941752433777 
model_pd.l_d.mean(): -24.729049682617188 
model_pd.lagr.mean(): -24.471031188964844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1746], device='cuda:0')), ('power', tensor([-24.9036], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.25801941752433777
epoch£º730	 i:1 	 global-step:14601	 l-p:0.12628775835037231
epoch£º730	 i:2 	 global-step:14602	 l-p:0.21065793931484222
epoch£º730	 i:3 	 global-step:14603	 l-p:0.16764208674430847
epoch£º730	 i:4 	 global-step:14604	 l-p:0.12392038106918335
epoch£º730	 i:5 	 global-step:14605	 l-p:0.12107830494642258
epoch£º730	 i:6 	 global-step:14606	 l-p:0.07562530785799026
epoch£º730	 i:7 	 global-step:14607	 l-p:0.13227495551109314
epoch£º730	 i:8 	 global-step:14608	 l-p:0.15303876996040344
epoch£º730	 i:9 	 global-step:14609	 l-p:0.19034190475940704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1115, 2.0897, 1.4259],
        [3.1115, 2.3901, 2.5207],
        [3.1115, 1.9756, 1.6844],
        [3.1115, 3.0758, 3.1083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.10856011509895325 
model_pd.l_d.mean(): -24.981849670410156 
model_pd.lagr.mean(): -24.873289108276367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0419], device='cuda:0')), ('power', tensor([-25.0238], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.10856011509895325
epoch£º731	 i:1 	 global-step:14621	 l-p:0.20601136982440948
epoch£º731	 i:2 	 global-step:14622	 l-p:0.13897261023521423
epoch£º731	 i:3 	 global-step:14623	 l-p:0.22435328364372253
epoch£º731	 i:4 	 global-step:14624	 l-p:0.1276703178882599
epoch£º731	 i:5 	 global-step:14625	 l-p:0.17716188728809357
epoch£º731	 i:6 	 global-step:14626	 l-p:0.14816069602966309
epoch£º731	 i:7 	 global-step:14627	 l-p:0.12102331221103668
epoch£º731	 i:8 	 global-step:14628	 l-p:0.15843810141086578
epoch£º731	 i:9 	 global-step:14629	 l-p:0.18866218626499176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1095, 1.8395, 1.2330],
        [3.1095, 3.1018, 3.1092],
        [3.1095, 3.1095, 3.1095],
        [3.1095, 3.0975, 3.1089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.13005602359771729 
model_pd.l_d.mean(): -24.715620040893555 
model_pd.lagr.mean(): -24.58556365966797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0712], device='cuda:0')), ('power', tensor([-24.7868], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:0.13005602359771729
epoch£º732	 i:1 	 global-step:14641	 l-p:0.1414114385843277
epoch£º732	 i:2 	 global-step:14642	 l-p:0.11526548117399216
epoch£º732	 i:3 	 global-step:14643	 l-p:0.14794717729091644
epoch£º732	 i:4 	 global-step:14644	 l-p:0.12331592291593552
epoch£º732	 i:5 	 global-step:14645	 l-p:0.14344190061092377
epoch£º732	 i:6 	 global-step:14646	 l-p:0.38969138264656067
epoch£º732	 i:7 	 global-step:14647	 l-p:0.4591612219810486
epoch£º732	 i:8 	 global-step:14648	 l-p:0.13532182574272156
epoch£º732	 i:9 	 global-step:14649	 l-p:0.12405379861593246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0785, 3.0785, 3.0785],
        [3.0785, 3.0392, 3.0748],
        [3.0785, 2.8049, 2.9774],
        [3.0785, 2.9685, 3.0577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.13291244208812714 
model_pd.l_d.mean(): -24.875789642333984 
model_pd.lagr.mean(): -24.742877960205078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0861], device='cuda:0')), ('power', tensor([-24.9619], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.13291244208812714
epoch£º733	 i:1 	 global-step:14661	 l-p:0.15446138381958008
epoch£º733	 i:2 	 global-step:14662	 l-p:0.15791559219360352
epoch£º733	 i:3 	 global-step:14663	 l-p:0.5878214240074158
epoch£º733	 i:4 	 global-step:14664	 l-p:0.28531548380851746
epoch£º733	 i:5 	 global-step:14665	 l-p:0.14552220702171326
epoch£º733	 i:6 	 global-step:14666	 l-p:0.1868745982646942
epoch£º733	 i:7 	 global-step:14667	 l-p:0.07374165952205658
Save?